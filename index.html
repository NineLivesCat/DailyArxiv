<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2025-07-24T00:00:00Z">2025-07-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">37</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Experimental Comparison of Whole-Body <span class="highlight-title">Control</span> Formulations for Humanoid
  <span class="highlight-title">Robot</span>s in Task Acceleration and Task Force Spaces <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sait Sovukluk, Grazia Zambella, Tobias Egle, Christian Ott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the experimental comparison of two different whole-body
control formulations for humanoid robots: inverse dynamics whole-body control
(ID-WBC) and passivity-based whole-body control (PB-WBC). The two controllers
fundamentally differ from each other as the first is formulated in task
acceleration space and the latter is in task force space with passivity
considerations. Even though both control methods predict stability under ideal
conditions in closed-loop dynamics, their robustness against joint friction,
sensor noise, unmodeled external disturbances, and non-perfect contact
conditions is not evident. Therefore, we analyze and experimentally compare the
two controllers on a humanoid robot platform through swing foot position and
orientation control, squatting with and without unmodeled additional weights,
and jumping. We also relate the observed performance and characteristic
differences with the controller formulations and highlight each controller's
advantages and disadvantages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for publication in 2025 IEEE/RSJ
  International Conference on Intelligent Robots and Systems (IROS 2025). -
  Link to video: https://youtu.be/Nfm50ycz-FU</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Monte-Carlo Compressed Sensing and Dictionary Learning Method
  for the Efficient Path <span class="highlight-title">Planning</span> of Remote Sensing <span class="highlight-title">Robot</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alghalya Al-Hajri, Ejmen Al-Ubejdij, Aiman Erbad, Ali Safa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Compressed Sensing (CS) has gained significant interest as a
technique for acquiring high-resolution sensory data using fewer measurements
than traditional Nyquist sampling requires. At the same time, autonomous
robotic platforms such as drones and rovers have become increasingly popular
tools for remote sensing and environmental monitoring tasks, including
measurements of temperature, humidity, and air quality. Within this context,
this paper presents, to the best of our knowledge, the first investigation into
how the structure of CS measurement matrices can be exploited to design
optimized sampling trajectories for robotic environmental data collection. We
propose a novel Monte Carlo optimization framework that generates measurement
matrices designed to minimize both the robot's traversal path length and the
signal reconstruction error within the CS framework. Central to our approach is
the application of Dictionary Learning (DL) to obtain a data-driven sparsifying
transform, which enhances reconstruction accuracy while further reducing the
number of samples that the robot needs to collect. We demonstrate the
effectiveness of our method through experiments reconstructing $NO_2$ pollution
maps over the Gulf region. The results indicate that our approach can reduce
robot travel distance to less than $10\%$ of a full-coverage path, while
improving reconstruction accuracy by over a factor of five compared to
traditional CS methods based on DCT and polynomial dictionaries, as well as by
a factor of two compared to previously-proposed Informative Path Planning (IPP)
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DSFormer: A Dual-Scale Cross-Learning Transformer for Visual Place
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiyang Jiang, Songhao Piao, Chao Gao, Lei Yu, Liguo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Place Recognition (VPR) is crucial for robust mobile robot
localization, yet it faces significant challenges in maintaining reliable
performance under varying environmental conditions and viewpoints. To address
this, we propose a novel framework that integrates Dual-Scale-Former
(DSFormer), a Transformer-based cross-learning module, with an innovative block
clustering strategy. DSFormer enhances feature representation by enabling
bidirectional information transfer between dual-scale features extracted from
the final two CNN layers, capturing both semantic richness and spatial details
through self-attention for long-range dependencies within each scale and shared
cross-attention for cross-scale learning. Complementing this, our block
clustering strategy repartitions the widely used San Francisco eXtra Large
(SF-XL) training dataset from multiple distinct perspectives, optimizing data
organization to further bolster robustness against viewpoint variations.
Together, these innovations not only yield a robust global embedding adaptable
to environmental changes but also reduce the required training data volume by
approximately 30\% compared to previous partitioning methods. Comprehensive
experiments demonstrate that our approach achieves state-of-the-art performance
across most benchmark datasets, surpassing advanced reranking methods like
DELG, Patch-NetVLAD, TransVPR, and R2Former as a global retrieval solution
using 512-dim global descriptors, while significantly improving computational
efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the Pre-Dressing Step: Unfolding Medical Garments Via
  Imitation Learning <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Blanco-Mulero, Júlia Borràs, Carme Torras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic-assisted dressing has the potential to significantly aid both
patients as well as healthcare personnel, reducing the workload and improving
the efficiency in clinical settings. While substantial progress has been made
in robotic dressing assistance, prior works typically assume that garments are
already unfolded and ready for use. However, in medical applications gowns and
aprons are often stored in a folded configuration, requiring an additional
unfolding step. In this paper, we introduce the pre-dressing step, the process
of unfolding garments prior to assisted dressing. We leverage imitation
learning for learning three manipulation primitives, including both high and
low acceleration motions. In addition, we employ a visual classifier to
categorise the garment state as closed, partly opened, and fully opened. We
conduct an empirical evaluation of the learned manipulation primitives as well
as their combinations. Our results show that highly dynamic motions are not
effective for unfolding freshly unpacked garments, where the combination of
motions can efficiently enhance the opening configuration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures, 2 tables. Accepted to IEEE/RSJ IROS 2025. Project
  website: https://sites.google.com/view/pre-dressing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Residual Koopman Model Predictive <span class="highlight-title">Control</span> for Enhanced Vehicle <span class="highlight-title">Dynamic</span>s
  with Small On-Track Data Input 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghao Fu, Cheng Hu, Haokun Xiong, Zhangpeng Bao, Wenyuan Du, Edoardo Ghignone, Michele Magno, Lei Xie, Hongye Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In vehicle trajectory tracking tasks, the simplest approach is the Pure
Pursuit (PP) Control. However, this single-point preview tracking strategy
fails to consider vehicle model constraints, compromising driving safety. Model
Predictive Control (MPC) as a widely adopted control method, optimizes control
actions by incorporating mechanistic models and physical constraints. While its
control performance critically depends on the accuracy of vehicle modeling.
Traditional vehicle modeling approaches face inherent trade-offs between
capturing nonlinear dynamics and maintaining computational efficiency, often
resulting in reduced control performance. To address these challenges, this
paper proposes Residual Koopman Model Predictive Control (RKMPC) framework.
This method uses two linear MPC architecture to calculate control inputs: a
Linear Model Predictive Control (LMPC) computes the baseline control input
based on the vehicle kinematic model, and a neural network-based RKMPC
calculates the compensation input. The final control command is obtained by
adding these two components. This design preserves the reliability and
interpretability of traditional mechanistic model while achieving performance
optimization through residual modeling. This method has been validated on the
Carsim-Matlab joint simulation platform and a physical 1:10 scale F1TENTH
racing car. Experimental results show that RKMPC requires only 20% of the
training data needed by traditional Koopman Model Predictive Control (KMPC)
while delivering superior tracking performance. Compared to traditional LMPC,
RKMPC reduces lateral error by 11.7%-22.1%, decreases heading error by
8.9%-15.8%, and improves front-wheel steering stability by up to 27.6%. The
implementation code is available at: https://github.com/ZJU-DDRX/Residual
Koopman.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ G2S-ICP <span class="highlight-title">SLAM</span>: Geometry-aware Gaussian Splatting ICP <span class="highlight-title">SLAM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gyuhyeon Pak, Hae Min Cho, Euntai Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a novel geometry-aware RGB-D Gaussian Splatting
SLAM system, named G2S-ICP SLAM. The proposed method performs high-fidelity 3D
reconstruction and robust camera pose tracking in real-time by representing
each scene element using a Gaussian distribution constrained to the local
tangent plane. This effectively models the local surface as a 2D Gaussian disk
aligned with the underlying geometry, leading to more consistent depth
interpretation across multiple viewpoints compared to conventional 3D
ellipsoid-based representations with isotropic uncertainty. To integrate this
representation into the SLAM pipeline, we embed the surface-aligned Gaussian
disks into a Generalized ICP framework by introducing anisotropic covariance
prior without altering the underlying registration formulation. Furthermore we
propose a geometry-aware loss that supervises photometric, depth, and normal
consistency. Our system achieves real-time operation while preserving both
visual and geometric fidelity. Extensive experiments on the Replica and
TUM-RGBD datasets demonstrate that G2S-ICP SLAM outperforms prior SLAM systems
in terms of localization accuracy, reconstruction completeness, while
maintaining the rendering quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AF-RLIO: Adaptive <span class="highlight-title">Fusion</span> of Radar-<span class="highlight-title">LiDAR</span>-Inertial Information for <span class="highlight-title">Robust</span>
  <span class="highlight-title">Odometry</span> in Challenging Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenglong Qian, Yang Xu, Xiufang Shi, Jiming Chen, Liang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In robotic navigation, maintaining precise pose estimation and navigation in
complex and dynamic environments is crucial. However, environmental challenges
such as smoke, tunnels, and adverse weather can significantly degrade the
performance of single-sensor systems like LiDAR or GPS, compromising the
overall stability and safety of autonomous robots. To address these challenges,
we propose AF-RLIO: an adaptive fusion approach that integrates 4D
millimeter-wave radar, LiDAR, inertial measurement unit (IMU), and GPS to
leverage the complementary strengths of these sensors for robust odometry
estimation in complex environments. Our method consists of three key modules.
Firstly, the pre-processing module utilizes radar data to assist LiDAR in
removing dynamic points and determining when environmental conditions are
degraded for LiDAR. Secondly, the dynamic-aware multimodal odometry selects
appropriate point cloud data for scan-to-map matching and tightly couples it
with the IMU using the Iterative Error State Kalman Filter. Lastly, the factor
graph optimization module balances weights between odometry and GPS data,
constructing a pose graph for optimization. The proposed approach has been
evaluated on datasets and tested in real-world robotic environments,
demonstrating its effectiveness and advantages over existing methods in
challenging conditions such as smoke and tunnels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Articulated Object Manipulation On The Fly with Foundation
  Model Reasoning and Part Grounding <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaojie Zhang, Yuanfei Wang, Ruihai Wu, Kunqi Xu, Yu Li, Liuyu Xiang, Hao Dong, Zhaofeng He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Articulated objects pose diverse manipulation challenges for robots. Since
their internal structures are not directly observable, robots must adaptively
explore and refine actions to generate successful manipulation trajectories.
While existing works have attempted cross-category generalization in adaptive
articulated object manipulation, two major challenges persist: (1) the
geometric diversity of real-world articulated objects complicates visual
perception and understanding, and (2) variations in object functions and
mechanisms hinder the development of a unified adaptive manipulation strategy.
To address these challenges, we propose AdaRPG, a novel framework that
leverages foundation models to extract object parts, which exhibit greater
local geometric similarity than entire objects, thereby enhancing visual
affordance generalization for functional primitive skills. To support this, we
construct a part-level affordance annotation dataset to train the affordance
model. Additionally, AdaRPG utilizes the common knowledge embedded in
foundation models to reason about complex mechanisms and generate high-level
control codes that invoke primitive skill functions based on part affordance
inference. Simulation and real-world experiments demonstrate AdaRPG's strong
generalization ability across novel articulated object categories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic
  Grounding for Generalizable <span class="highlight-title">Robot</span>ic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu Su, Weiwei Shang, Chen Qian, Fei Zhang, Shuang Cong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantics-driven 3D spatial constraints align highlevel semantic
representations with low-level action spaces, facilitating the unification of
task understanding and execution in robotic manipulation. The synergistic
reasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation
Models (VFMs) enables cross-modal 3D spatial constraint construction.
Nevertheless, existing methods have three key limitations: (1) coarse semantic
granularity in constraint modeling, (2) lack of real-time closed-loop planning,
(3) compromised robustness in semantically diverse environments. To address
these challenges, we propose ReSem3D, a unified manipulation framework for
semantically diverse environments, leveraging the synergy between VFMs and
MLLMs to achieve fine-grained visual grounding and dynamically constructs
hierarchical 3D spatial constraints for real-time manipulation. Specifically,
the framework is driven by hierarchical recursive reasoning in MLLMs, which
interact with VFMs to automatically construct 3D spatial constraints from
natural language instructions and RGB-D observations in two stages: part-level
extraction and region-level refinement. Subsequently, these constraints are
encoded as real-time optimization objectives in joint space, enabling reactive
behavior to dynamic disturbances. Extensive simulation and real-world
experiments are conducted in semantically rich household and sparse chemical
lab environments. The results demonstrate that ReSem3D performs diverse
manipulation tasks under zero-shot conditions, exhibiting strong adaptability
and generalization. Code and videos at https://resem3d.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages,9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Evaluation</span> of facial landmark <span class="highlight-title">localization</span> performance in a surgical
  setting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ines Frajtag, Marko Švaco, Filip Šuligoj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of robotics, computer vision, and their applications is becoming
increasingly widespread in various fields, including medicine. Many face
detection algorithms have found applications in neurosurgery, ophthalmology,
and plastic surgery. A common challenge in using these algorithms is variable
lighting conditions and the flexibility of detection positions to identify and
precisely localize patients. The proposed experiment tests the MediaPipe
algorithm for detecting facial landmarks in a controlled setting, using a
robotic arm that automatically adjusts positions while the surgical light and
the phantom remain in a fixed position. The results of this study demonstrate
that the improved accuracy of facial landmark detection under surgical lighting
significantly enhances the detection performance at larger yaw and pitch
angles. The increase in standard deviation/dispersion occurs due to imprecise
detection of selected facial landmarks. This analysis allows for a discussion
on the potential integration of the MediaPipe algorithm into medical
procedures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoRPI-PINN: A Physics-Informed Framework for Mobile <span class="highlight-title">Robot</span> Pure Inertial
  <span class="highlight-title">Navigation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arup Kumar Sahoo, Itzik Klein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental requirement for full autonomy in mobile robots is accurate
navigation even in situations where satellite navigation or cameras are
unavailable. In such practical situations, relying only on inertial sensors
will result in navigation solution drift due to the sensors' inherent noise and
error terms. One of the emerging solutions to mitigate drift is to maneuver the
robot in a snake-like slithering motion to increase the inertial
signal-to-noise ratio, allowing the regression of the mobile robot position. In
this work, we propose MoRPI-PINN as a physics-informed neural network framework
for accurate inertial-based mobile robot navigation. By embedding physical laws
and constraints into the training process, MoRPI-PINN is capable of providing
an accurate and robust navigation solution. Using real-world experiments, we
show accuracy improvements of over 85% compared to other approaches. MoRPI-PINN
is a lightweight approach that can be implemented even on edge devices and used
in any typical mobile robot application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Autonomous UAV <span class="highlight-title">Navigation</span> for Search and Rescue Missions Using Computer
  Vision and Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luka Šiktar, Branimir Ćaran, Bojan Šekoranja, Marko Švaco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a subsystem, using Unmanned Aerial Vehicles (UAV),
for search and rescue missions, focusing on people detection, face recognition
and tracking of identified individuals. The proposed solution integrates a UAV
with ROS2 framework, that utilizes multiple convolutional neural networks (CNN)
for search missions. System identification and PD controller deployment are
performed for autonomous UAV navigation. The ROS2 environment utilizes the
YOLOv11 and YOLOv11-pose CNNs for tracking purposes, and the dlib library CNN
for face recognition. The system detects a specific individual, performs face
recognition and starts tracking. If the individual is not yet known, the UAV
operator can manually locate the person, save their facial image and
immediately initiate the tracking process. The tracking process relies on
specific keypoints identified on the human body using the YOLOv11-pose CNN
model. These keypoints are used to track a specific individual and maintain a
safe distance. To enhance accurate tracking, system identification is
performed, based on measurement data from the UAVs IMU. The identified system
parameters are used to design PD controllers that utilize YOLOv11-pose to
estimate the distance between the UAVs camera and the identified individual.
The initial experiments, conducted on 14 known individuals, demonstrated that
the proposed subsystem can be successfully used in real time. The next step
involves implementing the system on a large experimental UAV for field use and
integrating autonomous navigation with GPS-guided control for rescue operations
planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper is accepted and presented on the 34th International
  Conference on Robotics in Alpe-Adria-Danube Region, RAAD 2025, Belgrade
  Serbia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Modular Residual Learning Framework to Enhance Model-Based Approach
  for <span class="highlight-title">Robust</span> Locomotion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min-Gyu Kim, Dongyun Kang, Hajun Kim, Hae-Won Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach that combines the advantages of both
model-based and learning-based frameworks to achieve robust locomotion. The
residual modules are integrated with each corresponding part of the model-based
framework, a footstep planner and dynamic model designed using heuristics, to
complement performance degradation caused by a model mismatch. By utilizing a
modular structure and selecting the appropriate learning-based method for each
residual module, our framework demonstrates improved control performance in
environments with high uncertainty, while also achieving higher learning
efficiency compared to baseline methods. Moreover, we observed that our
proposed methodology not only enhances control performance but also provides
additional benefits, such as making nominal controllers more robust to
parameter tuning. To investigate the feasibility of our framework, we
demonstrated residual modules combined with model predictive control in a real
quadrupedal robot. Despite uncertainties beyond the simulation, the robot
successfully maintains balance and tracks the commanded velocity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, IEEE RA-L accepted (July 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modular <span class="highlight-title">Robot</span> and Landmark Localisation Using Relative Bearing
  Measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18070v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18070v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Behzad Zamani, Jochen Trumpf, Chris Manzie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we propose a modular nonlinear least squares filtering approach
for systems composed of independent subsystems. The state and error covariance
estimate of each subsystem is updated independently, even when a relative
measurement simultaneously depends on the states of multiple subsystems. We
integrate the Covariance Intersection (CI) algorithm as part of our solution in
order to prevent double counting of information when subsystems share estimates
with each other. An alternative derivation of the CI algorithm based on least
squares estimation makes this integration possible. We particularise the
proposed approach to the robot-landmark localization problem. In this problem,
noisy measurements of the bearing angle to a stationary landmark position
measured relative to the SE(2) pose of a moving robot couple the estimation
problems for the robot pose and the landmark position. In a randomized
simulation study, we benchmark the proposed modular method against a monolithic
joint state filter to elucidate their respective trade-offs. In this study we
also include variants of the proposed method that achieve a graceful
degradation of performance with reduced communication and bandwidth
requirements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to RA-L</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenNav: Open-World <span class="highlight-title">Navigation</span> with Multimodal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18033v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18033v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingfeng Yuan, Letian Wang, Steven L. Waslander
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained large language models (LLMs) have demonstrated strong
common-sense reasoning abilities, making them promising for robotic navigation
and planning tasks. However, despite recent progress, bridging the gap between
language descriptions and actual robot actions in the open-world, beyond merely
invoking limited predefined motion primitives, remains an open challenge. In
this work, we aim to enable robots to interpret and decompose complex language
instructions, ultimately synthesizing a sequence of trajectory points to
complete diverse navigation tasks given open-set instructions and open-set
objects. We observe that multi-modal large language models (MLLMs) exhibit
strong cross-modal understanding when processing free-form language
instructions, demonstrating robust scene comprehension. More importantly,
leveraging their code-generation capability, MLLMs can interact with
vision-language perception models to generate compositional 2D bird-eye-view
value maps, effectively integrating semantic knowledge from MLLMs with spatial
information from maps to reinforce the robot's spatial understanding. To
further validate our approach, we effectively leverage large-scale autonomous
vehicle datasets (AVDs) to validate our proposed zero-shot vision-language
navigation framework in outdoor navigation tasks, demonstrating its capability
to execute a diverse range of free-form natural language navigation
instructions while maintaining robustness against object detection errors and
linguistic ambiguities. Furthermore, we validate our system on a Husky robot in
both indoor and outdoor scenes, demonstrating its real-world robustness and
applicability. Supplementary videos are available at
https://trailab.github.io/OpenNav-website/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dif<span class="highlight-title">fusion</span> Beats Autoregressive in Data-Constrained Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15857v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15857v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihir Prabhudesai, Menging Wu, Amir Zadeh, Katerina Fragkiadaki, Deepak Pathak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive (AR) models have long dominated the landscape of large
language models, driving progress across a wide range of tasks. Recently,
diffusion-based language models have emerged as a promising alternative, though
their advantages over AR models remain underexplored. In this paper, we
systematically study masked diffusion models in data-constrained settings-where
training involves repeated passes over limited data-and find that they
significantly outperform AR models when compute is abundant but data is scarce.
Diffusion models make better use of repeated data, achieving lower validation
loss and superior downstream performance. We interpret this advantage as
implicit data augmentation: masked diffusion exposes the model to a diverse
distribution of token orderings and prediction tasks, unlike AR's fixed
left-to-right factorization. We find new scaling laws for diffusion models and
derive a closed-form expression for the critical compute threshold at which
diffusion begins to outperform AR. These results suggest that when data, not
compute, is the bottleneck, diffusion models offer a compelling alternative to
the standard AR paradigm. Our code is available at:
https://diffusion-scaling.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Webpage: https://diffusion-scaling.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RUMI: Rummaging Using Mutual Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10450v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10450v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng Zhong, Nima Fazeli, Dmitry Berenson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Rummaging Using Mutual Information (RUMI), a method for
online generation of robot action sequences to gather information about the
pose of a known movable object in visually-occluded environments. Focusing on
contact-rich rummaging, our approach leverages mutual information between the
object pose distribution and robot trajectory for action planning. From an
observed partial point cloud, RUMI deduces the compatible object pose
distribution and approximates the mutual information of it with workspace
occupancy in real time. Based on this, we develop an information gain cost
function and a reachability cost function to keep the object within the robot's
reach. These are integrated into a model predictive control (MPC) framework
with a stochastic dynamics model, updating the pose distribution in a closed
loop. Key contributions include a new belief framework for object pose
estimation, an efficient information gain computation strategy, and a robust
MPC-based control scheme. RUMI demonstrates superior performance in both
simulated and real tasks compared to baseline methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 20 figures, accepted by IEEE Transactions on Robotics
  (T-RO), preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Gentle <span class="highlight-title">Grasp</span>ing Using Vision, Sound, and Touch <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.07926v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.07926v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ken Nakahara, Roberto Calandra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In our daily life, we often encounter objects that are fragile and can be
damaged by excessive grasping force, such as fruits. For these objects, it is
paramount to grasp gently -- not using the maximum amount of force possible,
but rather the minimum amount of force necessary. This paper proposes using
visual, tactile, and auditory signals to learn to grasp and regrasp objects
stably and gently. Specifically, we use audio signals as an indicator of
gentleness during the grasping, and then train an end-to-end action-conditional
model from raw visuo-tactile inputs that predicts both the stability and the
gentleness of future grasping candidates, thus allowing the selection and
execution of the most promising action. Experimental results on a
multi-fingered hand over 1,500 grasping trials demonstrated that our model is
useful for gentle grasping by validating the predictive performance (3.27%
higher accuracy than the vision-only variant) and providing interpretations of
their behavior. Finally, real-world experiments confirmed that the grasping
performance with the trained multi-modal model outperformed other baselines
(17% higher rate for stable and gentle grasps than vision-only). Our approach
requires neither tactile sensor calibration nor analytical force modeling,
drastically reducing the engineering effort to grasp fragile objects. Dataset
and videos are available at https://lasr.org/research/gentle-grasping.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages. Accepted by 2025 IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoboCar: A Rapidly Deployable Open-Source Platform for Autonomous
  Driving Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.03572v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.03572v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehdi Testouri, Gamal Elghazaly, Raphael Frank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces RoboCar, an open-source research platform for
autonomous driving developed at the University of Luxembourg. RoboCar provides
a modular, cost-effective framework for the development of experimental
Autonomous Driving Systems (ADS), utilizing the 2018 KIA Soul EV. The platform
integrates a robust hardware and software architecture that aligns with the
vehicle's existing systems, minimizing the need for extensive modifications. It
supports various autonomous driving functions and has undergone real-world
testing on public roads in Luxembourg City. This paper outlines the platform's
architecture, integration challenges, and initial test results, offering
insights into its application in advancing autonomous driving research. RoboCar
is available to anyone at https://github.com/sntubix/robocar and is released
under an open-source MIT license.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatio-Temporal Motion Retargeting for Quadruped <span class="highlight-title">Robot</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11557v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11557v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taerim Yoon, Dongho Kang, Seungmin Kim, Jin Cheng, Minsung Ahn, Stelian Coros, Sungjoon Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents a motion retargeting approach for legged robots, aimed at
transferring the dynamic and agile movements to robots from source motions. In
particular, we guide the imitation learning procedures by transferring motions
from source to target, effectively bridging the morphological disparities while
ensuring the physical feasibility of the target system. In the first stage, we
focus on motion retargeting at the kinematic level by generating kinematically
feasible whole-body motions from keypoint trajectories. Following this, we
refine the motion at the dynamic level by adjusting it in the temporal domain
while adhering to physical constraints. This process facilitates policy
training via reinforcement learning, enabling precise and robust motion
tracking. We demonstrate that our approach successfully transforms noisy motion
sources, such as hand-held camera videos, into robot-specific motions that
align with the morphology and physical properties of the target robots.
Moreover, we demonstrate terrain-aware motion retargeting to perform BackFlip
on top of a box. We successfully deployed these skills to four robots with
different dimensions and physical properties in the real world through hardware
experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 12 figures, videos available at
  https://taerimyoon.me/Spatio-Temporal-Motion-Retargeting-for-Quadruped-Robots/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging multi-source and heterogeneous signals for fatigue <span class="highlight-title">detection</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16859v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16859v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luobin Cui, Yanlai Wu, Tang Ying, Weikai Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fatigue detection plays a critical role in safety-critical applications such
as aviation, mining, and long-haul transport. However, most existing methods
rely on high-end sensors and controlled environments, limiting their
applicability in real world settings. This paper formally defines a practical
yet underexplored problem setting for real world fatigue detection, where
systems operating with context-appropriate sensors aim to leverage knowledge
from differently instrumented sources including those using impractical sensors
deployed in controlled environments. To tackle this challenge, we propose a
heterogeneous and multi-source fatigue detection framework that adaptively
utilizes the available modalities in the target domain while benefiting from
the diverse configurations present in source domains. Our experiments,
conducted using a realistic field-deployed sensor setup and two publicly
available datasets, demonstrate the practicality, robustness, and improved
generalization of our approach, paving the practical way for effective fatigue
monitoring in sensor-constrained scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>1figures,32pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Realtime Limb <span class="highlight-title">Trajectory</span> <span class="highlight-title">Optimization</span> for Humanoid Running Through
  Centroidal Angular Momentum <span class="highlight-title">Dynamic</span>s <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17351v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17351v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sait Sovukluk, Robert Schuller, Johannes Englsberger, Christian Ott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the essential aspects of humanoid robot running is determining the
limb-swinging trajectories. During the flight phases, where the ground reaction
forces are not available for regulation, the limb swinging trajectories are
significant for the stability of the next stance phase. Due to the conservation
of angular momentum, improper leg and arm swinging results in highly tilted and
unsustainable body configurations at the next stance phase landing. In such
cases, the robotic system fails to maintain locomotion independent of the
stability of the center of mass trajectories. This problem is more apparent for
fast and high flight time trajectories. This paper proposes a real-time
nonlinear limb trajectory optimization problem for humanoid running. The
optimization problem is tested on two different humanoid robot models, and the
generated trajectories are verified using a running algorithm for both robots
in a simulation environment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for publication at the IEEE
  International Conference on Robotics and Automation (ICRA), Atlanta 2025.
  Link to video: https://www.youtube.com/watch?v=czfHjwh_A0Y</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Efficient Numerical Function <span class="highlight-title">Optimization</span> Framework for Constrained
  Nonlinear <span class="highlight-title">Robot</span>ic Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17349v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17349v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sait Sovukluk, Christian Ott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a numerical function optimization framework designed for
constrained optimization problems in robotics. The tool is designed with
real-time considerations and is suitable for online trajectory and control
input optimization problems. The proposed framework does not require any
analytical representation of the problem and works with constrained block-box
optimization functions. The method combines first-order gradient-based line
search algorithms with constraint prioritization through nullspace projections
onto constraint Jacobian space. The tool is implemented in C++ and provided
online for community use, along with some numerical and robotic example
implementations presented in this paper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>\c{opyright} 2025 the authors. This work has been accepted to IFAC
  for publication under a Creative Commons Licence CC-BY-NC-ND. -
  Implementation: https://github.com/ssovukluk/ENFORCpp</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CA-Cut: Crop-Aligned Cutout for Data Augmentation to Learn More <span class="highlight-title">Robust</span>
  Under-Canopy <span class="highlight-title">Navigation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robel Mamo, Taeyeong Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art visual under-canopy navigation methods are designed with
deep learning-based perception models to distinguish traversable space from
crop rows. While these models have demonstrated successful performance, they
require large amounts of training data to ensure reliability in real-world
field deployment. However, data collection is costly, demanding significant
human resources for in-field sampling and annotation. To address this
challenge, various data augmentation techniques are commonly employed during
model training, such as color jittering, Gaussian blur, and horizontal flip, to
diversify training data and enhance model robustness. In this paper, we
hypothesize that utilizing only these augmentation techniques may lead to
suboptimal performance, particularly in complex under-canopy environments with
frequent occlusions, debris, and non-uniform spacing of crops. Instead, we
propose a novel augmentation method, so-called Crop-Aligned Cutout (CA-Cut)
which masks random regions out in input images that are spatially distributed
around crop rows on the sides to encourage trained models to capture high-level
contextual features even when fine-grained information is obstructed. Our
extensive experiments with a public cornfield dataset demonstrate that
masking-based augmentations are effective for simulating occlusions and
significantly improving robustness in semantic keypoint predictions for visual
navigation. In particular, we show that biasing the mask distribution toward
crop rows in CA-Cut is critical for enhancing both prediction accuracy and
generalizability across diverse environments achieving up to a 36.9% reduction
in prediction error. In addition, we conduct ablation studies to determine the
number of masks, the size of each mask, and the spatial distribution of masks
to maximize overall performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 12th European Conference on Mobile
  Robots (ECMR 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ B4P: Simultaneous <span class="highlight-title">Grasp</span> and Motion <span class="highlight-title">Planning</span> for Object Placement via
  Parallelized Bidirectional Forests and Path Repair 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.04598v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.04598v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin H. Leebron, Kejia Ren, Yiting Chen, Kaiyu Hang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot pick and place systems have traditionally decoupled grasp, placement,
and motion planning to build sequential optimization pipelines with the
assumption that the individual components will be able to work together.
However, this separation introduces sub-optimality, as grasp choices may limit
or even prohibit feasible motions for a robot to reach the target placement
pose, particularly in cluttered environments with narrow passages. To this end,
we propose a forest-based planning framework to simultaneously find grasp
configurations and feasible robot motions that explicitly satisfy downstream
placement configurations paired with the selected grasps. Our proposed
framework leverages a bidirectional sampling-based approach to build a start
forest, rooted at the feasible grasp regions, and a goal forest, rooted at the
feasible placement regions, to facilitate the search through randomly explored
motions that connect valid pairs of grasp and placement trees. We demonstrate
that the framework's inherent parallelism enables superlinear speedup, making
it scalable for applications for redundant robot arms (e.g., 7 Degrees of
Freedom) to work efficiently in highly cluttered environments. Extensive
experiments in simulation demonstrate the robustness and efficiency of the
proposed framework in comparison with multiple baselines under diverse
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentiable Motion Manifold Primitives for Reactive Motion Generation
  under Kino<span class="highlight-title">dynamic</span> Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12193v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12193v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghyeon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time motion generation -- which is essential for achieving reactive and
adaptive behavior -- under kinodynamic constraints for high-dimensional systems
is a crucial yet challenging problem. We address this with a two-step approach:
offline learning of a lower-dimensional trajectory manifold of task-relevant,
constraint-satisfying trajectories, followed by rapid online search within this
manifold. Extending the discrete-time Motion Manifold Primitives (MMP)
framework, we propose Differentiable Motion Manifold Primitives (DMMP), a novel
neural network architecture that encodes and generates continuous-time,
differentiable trajectories, trained using data collected offline through
trajectory optimizations, with a strategy that ensures constraint satisfaction
-- absent in existing methods. Experiments on dynamic throwing with a 7-DoF
robot arm demonstrate that DMMP outperforms prior methods in planning speed,
task success, and constraint satisfaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages and 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17596v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17596v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej K. Wozniak, Lianhang Liu, Yixi Cai, Patric Jensfelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While end-to-end autonomous driving models show promising results, their
practical deployment is often hindered by large model sizes, a reliance on
expensive LiDAR sensors and computationally intensive BEV feature
representations. This limits their scalability, especially for mass-market
vehicles equipped only with cameras. To address these challenges, we propose
PRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving
architecture operates using only camera data, without explicit BEV
representation and forgoing the need for LiDAR. PRIX leverages a visual feature
extractor coupled with a generative planning head to predict safe trajectories
from raw pixel inputs directly. A core component of our architecture is the
Context-aware Recalibration Transformer (CaRT), a novel module designed to
effectively enhance multi-level visual features for more robust planning. We
demonstrate through comprehensive experiments that PRIX achieves
state-of-the-art performance on the NavSim and nuScenes benchmarks, matching
the capabilities of larger, multimodal diffusion planners while being
significantly more efficient in terms of inference speed and model size, making
it a practical solution for real-world deployment. Our work is open-source and
the code will be at https://maxiuw.github.io/prix.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Target Tracking via <span class="highlight-title">LiDAR</span>-RADAR Sensor <span class="highlight-title">Fusion</span> for Autonomous Racing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20043v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20043v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcello Cellina, Matteo Corno, Sergio Matteo Savaresi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High Speed multi-vehicle Autonomous Racing will increase the safety and
performance of road-going Autonomous Vehicles. Precise vehicle detection and
dynamics estimation from a moving platform is a key requirement for planning
and executing complex autonomous overtaking maneuvers. To address this
requirement, we have developed a Latency-Aware EKF-based Multi Target Tracking
algorithm fusing LiDAR and RADAR measurements. The algorithm explots the
different sensor characteristics by explicitly integrating the Range Rate in
the EKF Measurement Function, as well as a-priori knowledge of the racetrack
during state prediction. It can handle Out-Of-Sequence Measurements via
Reprocessing using a double State and Measurement Buffer, ensuring sensor delay
compensation with no information loss. This algorithm has been implemented on
Team PoliMOVE's autonomous racecar, and was proved experimentally by completing
a number of fully autonomous overtaking maneuvers at speeds up to 275 km/h.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Conference, 6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compositional Coordination for Multi-<span class="highlight-title">Robot</span> Teams with Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16068v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16068v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhehui Huang, Guangyao Shi, Yuwei Wu, Vijay Kumar, Gaurav S. Sukhatme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-robot coordination has traditionally relied on a mission-specific and
expert-driven pipeline, where natural language mission descriptions are
manually translated by domain experts into mathematical formulation, algorithm
design, and executable code. This conventional process is labor-intensive,
inaccessible to non-experts, and inflexible to changes in mission requirements.
Here, we propose LAN2CB (Language to Collective Behavior), a novel framework
that leverages large language models (LLMs) to streamline and generalize the
multi-robot coordination pipeline. LAN2CB transforms natural language (NL)
mission descriptions into executable Python code for multi-robot systems
through two core modules: (1) Mission Analysis, which parses mission
descriptions into behavior trees, and (2) Code Generation, which leverages the
behavior tree and a structured knowledge base to generate robot control code.
We further introduce a dataset of natural language mission descriptions to
support development and benchmarking. Experiments in both simulation and
real-world environments demonstrate that LAN2CB enables robust and flexible
multi-robot coordination from natural language, significantly reducing manual
engineering effort and supporting broad generalization across diverse mission
types. Website: https://sites.google.com/view/lan-cb
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Terrain-Aware Adaptation for Two-Dimensional UAV Path Planners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17519v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17519v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kostas Karakontis, Thanos Petsanis, Athanasios Ch. Kapoutsis, Pavlos Ch. Kapoutsis, Elias B. Kosmatopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-UAV Coverage Path Planning (mCPP) algorithms in popular commercial
software typically treat a Region of Interest (RoI) only as a 2D plane,
ignoring important3D structure characteristics. This leads to incomplete
3Dreconstructions, especially around occluded or vertical surfaces. In this
paper, we propose a modular algorithm that can extend commercial
two-dimensional path planners to facilitate terrain-aware planning by adjusting
altitude and camera orientations. To demonstrate it, we extend the well-known
DARP (Divide Areas for Optimal Multi-Robot Coverage Path Planning) algorithm
and produce DARP-3D. We present simulation results in multiple 3D environments
and a real-world flight test using DJI hardware. Compared to baseline, our
approach consistently captures improved 3D reconstructions, particularly in
areas with significant vertical features. An open-source implementation of the
algorithm is available here:https://github.com/konskara/TerraPlan
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Differentiated Reward Method for <span class="highlight-title">Reinforcement</span> Learning based
  Multi-Vehicle Cooperative <span class="highlight-title">Decision</span>-Making Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00352v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00352v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Han, Lijun Zhang, Dejian Meng, Zhuang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) shows great potential for optimizing
multi-vehicle cooperative driving strategies through the state-action-reward
feedback loop, but it still faces challenges such as low sample efficiency.
This paper proposes a differentiated reward method based on steady-state
transition systems, which incorporates state transition gradient information
into the reward design by analyzing traffic flow characteristics, aiming to
optimize action selection and policy learning in multi-vehicle cooperative
decision-making. The performance of the proposed method is validated in RL
algorithms such as MAPPO, MADQN, and QMIX under varying autonomous vehicle
penetration. The results show that the differentiated reward method
significantly accelerates training convergence and outperforms centering reward
and others in terms of traffic efficiency, safety, and action rationality.
Additionally, the method demonstrates strong scalability and environmental
adaptability, providing a novel approach for multi-agent cooperative
decision-making in complex traffic scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safe, Task-Consistent Manipulation with Operational Space <span class="highlight-title">Control</span>
  Barrier Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.06736v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.06736v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Morton, Marco Pavone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe real-time control of robotic manipulators in unstructured environments
requires handling numerous safety constraints without compromising task
performance. Traditional approaches, such as artificial potential fields
(APFs), suffer from local minima, oscillations, and limited scalability, while
model predictive control (MPC) can be computationally expensive. Control
barrier functions (CBFs) offer a promising alternative due to their high level
of robustness and low computational cost, but these safety filters must be
carefully designed to avoid significant reductions in the overall performance
of the manipulator. In this work, we introduce an Operational Space Control
Barrier Function (OSCBF) framework that integrates safety constraints while
preserving task-consistent behavior. Our approach scales to hundreds of
simultaneous constraints while retaining real-time control rates, ensuring
collision avoidance, singularity prevention, and workspace containment even in
highly cluttered settings or during dynamic motions. By explicitly accounting
for the task hierarchy in the CBF objective, we prevent degraded performance
across both joint-space and operational-space tasks, when at the limit of
safety. We validate performance in both simulation and hardware, and release
our open-source high-performance code and media on our project webpage,
https://stanfordasl.github.io/oscbf/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compliant Residual DAgger: Improving Real-World Contact-Rich
  Manipulation with Human Corrections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16685v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16685v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaomeng Xu, Yifan Hou, Zeyi Liu, Shuran Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address key challenges in Dataset Aggregation (DAgger) for real-world
contact-rich manipulation: how to collect informative human correction data and
how to effectively update policies with this new data. We introduce Compliant
Residual DAgger (CR-DAgger), which contains two novel components: 1) a
Compliant Intervention Interface that leverages compliance control, allowing
humans to provide gentle, accurate delta action corrections without
interrupting the ongoing robot policy execution; and 2) a Compliant Residual
Policy formulation that learns from human corrections while incorporating force
feedback and force control. Our system significantly enhances performance on
precise contact-rich manipulation tasks using minimal correction data,
improving base policy success rates by over 50\% on two challenging tasks (book
flipping and belt assembly) while outperforming both retraining-from-scratch
and finetuning approaches. Through extensive real-world experiments, we provide
practical guidance for implementing effective DAgger in real-world robot
learning tasks. Result videos are available at:
https://compliant-residual-dagger.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hand Gesture Recognition for Collaborative <span class="highlight-title">Robot</span>s Using Lightweight Deep
  Learning in Real-Time <span class="highlight-title">Robot</span>ic Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.10055v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.10055v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Muhtadin, I Wayan Agus Darmawan, Muhammad Hilmi Rusydiansyah, I Ketut Eddy Purnama, Chastine Fatichah, Mauridhi Hery Purnomo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct and natural interaction is essential for intuitive human-robot
collaboration, eliminating the need for additional devices such as joysticks,
tablets, or wearable sensors. In this paper, we present a lightweight deep
learning-based hand gesture recognition system that enables humans to control
collaborative robots naturally and efficiently. This model recognizes eight
distinct hand gestures with only 1,103 parameters and a compact size of 22 KB,
achieving an accuracy of 93.5%. To further optimize the model for real-world
deployment on edge devices, we applied quantization and pruning using
TensorFlow Lite, reducing the final model size to just 7 KB. The system was
successfully implemented and tested on a Universal Robot UR5 collaborative
robot within a real-time robotic framework based on ROS2. The results
demonstrate that even extremely lightweight models can deliver accurate and
responsive hand gesture-based control for collaborative robots, opening new
possibilities for natural human-robot interaction in constrained environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Relative Pose <span class="highlight-title">Estimation</span> Framework with Dual Noise Tuning for
  Safe Approaching Maneuvers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16214v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16214v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Batu Candan, Simone Servadio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and robust relative pose estimation is crucial for enabling
challenging Active Debris Removal (ADR) missions targeting tumbling derelict
satellites such as ESA's ENVISAT. This work presents a complete pipeline
integrating advanced computer vision techniques with adaptive nonlinear
filtering to address this challenge. A Convolutional Neural Network (CNN),
enhanced with image preprocessing, detects structural markers (corners) from
chaser imagery, whose 2D coordinates are converted to 3D measurements using
camera modeling. These measurements are fused within an Unscented Kalman Filter
(UKF) framework, selected for its ability to handle nonlinear relative
dynamics, to estimate the full relative pose. Key contributions include the
integrated system architecture and a dual adaptive strategy within the UKF:
dynamic tuning of the measurement noise covariance compensates for varying CNN
measurement uncertainty, while adaptive tuning of the process noise covariance,
utilizing measurement residual analysis, accounts for unmodeled dynamics or
maneuvers online. This dual adaptation enhances robustness against both
measurement imperfections and dynamic model uncertainties. The performance of
the proposed adaptive integrated system is evaluated through high-fidelity
simulations using a realistic ENVISAT model, comparing estimates against ground
truth under various conditions, including measurement outages. This
comprehensive approach offers an enhanced solution for robust onboard relative
navigation, significantly advancing the capabilities required for safe
proximity operations during ADR missions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Designing Effective Human-<span class="highlight-title">Swarm</span> Interaction Interfaces: Insights from a
  User Study on Task Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.02250v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.02250v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wasura D. Wattearachchi, Erandi Lakshika, Kathryn Kasmarik, Michael Barlow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a systematic method of design for human-swarm
interaction interfaces, combining theoretical insights with empirical
evaluation. We first derived ten design principles from existing literature,
applying them to key information dimensions identified through goal-directed
task analysis and developed a tablet-based interface for a target search task.
We then conducted a user study with 31 participants where humans were required
to guide a robotic swarm to a target in the presence of three types of hazards
that pose a risk to the robots: Distributed, Moving, and Spreading. Performance
was measured based on the proximity of the robots to the target and the number
of deactivated robots at the end of the task. Results indicate that at least
one robot was brought closer to the target in 98% of tasks, demonstrating the
interface's success in fulfilling the primary objective of the task.
Additionally, in nearly 67% of tasks, more than 50% of the robots reached the
target. Moreover, particularly better performance was noted in moving hazards.
Additionally, the interface appeared to help minimise robot deactivation, as
evidenced by nearly 94% of tasks where participants managed to keep more than
50% of the robots active, ensuring that most of the swarm remained operational.
However, its effectiveness varied across hazards, with robot deactivation being
lowest in distributed hazard scenarios, suggesting that the interface provided
the most support in these conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Bilateral Teleoperation and Imitation Learning Using Sensorless
  Force <span class="highlight-title">Control</span> via Accurate <span class="highlight-title">Dynamic</span>s Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06174v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06174v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koki Yamane, Yunhan Li, Masashi Konosu, Koki Inami, Junji Oaki, Sho Sakaino, Toshiaki Tsuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the advancement of imitation learning has led to increased
interest in teleoperating low-cost manipulators to collect demonstration data.
However, most existing systems rely on unilateral control, which only transmits
target position values. While this approach is easy to implement and suitable
for slow, non-contact tasks, it struggles with fast or contact-rich operations
due to the absence of force feedback. This work demonstrates that fast
teleoperation with force feedback is feasible even with force-sensorless,
low-cost manipulators by leveraging 4-channel bilateral control. Based on
accurately identified manipulator dynamics, our method integrates nonlinear
terms compensation, velocity and external force estimation, and variable gain
corresponding to inertial variation. Furthermore, using data collected by
4-channel bilateral control, we show that incorporating force information into
both the input and output of learned policies improves performance in imitation
learning. These results highlight the practical effectiveness of our system for
high-fidelity teleoperation and data collection on affordable hardware.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 9 figures, Submitted to CoRL 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">179</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Captain Cinema: Towards Short Movie Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfei Xiao, Ceyuan Yang, Lvmin Zhang, Shengqu Cai, Yang Zhao, Yuwei Guo, Gordon Wetzstein, Maneesh Agrawala, Alan Yuille, Lu Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Captain Cinema, a generation framework for short movie generation.
Given a detailed textual description of a movie storyline, our approach firstly
generates a sequence of keyframes that outline the entire narrative, which
ensures long-range coherence in both the storyline and visual appearance (e.g.,
scenes and characters). We refer to this step as top-down keyframe planning.
These keyframes then serve as conditioning signals for a video synthesis model,
which supports long context learning, to produce the spatio-temporal dynamics
between them. This step is referred to as bottom-up video synthesis. To support
stable and efficient generation of multi-scene long narrative cinematic works,
we introduce an interleaved training strategy for Multimodal Diffusion
Transformers (MM-DiT), specifically adapted for long-context video data. Our
model is trained on a specially curated cinematic dataset consisting of
interleaved data pairs. Our experiments demonstrate that Captain Cinema
performs favorably in the automated creation of visually coherent and narrative
consistent short movies in high quality and efficiency. Project page:
https://thecinema.ai
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review. Project page: https://thecinema.ai</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying Prompted Artist Names from Generated Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grace Su, Sheng-Yu Wang, Aaron Hertzmann, Eli Shechtman, Jun-Yan Zhu, Richard Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A common and controversial use of text-to-image models is to generate
pictures by explicitly naming artists, such as "in the style of Greg
Rutkowski". We introduce a benchmark for prompted-artist recognition:
predicting which artist names were invoked in the prompt from the image alone.
The dataset contains 1.95M images covering 110 artists and spans four
generalization settings: held-out artists, increasing prompt complexity,
multiple-artist prompts, and different text-to-image models. We evaluate
feature similarity baselines, contrastive style descriptors, data attribution
methods, supervised classifiers, and few-shot prototypical networks.
Generalization patterns vary: supervised and few-shot models excel on seen
artists and complex prompts, whereas style descriptors transfer better when the
artist's style is pronounced; multi-artist prompts remain the most challenging.
Our benchmark reveals substantial headroom and provides a public testbed to
advance the responsible moderation of text-to-image models. We release the
dataset and benchmark to foster further research:
https://graceduansu.github.io/IdentifyingPromptedArtists/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page:
  https://graceduansu.github.io/IdentifyingPromptedArtists</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SIDA: Synthetic Image Driven Zero-shot Domain Adaptation <span class="chip">ACM MM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye-Chan Kim, SeungJu Cha, Si-Woo Kim, Taewhan Kim, Dong-Jin Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot domain adaptation is a method for adapting a model to a target
domain without utilizing target domain image data. To enable adaptation without
target images, existing studies utilize CLIP's embedding space and text
description to simulate target-like style features. Despite the previous
achievements in zero-shot domain adaptation, we observe that these text-driven
methods struggle to capture complex real-world variations and significantly
increase adaptation time due to their alignment process. Instead of relying on
text descriptions, we explore solutions leveraging image data, which provides
diverse and more fine-grained style cues. In this work, we propose SIDA, a
novel and efficient zero-shot domain adaptation method leveraging synthetic
images. To generate synthetic images, we first create detailed, source-like
images and apply image translation to reflect the style of the target domain.
We then utilize the style features of these synthetic images as a proxy for the
target domain. Based on these features, we introduce Domain Mix and Patch Style
Transfer modules, which enable effective modeling of real-world variations. In
particular, Domain Mix blends multiple styles to expand the intra-domain
representations, and Patch Style Transfer assigns different styles to
individual patches. We demonstrate the effectiveness of our method by showing
state-of-the-art performance in diverse zero-shot adaptation scenarios,
particularly in challenging domains. Moreover, our approach achieves high
efficiency by significantly reducing the overall adaptation time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM MM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Software Synthesis Guided by Constraint-Expressive Intermediate
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuqing Li, Anson Y. Lam, Yun Peng, Wenxuan Wang, Michael R. Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphical user interface (UI) software has undergone a fundamental
transformation from traditional two-dimensional (2D) desktop/web/mobile
interfaces to spatial three-dimensional (3D) environments. While existing work
has made remarkable success in automated 2D software generation, such as
HTML/CSS and mobile app interface code synthesis, the generation of 3D software
still remains under-explored. Current methods for 3D software generation
usually generate the 3D environments as a whole and cannot modify or control
specific elements in the software. Furthermore, these methods struggle to
handle the complex spatial and semantic constraints inherent in the real world.
To address the challenges, we present Scenethesis, a novel
requirement-sensitive 3D software synthesis approach that maintains formal
traceability between user specifications and generated 3D software. Scenethesis
is built upon ScenethesisLang, a domain-specific language that serves as a
granular constraint-aware intermediate representation (IR) to bridge natural
language requirements and executable 3D software. It serves both as a
comprehensive scene description language enabling fine-grained modification of
3D software elements and as a formal constraint-expressive specification
language capable of expressing complex spatial constraints. By decomposing 3D
software synthesis into stages operating on ScenethesisLang, Scenethesis
enables independent verification, targeted modification, and systematic
constraint satisfaction. Our evaluation demonstrates that Scenethesis
accurately captures over 80% of user requirements and satisfies more than 90%
of hard constraints while handling over 100 constraints simultaneously.
Furthermore, Scenethesis achieves a 42.8% improvement in BLIP-2 visual
evaluation scores compared to the state-of-the-art method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SynC: Synthetic Image Caption <span class="highlight-title">Dataset</span> Refinement with One-to-many
  <span class="highlight-title">Mapping</span> for Zero-shot Image Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Si-Woo Kim, MinJu Jeon, Ye-Chan Kim, Soeun Lee, Taewhan Kim, Dong-Jin Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot Image Captioning (ZIC) increasingly utilizes synthetic datasets
generated by text-to-image (T2I) models to mitigate the need for costly manual
annotation. However, these T2I models often produce images that exhibit
semantic misalignments with their corresponding input captions (e.g., missing
objects, incorrect attributes), resulting in noisy synthetic image-caption
pairs that can hinder model training. Existing dataset pruning techniques are
largely designed for removing noisy text in web-crawled data. However, these
methods are ill-suited for the distinct challenges of synthetic data, where
captions are typically well-formed, but images may be inaccurate
representations. To address this gap, we introduce SynC, a novel framework
specifically designed to refine synthetic image-caption datasets for ZIC.
Instead of conventional filtering or regeneration, SynC focuses on reassigning
captions to the most semantically aligned images already present within the
synthetic image pool. Our approach employs a one-to-many mapping strategy by
initially retrieving multiple relevant candidate images for each caption. We
then apply a cycle-consistency-inspired alignment scorer that selects the best
image by verifying its ability to retrieve the original caption via
image-to-text retrieval. Extensive evaluations demonstrate that SynC
consistently and significantly improves performance across various ZIC models
on standard benchmarks (MS-COCO, Flickr30k, NoCaps), achieving state-of-the-art
results in several scenarios. SynC offers an effective strategy for curating
refined synthetic data to enhance ZIC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM Multimedia 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DRWKV: Focusing on Object Edges for Low-Light Image Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuecheng Bai, Yuxiang Wang, Boyu Hu, Qinyuan Jie, Chuanzhi Xu, Hongru Xiao, Kechen Li, Vera Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-light image enhancement remains a challenging task, particularly in
preserving object edge continuity and fine structural details under extreme
illumination degradation. In this paper, we propose a novel model, DRWKV
(Detailed Receptance Weighted Key Value), which integrates our proposed Global
Edge Retinex (GER) theory, enabling effective decoupling of illumination and
edge structures for enhanced edge fidelity. Secondly, we introduce Evolving WKV
Attention, a spiral-scanning mechanism that captures spatial edge continuity
and models irregular structures more effectively. Thirdly, we design the
Bilateral Spectrum Aligner (Bi-SAB) and a tailored MS2-Loss to jointly align
luminance and chrominance features, improving visual naturalness and mitigating
artifacts. Extensive experiments on five LLIE benchmarks demonstrate that DRWKV
achieves leading performance in PSNR, SSIM, and NIQE while maintaining low
computational complexity. Furthermore, DRWKV enhances downstream performance in
low-light multi-object tracking tasks, validating its generalization
capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SafeWork-R1: Coevolving Safety and Intelligence under the
  AI-45$^{\circ}$ Law 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18576v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18576v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanghai AI Lab,  :, Yicheng Bao, Guanxu Chen, Mingkang Chen, Yunhao Chen, Chiyu Chen, Lingjie Chen, Sirui Chen, Xinquan Chen, Jie Cheng, Yu Cheng, Dengke Deng, Yizhuo Ding, Dan Ding, Xiaoshan Ding, Yi Ding, Zhichen Dong, Lingxiao Du, Yuyu Fan, Xinshun Feng, Yanwei Fu, Yuxuan Gao, Ruijun Ge, Tianle Gu, Lujun Gui, Jiaxuan Guo, Qianxi He, Yuenan Hou, Xuhao Hu, Hong Huang, Kaichen Huang, Shiyang Huang, Yuxian Jiang, Shanzhe Lei, Jie Li, Lijun Li, Hao Li, Juncheng Li, Xiangtian Li, Yafu Li, Lingyu Li, Xueyan Li, Haotian Liang, Dongrui Liu, Qihua Liu, Zhixuan Liu, Bangwei Liu, Huacan Liu, Yuexiao Liu, Zongkai Liu, Chaochao Lu, Yudong Lu, Xiaoya Lu, Zhenghao Lu, Qitan Lv, Caoyuan Ma, Jiachen Ma, Xiaoya Ma, Zhongtian Ma, Lingyu Meng, Ziqi Miao, Yazhe Niu, Yuezhang Peng, Yuan Pu, Han Qi, Chen Qian, Xingge Qiao, Jingjing Qu, Jiashu Qu, Wanying Qu, Wenwen Qu, Xiaoye Qu, Qihan Ren, Qingnan Ren, Qingyu Ren, Jing Shao, Wenqi Shao, Shuai Shao, Dongxing Shi, Xin Song, Xinhao Song, Yan Teng, Xuan Tong, Yingchun Wang, Xuhong Wang, Shujie Wang, Xin Wang, Yige Wang, Yixu Wang, Yuanfu Wang, Futing Wang, Ruofan Wang, Wenjie Wang, Yajie Wang, Muhao Wei, Xiaoyu Wen, Fenghua Weng, Yuqi Wu, Yingtong Xiong, Xingcheng Xu, Chao Yang, Yue Yang, Yang Yao, Yulei Ye, Zhenyun Yin, Yi Yu, Bo Zhang, Qiaosheng Zhang, Jinxuan Zhang, Yexin Zhang, Yinqiang Zheng, Hefeng Zhou, Zhanhui Zhou, Pengyu Zhu, Qingzi Zhu, Yubo Zhu, Bowen Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that
demonstrates the coevolution of capabilities and safety. It is developed by our
proposed SafeLadder framework, which incorporates large-scale, progressive,
safety-oriented reinforcement learning post-training, supported by a suite of
multi-principled verifiers. Unlike previous alignment methods such as RLHF that
simply learn human preferences, SafeLadder enables SafeWork-R1 to develop
intrinsic safety reasoning and self-reflection abilities, giving rise to safety
`aha' moments. Notably, SafeWork-R1 achieves an average improvement of
$46.54\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks
without compromising general capabilities, and delivers state-of-the-art safety
performance compared to leading proprietary models such as GPT-4.1 and Claude
Opus 4. To further bolster its reliability, we implement two distinct
inference-time intervention methods and a deliberative search mechanism,
enforcing step-level verification. Finally, we further develop
SafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and
SafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and
capability can co-evolve synergistically, highlighting the generalizability of
our framework in building robust, reliable, and trustworthy general-purpose AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>47 pages, 18 figures, authors are listed in alphabetical order by
  their last names</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HybridTM: Combining Transformer and Mamba for 3D Semantic <span class="highlight-title">Segmentation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18575v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18575v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Wang, Jinghua Hou, Zhe Liu, Yingying Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based methods have demonstrated remarkable capabilities in 3D
semantic segmentation through their powerful attention mechanisms, but the
quadratic complexity limits their modeling of long-range dependencies in
large-scale point clouds. While recent Mamba-based approaches offer efficient
processing with linear complexity, they struggle with feature representation
when extracting 3D features. However, effectively combining these complementary
strengths remains an open challenge in this field. In this paper, we propose
HybridTM, the first hybrid architecture that integrates Transformer and Mamba
for 3D semantic segmentation. In addition, we propose the Inner Layer Hybrid
Strategy, which combines attention and Mamba at a finer granularity, enabling
simultaneous capture of long-range dependencies and fine-grained local
features. Extensive experiments demonstrate the effectiveness and
generalization of our HybridTM on diverse indoor and outdoor datasets.
Furthermore, our HybridTM achieves state-of-the-art performance on ScanNet,
ScanNet200, and nuScenes benchmarks. The code will be made available at
https://github.com/deepinact/HybridTM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial Distribution Matching for Dif<span class="highlight-title">fusion</span> Distillation Towards
  Efficient Image and Video Synthesis <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanzuo Lu, Yuxi Ren, Xin Xia, Shanchuan Lin, Xing Wang, Xuefeng Xiao, Andy J. Ma, Xiaohua Xie, Jian-Huang Lai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distribution Matching Distillation (DMD) is a promising score distillation
technique that compresses pre-trained teacher diffusion models into efficient
one-step or multi-step student generators. Nevertheless, its reliance on the
reverse Kullback-Leibler (KL) divergence minimization potentially induces mode
collapse (or mode-seeking) in certain applications. To circumvent this inherent
drawback, we propose Adversarial Distribution Matching (ADM), a novel framework
that leverages diffusion-based discriminators to align the latent predictions
between real and fake score estimators for score distillation in an adversarial
manner. In the context of extremely challenging one-step distillation, we
further improve the pre-trained generator by adversarial distillation with
hybrid discriminators in both latent and pixel spaces. Different from the mean
squared error used in DMD2 pre-training, our method incorporates the
distributional loss on ODE pairs collected from the teacher model, and thus
providing a better initialization for score distillation fine-tuning in the
next stage. By combining the adversarial distillation pre-training with ADM
fine-tuning into a unified pipeline termed DMDX, our proposed method achieves
superior one-step performance on SDXL compared to DMD2 while consuming less GPU
time. Additional experiments that apply multi-step ADM distillation on
SD3-Medium, SD3.5-Large, and CogVideoX set a new benchmark towards efficient
image and video synthesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2025 (Highlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Facial Demorphing from a Single Morph Using a Latent Conditional GAN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nitish Shukla, Arun Ross
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A morph is created by combining two (or more) face images from two (or more)
identities to create a composite image that is highly similar to both
constituent identities, allowing the forged morph to be biometrically
associated with more than one individual. Morph Attack Detection (MAD) can be
used to detect a morph, but does not reveal the constituent images. Demorphing
- the process of deducing the constituent images - is thus vital to provide
additional evidence about a morph. Existing demorphing methods suffer from the
morph replication problem, where the outputs tend to look very similar to the
morph itself, or assume that train and test morphs are generated using the same
morph technique. The proposed method overcomes these issues. The method
decomposes a morph in latent space allowing it to demorph images created from
unseen morph techniques and face styles. We train our method on morphs created
from synthetic faces and test on morphs created from real faces using arbitrary
morph techniques. Our method outperforms existing methods by a considerable
margin and produces high fidelity demorphed face images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning-Based Age <span class="highlight-title">Estimation</span> and Gender Deep Learning-Based Age
  <span class="highlight-title">Estimation</span> and Gender Classification for Targeted Advertisement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Imran Zaman, Nisar Ahmed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel deep learning-based approach for simultaneous age
and gender classification from facial images, designed to enhance the
effectiveness of targeted advertising campaigns. We propose a custom
Convolutional Neural Network (CNN) architecture, optimized for both tasks,
which leverages the inherent correlation between age and gender information
present in facial features. Unlike existing methods that often treat these
tasks independently, our model learns shared representations, leading to
improved performance. The network is trained on a large, diverse dataset of
facial images, carefully pre-processed to ensure robustness against variations
in lighting, pose, and image quality. Our experimental results demonstrate a
significant improvement in gender classification accuracy, achieving 95%, and a
competitive mean absolute error of 5.77 years for age estimation. Critically,
we analyze the performance across different age groups, identifying specific
challenges in accurately estimating the age of younger individuals. This
analysis reveals the need for targeted data augmentation and model refinement
to address these biases. Furthermore, we explore the impact of different CNN
architectures and hyperparameter settings on the overall performance, providing
valuable insights for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthetic Data Augmentation for Enhanced Chicken Carcass Instance
  <span class="highlight-title">Segmentation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Feng, Chaitanya Pallerla, Xiaomin Lin, Pouya Sohrabipour Sr, Philip Crandall, Wan Shou, Yu She, Dongyi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The poultry industry has been driven by broiler chicken production and has
grown into the world's largest animal protein sector. Automated detection of
chicken carcasses on processing lines is vital for quality control, food
safety, and operational efficiency in slaughterhouses and poultry processing
plants. However, developing robust deep learning models for tasks like instance
segmentation in these fast-paced industrial environments is often hampered by
the need for laborious acquisition and annotation of large-scale real-world
image datasets. We present the first pipeline generating photo-realistic,
automatically labeled synthetic images of chicken carcasses. We also introduce
a new benchmark dataset containing 300 annotated real-world images, curated
specifically for poultry segmentation research. Using these datasets, this
study investigates the efficacy of synthetic data and automatic data annotation
to enhance the instance segmentation of chicken carcasses, particularly when
real annotated data from the processing line is scarce. A small real dataset
with varying proportions of synthetic images was evaluated in prominent
instance segmentation models. Results show that synthetic data significantly
boosts segmentation performance for chicken carcasses across all models. This
research underscores the value of synthetic data augmentation as a viable and
effective strategy to mitigate data scarcity, reduce manual annotation efforts,
and advance the development of robust AI-driven automated detection systems for
chicken carcasses in the poultry processing industry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for journal reviewing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VideoMind: An Omni-Modal Video <span class="highlight-title">Dataset</span> with Intent Grounding for
  Deep-Cognitive Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baoyao Yang, Wanyun Li, Dixin Chen, Junxiang Chen, Wenbin Yao, Haifeng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces VideoMind, a video-centric omni-modal dataset designed
for deep video content cognition and enhanced multi-modal feature
representation. The dataset comprises 103K video samples (3K reserved for
testing), each paired with audio and systematically detailed textual
descriptions. Specifically, every video and its audio is described across three
hierarchical layers (factual, abstract, and intent), progressing from surface
to depth. It contains over 22 million words, averaging ~225 words per sample.
VideoMind's key distinction from existing datasets is its provision of intent
expressions, which require contextual integration across the entire video and
are not directly observable. These deep-cognitive expressions are generated
using a Chain-of-Thought (COT) approach, prompting the mLLM through
step-by-step reasoning. Each description includes annotations for subject,
place, time, event, action, and intent, supporting downstream recognition
tasks. Crucially, we establish a gold-standard benchmark with 3,000 manually
validated samples for evaluating deep-cognitive video understanding. We design
hybrid-cognitive retrieval experiments, scored by multi-level retrieval
metrics, to appropriately assess deep video comprehension. Evaluation results
for models (e.g., InternVideo, VAST, UMT-L) are released. VideoMind serves as a
powerful benchmark for fine-grained cross-modal alignment and advances fields
requiring in-depth video understanding, such as emotion and intent recognition.
The data is publicly available on GitHub, HuggingFace, and OpenDataLab,
https://github.com/cdx-cindy/VideoMind.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages; 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A 3D Cross-modal Keypoint Descriptor for MR-US Matching and <span class="highlight-title">Registration</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniil Morozov, Reuben Dorent, Nazim Haouchine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intraoperative registration of real-time ultrasound (iUS) to preoperative
Magnetic Resonance Imaging (MRI) remains an unsolved problem due to severe
modality-specific differences in appearance, resolution, and field-of-view. To
address this, we propose a novel 3D cross-modal keypoint descriptor for MRI-iUS
matching and registration. Our approach employs a patient-specific
matching-by-synthesis approach, generating synthetic iUS volumes from
preoperative MRI. This enables supervised contrastive training to learn a
shared descriptor space.
  A probabilistic keypoint detection strategy is then employed to identify
anatomically salient and modality-consistent locations. During training, a
curriculum-based triplet loss with dynamic hard negative mining is used to
learn descriptors that are i) robust to iUS artifacts such as speckle noise and
limited coverage, and ii) rotation-invariant . At inference, the method detects
keypoints in MR and real iUS images and identifies sparse matches, which are
then used to perform rigid registration. Our approach is evaluated using 3D
MRI-iUS pairs from the ReMIND dataset. Experiments show that our approach
outperforms state-of-the-art keypoint matching methods across 11 patients, with
an average precision of $69.8\%$. For image registration, our method achieves a
competitive mean Target Registration Error of 2.39 mm on the ReMIND2Reg
benchmark.
  Compared to existing iUS-MR registration approach, our framework is
interpretable, requires no manual initialization, and shows robustness to iUS
field-of-view variation. Code is available at
https://github.com/morozovdd/CrossKEY.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Performance of Concept Probing: The Influence of the Data
  (Extended Version) <span class="chip">ECAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel de Sousa Ribeiro, Afonso Leote, João Leite
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept probing has recently garnered increasing interest as a way to help
interpret artificial neural networks, dealing both with their typically large
size and their subsymbolic nature, which ultimately renders them unfeasible for
direct human interpretation. Concept probing works by training additional
classifiers to map the internal representations of a model into human-defined
concepts of interest, thus allowing humans to peek inside artificial neural
networks. Research on concept probing has mainly focused on the model being
probed or the probing model itself, paying limited attention to the data
required to train such probing models. In this paper, we address this gap.
Focusing on concept probing in the context of image classification tasks, we
investigate the effect of the data used to train probing models on their
performance. We also make available concept labels for two widely used
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of the paper published in Proceedings of the
  European Conference on Artificial Intelligence (ECAI 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unposed 3DGS Reconstruction with Probabilistic Procrustes <span class="highlight-title">Mapping</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chong Cheng, Zijian Wang, Sicheng Yu, Yu Hu, Nanjie Yao, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has emerged as a core technique for 3D
representation. Its effectiveness largely depends on precise camera poses and
accurate point cloud initialization, which are often derived from pretrained
Multi-View Stereo (MVS) models. However, in unposed reconstruction task from
hundreds of outdoor images, existing MVS models may struggle with memory limits
and lose accuracy as the number of input images grows. To address this
limitation, we propose a novel unposed 3DGS reconstruction framework that
integrates pretrained MVS priors with the probabilistic Procrustes mapping
strategy. The method partitions input images into subsets, maps submaps into a
global space, and jointly optimizes geometry and poses with 3DGS. Technically,
we formulate the mapping of tens of millions of point clouds as a probabilistic
Procrustes problem and solve a closed-form alignment. By employing
probabilistic coupling along with a soft dustbin mechanism to reject uncertain
correspondences, our method globally aligns point clouds and poses within
minutes across hundreds of images. Moreover, we propose a joint optimization
framework for 3DGS and camera poses. It constructs Gaussians from
confidence-aware anchor points and integrates 3DGS differentiable rendering
with an analytical Jacobian to jointly refine scene and poses, enabling
accurate reconstruction and pose estimation. Experiments on Waymo and KITTI
datasets show that our method achieves accurate reconstruction from unposed
image sequences, setting a new state of the art for unposed 3DGS
reconstruction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TTS-VAR: A Test-Time Scaling Framework for Visual Auto-Regressive
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhekai Chen, Ruihang Chu, Yukang Chen, Shiwei Zhang, Yujie Wei, Yingya Zhang, Xihui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling visual generation models is essential for real-world content
creation, yet requires substantial training and computational expenses.
Alternatively, test-time scaling has garnered growing attention due to resource
efficiency and promising performance. In this work, we present TTS-VAR, the
first general test-time scaling framework for visual auto-regressive (VAR)
models, modeling the generation process as a path searching problem. To
dynamically balance computational efficiency with exploration capacity, we
first introduce an adaptive descending batch size schedule throughout the
causal generation process. Besides, inspired by VAR's hierarchical
coarse-to-fine multi-scale generation, our framework integrates two key
components: (i) At coarse scales, we observe that generated tokens are hard for
evaluation, possibly leading to erroneous acceptance of inferior samples or
rejection of superior samples. Noticing that the coarse scales contain
sufficient structural information, we propose clustering-based diversity
search. It preserves structural variety through semantic feature clustering,
enabling later selection on samples with higher potential. (ii) In fine scales,
resampling-based potential selection prioritizes promising candidates using
potential scores, which are defined as reward functions incorporating
multi-scale generation history. Experiments on the powerful VAR model Infinity
show a notable 8.7% GenEval score improvement (from 0.69 to 0.75). Key insights
reveal that early-stage structural features effectively influence final
quality, and resampling efficacy varies across generation scales. Code is
available at https://github.com/ali-vilab/TTS-VAR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 Tables, 9 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Elucidating the Design Space of Arbitrary-Noise-Based Dif<span class="highlight-title">fusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Qiu, Mengying Yang, Xinghua Ma, Dong Liang, Yuzhen Li, Fanding Li, Gongning Luo, Wei Wang, Kuanquan Wang, Shuo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  EDM elucidates the unified design space of diffusion models, yet its fixed
noise patterns restricted to pure Gaussian noise, limit advancements in image
restoration. Our study indicates that forcibly injecting Gaussian noise
corrupts the degraded images, overextends the image transformation distance,
and increases restoration complexity. To address this problem, our proposed EDA
Elucidates the Design space of Arbitrary-noise-based diffusion models.
Theoretically, EDA expands the freedom of noise pattern while preserving the
original module flexibility of EDM, with rigorous proof that increased noise
complexity incurs no additional computational overhead during restoration. EDA
is validated on three typical tasks: MRI bias field correction (global smooth
noise), CT metal artifact reduction (global sharp noise), and natural image
shadow removal (local boundary-aware noise). With only 5 sampling steps, EDA
outperforms most task-specific methods and achieves state-of-the-art
performance in bias field correction and shadow removal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COT-AD: Cotton Analysis <span class="highlight-title">Dataset</span> <span class="chip">ICIP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akbar Ali, Mahek Vyas, Soumyaratna Debnath, Chanda Grover Kamra, Jaidev Sanjay Khalane, Reuben Shibu Devanesan, Indra Deep Mastan, Subramanian Sankaranarayanan, Pankaj Khanna, Shanmuganathan Raman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents COT-AD, a comprehensive Dataset designed to enhance
cotton crop analysis through computer vision. Comprising over 25,000 images
captured throughout the cotton growth cycle, with 5,000 annotated images,
COT-AD includes aerial imagery for field-scale detection and segmentation and
high-resolution DSLR images documenting key diseases. The annotations cover
pest and disease recognition, vegetation, and weed analysis, addressing a
critical gap in cotton-specific agricultural datasets. COT-AD supports tasks
such as classification, segmentation, image restoration, enhancement, deep
generative model-based cotton crop synthesis, and early disease management,
advancing data-driven crop management
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Dataset publicly available at:
  https://ieee-dataport.org/documents/cot-adcotton-analysis-dataset. Accepted
  to IEEE International Conference on Image Processing (ICIP) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IntentVCNet: Bridging Spatio-Temporal Gaps for Intention-Oriented
  <span class="highlight-title">Control</span>lable Video Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18531v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18531v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianheng Qiu, Jingchun Gao, Jingyu Li, Huiyi Leong, Xuan Huang, Xi Wang, Xiaocheng Zhang, Kele Xu, Lan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intent-oriented controlled video captioning aims to generate targeted
descriptions for specific targets in a video based on customized user intent.
Current Large Visual Language Models (LVLMs) have gained strong instruction
following and visual comprehension capabilities. Although the LVLMs
demonstrated proficiency in spatial and temporal understanding respectively, it
was not able to perform fine-grained spatial control in time sequences in
direct response to instructions. This substantial spatio-temporal gap
complicates efforts to achieve fine-grained intention-oriented control in
video. Towards this end, we propose a novel IntentVCNet that unifies the
temporal and spatial understanding knowledge inherent in LVLMs to bridge the
spatio-temporal gap from both prompting and model perspectives. Specifically,
we first propose a prompt combination strategy designed to enable LLM to model
the implicit relationship between prompts that characterize user intent and
video sequences. We then propose a parameter efficient box adapter that
augments the object semantic information in the global visual context so that
the visual token has a priori information about the user intent. The final
experiment proves that the combination of the two strategies can further
enhance the LVLM's ability to model spatial details in video sequences, and
facilitate the LVLMs to accurately generate controlled intent-oriented
captions. Our proposed method achieved state-of-the-art results in several open
source LVLMs and was the runner-up in the IntentVC challenge. Our code is
available on https://github.com/thqiu0419/IntentVCNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gaussian<span class="highlight-title">Fusion</span>Occ: A Seamless Sensor <span class="highlight-title">Fusion</span> Approach for 3D Occupancy
  <span class="highlight-title">Prediction</span> Using 3D Gaussians 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomislav Pavković, Mohammad-Ali Nikouei Mahani, Johannes Niedermayer, Johannes Betz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D semantic occupancy prediction is one of the crucial tasks of autonomous
driving. It enables precise and safe interpretation and navigation in complex
environments. Reliable predictions rely on effective sensor fusion, as
different modalities can contain complementary information. Unlike conventional
methods that depend on dense grid representations, our approach,
GaussianFusionOcc, uses semantic 3D Gaussians alongside an innovative sensor
fusion mechanism. Seamless integration of data from camera, LiDAR, and radar
sensors enables more precise and scalable occupancy prediction, while 3D
Gaussian representation significantly improves memory efficiency and inference
speed. GaussianFusionOcc employs modality-agnostic deformable attention to
extract essential features from each sensor type, which are then used to refine
Gaussian properties, resulting in a more accurate representation of the
environment. Extensive testing with various sensor combinations demonstrates
the versatility of our approach. By leveraging the robustness of multi-modal
fusion and the efficiency of Gaussian representation, GaussianFusionOcc
outperforms current state-of-the-art models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object <span class="highlight-title">segmentation</span> in the wild with foundation models: application to
  vision assisted neuro-prostheses for upper limbs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bolutife Atoki, Jenny Benois-Pineau, Renaud Péteri, Fabien Baldacci, Aymar de Rugy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we address the problem of semantic object segmentation using
foundation models. We investigate whether foundation models, trained on a large
number and variety of objects, can perform object segmentation without
fine-tuning on specific images containing everyday objects, but in highly
cluttered visual scenes. The ''in the wild'' context is driven by the target
application of vision guided upper limb neuroprostheses. We propose a method
for generating prompts based on gaze fixations to guide the Segment Anything
Model (SAM) in our segmentation scenario, and fine-tune it on egocentric visual
data. Evaluation results of our approach show an improvement of the IoU
segmentation quality metric by up to 0.51 points on real-world challenging data
of Grasping-in-the-Wild corpus which is made available on the RoboFlow Platform
(https://universe.roboflow.com/iwrist/grasping-in-the-wild)
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Large Scale Geostatistical Methane Monitoring with Part-based
  Object <span class="highlight-title">Detection</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adhemar de Senneville, Xavier Bou, Thibaud Ehret, Rafael Grompone, Jean Louis Bonne, Nicolas Dumelie, Thomas Lauvaux, Gabriele Facciolo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection is one of the main applications of computer vision in remote
sensing imagery. Despite its increasing availability, the sheer volume of
remote sensing data poses a challenge when detecting rare objects across large
geographic areas. Paradoxically, this common challenge is crucial to many
applications, such as estimating environmental impact of certain human
activities at scale. In this paper, we propose to address the problem by
investigating the methane production and emissions of bio-digesters in France.
We first introduce a novel dataset containing bio-digesters, with small
training and validation sets, and a large test set with a high imbalance
towards observations without objects since such sites are rare. We develop a
part-based method that considers essential bio-digester sub-elements to boost
initial detections. To this end, we apply our method to new, unseen regions to
build an inventory of bio-digesters. We then compute geostatistical estimates
of the quantity of methane produced that can be attributed to these
infrastructures in a given area at a given time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explaining How Visual, Textual and Multimodal Encoders Share Concepts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clément Cornet, Romaric Besançon, Hervé Le Borgne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse autoencoders (SAEs) have emerged as a powerful technique for
extracting human-interpretable features from neural networks activations.
Previous works compared different models based on SAE-derived features but
those comparisons have been restricted to models within the same modality. We
propose a novel indicator allowing quantitative comparison of models across SAE
features, and use it to conduct a comparative study of visual, textual and
multimodal encoders. We also propose to quantify the Comparative Sharedness of
individual features between different classes of models. With these two new
tools, we conduct several studies on 21 encoders of the three types, with two
significantly different sizes, and considering generalist and domain specific
datasets. The results allow to revisit previous studies at the light of
encoders trained in a multimodal context and to quantify to which extent all
these models share some representations or features. They also suggest that
visual features that are specific to VLMs among vision encoders are shared with
text encoders, highlighting the impact of text pretraining. The code is
available at https://github.com/CEA-LIST/SAEshareConcepts
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human Scanpath <span class="highlight-title">Prediction</span> in Target-Present Visual Search with
  Semantic-Foveal Bayesian Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João Luzio, Alexandre Bernardino, Plinio Moreno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In goal-directed visual tasks, human perception is guided by both top-down
and bottom-up cues. At the same time, foveal vision plays a crucial role in
directing attention efficiently. Modern research on bio-inspired computational
attention models has taken advantage of advancements in deep learning by
utilizing human scanpath data to achieve new state-of-the-art performance. In
this work, we assess the performance of SemBA-FAST, i.e. Semantic-based
Bayesian Attention for Foveal Active visual Search Tasks, a top-down framework
designed for predicting human visual attention in target-present visual search.
SemBA-FAST integrates deep object detection with a probabilistic semantic
fusion mechanism to generate attention maps dynamically, leveraging pre-trained
detectors and artificial foveation to update top-down knowledge and improve
fixation prediction sequentially. We evaluate SemBA-FAST on the COCO-Search18
benchmark dataset, comparing its performance against other scanpath prediction
models. Our methodology achieves fixation sequences that closely match human
ground-truth scanpaths. Notably, it surpasses baseline and other top-down
approaches and competes, in some cases, with scanpath-informed models. These
findings provide valuable insights into the capabilities of semantic-foveal
probabilistic frameworks for human-like attention modelling, with implications
for real-time cognitive computing and robotics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in the 2025 IEEE International Conference on
  Development and Learning (ICDL)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Delving into <span class="highlight-title">Mapping</span> Uncertainty for Mapless <span class="highlight-title">Trajectory</span> <span class="highlight-title">Prediction</span> <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongzheng Zhang, Xuchong Qiu, Boran Zhang, Guantian Zheng, Xunjiang Gu, Guoxuan Chi, Huan-ang Gao, Leichen Wang, Zi<span class="highlight-author">ming Liu</span>, Xinrun Li, Igor Gilitschenski, Hongyang Li, <span class="highlight-author">Hang Zhao</span>, Hao Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in autonomous driving are moving towards mapless approaches,
where High-Definition (HD) maps are generated online directly from sensor data,
reducing the need for expensive labeling and maintenance. However, the
reliability of these online-generated maps remains uncertain. While
incorporating map uncertainty into downstream trajectory prediction tasks has
shown potential for performance improvements, current strategies provide
limited insights into the specific scenarios where this uncertainty is
beneficial. In this work, we first analyze the driving scenarios in which
mapping uncertainty has the greatest positive impact on trajectory prediction
and identify a critical, previously overlooked factor: the agent's kinematic
state. Building on these insights, we propose a novel Proprioceptive Scenario
Gating that adaptively integrates map uncertainty into trajectory prediction
based on forecasts of the ego vehicle's future kinematics. This lightweight,
self-supervised approach enhances the synergy between online mapping and
trajectory prediction, providing interpretability around where uncertainty is
advantageous and outperforming previous integration methods. Additionally, we
introduce a Covariance-based Map Uncertainty approach that better aligns with
map geometry, further improving trajectory prediction. Extensive ablation
studies confirm the effectiveness of our approach, achieving up to 23.6%
improvement in mapless trajectory prediction performance over the
state-of-the-art method using the real-world nuScenes driving dataset. Our
code, data, and models are publicly available at
https://github.com/Ethan-Zheng136/Map-Uncertainty-for-Trajectory-Prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IROS 2025, Project Page:
  https://ethan-zheng136.github.io/Dev-Unc/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforced <span class="highlight-title">Embodied</span> Active Defense: Exploiting Adaptive Interaction for
  <span class="highlight-title">Robust</span> Visual Perception in Adversarial 3D Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Yang, Lingxuan Wu, Lizhong Wang, Chengyang Ying, Hang Su, Jun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks in 3D environments have emerged as a critical threat to
the reliability of visual perception systems, particularly in safety-sensitive
applications such as identity verification and autonomous driving. These
attacks employ adversarial patches and 3D objects to manipulate deep neural
network (DNN) predictions by exploiting vulnerabilities within complex scenes.
Existing defense mechanisms, such as adversarial training and purification,
primarily employ passive strategies to enhance robustness. However, these
approaches often rely on pre-defined assumptions about adversarial tactics,
limiting their adaptability in dynamic 3D settings. To address these
challenges, we introduce Reinforced Embodied Active Defense (Rein-EAD), a
proactive defense framework that leverages adaptive exploration and interaction
with the environment to improve perception robustness in 3D adversarial
contexts. By implementing a multi-step objective that balances immediate
prediction accuracy with predictive entropy minimization, Rein-EAD optimizes
defense strategies over a multi-step horizon. Additionally, Rein-EAD involves
an uncertainty-oriented reward-shaping mechanism that facilitates efficient
policy updates, thereby reducing computational overhead and supporting
real-world applicability without the need for differentiable environments.
Comprehensive experiments validate the effectiveness of Rein-EAD, demonstrating
a substantial reduction in attack success rates while preserving standard
accuracy across diverse tasks. Notably, Rein-EAD exhibits robust generalization
to unseen and adaptive attacks, making it suitable for real-world complex
tasks, including 3D object classification, face recognition and autonomous
driving.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2404.00540</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A COCO-Formatted Instance-Level <span class="highlight-title">Dataset</span> for Plasmodium Falciparum
  <span class="highlight-title">Detection</span> in Giemsa-Stained Blood Smears <span class="chip">MICCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frauke Wilm, Luis Carlos Rivera Monroy, Mathias Öttl, Lukas Mürdter, Leonid Mill, Andreas Maier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate detection of Plasmodium falciparum in Giemsa-stained blood smears is
an essential component of reliable malaria diagnosis, especially in developing
countries. Deep learning-based object detection methods have demonstrated
strong potential for automated Malaria diagnosis, but their adoption is limited
by the scarcity of datasets with detailed instance-level annotations. In this
work, we present an enhanced version of the publicly available NIH malaria
dataset, with detailed bounding box annotations in COCO format to support
object detection training. We validated the revised annotations by training a
Faster R-CNN model to detect infected and non-infected red blood cells, as well
as white blood cells. Cross-validation on the original dataset yielded F1
scores of up to 0.88 for infected cell detection. These results underscore the
importance of annotation volume and consistency, and demonstrate that automated
annotation refinement combined with targeted manual correction can produce
training data of sufficient quality for robust detection performance. The
updated annotations set is publicly available via GitHub:
https://github.com/MIRA-Vision-Microscopy/malaria-thin-smear-coco.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures, 2 tables, accepted at MICCAI 2025 Open Data</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Q-Former Autoencoder: A Modern Framework for Medical Anomaly <span class="highlight-title">Detection</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Dalmonte, Emirhan Bayar, Emre Akbas, Mariana-Iuliana Georgescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection in medical images is an important yet challenging task due
to the diversity of possible anomalies and the practical impossibility of
collecting comprehensively annotated data sets. In this work, we tackle
unsupervised medical anomaly detection proposing a modernized autoencoder-based
framework, the Q-Former Autoencoder, that leverages state-of-the-art pretrained
vision foundation models, such as DINO, DINOv2 and Masked Autoencoder. Instead
of training encoders from scratch, we directly utilize frozen vision foundation
models as feature extractors, enabling rich, multi-stage, high-level
representations without domain-specific fine-tuning. We propose the usage of
the Q-Former architecture as the bottleneck, which enables the control of the
length of the reconstruction sequence, while efficiently aggregating multiscale
features. Additionally, we incorporate a perceptual loss computed using
features from a pretrained Masked Autoencoder, guiding the reconstruction
towards semantically meaningful structures. Our framework is evaluated on four
diverse medical anomaly detection benchmarks, achieving state-of-the-art
results on BraTS2021, RESC, and RSNA. Our results highlight the potential of
vision foundation model encoders, pretrained on natural images, to generalize
effectively to medical image analysis tasks without further fine-tuning. We
release the code and models at https://github.com/emirhanbayar/QFAE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> CRUISE: Cooperative Reconstruction and Editing in V2X Scenarios using
  Gaussian Splatting <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Xu, Saining Zhang, Peishuo Li, Baijun Ye, Xiaoxue Chen, Huan-ang Gao, Jv Zheng, Xiaowei Song, Ziqiao Peng, Run Miao, Jinrang Jia, Yifeng Shi, Guangqi Yi, <span class="highlight-author">Hang Zhao</span>, Hao Tang, Hongyang Li, Kaicheng Yu, Hao Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vehicle-to-everything (V2X) communication plays a crucial role in autonomous
driving, enabling cooperation between vehicles and infrastructure. While
simulation has significantly contributed to various autonomous driving tasks,
its potential for data generation and augmentation in V2X scenarios remains
underexplored. In this paper, we introduce CRUISE, a comprehensive
reconstruction-and-synthesis framework designed for V2X driving environments.
CRUISE employs decomposed Gaussian Splatting to accurately reconstruct
real-world scenes while supporting flexible editing. By decomposing dynamic
traffic participants into editable Gaussian representations, CRUISE allows for
seamless modification and augmentation of driving scenes. Furthermore, the
framework renders images from both ego-vehicle and infrastructure views,
enabling large-scale V2X dataset augmentation for training and evaluation. Our
experimental results demonstrate that: 1) CRUISE reconstructs real-world V2X
driving scenes with high fidelity; 2) using CRUISE improves 3D detection across
ego-vehicle, infrastructure, and cooperative views, as well as cooperative 3D
tracking on the V2X-Seq benchmark; and 3) CRUISE effectively generates
challenging corner cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IROS 2025, Code: https://github.com/SainingZhang/CRUISE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Physically Realizable Adversarial Object Attack against
  <span class="highlight-title">LiDAR</span>-based <span class="highlight-title">Detection</span>: Clarifying Problem Formulation and Experimental
  Protocols 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luo Cheng, Hanwei Zhang, Lijun Zhang, Holger Hermanns
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial robustness in LiDAR-based 3D object detection is a critical
research area due to its widespread application in real-world scenarios. While
many digital attacks manipulate point clouds or meshes, they often lack
physical realizability, limiting their practical impact. Physical adversarial
object attacks remain underexplored and suffer from poor reproducibility due to
inconsistent setups and hardware differences. To address this, we propose a
device-agnostic, standardized framework that abstracts key elements of physical
adversarial object attacks, supports diverse methods, and provides open-source
code with benchmarking protocols in simulation and real-world settings. Our
framework enables fair comparison, accelerates research, and is validated by
successfully transferring simulated attacks to a physical LiDAR system. Beyond
the framework, we offer insights into factors influencing attack success and
advance understanding of adversarial robustness in real-world LiDAR perception.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PDB-Eval: An <span class="highlight-title">Evaluation</span> of Large Multimodal Models for Description and
  Explanation of Personalized Driving Behavior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junda Wu, Jessica Echterhoff, Kyungtae Han, Amr Abdelraouf, Rohit Gupta, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding a driver's behavior and intentions is important for potential
risk assessment and early accident prevention. Safety and driver assistance
systems can be tailored to individual drivers' behavior, significantly
enhancing their effectiveness. However, existing datasets are limited in
describing and explaining general vehicle movements based on external visual
evidence. This paper introduces a benchmark, PDB-Eval, for a detailed
understanding of Personalized Driver Behavior, and aligning Large Multimodal
Models (MLLMs) with driving comprehension and reasoning. Our benchmark consists
of two main components, PDB-X and PDB-QA. PDB-X can evaluate MLLMs'
understanding of temporal driving scenes. Our dataset is designed to find valid
visual evidence from the external view to explain the driver's behavior from
the internal view. To align MLLMs' reasoning abilities with driving tasks, we
propose PDB-QA as a visual explanation question-answering task for MLLM
instruction fine-tuning. As a generic learning task for generative models like
MLLMs, PDB-QA can bridge the domain gap without harming MLLMs'
generalizability. Our evaluation indicates that fine-tuning MLLMs on
fine-grained descriptions and explanations can effectively bridge the gap
between MLLMs and the driving domain, which improves zero-shot performance on
question-answering tasks by up to 73.2%. We further evaluate the MLLMs
fine-tuned on PDB-X in Brain4Cars' intention prediction and AIDE's recognition
tasks. We observe up to 12.5% performance improvements on the turn intention
prediction task in Brain4Cars, and consistent performance improvements up to
11.0% on all tasks in AIDE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DSFormer: A Dual-Scale Cross-Learning Transformer for Visual Place
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiyang Jiang, Songhao Piao, Chao Gao, Lei Yu, Liguo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Place Recognition (VPR) is crucial for robust mobile robot
localization, yet it faces significant challenges in maintaining reliable
performance under varying environmental conditions and viewpoints. To address
this, we propose a novel framework that integrates Dual-Scale-Former
(DSFormer), a Transformer-based cross-learning module, with an innovative block
clustering strategy. DSFormer enhances feature representation by enabling
bidirectional information transfer between dual-scale features extracted from
the final two CNN layers, capturing both semantic richness and spatial details
through self-attention for long-range dependencies within each scale and shared
cross-attention for cross-scale learning. Complementing this, our block
clustering strategy repartitions the widely used San Francisco eXtra Large
(SF-XL) training dataset from multiple distinct perspectives, optimizing data
organization to further bolster robustness against viewpoint variations.
Together, these innovations not only yield a robust global embedding adaptable
to environmental changes but also reduce the required training data volume by
approximately 30\% compared to previous partitioning methods. Comprehensive
experiments demonstrate that our approach achieves state-of-the-art performance
across most benchmark datasets, surpassing advanced reranking methods like
DELG, Patch-NetVLAD, TransVPR, and R2Former as a global retrieval solution
using 512-dim global descriptors, while significantly improving computational
efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiagR1: A Vision-Language Model Trained via <span class="highlight-title">Reinforcement</span> Learning for
  Digestive Pathology Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minxi Ouyang, Lianghui Zhu, Yaqing Bao, Qiang Huang, Jingli Ouyang, Tian Guan, Xitong Ling, Jiawen Li, Song Duan, Wenbin Dai, Li Zheng, Xuemei Zhang, Yonghong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large models have shown great potential in automating pathology
image analysis. However, current multimodal models for gastrointestinal
pathology are constrained by both data quality and reasoning transparency:
pervasive noise and incomplete annotations in public datasets predispose vision
language models to factual hallucinations when generating diagnostic text,
while the absence of explicit intermediate reasoning chains renders the outputs
difficult to audit and thus less trustworthy in clinical practice. To address
these issues, we construct a large scale gastrointestinal pathology dataset
containing both microscopic descriptions and diagnostic conclusions, and
propose a prompt argumentation strategy that incorporates lesion classification
and anatomical site information. This design guides the model to better capture
image specific features and maintain semantic consistency in generation.
Furthermore, we employ a post training pipeline that combines supervised fine
tuning with Group Relative Policy Optimization (GRPO) to improve reasoning
quality and output structure. Experimental results on real world pathology
report generation tasks demonstrate that our approach significantly outperforms
state of the art open source and proprietary baselines in terms of generation
quality, structural completeness, and clinical relevance. Our solution
outperforms state of the art models with 18.7% higher clinical relevance, 32.4%
improved structural completeness, and 41.2% fewer diagnostic errors,
demonstrating superior accuracy and clinical utility compared to existing
solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NLML-HPE: Head Pose <span class="highlight-title">Estimation</span> with Limited Data via Manifold Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahdi Ghafourian, Federico M. Sukno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Head pose estimation (HPE) plays a critical role in various computer vision
applications such as human-computer interaction and facial recognition. In this
paper, we propose a novel deep learning approach for head pose estimation with
limited training data via non-linear manifold learning called NLML-HPE. This
method is based on the combination of tensor decomposition (i.e., Tucker
decomposition) and feed forward neural networks. Unlike traditional
classification-based approaches, our method formulates head pose estimation as
a regression problem, mapping input landmarks into a continuous representation
of pose angles. To this end, our method uses tensor decomposition to split each
Euler angle (yaw, pitch, roll) to separate subspaces and models each dimension
of the underlying manifold as a cosine curve. We address two key challenges: 1.
Almost all HPE datasets suffer from incorrect and inaccurate pose annotations.
Hence, we generated a precise and consistent 2D head pose dataset for our
training set by rotating 3D head models for a fixed set of poses and rendering
the corresponding 2D images. 2. We achieved real-time performance with limited
training data as our method accurately captures the nature of rotation of an
object from facial landmarks. Once the underlying manifold for rotation around
each axis is learned, the model is very fast in predicting unseen data. Our
training and testing code is available online along with our trained models:
https: //github.com/MahdiGhafoorian/NLML_HPE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-Supervised</span> Ultrasound-Video <span class="highlight-title">Segmentation</span> with Feature <span class="highlight-title">Prediction</span>
  and 3D Localised Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward Ellis, Robert Mendel, Andrew Bulpitt, Nasim Parsa, Michael F Byrne, Sharib Ali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acquiring and annotating large datasets in ultrasound imaging is challenging
due to low contrast, high noise, and susceptibility to artefacts. This process
requires significant time and clinical expertise. Self-supervised learning
(SSL) offers a promising solution by leveraging unlabelled data to learn useful
representations, enabling improved segmentation performance when annotated data
is limited. Recent state-of-the-art developments in SSL for video data include
V-JEPA, a framework solely based on feature prediction, avoiding pixel level
reconstruction or negative samples. We hypothesise that V-JEPA is well-suited
to ultrasound imaging, as it is less sensitive to noisy pixel-level detail
while effectively leveraging temporal information. To the best of our
knowledge, this is the first study to adopt V-JEPA for ultrasound video data.
Similar to other patch-based masking SSL techniques such as VideoMAE, V-JEPA is
well-suited to ViT-based models. However, ViTs can underperform on small
medical datasets due to lack of inductive biases, limited spatial locality and
absence of hierarchical feature learning. To improve locality understanding, we
propose a novel 3D localisation auxiliary task to improve locality in ViT
representations during V-JEPA pre-training. Our results show V-JEPA with our
auxiliary task improves segmentation performance significantly across various
frozen encoder configurations, with gains up to 3.4\% using 100\% and up to
8.35\% using only 10\% of the training data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DCFFSNet: Deep Connectivity Feature <span class="highlight-title">Fusion</span> Separation Network for
  Medical Image <span class="highlight-title">Segmentation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xun Ye, Ruixiang Tang, Mingda Zhang, Jianglong Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation leverages topological connectivity theory to
enhance edge precision and regional consistency. However, existing deep
networks integrating connectivity often forcibly inject it as an additional
feature module, resulting in coupled feature spaces with no standardized
mechanism to quantify different feature strengths. To address these issues, we
propose DCFFSNet (Dual-Connectivity Feature Fusion-Separation Network). It
introduces an innovative feature space decoupling strategy. This strategy
quantifies the relative strength between connectivity features and other
features. It then builds a deep connectivity feature fusion-separation
architecture. This architecture dynamically balances multi-scale feature
expression. Experiments were conducted on the ISIC2018, DSB2018, and MoNuSeg
datasets. On ISIC2018, DCFFSNet outperformed the next best model (CMUNet) by
1.3% (Dice) and 1.2% (IoU). On DSB2018, it surpassed TransUNet by 0.7% (Dice)
and 0.9% (IoU). On MoNuSeg, it exceeded CSCAUNet by 0.8% (Dice) and 0.9% (IoU).
The results demonstrate that DCFFSNet exceeds existing mainstream methods
across all metrics. It effectively resolves segmentation fragmentation and
achieves smooth edge transitions. This significantly enhances clinical
usability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages , 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Iwin Transformer: Hierarchical Vision Transformer using Interleaved
  Windows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simin Huo, Ning Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Iwin Transformer, a novel position-embedding-free hierarchical
vision transformer, which can be fine-tuned directly from low to high
resolution, through the collaboration of innovative interleaved window
attention and depthwise separable convolution. This approach uses attention to
connect distant tokens and applies convolution to link neighboring tokens,
enabling global information exchange within a single module, overcoming Swin
Transformer's limitation of requiring two consecutive blocks to approximate
global attention. Extensive experiments on visual benchmarks demonstrate that
Iwin Transformer exhibits strong competitiveness in tasks such as image
classification (87.4 top-1 accuracy on ImageNet-1K), semantic segmentation and
video action recognition. We also validate the effectiveness of the core
component in Iwin as a standalone module that can seamlessly replace the
self-attention module in class-conditional image generation. The concepts and
methods introduced by the Iwin Transformer have the potential to inspire future
research, like Iwin 3D Attention in video generation. The code and models are
available at https://github.com/cominder/Iwin-Transformer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 10 figures, Submitted to IEEE Transactions on Pattern
  Analysis and Machine Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HumanMaterial: Human Material <span class="highlight-title">Estimation</span> from a Single Image via
  Progressive Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Jiang, Jiahao Xia, Jiongming Qin, Yusen Wang, Tuo Cao, Chunxia Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Full-body Human inverse rendering based on physically-based rendering aims to
acquire high-quality materials, which helps achieve photo-realistic rendering
under arbitrary illuminations. This task requires estimating multiple material
maps and usually relies on the constraint of rendering result. The absence of
constraints on the material maps makes inverse rendering an ill-posed task.
Previous works alleviated this problem by building material dataset for
training, but their simplified material data and rendering equation lead to
rendering results with limited realism, especially that of skin. To further
alleviate this problem, we construct a higher-quality dataset (OpenHumanBRDF)
based on scanned real data and statistical material data. In addition to the
normal, diffuse albedo, roughness, specular albedo, we produce displacement and
subsurface scattering to enhance the realism of rendering results, especially
for the skin. With the increase in prediction tasks for more materials, using
an end-to-end model as in the previous work struggles to balance the importance
among various material maps, and leads to model underfitting. Therefore, we
design a model (HumanMaterial) with progressive training strategy to make full
use of the supervision information of the material maps and improve the
performance of material estimation. HumanMaterial first obtain the initial
material results via three prior models, and then refine the results by a
finetuning model. Prior models estimate different material maps, and each map
has different significance for rendering results. Thus, we design a Controlled
PBR Rendering (CPR) loss, which enhances the importance of the materials to be
optimized during the training of prior models. Extensive experiments on
OpenHumanBRDF dataset and real data demonstrate that our method achieves
state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Consistent <span class="highlight-title">Long-Term</span> Pose Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18382v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18382v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yayuan Li, Filippos Bellos, Jason Corso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current approaches to pose generation rely heavily on intermediate
representations, either through two-stage pipelines with quantization or
autoregressive models that accumulate errors during inference. This fundamental
limitation leads to degraded performance, particularly in long-term pose
generation where maintaining temporal coherence is crucial. We propose a novel
one-stage architecture that directly generates poses in continuous coordinate
space from minimal context - a single RGB image and text description - while
maintaining consistent distributions between training and inference. Our key
innovation is eliminating the need for intermediate representations or
token-based generation by operating directly on pose coordinates through a
relative movement prediction mechanism that preserves spatial relationships,
and a unified placeholder token approach that enables single-forward generation
with identical behavior during training and inference. Through extensive
experiments on Penn Action and First-Person Hand Action Benchmark (F-PHAB)
datasets, we demonstrate that our approach significantly outperforms existing
quantization-based and autoregressive methods, especially in long-term
generation scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Effective Human-in-the-<span class="highlight-title">Loop</span> Assistive AI Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filippos Bellos, Yayuan Li, Cary Shu, Ruey Day, Jeffrey M. Siskind, Jason J. Corso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective human-AI collaboration for physical task completion has significant
potential in both everyday activities and professional domains. AI agents
equipped with informative guidance can enhance human performance, but
evaluating such collaboration remains challenging due to the complexity of
human-in-the-loop interactions. In this work, we introduce an evaluation
framework and a multimodal dataset of human-AI interactions designed to assess
how AI guidance affects procedural task performance, error reduction and
learning outcomes. Besides, we develop an augmented reality (AR)-equipped AI
agent that provides interactive guidance in real-world tasks, from cooking to
battlefield medicine. Through human studies, we share empirical insights into
AI-assisted human performance and demonstrate that AI-assisted collaboration
improves task completion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MVG4D: Image Matrix-Based <span class="highlight-title">Multi-View</span> and Motion Generation for 4D
  Content Creation from a Single Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaotian Chen, DongFu Yin, Fei Richard Yu, Xuanchen Li, Xinhao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in generative modeling have significantly enhanced digital content
creation, extending from 2D images to complex 3D and 4D scenes. Despite
substantial progress, producing high-fidelity and temporally consistent dynamic
4D content remains a challenge. In this paper, we propose MVG4D, a novel
framework that generates dynamic 4D content from a single still image by
combining multi-view synthesis with 4D Gaussian Splatting (4D GS). At its core,
MVG4D employs an image matrix module that synthesizes temporally coherent and
spatially diverse multi-view images, providing rich supervisory signals for
downstream 3D and 4D reconstruction. These multi-view images are used to
optimize a 3D Gaussian point cloud, which is further extended into the temporal
domain via a lightweight deformation network. Our method effectively enhances
temporal consistency, geometric fidelity, and visual realism, addressing key
challenges in motion discontinuity and background degradation that affect prior
4D GS-based methods. Extensive experiments on the Objaverse dataset demonstrate
that MVG4D outperforms state-of-the-art baselines in CLIP-I, PSNR, FVD, and
time efficiency. Notably, it reduces flickering artifacts and sharpens
structural details across views and time, enabling more immersive AR/VR
experiences. MVG4D sets a new direction for efficient and controllable 4D
generation from minimal inputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniSegDiff: Boosting Unified Lesion <span class="highlight-title">Segmentation</span> via a Staged Dif<span class="highlight-title">fusion</span>
  Model <span class="chip">MICCAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18362v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18362v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilong Hu, Shijie Chang, Lihe Zhang, Feng Tian, Weibing Sun, Huchuan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Diffusion Probabilistic Model (DPM) has demonstrated remarkable
performance across a variety of generative tasks. The inherent randomness in
diffusion models helps address issues such as blurring at the edges of medical
images and labels, positioning Diffusion Probabilistic Models (DPMs) as a
promising approach for lesion segmentation. However, we find that the current
training and inference strategies of diffusion models result in an uneven
distribution of attention across different timesteps, leading to longer
training times and suboptimal solutions. To this end, we propose UniSegDiff, a
novel diffusion model framework designed to address lesion segmentation in a
unified manner across multiple modalities and organs. This framework introduces
a staged training and inference approach, dynamically adjusting the prediction
targets at different stages, forcing the model to maintain high attention
across all timesteps, and achieves unified lesion segmentation through
pre-training the feature extraction network for segmentation. We evaluate
performance on six different organs across various imaging modalities.
Comprehensive experimental results demonstrate that UniSegDiff significantly
outperforms previous state-of-the-art (SOTA) approaches. The code is available
at https://github.com/HUYILONG-Z/UniSegDiff.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Deformable</span> Convolution Module with <span class="highlight-title">Global</span>ly Learned Relative Offsets for
  Fundus Vessel <span class="highlight-title">Segmentation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lexuan Zhu, Yuxuan Li, Yuning Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deformable convolution can adaptively change the shape of convolution kernel
by learning offsets to deal with complex shape features. We propose a novel
plug and play deformable convolutional module that uses attention and
feedforward networks to learn offsets, so that the deformable patterns can
capture long-distance global features. Compared with previously existing
deformable convolutions, the proposed module learns the sub pixel displacement
field and adaptively warps the feature maps across all channels rather than
directly deforms the convolution kernel , which is equivalent to a relative
deformation of the kernel sampling grids, achieving global feature deformation
and the decoupling of kernel size and learning network. Considering that the
fundus blood vessels have globally self similar complex edges, we design a deep
learning model for fundus blood vessel segmentation, GDCUnet, based on the
proposed convolutional module. Empirical evaluations under the same
configuration and unified framework show that GDCUnet has achieved state of the
art performance on public datasets. Further ablation experiments demonstrated
that the proposed deformable convolutional module could more significantly
learn the complex features of fundus blood vessels, enhancing the model
representation and generalization capabilities.The proposed module is similar
to the interface of conventional convolution, we suggest applying it to more
machine vision tasks with complex global self similar features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VB-Mitigator: An Open-source Framework for Evaluating and Advancing
  Visual Bias Mitigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18348v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18348v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ioannis Sarridis, Christos Koutlis, Symeon Papadopoulos, Christos Diou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bias in computer vision models remains a significant challenge, often
resulting in unfair, unreliable, and non-generalizable AI systems. Although
research into bias mitigation has intensified, progress continues to be
hindered by fragmented implementations and inconsistent evaluation practices.
Disparate datasets and metrics used across studies complicate reproducibility,
making it difficult to fairly assess and compare the effectiveness of various
approaches. To overcome these limitations, we introduce the Visual Bias
Mitigator (VB-Mitigator), an open-source framework designed to streamline the
development, evaluation, and comparative analysis of visual bias mitigation
techniques. VB-Mitigator offers a unified research environment encompassing 12
established mitigation methods, 7 diverse benchmark datasets. A key strength of
VB-Mitigator is its extensibility, allowing for seamless integration of
additional methods, datasets, metrics, and models. VB-Mitigator aims to
accelerate research toward fairness-aware computer vision models by serving as
a foundational codebase for the research community to develop and assess their
approaches. To this end, we also recommend best evaluation practices and
provide a comprehensive performance comparison among state-of-the-art
methodologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EgoExoBench: A <span class="highlight-title">Benchmark</span> for First- and Third-person View Video
  Understanding in MLLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18342v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18342v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuping He, Yifei Huang, Guo Chen, Baoqi Pei, Jilan Xu, Tong Lu, Jiangmiao Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transferring and integrating knowledge across first-person (egocentric) and
third-person (exocentric) viewpoints is intrinsic to human intelligence,
enabling humans to learn from others and convey insights from their own
experiences. Despite rapid progress in multimodal large language models
(MLLMs), their ability to perform such cross-view reasoning remains unexplored.
To address this, we introduce EgoExoBench, the first benchmark for
egocentric-exocentric video understanding and reasoning. Built from publicly
available datasets, EgoExoBench comprises over 7,300 question-answer pairs
spanning eleven sub-tasks organized into three core challenges: semantic
alignment, viewpoint association, and temporal reasoning. We evaluate 13
state-of-the-art MLLMs and find that while these models excel on single-view
tasks, they struggle to align semantics across perspectives, accurately
associate views, and infer temporal dynamics in the ego-exo context. We hope
EgoExoBench can serve as a valuable resource for research on embodied agents
and intelligent assistants seeking human-like cross-view intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Bird Classification with Primary Color Additives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ezhini Rasendiran R, Chandresh Kumar Maurya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of classifying bird species using their song
recordings, a challenging task due to environmental noise, overlapping
vocalizations, and missing labels. Existing models struggle with low-SNR or
multi-species recordings. We hypothesize that birds can be classified by
visualizing their pitch pattern, speed, and repetition, collectively called
motifs. Deep learning models applied to spectrogram images help, but similar
motifs across species cause confusion. To mitigate this, we embed frequency
information into spectrograms using primary color additives. This enhances
species distinction and improves classification accuracy. Our experiments show
that the proposed approach achieves statistically significant gains over models
without colorization and surpasses the BirdCLEF 2024 winner, improving F1 by
7.3%, ROC-AUC by 6.2%, and CMAP by 6.6%. These results demonstrate the
effectiveness of incorporating frequency information via colorization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages (Accepted to Interspeech 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting <span class="highlight-title">Multi-View</span> Indoor 3D Object <span class="highlight-title">Detection</span> via Adaptive 3D Volume
  Construction <span class="chip">ICCV2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runmin Zhang, Zhu Yu, Si-Yuan Cao, Lingyu Zhu, Guangyi Zhang, Xiaokai Bai, Hui-Liang Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents SGCDet, a novel multi-view indoor 3D object detection
framework based on adaptive 3D volume construction. Unlike previous approaches
that restrict the receptive field of voxels to fixed locations on images, we
introduce a geometry and context aware aggregation module to integrate
geometric and contextual information within adaptive regions in each image and
dynamically adjust the contributions from different views, enhancing the
representation capability of voxel features. Furthermore, we propose a sparse
volume construction strategy that adaptively identifies and selects voxels with
high occupancy probabilities for feature refinement, minimizing redundant
computation in free space. Benefiting from the above designs, our framework
achieves effective and efficient volume construction in an adaptive way. Better
still, our network can be supervised using only 3D bounding boxes, eliminating
the dependence on ground-truth scene geometry. Experimental results demonstrate
that SGCDet achieves state-of-the-art performance on the ScanNet, ScanNet200
and ARKitScenes datasets. The source code is available at
https://github.com/RM-Zhang/SGCDet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GVCCS: A <span class="highlight-title">Dataset</span> for Contrail Identification and Tracking on Visible
  Whole Sky Camera Sequences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Jarry, Ramon Dalmau, Philippe Very, Franck Ballerini, Stephania-Denisa Bocu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aviation's climate impact includes not only CO2 emissions but also
significant non-CO2 effects, especially from contrails. These ice clouds can
alter Earth's radiative balance, potentially rivaling the warming effect of
aviation CO2. Physics-based models provide useful estimates of contrail
formation and climate impact, but their accuracy depends heavily on the quality
of atmospheric input data and on assumptions used to represent complex
processes like ice particle formation and humidity-driven persistence.
Observational data from remote sensors, such as satellites and ground cameras,
could be used to validate and calibrate these models. However, existing
datasets don't explore all aspect of contrail dynamics and formation: they
typically lack temporal tracking, and do not attribute contrails to their
source flights. To address these limitations, we present the Ground Visible
Camera Contrail Sequences (GVCCS), a new open data set of contrails recorded
with a ground-based all-sky camera in the visible range. Each contrail is
individually labeled and tracked over time, allowing a detailed analysis of its
lifecycle. The dataset contains 122 video sequences (24,228 frames) and
includes flight identifiers for contrails that form above the camera. As
reference, we also propose a unified deep learning framework for contrail
analysis using a panoptic segmentation model that performs semantic
segmentation (contrail pixel identification), instance segmentation (individual
contrail separation), and temporal tracking in a single architecture. By
providing high-quality, temporally resolved annotations and a benchmark for
model evaluation, our work supports improved contrail monitoring and will
facilitate better calibration of physical models. This sets the groundwork for
more accurate climate impact understanding and assessments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Low-rankness: Guaranteed Matrix Recovery via Modified Nuclear
  Norm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiangjun Peng, Yisi Luo, Xiangyong Cao, Shuang Xu, Deyu Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The nuclear norm (NN) has been widely explored in matrix recovery problems,
such as Robust PCA and matrix completion, leveraging the inherent global
low-rank structure of the data. In this study, we introduce a new modified
nuclear norm (MNN) framework, where the MNN family norms are defined by
adopting suitable transformations and performing the NN on the transformed
matrix. The MNN framework offers two main advantages: (1) it jointly captures
both local information and global low-rankness without requiring trade-off
parameter tuning; (2) Under mild assumptions on the transformation, we provided
exact theoretical recovery guarantees for both Robust PCA and MC tasks-an
achievement not shared by existing methods that combine local and global
information. Thanks to its general and flexible design, MNN can accommodate
various proven transformations, enabling a unified and effective approach to
structured low-rank recovery. Extensive experiments demonstrate the
effectiveness of our method. Code and supplementary material are available at
https://github.com/andrew-pengjj/modified_nuclear_norm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-<span class="highlight-title">Dataset</span> <span class="highlight-title">Benchmark</span> for <span class="highlight-title">Semi-Supervised</span> Semantic <span class="highlight-title">Segmentation</span> in
  ECG Delineation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minje Park, Jeonghwa Lim, Taehyung Yu, Sunghoon Joo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electrocardiogram (ECG) delineation, the segmentation of meaningful waveform
features, is critical for clinical diagnosis. Despite recent advances using
deep learning, progress has been limited by the scarcity of publicly available
annotated datasets. Semi-supervised learning presents a promising solution by
leveraging abundant unlabeled ECG data. In this study, we present the first
systematic benchmark for semi-supervised semantic segmentation (SemiSeg) in ECG
delineation. We curated and unified multiple public datasets, including
previously underused sources, to support robust and diverse evaluation. We
adopted five representative SemiSeg algorithms from computer vision,
implemented them on two different architectures: the convolutional network and
the transformer, and evaluated them in two different settings: in-domain and
cross-domain. Additionally, we propose ECG-specific training configurations and
augmentation strategies and introduce a standardized evaluation framework. Our
results show that the transformer outperforms the convolutional network in
semi-supervised ECG delineation. We anticipate that our benchmark will serve as
a foundation for advancing semi-supervised ECG delineation methods and will
facilitate further research in this domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Large Vision-Language Models' Understanding for <span class="highlight-title">Field</span> Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaomei Zhang, Hanyu Zheng, Xiangyu Zhu, Jinghuan Wei, Junhong Zou, Zhen Lei, Zhaoxiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) have shown impressive capabilities
across a range of tasks that integrate visual and textual understanding, such
as image captioning and visual question answering. These models are trained on
large-scale image and video datasets paired with text, enabling them to bridge
visual perception and natural language processing. However, their application
to scientific domains, especially in interpreting complex field data commonly
used in the natural sciences, remains underexplored. In this work, we introduce
FieldLVLM, a novel framework designed to improve large vision-language models'
understanding of field data. FieldLVLM consists of two main components: a
field-aware language generation strategy and a data-compressed multimodal model
tuning. The field-aware language generation strategy leverages a
special-purpose machine learning pipeline to extract key physical features from
field data, such as flow classification, Reynolds number, and vortex patterns.
This information is then converted into structured textual descriptions that
serve as a dataset. The data-compressed multimodal model tuning focuses on
LVLMs with these generated datasets, using a data compression strategy to
reduce the complexity of field inputs and retain only the most informative
values. This ensures compatibility with the models language decoder and guides
its learning more effectively. Experimental results on newly proposed benchmark
datasets demonstrate that FieldLVLM significantly outperforms existing methods
in tasks involving scientific field data. Our findings suggest that this
approach opens up new possibilities for applying large vision-language models
to scientific research, helping bridge the gap between large models and
domain-specific discovery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LMM-Det: Make Large Multimodal Models Excel in Object <span class="highlight-title">Detection</span> <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jincheng Li, Chunyu Xie, Ji Ao, Dawei Leng, Yuhui Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large multimodal models (LMMs) have garnered wide-spread attention and
interest within the artificial intelligence research and industrial
communities, owing to their remarkable capability in multimodal understanding,
reasoning, and in-context learning, among others. While LMMs have demonstrated
promising results in tackling multimodal tasks like image captioning, visual
question answering, and visual grounding, the object detection capabilities of
LMMs exhibit a significant gap compared to specialist detectors. To bridge the
gap, we depart from the conventional methods of integrating heavy detectors
with LMMs and propose LMM-Det, a simple yet effective approach that leverages a
Large Multimodal Model for vanilla object Detection without relying on
specialized detection modules. Specifically, we conduct a comprehensive
exploratory analysis when a large multimodal model meets with object detection,
revealing that the recall rate degrades significantly compared with specialist
detection models. To mitigate this, we propose to increase the recall rate by
introducing data distribution adjustment and inference optimization tailored
for object detection. We re-organize the instruction conversations to enhance
the object detection capabilities of large multimodal models. We claim that a
large multimodal model possesses detection capability without any extra
detection modules. Extensive experiments support our claim and show the
effectiveness of the versatile LMM-Det. The datasets, models, and codes are
available at https://github.com/360CVGroup/LMM-Det.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> TCM-Tongue: A Standardized Tongue Image <span class="highlight-title">Dataset</span> with Pathological
  Annotations for AI-Assisted TCM Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuebo Jin, Long<span class="highlight-author">fei Gao</span>, Anshuo Tong, Zhengyang Chen, Jianlei Kong, Ning Sun, Huijun Ma, Qiang Wang, Yuting Bai, Tingli Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional Chinese medicine (TCM) tongue diagnosis, while clinically
valuable, faces standardization challenges due to subjective interpretation and
inconsistent imaging protocols, compounded by the lack of large-scale,
annotated datasets for AI development. To address this gap, we present the
first specialized dataset for AI-driven TCM tongue diagnosis, comprising 6,719
high-quality images captured under standardized conditions and annotated with
20 pathological symptom categories (averaging 2.54 clinically validated labels
per image, all verified by licensed TCM practitioners). The dataset supports
multiple annotation formats (COCO, TXT, XML) for broad usability and has been
benchmarked using nine deep learning models (YOLOv5/v7/v8 variants, SSD, and
MobileNetV2) to demonstrate its utility for AI development. This resource
provides a critical foundation for advancing reliable computational tools in
TCM, bridging the data shortage that has hindered progress in the field, and
facilitating the integration of AI into both research and clinical practice
through standardized, high-quality diagnostic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 11 figures, 2 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dissecting the Dental Lung Cancer Axis via Mendelian Randomization and
  Mediation Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenran Zhang, Huihuan Luo, Linda Wei, Ping Nie, Yiqun Wu, Dedong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Periodontitis and dental caries are common oral diseases affecting billions
globally. While observational studies suggest links between these conditions
and lung cancer, causality remains uncertain. This study used two sample
Mendelian randomization (MR) to explore causal relationships between dental
traits (periodontitis, dental caries) and lung cancer subtypes, and to assess
mediation by pulmonary function. Genetic instruments were derived from the
largest available genome wide association studies, including data from 487,823
dental caries and 506,594 periodontitis cases, as well as lung cancer data from
the Transdisciplinary Research of Cancer in Lung consortium. Inverse variance
weighting was the main analytical method; lung function mediation was assessed
using the delta method. The results showed a significant positive causal effect
of dental caries on overall lung cancer and its subtypes. Specifically, a one
standard deviation increase in dental caries incidence was associated with a
188.0% higher risk of squamous cell lung carcinoma (OR = 2.880, 95% CI =
1.236--6.713, p = 0.014), partially mediated by declines in forced vital
capacity (FVC) and forced expiratory volume in one second (FEV1), accounting
for 5.124% and 5.890% of the total effect. No causal effect was found for
periodontitis. These findings highlight a causal role of dental caries in lung
cancer risk and support integrating dental care and pulmonary function
monitoring into cancer prevention strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Articulated Object Manipulation On The Fly with Foundation
  Model Reasoning and Part Grounding <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaojie Zhang, Yuanfei Wang, Ruihai Wu, Kunqi Xu, Yu Li, Liuyu Xiang, Hao Dong, Zhaofeng He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Articulated objects pose diverse manipulation challenges for robots. Since
their internal structures are not directly observable, robots must adaptively
explore and refine actions to generate successful manipulation trajectories.
While existing works have attempted cross-category generalization in adaptive
articulated object manipulation, two major challenges persist: (1) the
geometric diversity of real-world articulated objects complicates visual
perception and understanding, and (2) variations in object functions and
mechanisms hinder the development of a unified adaptive manipulation strategy.
To address these challenges, we propose AdaRPG, a novel framework that
leverages foundation models to extract object parts, which exhibit greater
local geometric similarity than entire objects, thereby enhancing visual
affordance generalization for functional primitive skills. To support this, we
construct a part-level affordance annotation dataset to train the affordance
model. Additionally, AdaRPG utilizes the common knowledge embedded in
foundation models to reason about complex mechanisms and generate high-level
control codes that invoke primitive skill functions based on part affordance
inference. Simulation and real-world experiments demonstrate AdaRPG's strong
generalization ability across novel articulated object categories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic
  Grounding for Generalizable <span class="highlight-title">Robot</span>ic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu Su, Weiwei Shang, Chen Qian, Fei Zhang, Shuang Cong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantics-driven 3D spatial constraints align highlevel semantic
representations with low-level action spaces, facilitating the unification of
task understanding and execution in robotic manipulation. The synergistic
reasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation
Models (VFMs) enables cross-modal 3D spatial constraint construction.
Nevertheless, existing methods have three key limitations: (1) coarse semantic
granularity in constraint modeling, (2) lack of real-time closed-loop planning,
(3) compromised robustness in semantically diverse environments. To address
these challenges, we propose ReSem3D, a unified manipulation framework for
semantically diverse environments, leveraging the synergy between VFMs and
MLLMs to achieve fine-grained visual grounding and dynamically constructs
hierarchical 3D spatial constraints for real-time manipulation. Specifically,
the framework is driven by hierarchical recursive reasoning in MLLMs, which
interact with VFMs to automatically construct 3D spatial constraints from
natural language instructions and RGB-D observations in two stages: part-level
extraction and region-level refinement. Subsequently, these constraints are
encoded as real-time optimization objectives in joint space, enabling reactive
behavior to dynamic disturbances. Extensive simulation and real-world
experiments are conducted in semantically rich household and sparse chemical
lab environments. The results demonstrate that ReSem3D performs diverse
manipulation tasks under zero-shot conditions, exhibiting strong adaptability
and generalization. Code and videos at https://resem3d.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages,9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting Gaussian Agnostic Representation Learning with Dif<span class="highlight-title">fusion</span>
  Priors for Enhanced Infrared Small Target <span class="highlight-title">Detection</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyao Li, Yahao Lu, Xingyuan Guo, Xiaoyu Xian, Tiantian Wang, Yukai Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Infrared small target detection (ISTD) plays a vital role in numerous
practical applications. In pursuit of determining the performance boundaries,
researchers employ large and expensive manual-labeling data for representation
learning. Nevertheless, this approach renders the state-of-the-art ISTD methods
highly fragile in real-world challenges. In this paper, we first study the
variation in detection performance across several mainstream methods under
various scarcity -- namely, the absence of high-quality infrared data -- that
challenge the prevailing theories about practical ISTD. To address this
concern, we introduce the Gaussian Agnostic Representation Learning.
Specifically, we propose the Gaussian Group Squeezer, leveraging Gaussian
sampling and compression for non-uniform quantization. By exploiting a diverse
array of training samples, we enhance the resilience of ISTD models against
various challenges. Then, we introduce two-stage diffusion models for
real-world reconstruction. By aligning quantized signals closely with
real-world distributions, we significantly elevate the quality and fidelity of
the synthetic samples. Comparative evaluations against state-of-the-art
detection methods in various scarcity scenarios demonstrate the efficacy of the
proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Neural Networks. We propose the Gaussian Group Squeezer,
  leveraging Gaussian sampling and compression with diffusion models for
  channel-based data augmentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> LONG3R: Long Sequence Streaming 3D Reconstruction <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoguang Chen, Minghui Qin, Tianyuan Yuan, Zhe Liu, <span class="highlight-author">Hang Zhao</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in multi-view scene reconstruction have been significant,
yet existing methods face limitations when processing streams of input images.
These methods either rely on time-consuming offline optimization or are
restricted to shorter sequences, hindering their applicability in real-time
scenarios. In this work, we propose LONG3R (LOng sequence streaming 3D
Reconstruction), a novel model designed for streaming multi-view 3D scene
reconstruction over longer sequences. Our model achieves real-time processing
by operating recurrently, maintaining and updating memory with each new
observation. We first employ a memory gating mechanism to filter relevant
memory, which, together with a new observation, is fed into a dual-source
refined decoder for coarse-to-fine interaction. To effectively capture
long-sequence memory, we propose a 3D spatio-temporal memory that dynamically
prunes redundant spatial information while adaptively adjusting resolution
along the scene. To enhance our model's performance on long sequences while
maintaining training efficiency, we employ a two-stage curriculum training
strategy, each stage targeting specific capabilities. Experiments demonstrate
that LONG3R outperforms state-of-the-art streaming methods, particularly for
longer sequences, while maintaining real-time inference speed. Project page:
https://zgchen33.github.io/LONG3R/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2025. Project page:
  https://zgchen33.github.io/LONG3R/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Evaluation</span> of facial landmark <span class="highlight-title">localization</span> performance in a surgical
  setting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ines Frajtag, Marko Švaco, Filip Šuligoj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of robotics, computer vision, and their applications is becoming
increasingly widespread in various fields, including medicine. Many face
detection algorithms have found applications in neurosurgery, ophthalmology,
and plastic surgery. A common challenge in using these algorithms is variable
lighting conditions and the flexibility of detection positions to identify and
precisely localize patients. The proposed experiment tests the MediaPipe
algorithm for detecting facial landmarks in a controlled setting, using a
robotic arm that automatically adjusts positions while the surgical light and
the phantom remain in a fixed position. The results of this study demonstrate
that the improved accuracy of facial landmark detection under surgical lighting
significantly enhances the detection performance at larger yaw and pitch
angles. The increase in standard deviation/dispersion occurs due to imprecise
detection of selected facial landmarks. This analysis allows for a discussion
on the potential integration of the MediaPipe algorithm into medical
procedures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DepthDark: <span class="highlight-title">Robust</span> Monocular Depth <span class="highlight-title">Estimation</span> for Low-Light Environments <span class="chip">ACM MM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longjian Zeng, Zunjie Zhu, Rongfeng Lu, Ming Lu, Bolun Zheng, Chenggang Yan, Anke Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, foundation models for monocular depth estimation have
received increasing attention. Current methods mainly address typical daylight
conditions, but their effectiveness notably decreases in low-light
environments. There is a lack of robust foundational models for monocular depth
estimation specifically designed for low-light scenarios. This largely stems
from the absence of large-scale, high-quality paired depth datasets for
low-light conditions and the effective parameter-efficient fine-tuning (PEFT)
strategy. To address these challenges, we propose DepthDark, a robust
foundation model for low-light monocular depth estimation. We first introduce a
flare-simulation module and a noise-simulation module to accurately simulate
the imaging process under nighttime conditions, producing high-quality paired
depth datasets for low-light conditions. Additionally, we present an effective
low-light PEFT strategy that utilizes illumination guidance and multiscale
feature fusion to enhance the model's capability in low-light environments. Our
method achieves state-of-the-art depth estimation performance on the
challenging nuScenes-Night and RobotCar-Night datasets, validating its
effectiveness using limited training data and computing resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM 2025 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DATA: Domain-And-Time Alignment for High-Quality Feature <span class="highlight-title">Fusion</span> in
  Collaborative Perception <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengchang Tian, Jianwei Ma, Yan Huang, Zhanye Chen, Honghao Wei, Hui Zhang, Wei Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature-level fusion shows promise in collaborative perception (CP) through
balanced performance and communication bandwidth trade-off. However, its
effectiveness critically relies on input feature quality. The acquisition of
high-quality features faces domain gaps from hardware diversity and deployment
conditions, alongside temporal misalignment from transmission delays. These
challenges degrade feature quality with cumulative effects throughout the
collaborative network. In this paper, we present the Domain-And-Time Alignment
(DATA) network, designed to systematically align features while maximizing
their semantic representations for fusion. Specifically, we propose a
Consistency-preserving Domain Alignment Module (CDAM) that reduces domain gaps
through proximal-region hierarchical downsampling and observability-constrained
discriminator. We further propose a Progressive Temporal Alignment Module
(PTAM) to handle transmission delays via multi-scale motion modeling and
two-stage compensation. Building upon the aligned features, an Instance-focused
Feature Aggregation Module (IFAM) is developed to enhance semantic
representations. Extensive experiments demonstrate that DATA achieves
state-of-the-art performance on three typical datasets, maintaining robustness
with severe communication delays and pose errors. The code will be released at
https://github.com/ChengchangTian/DATA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025, accepted as poster. 22 pages including supplementary
  materials</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PS-GS: Gaussian Splatting for <span class="highlight-title">Multi-View</span> Photometric Stereo 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixiao Chen, Bin Liang, Hanzhi Guo, Yongqing Cheng, Jiayi Zhao, Dongdong Weng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating inverse rendering with multi-view photometric stereo (MVPS)
yields more accurate 3D reconstructions than the inverse rendering approaches
that rely on fixed environment illumination. However, efficient inverse
rendering with MVPS remains challenging. To fill this gap, we introduce the
Gaussian Splatting for Multi-view Photometric Stereo (PS-GS), which efficiently
and jointly estimates the geometry, materials, and lighting of the object that
is illuminated by diverse directional lights (multi-light). Our method first
reconstructs a standard 2D Gaussian splatting model as the initial geometry.
Based on the initialization model, it then proceeds with the deferred inverse
rendering by the full rendering equation containing a lighting-computing
multi-layer perceptron. During the whole optimization, we regularize the
rendered normal maps by the uncalibrated photometric stereo estimated normals.
We also propose the 2D Gaussian ray-tracing for single directional light to
refine the incident lighting. The regularizations and the use of multi-view and
multi-light images mitigate the ill-posed problem of inverse rendering. After
optimization, the reconstructed object can be used for novel-view synthesis,
relighting, and material and shape editing. Experiments on both synthetic and
real datasets demonstrate that our method outperforms prior works in terms of
reconstruction accuracy and computational efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Test-time Adaptation via <span class="highlight-title">Graph</span> Spectral Driven Point Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Wei, Qin Yang, Yijie Fang, Mingrui Zhu, Nannan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While test-time adaptation (TTA) methods effectively address domain shifts by
dynamically adapting pre-trained models to target domain data during online
inference, their application to 3D point clouds is hindered by their irregular
and unordered structure. Current 3D TTA methods often rely on computationally
expensive spatial-domain optimizations and may require additional training
data. In contrast, we propose Graph Spectral Domain Test-Time Adaptation
(GSDTTA), a novel approach for 3D point cloud classification that shifts
adaptation to the graph spectral domain, enabling more efficient adaptation by
capturing global structural properties with fewer parameters. Point clouds in
target domain are represented as outlier-aware graphs and transformed into
graph spectral domain by Graph Fourier Transform (GFT). For efficiency,
adaptation is performed by optimizing only the lowest 10% of frequency
components, which capture the majority of the point cloud's energy. An inverse
GFT (IGFT) is then applied to reconstruct the adapted point cloud with the
graph spectral-driven point shift. This process is enhanced by an
eigenmap-guided self-training strategy that iteratively refines both the
spectral adjustments and the model parameters. Experimental results and
ablation studies on benchmark datasets demonstrate the effectiveness of GSDTTA,
outperforming existing TTA methods for 3D point cloud classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LEAF: Latent Dif<span class="highlight-title">fusion</span> with Efficient Encoder Distillation for Aligned
  Features in Medical Image <span class="highlight-title">Segmentation</span> <span class="chip">MICCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qilin Huang, Tianyu Lin, Zhiguang Chen, Fudan Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging the powerful capabilities of diffusion models has yielded quite
effective results in medical image segmentation tasks. However, existing
methods typically transfer the original training process directly without
specific adjustments for segmentation tasks. Furthermore, the commonly used
pre-trained diffusion models still have deficiencies in feature extraction.
Based on these considerations, we propose LEAF, a medical image segmentation
model grounded in latent diffusion models. During the fine-tuning process, we
replace the original noise prediction pattern with a direct prediction of the
segmentation map, thereby reducing the variance of segmentation results. We
also employ a feature distillation method to align the hidden states of the
convolutional layers with the features from a transformer-based vision encoder.
Experimental results demonstrate that our method enhances the performance of
the original diffusion model across multiple segmentation datasets for
different disease types. Notably, our approach does not alter the model
architecture, nor does it increase the number of parameters or computation
during the inference phase, making it highly efficient.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MICCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> TeE<span class="highlight-title">Fusion</span>: Blending Text Embeddings to Distill Classifier-Free Guidance <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghao Fu, Guo-Hua Wang, Xiaohao Chen, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kai<span class="highlight-author">fu Zhang</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-to-image synthesis largely benefit from sophisticated
sampling strategies and classifier-free guidance (CFG) to ensure high-quality
generation. However, CFG's reliance on two forward passes, especially when
combined with intricate sampling algorithms, results in prohibitively high
inference costs. To address this, we introduce TeEFusion (\textbf{Te}xt
\textbf{E}mbeddings \textbf{Fusion}), a novel and efficient distillation method
that directly incorporates the guidance magnitude into the text embeddings and
distills the teacher model's complex sampling strategy. By simply fusing
conditional and unconditional text embeddings using linear operations,
TeEFusion reconstructs the desired guidance without adding extra parameters,
simultaneously enabling the student model to learn from the teacher's output
produced via its sophisticated sampling approach. Extensive experiments on
state-of-the-art models such as SD3 demonstrate that our method allows the
student to closely mimic the teacher's performance with a far simpler and more
efficient sampling strategy. Consequently, the student model achieves inference
speeds up to 6$\times$ faster than the teacher model, while maintaining image
quality at levels comparable to those obtained through the teacher's complex
sampling approach. The code is publicly available at
\href{https://github.com/AIDC-AI/TeEFusion}{github.com/AIDC-AI/TeEFusion}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2025. The code is publicly available at
  https://github.com/AIDC-AI/TeEFusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MatSSL: <span class="highlight-title">Robust</span> <span class="highlight-title">Self-Supervised</span> Representation Learning for
  Metallo<span class="highlight-title">graph</span>ic Image <span class="highlight-title">Segmentation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoang Hai Nam Nguyen, Phan Nguyen Duc Hieu, Ho Won Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  MatSSL is a streamlined self-supervised learning (SSL) architecture that
employs Gated Feature Fusion at each stage of the backbone to integrate
multi-level representations effectively. Current micrograph analysis of
metallic materials relies on supervised methods, which require retraining for
each new dataset and often perform inconsistently with only a few labeled
samples. While SSL offers a promising alternative by leveraging unlabeled data,
most existing methods still depend on large-scale datasets to be effective.
MatSSL is designed to overcome this limitation. We first perform
self-supervised pretraining on a small-scale, unlabeled dataset and then
fine-tune the model on multiple benchmark datasets. The resulting segmentation
models achieve 69.13% mIoU on MetalDAM, outperforming the 66.73% achieved by an
ImageNet-pretrained encoder, and delivers consistently up to nearly 40%
improvement in average mIoU on the Environmental Barrier Coating benchmark
dataset (EBC) compared to models pretrained with MicroNet. This suggests that
MatSSL enables effective adaptation to the metallographic domain using only a
small amount of unlabeled data, while preserving the rich and transferable
features learned from large-scale pretraining on natural images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChronoSelect: <span class="highlight-title">Robust</span> Learning with Noisy Labels via <span class="highlight-title">Dynamic</span>s Temporal
  Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianchao Wang, Qingfeng Li, Pengcheng Zheng, Xiaorong Pu, Yazhou Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training deep neural networks on real-world datasets is often hampered by the
presence of noisy labels, which can be memorized by over-parameterized models,
leading to significant degradation in generalization performance. While
existing methods for learning with noisy labels (LNL) have made considerable
progress, they fundamentally suffer from static snapshot evaluations and fail
to leverage the rich temporal dynamics of learning evolution. In this paper, we
propose ChronoSelect (chrono denoting its temporal nature), a novel framework
featuring an innovative four-stage memory architecture that compresses
prediction history into compact temporal distributions. Our unique sliding
update mechanism with controlled decay maintains only four dynamic memory units
per sample, progressively emphasizing recent patterns while retaining essential
historical knowledge. This enables precise three-way sample partitioning into
clean, boundary, and noisy subsets through temporal trajectory analysis and
dual-branch consistency. Theoretical guarantees prove the mechanism's
convergence and stability under noisy conditions. Extensive experiments
demonstrate ChronoSelect's state-of-the-art performance across synthetic and
real-world benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differential-UMamba: Rethinking Tumor <span class="highlight-title">Segmentation</span> Under Limited Data
  Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhruv Jain, Romain Modzelewski, Romain Hérault, Clement Chatelain, Eva Torfeh, Sebastien Thureau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In data-scarce scenarios, deep learning models often overfit to noise and
irrelevant patterns, which limits their ability to generalize to unseen
samples. To address these challenges in medical image segmentation, we
introduce Diff-UMamba, a novel architecture that combines the UNet framework
with the mamba mechanism for modeling long-range dependencies. At the heart of
Diff-UMamba is a Noise Reduction Module (NRM), which employs a signal
differencing strategy to suppress noisy or irrelevant activations within the
encoder. This encourages the model to filter out spurious features and enhance
task-relevant representations, thereby improving its focus on clinically
meaningful regions. As a result, the architecture achieves improved
segmentation accuracy and robustness, particularly in low-data settings.
Diff-UMamba is evaluated on multiple public datasets, including MSD (lung and
pancreas) and AIIB23, demonstrating consistent performance gains of 1-3% over
baseline methods across diverse segmentation tasks. To further assess
performance under limited-data conditions, additional experiments are conducted
on the BraTS-21 dataset by varying the proportion of available training
samples. The approach is also validated on a small internal non-small cell lung
cancer (NSCLC) dataset for gross tumor volume (GTV) segmentation in cone beam
CT (CBCT), where it achieves a 4-5% improvement over the baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Unsupervised</span> Domain Adaptation for 3D <span class="highlight-title">LiDAR</span> Semantic <span class="highlight-title">Segmentation</span> Using
  Contrastive Learning and Multi-Model Pseudo Labeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Kaushik, Norbert Haala, Uwe Soergel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Addressing performance degradation in 3D LiDAR semantic segmentation due to
domain shifts (e.g., sensor type, geographical location) is crucial for
autonomous systems, yet manual annotation of target data is prohibitive. This
study addresses the challenge using Unsupervised Domain Adaptation (UDA) and
introduces a novel two-stage framework to tackle it. Initially, unsupervised
contrastive learning at the segment level is used to pre-train a backbone
network, enabling it to learn robust, domain-invariant features without labels.
Subsequently, a multi-model pseudo-labeling strategy is introduced, utilizing
an ensemble of diverse state-of-the-art architectures (including projection,
voxel, hybrid, and cylinder-based methods). Predictions from these models are
aggregated via hard voting to generate high-quality, refined pseudo-labels for
the unlabeled target domain, mitigating single-model biases. The contrastively
pre-trained network is then fine-tuned using these robust pseudo-labels.
Experiments adapting from SemanticKITTI to unlabeled target datasets
(SemanticPOSS, SemanticSlamantic) demonstrate significant improvements in
segmentation accuracy compared to direct transfer and single-model UDA
approaches. These results highlight the effectiveness of combining contrastive
pre-training with refined ensemble pseudo-labeling for bridging complex domain
gaps without requiring target domain annotations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-Time Object <span class="highlight-title">Detection</span> and Classification using YOLO for Edge FPGAs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18174v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18174v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rashed Al Amin, Roman Obermaisser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection and classification are crucial tasks across various
application domains, particularly in the development of safe and reliable
Advanced Driver Assistance Systems (ADAS). Existing deep learning-based methods
such as Convolutional Neural Networks (CNNs), Single Shot Detectors (SSDs), and
You Only Look Once (YOLO) have demonstrated high performance in terms of
accuracy and computational speed when deployed on Field-Programmable Gate
Arrays (FPGAs). However, despite these advances, state-of-the-art YOLO-based
object detection and classification systems continue to face challenges in
achieving resource efficiency suitable for edge FPGA platforms. To address this
limitation, this paper presents a resource-efficient real-time object detection
and classification system based on YOLOv5 optimized for FPGA deployment. The
proposed system is trained on the COCO and GTSRD datasets and implemented on
the Xilinx Kria KV260 FPGA board. Experimental results demonstrate a
classification accuracy of 99%, with a power consumption of 3.5W and a
processing speed of 9 frames per second (FPS). These findings highlight the
effectiveness of the proposed approach in enabling real-time,
resource-efficient object detection and classification for edge computing
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for the 67th International Symposium on
  ELMAR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WaveMamba: Wavelet-Driven Mamba <span class="highlight-title">Fusion</span> for RGB-Infrared Object <span class="highlight-title">Detection</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haodong Zhu, Wenhao Dong, Linlin Yang, Hong Li, Yuguang Yang, Yangyang Ren, Qingcheng Zhu, Zichao Feng, Changbai Li, Shaohui Lin, Runqi Wang, Xiaoyan Luo, Baochang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging the complementary characteristics of visible (RGB) and infrared
(IR) imagery offers significant potential for improving object detection. In
this paper, we propose WaveMamba, a cross-modality fusion method that
efficiently integrates the unique and complementary frequency features of RGB
and IR decomposed by Discrete Wavelet Transform (DWT). An improved detection
head incorporating the Inverse Discrete Wavelet Transform (IDWT) is also
proposed to reduce information loss and produce the final detection results.
The core of our approach is the introduction of WaveMamba Fusion Block (WMFB),
which facilitates comprehensive fusion across low-/high-frequency sub-bands.
Within WMFB, the Low-frequency Mamba Fusion Block (LMFB), built upon the Mamba
framework, first performs initial low-frequency feature fusion with channel
swapping, followed by deep fusion with an advanced gated attention mechanism
for enhanced integration. High-frequency features are enhanced using a strategy
that applies an ``absolute maximum" fusion approach. These advancements lead to
significant performance gains, with our method surpassing state-of-the-art
approaches and achieving average mAP improvements of 4.5% on four benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeoAvatar: Adaptive <span class="highlight-title">Geometric</span>al Gaussian Splatting for 3D Head Avatar <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        SeungJun Moon, Hah Min Lew, Seungeun Lee, Ji-Su Kang, Gyeong-Moon Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent progress in 3D head avatar generation, balancing identity
preservation, i.e., reconstruction, with novel poses and expressions, i.e.,
animation, remains a challenge. Existing methods struggle to adapt Gaussians to
varying geometrical deviations across facial regions, resulting in suboptimal
quality. To address this, we propose GeoAvatar, a framework for adaptive
geometrical Gaussian Splatting. GeoAvatar leverages Adaptive Pre-allocation
Stage (APS), an unsupervised method that segments Gaussians into rigid and
flexible sets for adaptive offset regularization. Then, based on mouth anatomy
and dynamics, we introduce a novel mouth structure and the part-wise
deformation strategy to enhance the animation fidelity of the mouth. Finally,
we propose a regularization loss for precise rigging between Gaussians and 3DMM
faces. Moreover, we release DynamicFace, a video dataset with highly expressive
facial motions. Extensive experiments show the superiority of GeoAvatar
compared to state-of-the-art methods in reconstruction and novel animation
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025, Project page: https://hahminlew.github.io/geoavatar/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Degradation-Consistent Learning via Bidirectional Dif<span class="highlight-title">fusion</span> for
  Low-Light Image Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhong He, Minglong Xue, Zhipu Liu, Mingliang Zhou, Aoxiang Ning, Palaiahnakote Shivakumara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-light image enhancement aims to improve the visibility of degraded images
to better align with human visual perception. While diffusion-based methods
have shown promising performance due to their strong generative capabilities.
However, their unidirectional modelling of degradation often struggles to
capture the complexity of real-world degradation patterns, leading to
structural inconsistencies and pixel misalignments. To address these
challenges, we propose a bidirectional diffusion optimization mechanism that
jointly models the degradation processes of both low-light and normal-light
images, enabling more precise degradation parameter matching and enhancing
generation quality. Specifically, we perform bidirectional diffusion-from
low-to-normal light and from normal-to-low light during training and introduce
an adaptive feature interaction block (AFI) to refine feature representation.
By leveraging the complementarity between these two paths, our approach imposes
an implicit symmetry constraint on illumination attenuation and noise
distribution, facilitating consistent degradation learning and improving the
models ability to perceive illumination and detail degradation. Additionally,
we design a reflection-aware correction module (RACM) to guide color
restoration post-denoising and suppress overexposed regions, ensuring content
consistency and generating high-quality images that align with human visual
perception. Extensive experiments on multiple benchmark datasets demonstrate
that our method outperforms state-of-the-art methods in both quantitative and
qualitative evaluations while generalizing effectively to diverse degradation
scenarios. Code at https://github.com/hejh8/BidDiff
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10page</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Information Entropy-Based Framework for Quantifying Tortuosity in
  Meibomian Gland Uneven Atrophy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kesheng Wang, Xiaoyu Chen, Chunlei He, Fenfen Li, Xinxin Yu, Dexing Kong, Shoujun Huang, Qi Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the medical image analysis field, precise quantification of curve
tortuosity plays a critical role in the auxiliary diagnosis and pathological
assessment of various diseases. In this study, we propose a novel framework for
tortuosity quantification and demonstrate its effectiveness through the
evaluation of meibomian gland atrophy uniformity,serving as a representative
application scenario.
  We introduce an information entropy-based tortuosity quantification framework
that integrates probability modeling with entropy theory and incorporates
domain transformation of curve data. Unlike traditional methods such as
curvature or arc-chord ratio, this approach evaluates the tortuosity of a
target curve by comparing it to a designated reference curve. Consequently, it
is more suitable for tortuosity assessment tasks in medical data where
biologically plausible reference curves are available, providing a more robust
and objective evaluation metric without relying on idealized straight-line
comparisons.
  First, we conducted numerical simulation experiments to preliminarily assess
the stability and validity of the method. Subsequently, the framework was
applied to quantify the spatial uniformity of meibomian gland atrophy and to
analyze the difference in this uniformity between \textit{Demodex}-negative and
\textit{Demodex}-positive patient groups. The results demonstrated a
significant difference in tortuosity-based uniformity between the two groups,
with an area under the curve of 0.8768, sensitivity of 0.75, and specificity of
0.93. These findings highlight the clinical utility of the proposed framework
in curve tortuosity analysis and its potential as a generalizable tool for
quantitative morphological evaluation in medical diagnostics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript contains 7 figures. All comments are welcome</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning for Glioblastoma Morpho-pathological Features
  Identification: A BraTS-Pathology Challenge Solution <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18133v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18133v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juexin Zhang, Ying Weng, Ke Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Glioblastoma, a highly aggressive brain tumor with diverse molecular and
pathological features, poses a diagnostic challenge due to its heterogeneity.
Accurate diagnosis and assessment of this heterogeneity are essential for
choosing the right treatment and improving patient outcomes. Traditional
methods rely on identifying specific features in tissue samples, but deep
learning offers a promising approach for improved glioblastoma diagnosis. In
this paper, we present our approach to the BraTS-Path Challenge 2024. We
leverage a pre-trained model and fine-tune it on the BraTS-Path training
dataset. Our model demonstrates poor performance on the challenging BraTS-Path
validation set, as rigorously assessed by the Synapse online platform. The
model achieves an accuracy of 0.392229, a recall of 0.392229, and a F1-score of
0.392229, indicating a consistent ability to correctly identify instances under
the target condition. Notably, our model exhibits perfect specificity of
0.898704, showing an exceptional capacity to correctly classify negative cases.
Moreover, a Matthews Correlation Coefficient (MCC) of 0.255267 is calculated,
to signify a limited positive correlation between predicted and actual values
and highlight our model's overall predictive power. Our solution also achieves
the second place during the testing phase.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the International Brain Tumor Segmentation (BraTS)
  challenge organized at MICCAI 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ U-Net Based Healthy 3D Brain Tissue Inpainting <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juexin Zhang, Ying Weng, Ke Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel approach to synthesize healthy 3D brain tissue
from masked input images, specifically focusing on the task of 'ASNR-MICCAI
BraTS Local Synthesis of Tissue via Inpainting'. Our proposed method employs a
U-Net-based architecture, which is designed to effectively reconstruct the
missing or corrupted regions of brain MRI scans. To enhance our model's
generalization capabilities and robustness, we implement a comprehensive data
augmentation strategy that involves randomly masking healthy images during
training. Our model is trained on the BraTS-Local-Inpainting dataset and
demonstrates the exceptional performance in recovering healthy brain tissue.
The evaluation metrics employed, including Structural Similarity Index (SSIM),
Peak Signal-to-Noise Ratio (PSNR), and Mean Squared Error (MSE), consistently
yields impressive results. On the BraTS-Local-Inpainting validation set, our
model achieved an SSIM score of 0.841, a PSNR score of 23.257, and an MSE score
of 0.007. Notably, these evaluation metrics exhibit relatively low standard
deviations, i.e., 0.103 for SSIM score, 4.213 for PSNR score and 0.007 for MSE
score, which indicates that our model's reliability and consistency across
various input scenarios. Our method also secured first place in the challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the International Brain Tumor Segmentation (BraTS)
  challenge organized at MICCAI 2024 conference. Included 7 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameter-Efficient Fine-Tuning of 3D DDPM for MRI Image Generation
  Using Tensor Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binghua Li, Ziqing Chang, Tong Liang, Chao Li, Toshihisa Tanaka, Shigeki Aoki, Qibin Zhao, Zhe Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the challenge of parameter-efficient fine-tuning (PEFT) for
three-dimensional (3D) U-Net-based denoising diffusion probabilistic models
(DDPMs) in magnetic resonance imaging (MRI) image generation. Despite its
practical significance, research on parameter-efficient representations of 3D
convolution operations remains limited. To bridge this gap, we propose Tensor
Volumetric Operator (TenVOO), a novel PEFT method specifically designed for
fine-tuning DDPMs with 3D convolutional backbones. Leveraging tensor network
modeling, TenVOO represents 3D convolution kernels with lower-dimensional
tensors, effectively capturing complex spatial dependencies during fine-tuning
with few parameters. We evaluate TenVOO on three downstream brain MRI
datasets-ADNI, PPMI, and BraTS2021-by fine-tuning a DDPM pretrained on 59,830
T1-weighted brain MRI scans from the UK Biobank. Our results demonstrate that
TenVOO achieves state-of-the-art performance in multi-scale structural
similarity index measure (MS-SSIM), outperforming existing approaches in
capturing spatial dependencies while requiring only 0.3% of the trainable
parameters of the original model. Our code is available at:
https://github.com/xiaovhua/tenvoo
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ T2VWorldBench: A <span class="highlight-title">Benchmark</span> for Evaluating World Knowledge in
  Text-to-Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubin Chen, Xuyang Guo, Zhenmei Shi, Zhao Song, Jiahao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-video (T2V) models have shown remarkable performance in generating
visually reasonable scenes, while their capability to leverage world knowledge
for ensuring semantic consistency and factual accuracy remains largely
understudied. In response to this challenge, we propose T2VWorldBench, the
first systematic evaluation framework for evaluating the world knowledge
generation abilities of text-to-video models, covering 6 major categories, 60
subcategories, and 1,200 prompts across a wide range of domains, including
physics, nature, activity, culture, causality, and object. To address both
human preference and scalable evaluation, our benchmark incorporates both human
evaluation and automated evaluation using vision-language models (VLMs). We
evaluated the 10 most advanced text-to-video models currently available,
ranging from open source to commercial models, and found that most models are
unable to understand world knowledge and generate truly correct videos. These
findings point out a critical gap in the capability of current text-to-video
models to leverage world knowledge, providing valuable research opportunities
and entry points for constructing models with robust capabilities for
commonsense reasoning and factual generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributional Uncertainty for Out-of-Distribution <span class="highlight-title">Detection</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        JinYoung Kim, DaeUng Jo, Kimin Yun, Jeonghyo Song, Youngjoon Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating uncertainty from deep neural networks is a widely used approach
for detecting out-of-distribution (OoD) samples, which typically exhibit high
predictive uncertainty. However, conventional methods such as Monte Carlo (MC)
Dropout often focus solely on either model or data uncertainty, failing to
align with the semantic objective of OoD detection. To address this, we propose
the Free-Energy Posterior Network, a novel framework that jointly models
distributional uncertainty and identifying OoD and misclassified regions using
free energy. Our method introduces two key contributions: (1) a
free-energy-based density estimator parameterized by a Beta distribution, which
enables fine-grained uncertainty estimation near ambiguous or unseen regions;
and (2) a loss integrated within a posterior network, allowing direct
uncertainty estimation from learned parameters without requiring stochastic
sampling. By integrating our approach with the residual prediction branch (RPL)
framework, the proposed method goes beyond post-hoc energy thresholding and
enables the network to learn OoD regions by leveraging the variance of the Beta
distribution, resulting in a semantically meaningful and computationally
efficient solution for uncertainty-aware segmentation. We validate the
effectiveness of our method on challenging real-world benchmarks, including
Fishyscapes, RoadAnomaly, and Segment-Me-If-You-Can.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages , 3 figures , IEEE International Conference on Advanced
  Visual and Signal-Based Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multimodal Seq2Seq Transformer for Predicting Brain Responses to
  Naturalistic Stimuli 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianyi He, Yuan Chang Leong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Algonauts 2025 Challenge called on the community to develop encoding
models that predict whole-brain fMRI responses to naturalistic multimodal
movies. In this submission, we propose a sequence-to-sequence Transformer that
autoregressively predicts fMRI activity from visual, auditory, and language
inputs. Stimulus features were extracted using pretrained models including
VideoMAE, HuBERT, Qwen, and BridgeTower. The decoder integrates information
from prior brain states, current stimuli, and episode-level summaries via dual
cross-attention mechanisms that attend to both perceptual information extracted
from the stimulus as well as narrative information provided by high-level
summaries of narrative content. One core innovation of our approach is the use
of sequences of multimodal context to predict sequences of brain activity,
enabling the model to capture long-range temporal structure in both stimuli and
neural responses. Another is the combination of a shared encoder with partial
subject-specific decoder, which leverages common structure across subjects
while accounting for individual variability. Our model achieves strong
performance on both in-distribution and out-of-distribution data, demonstrating
the effectiveness of temporally-aware, multimodal sequence modeling for brain
activity prediction. The code is available at
https://github.com/Angelneer926/Algonauts_challenge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Dataset</span>s and Recipes for Video Temporal Grounding via <span class="highlight-title">Reinforcement</span>
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruizhe Chen, Zhiting Fan, Tianze Luo, Heqing Zou, Zhaopeng Feng, Guiyang Xie, Hansheng Zhang, Zhuochen Wang, Zuozhu Liu, Huaijian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Temporal Grounding (VTG) aims to localize relevant temporal segments in
videos given natural language queries. Despite recent progress with large
vision-language models (LVLMs) and instruction-tuning, existing approaches
often suffer from limited temporal awareness and poor generalization. In this
work, we introduce a two-stage training framework that integrates supervised
fine-tuning with reinforcement learning (RL) to improve both the accuracy and
robustness of VTG models. Our approach first leverages high-quality curated
cold start data for SFT initialization, followed by difficulty-controlled RL to
further enhance temporal localization and reasoning abilities. Comprehensive
experiments on multiple VTG benchmarks demonstrate that our method consistently
outperforms existing models, particularly in challenging and open-domain
scenarios. We conduct an in-depth analysis of training strategies and dataset
curation, highlighting the importance of both high-quality cold start data and
difficulty-controlled RL. To facilitate further research and industrial
adoption, we release all intermediate datasets, models, and code to the
community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparison of <span class="highlight-title">Segmentation</span> Methods in Remote Sensing for Land Use Land
  Cover 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naman Srivastava, Joel D Joy, Yash Dixit, Swarup E, Rakshit Ramesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Land Use Land Cover (LULC) mapping is essential for urban and resource
planning, and is one of the key elements in developing smart and sustainable
cities.This study evaluates advanced LULC mapping techniques, focusing on
Look-Up Table (LUT)-based Atmospheric Correction applied to Cartosat
Multispectral (MX) sensor images, followed by supervised and semi-supervised
learning models for LULC prediction. We explore DeeplabV3+ and Cross-Pseudo
Supervision (CPS). The CPS model is further refined with dynamic weighting,
enhancing pseudo-label reliability during training. This comprehensive approach
analyses the accuracy and utility of LULC mapping techniques for various urban
planning applications. A case study of Hyderabad, India, illustrates
significant land use changes due to rapid urbanization. By analyzing Cartosat
MX images over time, we highlight shifts such as urban sprawl, shrinking green
spaces, and expanding industrial areas. This demonstrates the practical utility
of these techniques for urban planners and policymakers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TextSAM-EUS: Text Prompt Learning for SAM to Accurately Segment
  Pancreatic Tumor in Endoscopic Ultrasound <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18082v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18082v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Spiegler, Taha Koleilat, Arash Harirpoush, Corey S. Miller, Hassan Rivaz, Marta Kersten-Oertel, Yiming Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pancreatic cancer carries a poor prognosis and relies on endoscopic
ultrasound (EUS) for targeted biopsy and radiotherapy. However, the speckle
noise, low contrast, and unintuitive appearance of EUS make segmentation of
pancreatic tumors with fully supervised deep learning (DL) models both
error-prone and dependent on large, expert-curated annotation datasets. To
address these challenges, we present TextSAM-EUS, a novel, lightweight,
text-driven adaptation of the Segment Anything Model (SAM) that requires no
manual geometric prompts at inference. Our approach leverages text prompt
learning (context optimization) through the BiomedCLIP text encoder in
conjunction with a LoRA-based adaptation of SAM's architecture to enable
automatic pancreatic tumor segmentation in EUS, tuning only 0.86% of the total
parameters. On the public Endoscopic Ultrasound Database of the Pancreas,
TextSAM-EUS with automatic prompts attains 82.69% Dice and 85.28% normalized
surface distance (NSD), and with manual geometric prompts reaches 83.10% Dice
and 85.70% NSD, outperforming both existing state-of-the-art (SOTA) supervised
DL models and foundation models (e.g., SAM and its variants). As the first
attempt to incorporate prompt learning in SAM-based medical image segmentation,
TextSAM-EUS offers a practical option for efficient and robust automatic EUS
segmentation. Our code will be publicly available upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV 2025 Workshop CVAMD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adapting Large VLMs with Iterative and Manual Instructions for
  Generative Low-light Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoran Sun, Liyan Wang, Cong Wang, Yeying Jin, Kin-man Lam, Zhixun Su, Yang Yang, Jinshan Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most existing low-light image enhancement (LLIE) methods rely on pre-trained
model priors, low-light inputs, or both, while neglecting the semantic guidance
available from normal-light images. This limitation hinders their effectiveness
in complex lighting conditions. In this paper, we propose VLM-IMI, a novel
framework that leverages large vision-language models (VLMs) with iterative and
manual instructions (IMIs) for LLIE. VLM-IMI incorporates textual descriptions
of the desired normal-light content as enhancement cues, enabling semantically
informed restoration. To effectively integrate cross-modal priors, we introduce
an instruction prior fusion module, which dynamically aligns and fuses image
and text features, promoting the generation of detailed and semantically
coherent outputs. During inference, we adopt an iterative and manual
instruction strategy to refine textual instructions, progressively improving
visual quality. This refinement enhances structural fidelity, semantic
alignment, and the recovery of fine details under extremely low-light
conditions. Extensive experiments across diverse scenarios demonstrate that
VLM-IMI outperforms state-of-the-art methods in both quantitative metrics and
perceptual quality. The source code is available at
https://github.com/sunxiaoran01/VLM-IMI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BokehDiff: Neural Lens Blur with One-Step Dif<span class="highlight-title">fusion</span> <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengxuan Zhu, Qingnan Fan, Qi Zhang, Jinwei Chen, Huaqi Zhang, Chao Xu, Boxin Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce BokehDiff, a novel lens blur rendering method that achieves
physically accurate and visually appealing outcomes, with the help of
generative diffusion prior. Previous methods are bounded by the accuracy of
depth estimation, generating artifacts in depth discontinuities. Our method
employs a physics-inspired self-attention module that aligns with the image
formation process, incorporating depth-dependent circle of confusion constraint
and self-occlusion effects. We adapt the diffusion model to the one-step
inference scheme without introducing additional noise, and achieve results of
high quality and fidelity. To address the lack of scalable paired data, we
propose to synthesize photorealistic foregrounds with transparency with
diffusion models, balancing authenticity and scene diversity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Scene Transition Awareness in Video Generation via
  Post-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanwen Shen, Jiajie Lu, Yupeng Cao, Xiaonan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in AI-generated video have shown strong performance on
\emph{text-to-video} tasks, particularly for short clips depicting a single
scene. However, current models struggle to generate longer videos with coherent
scene transitions, primarily because they cannot infer when a transition is
needed from the prompt. Most open-source models are trained on datasets
consisting of single-scene video clips, which limits their capacity to learn
and respond to prompts requiring multiple scenes. Developing scene transition
awareness is essential for multi-scene generation, as it allows models to
identify and segment videos into distinct clips by accurately detecting
transitions.
  To address this, we propose the \textbf{Transition-Aware Video} (TAV)
dataset, which consists of preprocessed video clips with multiple scene
transitions. Our experiment shows that post-training on the \textbf{TAV}
dataset improves prompt-based scene transition understanding, narrows the gap
between required and generated scenes, and maintains image quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs
  and VLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duy Nguyen, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inference-time steering methods offer a lightweight alternative to
fine-tuning large language models (LLMs) and vision-language models (VLMs) by
modifying internal activations at test time without updating model weights.
However, most existing approaches rely on fixed, global intervention vectors,
overlook the causal influence of individual input tokens, and fail to leverage
informative gradients from the model's logits, particularly in multimodal
settings where visual and textual inputs contribute unevenly. To address these
limitations, we introduce GrAInS, an inference-time steering approach that
operates across both language-only and vision-language models and tasks. GrAInS
uses contrastive, gradient-based attribution via Integrated Gradients to
identify the top-k most influential tokens, both positively and negatively
attributed based on their contribution to preferred versus dispreferred
outputs. These tokens are then used to construct directional steering vectors
that capture semantic shifts from undesirable to desirable behavior. During
inference, GrAInS adjusts hidden activations at transformer layers guided by
token-level attribution signals, and normalizes activations to preserve
representational scale. This enables fine-grained, interpretable, and modular
control over model behavior, without retraining or auxiliary supervision.
Empirically, GrAInS consistently outperforms both fine-tuning and existing
steering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using
Llama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514
with LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all
while preserving the model's fluency and general capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages. Code: https://github.com/duykhuongnguyen/GrAInS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NWaaS: Nonintrusive Watermarking as a Service for X-to-Image DNN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18036v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18036v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan An, Guang Hua, Yu Guo, Hangcheng Cao, Susanto Rahardja, Yuguang Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The intellectual property of deep neural network (DNN) models can be
protected with DNN watermarking, which embeds copyright watermarks into model
parameters (white-box), model behavior (black-box), or model outputs
(box-free), and the watermarks can be subsequently extracted to verify model
ownership or detect model theft. Despite recent advances, these existing
methods are inherently intrusive, as they either modify the model parameters or
alter the structure. This natural intrusiveness raises concerns about
watermarking-induced shifts in model behavior and the additional cost of
fine-tuning, further exacerbated by the rapidly growing model size. As a
result, model owners are often reluctant to adopt DNN watermarking in practice,
which limits the development of practical Watermarking as a Service (WaaS)
systems. To address this issue, we introduce Nonintrusive Watermarking as a
Service (NWaaS), a novel trustless paradigm designed for X-to-Image models, in
which we hypothesize that with the model untouched, an owner-defined watermark
can still be extracted from model outputs. Building on this concept, we propose
ShadowMark, a concrete implementation of NWaaS which addresses critical
deployment challenges by establishing a robust and nonintrusive side channel in
the protected model's black-box API, leveraging a key encoder and a watermark
decoder. It is significantly distinctive from existing solutions by attaining
the so-called absolute fidelity and being applicable to different DNN
architectures, while being also robust against existing attacks, eliminating
the fidelity-robustness trade-off. Extensive experiments on image-to-image,
noise-to-image, noise-and-text-to-image, and text-to-image models, demonstrate
the efficacy and practicality of ShadowMark for real-world deployment of
nonintrusive DNN watermarking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViGText: Deepfake Image <span class="highlight-title">Detection</span> with Vision-Language Model
  Explanations and <span class="highlight-title">Graph</span> Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18031v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18031v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad ALBarqawi, Mahmoud Nazzal, Issa Khalil, Abdallah Khreishah, NhatHai Phan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid rise of deepfake technology, which produces realistic but
fraudulent digital content, threatens the authenticity of media. Traditional
deepfake detection approaches often struggle with sophisticated, customized
deepfakes, especially in terms of generalization and robustness against
malicious attacks. This paper introduces ViGText, a novel approach that
integrates images with Vision Large Language Model (VLLM) Text explanations
within a Graph-based framework to improve deepfake detection. The novelty of
ViGText lies in its integration of detailed explanations with visual data, as
it provides a more context-aware analysis than captions, which often lack
specificity and fail to reveal subtle inconsistencies. ViGText systematically
divides images into patches, constructs image and text graphs, and integrates
them for analysis using Graph Neural Networks (GNNs) to identify deepfakes.
Through the use of multi-level feature extraction across spatial and frequency
domains, ViGText captures details that enhance its robustness and accuracy to
detect sophisticated deepfakes. Extensive experiments demonstrate that ViGText
significantly enhances generalization and achieves a notable performance boost
when it detects user-customized deepfakes. Specifically, average F1 scores rise
from 72.45% to 98.32% under generalization evaluation, and reflects the model's
superior ability to generalize to unseen, fine-tuned variations of stable
diffusion models. As for robustness, ViGText achieves an increase of 11.1% in
recall compared to other deepfake detection approaches. When facing targeted
attacks that exploit its graph-based architecture, ViGText limits
classification performance degradation to less than 4%. ViGText uses detailed
visual and textual analysis to set a new standard for detecting deepfakes,
helping ensure media authenticity and information integrity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emotion Recognition from Skeleton Data: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18026v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18026v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haifeng Lu, Jiuyi Chen, Zhen Zhang, Ruida Liu, Runhao Zeng, Xiping Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion recognition through body movements has emerged as a compelling and
privacy-preserving alternative to traditional methods that rely on facial
expressions or physiological signals. Recent advancements in 3D skeleton
acquisition technologies and pose estimation algorithms have significantly
enhanced the feasibility of emotion recognition based on full-body motion. This
survey provides a comprehensive and systematic review of skeleton-based emotion
recognition techniques. First, we introduce psychological models of emotion and
examine the relationship between bodily movements and emotional expression.
Next, we summarize publicly available datasets, highlighting the differences in
data acquisition methods and emotion labeling strategies. We then categorize
existing methods into posture-based and gait-based approaches, analyzing them
from both data-driven and technical perspectives. In particular, we propose a
unified taxonomy that encompasses four primary technical paradigms: Traditional
approaches, Feat2Net, FeatFusionNet, and End2EndNet. Representative works
within each category are reviewed and compared, with benchmarking results
across commonly used datasets. Finally, we explore the extended applications of
emotion recognition in mental health assessment, such as detecting depression
and autism, and discuss the open challenges and future research directions in
this rapidly evolving field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 5 figures, 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-fidelity 3D Gaussian Inpainting: preserving <span class="highlight-title">multi-view</span> consistency
  and photorealistic details 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Zhou, Dinghao Li, Nannan Li, Mingjie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in multi-view 3D reconstruction and novel-view synthesis,
particularly through Neural Radiance Fields (NeRF) and 3D Gaussian Splatting
(3DGS), have greatly enhanced the fidelity and efficiency of 3D content
creation. However, inpainting 3D scenes remains a challenging task due to the
inherent irregularity of 3D structures and the critical need for maintaining
multi-view consistency. In this work, we propose a novel 3D Gaussian inpainting
framework that reconstructs complete 3D scenes by leveraging sparse inpainted
views. Our framework incorporates an automatic Mask Refinement Process and
region-wise Uncertainty-guided Optimization. Specifically, we refine the
inpainting mask using a series of operations, including Gaussian scene
filtering and back-projection, enabling more accurate localization of occluded
regions and realistic boundary restoration. Furthermore, our Uncertainty-guided
Fine-grained Optimization strategy, which estimates the importance of each
region across multi-view images during training, alleviates multi-view
inconsistencies and enhances the fidelity of fine details in the inpainted
results. Comprehensive experiments conducted on diverse datasets demonstrate
that our approach outperforms existing state-of-the-art methods in both visual
quality and view consistency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Celeb-DF++: A Large-scale Challenging Video DeepFake <span class="highlight-title">Benchmark</span> for
  Generalizable Forensics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuezun Li, Delong Zhu, Xinjie Cui, Siwei Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of AI technologies has significantly increased the
diversity of DeepFake videos circulating online, posing a pressing challenge
for \textit{generalizable forensics}, \ie, detecting a wide range of unseen
DeepFake types using a single model. Addressing this challenge requires
datasets that are not only large-scale but also rich in forgery diversity.
However, most existing datasets, despite their scale, include only a limited
variety of forgery types, making them insufficient for developing generalizable
detection methods. Therefore, we build upon our earlier Celeb-DF dataset and
introduce {Celeb-DF++}, a new large-scale and challenging video DeepFake
benchmark dedicated to the generalizable forensics challenge. Celeb-DF++ covers
three commonly encountered forgery scenarios: Face-swap (FS), Face-reenactment
(FR), and Talking-face (TF). Each scenario contains a substantial number of
high-quality forged videos, generated using a total of 22 various recent
DeepFake methods. These methods differ in terms of architectures, generation
pipelines, and targeted facial regions, covering the most prevalent DeepFake
cases witnessed in the wild. We also introduce evaluation protocols for
measuring the generalizability of 24 recent detection methods, highlighting the
limitations of existing detection methods and the difficulty of our new
dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/OUC-VAS/Celeb-DF-PP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Direct Dual-Energy CT Material Decomposition using Model-based Denoising
  Dif<span class="highlight-title">fusion</span> Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Xu, Alexandre Bousse, Alessandro Perelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dual-energy X-ray Computed Tomography (DECT) constitutes an advanced
technology which enables automatic decomposition of materials in clinical
images without manual segmentation using the dependency of the X-ray linear
attenuation with energy. However, most methods perform material decomposition
in the image domain as a post-processing step after reconstruction but this
procedure does not account for the beam-hardening effect and it results in
sub-optimal results. In this work, we propose a deep learning procedure called
Dual-Energy Decomposition Model-based Diffusion (DEcomp-MoD) for quantitative
material decomposition which directly converts the DECT projection data into
material images. The algorithm is based on incorporating the knowledge of the
spectral DECT model into the deep learning training loss and combining a
score-based denoising diffusion learned prior in the material image domain.
Importantly the inference optimization loss takes as inputs directly the
sinogram and converts to material images through a model-based conditional
diffusion model which guarantees consistency of the results. We evaluate the
performance with both quantitative and qualitative estimation of the proposed
DEcomp-MoD method on synthetic DECT sinograms from the low-dose AAPM dataset.
Finally, we show that DEcomp-MoD outperform state-of-the-art unsupervised
score-based model and supervised deep learning networks, with the potential to
be deployed for clinical diagnosis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 10 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GRR-CoCa: Leveraging LLM Mechanisms in Multimodal Model Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jake R. Patock, Nicole Catherine Lewis, Kevin McCoy, Christina Gomez, Canling Chen, Lorenzo Luzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art (SOTA) image and text generation models are multimodal
models that have many similarities to large language models (LLMs). Despite
achieving strong performances, leading foundational multimodal model
architectures frequently lag behind the architectural sophistication of
contemporary LLMs. We propose GRR-CoCa, an improved SOTA Contrastive Captioner
(CoCa) model that incorporates Gaussian error gated linear units, root mean
squared normalization, and rotary positional embedding into the textual
decoders and the vision transformer (ViT) encoder. Each architectural
modification has been shown to improve model performance in LLMs, but has yet
to be adopted in CoCa. We benchmarked GRR-CoCa against Baseline CoCa, a model
with the same modified textual decoders but with CoCa's original ViT encoder.
We used standard pretraining and fine-tuning workflows to benchmark the models
on contrastive and generative tasks. Our GRR-CoCa significantly outperformed
Baseline CoCa on the pretraining dataset and three diverse fine-tuning
datasets. Pretraining improvements were 27.25% in contrastive loss, 3.71% in
perplexity, and 7.15% in CoCa loss. The average fine-tuning improvements were
13.66% in contrastive loss, 5.18% in perplexity, and 5.55% in CoCa loss. We
show that GRR-CoCa's modified architecture improves performance and
generalization across vision-language domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Registration</span> beyond Points: General Affine Subspace Alignment via
  Geodesic Distance on Grassmann Manifold 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17998v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17998v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeho Shin, Hyeonjae Gil, Junwoo Jang, Maani Ghaffari, Ayoung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Affine Grassmannian has been favored for expressing proximity between lines
and planes due to its theoretical exactness in measuring distances among
features. Despite this advantage, the existing method can only measure the
proximity without yielding the distance as an explicit function of rigid body
transformation. Thus, an optimizable distance function on the manifold has
remained underdeveloped, stifling its application in registration problems.
This paper is the first to explicitly derive an optimizable cost function
between two Grassmannian features with respect to rigid body transformation
($\mathbf{R}$ and $\mathbf{t}$). Specifically, we present a rigorous
mathematical proof demonstrating that the bases of high-dimensional linear
subspaces can serve as an explicit representation of the cost. Finally, we
propose an optimizable cost function based on the transformed bases that can be
applied to the registration problem of any affine subspace. Compared to vector
parameter-based approaches, our method is able to find a globally optimal
solution by directly minimizing the geodesic distance which is agnostic to
representation ambiguity. The resulting cost function and its extension to the
inlier-set maximizing \ac{BnB} solver have been demonstrated to improve the
convergence of existing solutions or outperform them in various computer vision
tasks. The code is available on
https://github.com/joomeok/GrassmannRegistration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the interplay of label bias with subgroup size and
  separability: A case study in mammo<span class="highlight-title">graph</span>ic density classification <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emma A. M. Stanley, Raghav Mehta, Mélanie Roschewitz, Nils D. Forkert, Ben Glocker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Systematic mislabelling affecting specific subgroups (i.e., label bias) in
medical imaging datasets represents an understudied issue concerning the
fairness of medical AI systems. In this work, we investigated how size and
separability of subgroups affected by label bias influence the learned features
and performance of a deep learning model. Therefore, we trained deep learning
models for binary tissue density classification using the EMory BrEast imaging
Dataset (EMBED), where label bias affected separable subgroups (based on
imaging manufacturer) or non-separable "pseudo-subgroups". We found that
simulated subgroup label bias led to prominent shifts in the learned feature
representations of the models. Importantly, these shifts within the feature
space were dependent on both the relative size and the separability of the
subgroup affected by label bias. We also observed notable differences in
subgroup performance depending on whether a validation set with clean labels
was used to define the classification threshold for the model. For instance,
with label bias affecting the majority separable subgroup, the true positive
rate for that subgroup fell from 0.898, when the validation set had clean
labels, to 0.518, when the validation set had biased labels. Our work
represents a key contribution toward understanding the consequences of label
bias on subgroup fairness in medical imaging AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MICCAI Workshop on Fairness of AI in Medical Imaging
  (FAIMI) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AG-VPReID.VIR: Bridging Aerial and Ground Platforms for Video-based
  Visible-Infrared Person Re-ID 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huy Nguyen, Kien Nguyen, Akila Pemasiri, Akmal Jahan, Clinton Fookes, Sridha Sridharan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Person re-identification (Re-ID) across visible and infrared modalities is
crucial for 24-hour surveillance systems, but existing datasets primarily focus
on ground-level perspectives. While ground-based IR systems offer nighttime
capabilities, they suffer from occlusions, limited coverage, and vulnerability
to obstructions--problems that aerial perspectives uniquely solve. To address
these limitations, we introduce AG-VPReID.VIR, the first aerial-ground
cross-modality video-based person Re-ID dataset. This dataset captures 1,837
identities across 4,861 tracklets (124,855 frames) using both UAV-mounted and
fixed CCTV cameras in RGB and infrared modalities. AG-VPReID.VIR presents
unique challenges including cross-viewpoint variations, modality discrepancies,
and temporal dynamics. Additionally, we propose TCC-VPReID, a novel
three-stream architecture designed to address the joint challenges of
cross-platform and cross-modality person Re-ID. Our approach bridges the domain
gaps between aerial-ground perspectives and RGB-IR modalities, through
style-robust feature learning, memory-based cross-view adaptation, and
intermediary-guided temporal modeling. Experiments show that AG-VPReID.VIR
presents distinctive challenges compared to existing datasets, with our
TCC-VPReID framework achieving significant performance gains across multiple
evaluation protocols. Dataset and code are available at
https://github.com/agvpreid25/AG-VPReID.VIR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted atIEEE International Joint Conference on Biometrics (IJCB)
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dif<span class="highlight-title">fusion</span> Beats Autoregressive in Data-Constrained Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15857v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15857v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihir Prabhudesai, Menging Wu, Amir Zadeh, Katerina Fragkiadaki, Deepak Pathak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive (AR) models have long dominated the landscape of large
language models, driving progress across a wide range of tasks. Recently,
diffusion-based language models have emerged as a promising alternative, though
their advantages over AR models remain underexplored. In this paper, we
systematically study masked diffusion models in data-constrained settings-where
training involves repeated passes over limited data-and find that they
significantly outperform AR models when compute is abundant but data is scarce.
Diffusion models make better use of repeated data, achieving lower validation
loss and superior downstream performance. We interpret this advantage as
implicit data augmentation: masked diffusion exposes the model to a diverse
distribution of token orderings and prediction tasks, unlike AR's fixed
left-to-right factorization. We find new scaling laws for diffusion models and
derive a closed-form expression for the critical compute threshold at which
diffusion begins to outperform AR. These results suggest that when data, not
compute, is the bottleneck, diffusion models offer a compelling alternative to
the standard AR paradigm. Our code is available at:
https://diffusion-scaling.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Webpage: https://diffusion-scaling.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MambaNeXt-YOLO: A Hybrid State Space Model for Real-time Object
  <span class="highlight-title">Detection</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03654v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03654v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaochun Lei, Siqi Wu, Weilin Wu, Zetao Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time object detection is a fundamental but challenging task in computer
vision, particularly when computational resources are limited. Although
YOLO-series models have set strong benchmarks by balancing speed and accuracy,
the increasing need for richer global context modeling has led to the use of
Transformer-based architectures. Nevertheless, Transformers have high
computational complexity because of their self-attention mechanism, which
limits their practicality for real-time and edge deployments. To overcome these
challenges, recent developments in linear state space models, such as Mamba,
provide a promising alternative by enabling efficient sequence modeling with
linear complexity. Building on this insight, we propose MambaNeXt-YOLO, a novel
object detection framework that balances accuracy and efficiency through three
key contributions: (1) MambaNeXt Block: a hybrid design that integrates CNNs
with Mamba to effectively capture both local features and long-range
dependencies; (2) Multi-branch Asymmetric Fusion Pyramid Network (MAFPN): an
enhanced feature pyramid architecture that improves multi-scale object
detection across various object sizes; and (3) Edge-focused Efficiency: our
method achieved 66.6% mAP at 31.9 FPS on the PASCAL VOC dataset without any
pre-training and supports deployment on edge devices such as the NVIDIA Jetson
Xavier NX and Orin NX.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is under consideration at Image and Vision Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling RL to Long Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07966v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07966v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, Sifei Liu, Hongxu Yin, Yao Lu, Song Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a full-stack framework that scales up reasoning in
vision-language models (VLMs) to long videos, leveraging reinforcement
learning. We address the unique challenges of long video reasoning by
integrating three critical components: (1) a large-scale dataset,
LongVideo-Reason, comprising 104K long video QA pairs with high-quality
reasoning annotations across diverse domains such as sports, games, and vlogs;
(2) a two-stage training pipeline that extends VLMs with chain-of-thought
supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a
training infrastructure for long video RL, named Multi-modal Reinforcement
Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a
vLLM-based engine tailored for long video, using cached video embeddings for
efficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves
strong performance on video benchmarks, reaching 65.0% and 70.7% accuracy on
VideoMME without and with subtitles, respectively, and consistently
outperforming LongVILA-R1 across multiple benchmarks. Moreover, LongVILA-R1
shows steady performance improvements as the number of input video frames
increases. Notably, our MR-SP system achieves up to 2.1x speedup on long video
RL training. In addition, we release our training system for public
availability that supports RL training on various modalities (video, text, and
audio), various models (VILA and Qwen series), and even image and video
generation models. On a single A100 node (8 GPUs), it supports RL training on
hour-long videos (e.g., 3,600 frames / around 256k tokens).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code at https://github.com/NVlabs/Long-RL and model at
  https://huggingface.co/Efficient-Large-Model/LongVILA-R1-7B</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BiECVC: Gated Diversification of Bidirectional Contexts for Learned
  Video Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09193v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09193v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Jiang, Junru Li, Kai Zhang, Li Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent forward prediction-based learned video compression (LVC) methods have
achieved impressive results, even surpassing VVC reference software VTM under
the Low Delay B (LDB) configuration. In contrast, learned bidirectional video
compression (BVC) remains underexplored and still lags behind its forward-only
counterparts. This performance gap is mainly due to the limited ability to
extract diverse and accurate contexts: most existing BVCs primarily exploit
temporal motion while neglecting non-local correlations across frames.
Moreover, they lack the adaptability to dynamically suppress harmful contexts
arising from fast motion or occlusion. To tackle these challenges, we propose
BiECVC, a BVC framework that incorporates diversified local and non-local
context modeling along with adaptive context gating. For local context
enhancement, BiECVC reuses high-quality features from lower layers and aligns
them using decoded motion vectors without introducing extra motion overhead. To
model non-local dependencies efficiently, we adopt a linear attention mechanism
that balances performance and complexity. To further mitigate the impact of
inaccurate context prediction, we introduce Bidirectional Context Gating,
inspired by data-dependent decay in recent autoregressive language models, to
dynamically filter contextual information based on conditional coding results.
Extensive experiments demonstrate that BiECVC achieves state-of-the-art
performance, reducing the bit-rate by 13.4% and 15.7% compared to VTM 13.2
under the Random Access (RA) configuration with intra periods of 32 and 64,
respectively. To our knowledge, BiECVC is the first learned video codec to
surpass VTM 13.2 RA across all standard test datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACMMM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01804v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01804v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Mai, Peter Hedman, George Kopanas, Dor Verbin, David Futschik, Qiangeng Xu, Falko Kuester, Jonathan T. Barron, Yinda Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Exact Volumetric Ellipsoid Rendering (EVER), a method for
real-time differentiable emission-only volume rendering. Unlike recent
rasterization based approach by 3D Gaussian Splatting (3DGS), our primitive
based representation allows for exact volume rendering, rather than alpha
compositing 3D Gaussian billboards. As such, unlike 3DGS our formulation does
not suffer from popping artifacts and view dependent density, but still
achieves frame rates of $\sim\!30$ FPS at 720p on an NVIDIA RTX4090. Since our
approach is built upon ray tracing it enables effects such as defocus blur and
camera distortion (e.g. such as from fisheye cameras), which are difficult to
achieve by rasterization. We show that our method is more accurate with fewer
blending issues than 3DGS and follow-up work on view-consistent rendering,
especially on the challenging large-scale scenes from the Zip-NeRF dataset
where it achieves sharpest results among real-time techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://half-potato.gitlab.io/posts/ever</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are
  Important 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.04704v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.04704v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manlai Liang, JiaMing Zhang, Xiong Li, Jinlong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing size of the Key-Value (KV) cache during the Large Language
Models long-context inference is the main obstacle for its balance between the
deployment cost and task accuracy. To reduce the KV cache size in such
scenarios, most previous efforts leveraged on the attention weight to evict
non-critical cache tokens. But there is a trade-off in those methods, they
usually require major modification of the inference infrastructure and
significant computation overhead. Based on the fact that the Large Language
models are autoregressive models, we propose LagKV, a KV compression strategy
only relying on straight forward comparison among KV themselves. It is a
totally attention free method which offers easy integration to the main stream
inference platform and comparable performance comparing to other complicated KV
compression methods. Results on RULER benchmark show that, our approach
outperforms SnapKV and StreamingLLM in different compression ratios. Especially
in the 64-digit passkey retrieval task, our method outperforms the attention
weight based method $H_2O$ over $50\%$ with same compression ratios. Our code
is available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Outdoor Monocular <span class="highlight-title">SLAM</span> with <span class="highlight-title">Global</span> Scale-Consistent 3D Gaussian
  Pointmaps <span class="chip">ICCV2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.03737v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.03737v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chong Cheng, Sicheng Yu, Zijian Wang, Yifan Zhou, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has become a popular solution in SLAM due to its
high-fidelity and real-time novel view synthesis performance. However, some
previous 3DGS SLAM methods employ a differentiable rendering pipeline for
tracking, lack geometric priors in outdoor scenes. Other approaches introduce
separate tracking modules, but they accumulate errors with significant camera
movement, leading to scale drift. To address these challenges, we propose a
robust RGB-only outdoor 3DGS SLAM method: S3PO-GS. Technically, we establish a
self-consistent tracking module anchored in the 3DGS pointmap, which avoids
cumulative scale drift and achieves more precise and robust tracking with fewer
iterations. Additionally, we design a patch-based pointmap dynamic mapping
module, which introduces geometric priors while avoiding scale ambiguity. This
significantly enhances tracking accuracy and the quality of scene
reconstruction, making it particularly suitable for complex outdoor
environments. Our experiments on the Waymo, KITTI, and DL3DV datasets
demonstrate that S3PO-GS achieves state-of-the-art results in novel view
synthesis and outperforms other 3DGS SLAM methods in tracking accuracy. Project
page: https://3dagentworld.github.io/S3PO-GS/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Gentle <span class="highlight-title">Grasp</span>ing Using Vision, Sound, and Touch <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.07926v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.07926v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ken Nakahara, Roberto Calandra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In our daily life, we often encounter objects that are fragile and can be
damaged by excessive grasping force, such as fruits. For these objects, it is
paramount to grasp gently -- not using the maximum amount of force possible,
but rather the minimum amount of force necessary. This paper proposes using
visual, tactile, and auditory signals to learn to grasp and regrasp objects
stably and gently. Specifically, we use audio signals as an indicator of
gentleness during the grasping, and then train an end-to-end action-conditional
model from raw visuo-tactile inputs that predicts both the stability and the
gentleness of future grasping candidates, thus allowing the selection and
execution of the most promising action. Experimental results on a
multi-fingered hand over 1,500 grasping trials demonstrated that our model is
useful for gentle grasping by validating the predictive performance (3.27%
higher accuracy than the vision-only variant) and providing interpretations of
their behavior. Finally, real-world experiments confirmed that the grasping
performance with the trained multi-modal model outperformed other baselines
(17% higher rate for stable and gentle grasps than vision-only). Our approach
requires neither tactile sensor calibration nor analytical force modeling,
drastically reducing the engineering effort to grasp fragile objects. Dataset
and videos are available at https://lasr.org/research/gentle-grasping.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages. Accepted by 2025 IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ External Knowledge Injection for CLIP-Based Class-Incremental Learning <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.08510v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.08510v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Da-Wei Zhou, Kai-Wen Li, Jingyi Ning, Han-Jia Ye, Lijun Zhang, De-Chuan Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-Incremental Learning (CIL) enables learning systems to continuously
adapt to evolving data streams. With the advancement of pre-training,
leveraging pre-trained vision-language models (e.g., CLIP) offers a promising
starting point for CIL. However, CLIP makes decisions by matching visual
embeddings to class names, overlooking the rich contextual information conveyed
through language. For instance, the concept of ``cat'' can be decomposed into
features like tail, fur, and face for recognition. Besides, since the model is
continually updated, these detailed features are overwritten in CIL, requiring
external knowledge for compensation. In this paper, we introduce ExterNal
knowledGe INjEction (ENGINE) for CLIP-based CIL. To enhance knowledge transfer
from outside the dataset, we propose a dual-branch injection tuning framework
that encodes informative knowledge from both visual and textual modalities. The
visual branch is enhanced with data augmentation to enrich the visual features,
while the textual branch leverages GPT-4 to rewrite discriminative descriptors.
In addition to this on-the-fly knowledge injection, we also implement
post-tuning knowledge by re-ranking the prediction results during inference.
With the injected knowledge, the model can better capture informative features
for downstream tasks as data evolves. Extensive experiments demonstrate the
state-of-the-art performance of ENGINE. Code is available at:
https://github.com/LAMDA-CL/ICCV25-ENGINE
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV 2025. Code is available at:
  https://github.com/LAMDA-CL/ICCV25-ENGINE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MC3D-AD: A Unified Geometry-aware Reconstruction Model for
  Multi-category 3D Anomaly <span class="highlight-title">Detection</span> <span class="chip">IJCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.01969v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.01969v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Cheng, Can Gao, Jie Zhou, Jiajun Wen, Tao Dai, Jinbao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Anomaly Detection (AD) is a promising means of controlling the quality of
manufactured products. However, existing methods typically require carefully
training a task-specific model for each category independently, leading to high
cost, low efficiency, and weak generalization. Therefore, this paper presents a
novel unified model for Multi-Category 3D Anomaly Detection (MC3D-AD) that aims
to utilize both local and global geometry-aware information to reconstruct
normal representations of all categories. First, to learn robust and
generalized features of different categories, we propose an adaptive
geometry-aware masked attention module that extracts geometry variation
information to guide mask attention. Then, we introduce a local geometry-aware
encoder reinforced by the improved mask attention to encode group-level feature
tokens. Finally, we design a global query decoder that utilizes point cloud
position embeddings to improve the decoding process and reconstruction ability.
This leads to local and global geometry-aware reconstructed feature tokens for
the AD task. MC3D-AD is evaluated on two publicly available Real3D-AD and
Anomaly-ShapeNet datasets, and exhibits significant superiority over current
state-of-the-art single-category methods, achieving 3.1\% and 9.3\% improvement
in object-level AUROC over Real3D-AD and Anomaly-ShapeNet, respectively. The
code is available at https://github.com/iCAN-SZU/MC3D-AD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages of main text, 3 pages of appendix, accepted to IJCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Diffuse and Disperse: Image Generation with Representation
  Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09027v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09027v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runqian Wang, <span class="highlight-author">Kaiming He</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of diffusion-based generative models over the past decade has
largely proceeded independently of progress in representation learning. These
diffusion models typically rely on regression-based objectives and generally
lack explicit regularization. In this work, we propose \textit{Dispersive
Loss}, a simple plug-and-play regularizer that effectively improves
diffusion-based generative models. Our loss function encourages internal
representations to disperse in the hidden space, analogous to contrastive
self-supervised learning, with the key distinction that it requires no positive
sample pairs and therefore does not interfere with the sampling process used
for regression. Compared to the recent method of representation alignment
(REPA), our approach is self-contained and minimalist, requiring no
pre-training, no additional parameters, and no external data. We evaluate
Dispersive Loss on the ImageNet dataset across a range of models and report
consistent improvements over widely used and strong baselines. We hope our work
will help bridge the gap between generative modeling and representation
learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Robust</span> sensitivity <span class="highlight-title">control</span> in digital pathology via tile score
  distribution matching <span class="chip">MICCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20144v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20144v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arthur Pignet, John Klein, Genevieve Robin, Antoine Olivier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deploying digital pathology models across medical centers is challenging due
to distribution shifts. Recent advances in domain generalization improve model
transferability in terms of aggregated performance measured by the Area Under
Curve (AUC). However, clinical regulations often require to control the
transferability of other metrics, such as prescribed sensitivity levels. We
introduce a novel approach to control the sensitivity of whole slide image
(WSI) classification models, based on optimal transport and Multiple Instance
Learning (MIL). Validated across multiple cohorts and tasks, our method enables
robust sensitivity control with only a handful of calibration samples,
providing a practical solution for reliable deployment of computational
pathology systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera ready version. Accepted at MICCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Adaptive Prompting for Compositional Zero-Shot Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20292v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20292v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyle Stein, Arash Mahyari, Guillermo Francia, Eman El-Sheikh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) have demonstrated impressive multimodal
capabilities in learning joint representations of visual and textual data,
making them powerful tools for tasks such as Compositional Zero-Shot Learning
(CZSL). CZSL requires models to generalize to novel combinations of visual
primitives--such as attributes and objects--that were not explicitly
encountered during training. Recent works in prompting for CZSL have focused on
modifying inputs for the text encoder, often using static prompts that do not
change across varying visual contexts. However, these approaches struggle to
fully capture varying visual contexts, as they focus on text adaptation rather
than leveraging visual features for compositional reasoning. To address this,
we propose a Visual Adaptive Prompting System (VAPS) that leverages a learnable
visual prompt repository and similarity-based retrieval mechanism within the
framework of VLMs to bridge the gap between semantic and visual features. Our
method introduces a dynamic visual prompt repository mechanism that selects the
most relevant attribute and object prompts based on the visual features of the
image. Our proposed system includes a visual prompt adapter that encourages the
model to learn a more generalizable embedding space. Experiments on three CZSL
benchmarks, across both closed and open-world scenarios, demonstrate
state-of-the-art results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Transfer Learning-Based Method for Water Body <span class="highlight-title">Segmentation</span> in Remote
  Sensing Imagery: A Case Study of the Zhada Tulin Area 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.10084v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.10084v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Chen, Xin Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Tibetan Plateau, known as the Asian Water Tower, faces significant water
security challenges due to its high sensitivity to climate change. Advancing
Earth observation for sustainable water monitoring is thus essential for
building climate resilience in this region. This study proposes a two-stage
transfer learning strategy using the SegFormer model to overcome domain shift
and data scarcit--key barriers in developing robust AI for climate-sensitive
applications. After pre-training on a diverse source domain, our model was
fine-tuned for the arid Zhada Tulin area. Experimental results show a
substantial performance boost: the Intersection over Union (IoU) for water body
segmentation surged from 25.50% (direct transfer) to 64.84%. This AI-driven
accuracy is crucial for disaster risk reduction, particularly in monitoring
flash flood-prone systems. More importantly, the high-precision map reveals a
highly concentrated spatial distribution of water, with over 80% of the water
area confined to less than 20% of the river channel length. This quantitative
finding provides crucial evidence for understanding hydrological processes and
designing targeted water management and climate adaptation strategies. Our work
thus demonstrates an effective technical solution for monitoring arid plateau
regions and contributes to advancing AI-powered Earth observation for disaster
preparedness in critical transboundary river headwaters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PLOT-TAL: Prompt Learning with Optimal Transport for Few-Shot Temporal
  Action <span class="highlight-title">Localization</span> <span class="chip">ICCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18915v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18915v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward Fish, Andrew Gilbert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot temporal action localization (TAL) methods that adapt large models
via single-prompt tuning often fail to produce precise temporal boundaries.
This stems from the model learning a non-discriminative mean representation of
an action from sparse data, which compromises generalization. We address this
by proposing a new paradigm based on multi-prompt ensembles, where a set of
diverse, learnable prompts for each action is encouraged to specialize on
compositional sub-events. To enforce this specialization, we introduce
PLOT-TAL, a framework that leverages Optimal Transport (OT) to find a globally
optimal alignment between the prompt ensemble and the video's temporal
features. Our method establishes a new state-of-the-art on the challenging
few-shot benchmarks of THUMOS'14 and EPIC-Kitchens, without requiring complex
meta-learning. The significant performance gains, particularly at high IoU
thresholds, validate our hypothesis and demonstrate the superiority of learning
distributed, compositional representations for precise temporal localization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCVWS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Dynamic</span> <span class="highlight-title">mapping</span> from static labels: remote sensing <span class="highlight-title">dynamic</span> sample
  generation with temporal-spectral embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Yuan, Shuang Chen, Tianwu Lin, Jincheng Yuan, Geng Tian, Yang Xu, Jie Wang, Peng Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate remote sensing geographic mapping requires timely and representative
samples. However, rapid land surface changes often render static samples
obsolete within months, making manual sample updates labor-intensive and
unsustainable. To address this challenge, we propose TasGen, a two-stage
Temporal spectral-aware Automatic Sample Generation method for generating
dynamic training samples from single-date static labels without human
intervention. Land surface dynamics often manifest as anomalies in
temporal-spectral sequences. %These anomalies are multivariate yet unified:
temporal, spectral, or joint anomalies stem from different mechanisms and
cannot be naively coupled, as this may obscure the nature of changes. Yet, any
land surface state corresponds to a coherent temporal-spectral signature, which
would be lost if the two dimensions are modeled separately. To effectively
capture these dynamics, TasGen first disentangles temporal and spectral
features to isolate their individual contributions, and then couples them to
model their synergistic interactions. In the first stage, we introduce a
hierarchical temporal-spectral variational autoencoder (HTS-VAE) with a
dual-dimension embedding to learn low-dimensional latent patterns of normal
samples by first disentangling and then jointly embedding temporal and spectral
information. This temporal-spectral embedding enables robust anomaly detection
by identifying deviations from learned joint patterns. In the second stage, a
classifier trained on stable samples relabels change points across time to
generate dynamic samples. To not only detect but also explain surface dynamics,
we further propose an anomaly interpretation method based on Gibbs sampling,
which attributes changes to specific spectral-temporal dimensions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Faithful, Interpretable Chest X-ray Diagnosis with Anti-Aliased B-cos
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16761v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16761v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcel Kleinmann, Shashank Agnihotri, Margret Keuper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Faithfulness and interpretability are essential for deploying deep neural
networks (DNNs) in safety-critical domains such as medical imaging. B-cos
networks offer a promising solution by replacing standard linear layers with a
weight-input alignment mechanism, producing inherently interpretable,
class-specific explanations without post-hoc methods. While maintaining
diagnostic performance competitive with state-of-the-art DNNs, standard B-cos
models suffer from severe aliasing artifacts in their explanation maps, making
them unsuitable for clinical use where clarity is essential. In this work, we
address these limitations by introducing anti-aliasing strategies using
FLCPooling (FLC) and BlurPool (BP) to significantly improve explanation
quality. Our experiments on chest X-ray datasets demonstrate that the modified
$\text{B-cos}_\text{FLC}$ and $\text{B-cos}_\text{BP}$ preserve strong
predictive performance while providing faithful and artifact-free explanations
suitable for clinical application in multi-class and multi-label settings. Code
available at: GitHub repository (url:
https://github.com/mkleinma/B-cos-medical-paper).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ crossMoDA Challenge: Evolution of Cross-Modality Domain Adaptation
  Techniques for Vestibular Schwannoma and Cochlea <span class="highlight-title">Segmentation</span> from 2021 to
  2023 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12006v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12006v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navodini Wijethilake, Reuben Dorent, Marina Ivory, Aaron Kujawa, Stefan Cornelissen, Patrick Langenhuizen, Mohamed Okasha, Anna Oviedova, Hexin Dong, Bogyeong Kang, Guillaume Sallé, Luyi Han, Ziyuan Zhao, Han Liu, Yubo Fan, Tao Yang, Shahad Hardan, Hussain Alasmawi, Santosh Sanjeev, Yuzhou Zhuang, Satoshi Kondo, Maria Baldeon Calisto, Shaikh Muhammad Uzair Noman, Cancan Chen, Ipek Oguz, Rongguo Zhang, Mina Rezaei, Susana K. Lai-Yuen, Satoshi Kasai, Yunzhi Huang, Chih-Cheng Hung, Mohammad Yaqub, Lisheng Wang, Benoit M. Dawant, Cuntai Guan, Ritse Mann, Vincent Jaouen, Tae-Eui Kam, Li Zhang, Jonathan Shapey, Tom Vercauteren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The cross-Modality Domain Adaptation (crossMoDA) challenge series, initiated
in 2021 in conjunction with the International Conference on Medical Image
Computing and Computer Assisted Intervention (MICCAI), focuses on unsupervised
cross-modality segmentation, learning from contrast-enhanced T1 (ceT1) and
transferring to T2 MRI. The task is an extreme example of domain shift chosen
to serve as a meaningful and illustrative benchmark. From a clinical
application perspective, it aims to automate Vestibular Schwannoma (VS) and
cochlea segmentation on T2 scans for more cost-effective VS management. Over
time, the challenge objectives have evolved to enhance its clinical relevance.
The challenge evolved from using single-institutional data and basic
segmentation in 2021 to incorporating multi-institutional data and Koos grading
in 2022, and by 2023, it included heterogeneous routine data and
sub-segmentation of intra- and extra-meatal tumour components. In this work, we
report the findings of the 2022 and 2023 editions and perform a retrospective
analysis of the challenge progression over the years. The observations from the
successive challenge contributions indicate that the number of outliers
decreases with an expanding dataset. This is notable since the diversity of
scanning protocols of the datasets concurrently increased. The winning approach
of the 2023 edition reduced the number of outliers on the 2021 and 2022 testing
data, demonstrating how increased data heterogeneity can enhance segmentation
performance even on homogeneous data. However, the cochlea Dice score declined
in 2023, likely due to the added complexity from tumour sub-annotations
affecting overall segmentation performance. While progress is still needed for
clinically acceptable VS segmentation, the plateauing performance suggests that
a more challenging cross-modal task may better serve future benchmarking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DEFAME: <span class="highlight-title">Dynamic</span> Evidence-based FAct-checking with Multimodal Experts <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10510v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10510v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Braun, Mark Rothermel, Marcus Rohrbach, Anna Rohrbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of disinformation demands reliable and scalable
fact-checking solutions. We present Dynamic Evidence-based FAct-checking with
Multimodal Experts (DEFAME), a modular, zero-shot MLLM pipeline for
open-domain, text-image claim verification. DEFAME operates in a six-stage
process, dynamically selecting the tools and search depth to extract and
evaluate textual and visual evidence. Unlike prior approaches that are
text-only, lack explainability, or rely solely on parametric knowledge, DEFAME
performs end-to-end verification, accounting for images in claims and evidence
while generating structured, multimodal reports. Evaluation on the popular
benchmarks VERITE, AVerITeC, and MOCHEG shows that DEFAME surpasses all
previous methods, establishing itself as the new state-of-the-art fact-checking
system for uni- and multimodal fact-checking. Moreover, we introduce a new
multimodal benchmark, ClaimReview2024+, featuring claims after the knowledge
cutoff of GPT-4o, avoiding data leakage. Here, DEFAME drastically outperforms
the GPT-4o baselines, showing temporal generalizability and the potential for
real-time fact-checking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025 version. 9 pages main paper, 35 pages with appendix, 18
  figures and 7 tables. Corrected two inconsistent numbers in Table 2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Personalization Toolkit: Training Free Personalization of Large Vision
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02452v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02452v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soroush Seifi, Vaggelis Dorovatas, Daniel Olmeda Reino, Rahaf Aljundi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalization of Large Vision-Language Models (LVLMs) involves customizing
models to recognize specific users and object instances, and to generate
contextually tailored responses. Existing approaches typically rely on
time-consuming test-time training for each user or object, making them
impractical for real-world deployment, a limitation reflected in current
personalization benchmarks, which are focused on object-centric, single-concept
evaluations. In this paper, we present a novel training-free approach to LVLM
personalization and introduce a comprehensive real-world benchmark designed to
rigorously evaluate various aspects of the personalization task. Our method
leverages pre-trained vision foundation models to extract distinctive features,
applies retrieval-augmented generation (RAG) techniques to identify instances
within visual inputs, and employs visual prompting strategies to guide model
outputs. Our model-agnostic vision toolkit enables efficient and flexible
multi-concept personalization across both images and videos, without any
additional training. We achieve state-of-the-art results, surpassing existing
training-based methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Residual Prior-driven Frequency-aware Network for Image <span class="highlight-title">Fusion</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06735v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06735v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guan Zheng, Xue Wang, Wenhua Qian, Peng Liu, Runzhuo Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image fusion aims to integrate complementary information across modalities to
generate high-quality fused images, thereby enhancing the performance of
high-level vision tasks. While global spatial modeling mechanisms show
promising results, constructing long-range feature dependencies in the spatial
domain incurs substantial computational costs. Additionally, the absence of
ground-truth exacerbates the difficulty of capturing complementary features
effectively. To tackle these challenges, we propose a Residual Prior-driven
Frequency-aware Network, termed as RPFNet. Specifically, RPFNet employs a
dual-branch feature extraction framework: the Residual Prior Module (RPM)
extracts modality-specific difference information from residual maps, thereby
providing complementary priors for fusion; the Frequency Domain Fusion Module
(FDFM) achieves efficient global feature modeling and integration through
frequency-domain convolution. Additionally, the Cross Promotion Module (CPM)
enhances the synergistic perception of local details and global structures
through bidirectional feature interaction. During training, we incorporate an
auxiliary decoder and saliency structure loss to strengthen the model's
sensitivity to modality-specific differences. Furthermore, a combination of
adaptive weight-based frequency contrastive loss and SSIM loss effectively
constrains the solution space, facilitating the joint capture of local details
and global features while ensuring the retention of complementary information.
Extensive experiments validate the fusion performance of RPFNet, which
effectively integrates discriminative features, enhances texture details and
salient objects, and can effectively facilitate the deployment of the
high-level vision task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CA-Cut: Crop-Aligned Cutout for Data Augmentation to Learn More <span class="highlight-title">Robust</span>
  Under-Canopy <span class="highlight-title">Navigation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robel Mamo, Taeyeong Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art visual under-canopy navigation methods are designed with
deep learning-based perception models to distinguish traversable space from
crop rows. While these models have demonstrated successful performance, they
require large amounts of training data to ensure reliability in real-world
field deployment. However, data collection is costly, demanding significant
human resources for in-field sampling and annotation. To address this
challenge, various data augmentation techniques are commonly employed during
model training, such as color jittering, Gaussian blur, and horizontal flip, to
diversify training data and enhance model robustness. In this paper, we
hypothesize that utilizing only these augmentation techniques may lead to
suboptimal performance, particularly in complex under-canopy environments with
frequent occlusions, debris, and non-uniform spacing of crops. Instead, we
propose a novel augmentation method, so-called Crop-Aligned Cutout (CA-Cut)
which masks random regions out in input images that are spatially distributed
around crop rows on the sides to encourage trained models to capture high-level
contextual features even when fine-grained information is obstructed. Our
extensive experiments with a public cornfield dataset demonstrate that
masking-based augmentations are effective for simulating occlusions and
significantly improving robustness in semantic keypoint predictions for visual
navigation. In particular, we show that biasing the mask distribution toward
crop rows in CA-Cut is critical for enhancing both prediction accuracy and
generalizability across diverse environments achieving up to a 36.9% reduction
in prediction error. In addition, we conduct ablation studies to determine the
number of masks, the size of each mask, and the spatial distribution of masks
to maximize overall performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 12th European Conference on Mobile
  Robots (ECMR 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Endo<span class="highlight-title">Control</span>Mag: <span class="highlight-title">Robust</span> Endoscopic Vascular Motion Magnification with
  Periodic Reference Resetting and Hierarchical Tissue-aware Dual-Mask <span class="highlight-title">Control</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15292v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15292v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        An Wang, Rulin Zhou, Mengya Xu, Yiru Ye, Longfei Gou, Yiting Chang, Hao Chen, Chwee Ming Lim, Jiankun Wang, Hongliang Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visualizing subtle vascular motions in endoscopic surgery is crucial for
surgical precision and decision-making, yet remains challenging due to the
complex and dynamic nature of surgical scenes. To address this, we introduce
EndoControlMag, a training-free, Lagrangian-based framework with
mask-conditioned vascular motion magnification tailored to endoscopic
environments. Our approach features two key modules: a Periodic Reference
Resetting (PRR) scheme that divides videos into short overlapping clips with
dynamically updated reference frames to prevent error accumulation while
maintaining temporal coherence, and a Hierarchical Tissue-aware Magnification
(HTM) framework with dual-mode mask dilation. HTM first tracks vessel cores
using a pretrained visual tracking model to maintain accurate localization
despite occlusions and view changes. It then applies one of two adaptive
softening strategies to surrounding tissues: motion-based softening that
modulates magnification strength proportional to observed tissue displacement,
or distance-based exponential decay that simulates biomechanical force
attenuation. This dual-mode approach accommodates diverse surgical
scenarios-motion-based softening excels with complex tissue deformations while
distance-based softening provides stability during unreliable optical flow
conditions. We evaluate EndoControlMag on our EndoVMM24 dataset spanning four
different surgery types and various challenging scenarios, including
occlusions, instrument disturbance, view changes, and vessel deformations.
Quantitative metrics, visual assessments, and expert surgeon evaluations
demonstrate that EndoControlMag significantly outperforms existing methods in
both magnification accuracy and visual quality while maintaining robustness
across challenging surgical conditions. The code, dataset, and video results
are available at https://szupc.github.io/EndoControlMag/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Att-Adapter: A <span class="highlight-title">Robust</span> and Precise Domain-Specific Multi-Attributes T2I
  Dif<span class="highlight-title">fusion</span> Adapter via Conditional Variational Autoencoder <span class="chip">ICCV'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.11937v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.11937v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonwoong Cho, Yan-Ying Chen, Matthew Klenk, David I. Inouye, Yanxia Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-Image (T2I) Diffusion Models have achieved remarkable performance in
generating high quality images. However, enabling precise control of continuous
attributes, especially multiple attributes simultaneously, in a new domain
(e.g., numeric values like eye openness or car width) with text-only guidance
remains a significant challenge. To address this, we introduce the Attribute
(Att) Adapter, a novel plug-and-play module designed to enable fine-grained,
multi-attributes control in pretrained diffusion models. Our approach learns a
single control adapter from a set of sample images that can be unpaired and
contain multiple visual attributes. The Att-Adapter leverages the decoupled
cross attention module to naturally harmonize the multiple domain attributes
with text conditioning. We further introduce Conditional Variational
Autoencoder (CVAE) to the Att-Adapter to mitigate overfitting, matching the
diverse nature of the visual world. Evaluations on two public datasets show
that Att-Adapter outperforms all LoRA-based baselines in controlling continuous
attributes. Additionally, our method enables a broader control range and also
improves disentanglement across multiple attributes, surpassing StyleGAN-based
techniques. Notably, Att-Adapter is flexible, requiring no paired synthetic
data for training, and is easily scalable to multiple attributes within a
single model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV'25 (Highlight), The project page is available at
  https://tri-mac.github.io/att-adapter/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Frame Sampling for Video Classification: A Semi-Optimal Policy
  Approach with Reduced Search Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05260v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05260v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junho Lee, Jeongwoo Shin, Seung Woo Ko, Seongsu Ha, Joonseok Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a video with $T$ frames, frame sampling is a task to select $N \ll T$
frames, so as to maximize the performance of a fixed video classifier. Not just
brute-force search, but most existing methods suffer from its vast search space
of $\binom{T}{N}$, especially when $N$ gets large. To address this challenge,
we introduce a novel perspective of reducing the search space from $O(T^N)$ to
$O(T)$. Instead of exploring the entire $O(T^N)$ space, our proposed
semi-optimal policy selects the top $N$ frames based on the independently
estimated value of each frame using per-frame confidence, significantly
reducing the computational complexity. We verify that our semi-optimal policy
can efficiently approximate the optimal policy, particularly under practical
settings. Additionally, through extensive experiments on various datasets and
model architectures, we demonstrate that learning our semi-optimal policy
ensures stable and high performance regardless of the size of $N$ and $T$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision Transformers in Precision Agriculture: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.21706v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.21706v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saber Mehdipour, Seyed Abolghasem Mirroshandel, Seyed Amirhossein Tabatabaei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting plant diseases is a crucial aspect of modern agriculture, as it
plays a key role in maintaining crop health and increasing overall yield.
Traditional approaches, though still valuable, often rely on manual inspection
or conventional machine learning techniques, both of which face limitations in
scalability and accuracy. Recently, Vision Transformers (ViTs) have emerged as
a promising alternative, offering advantages such as improved handling of
long-range dependencies and better scalability for visual tasks. This review
explores the application of ViTs in precision agriculture, covering a range of
tasks. We begin by introducing the foundational architecture of ViTs and
discussing their transition from Natural Language Processing (NLP) to Computer
Vision. The discussion includes the concept of inductive bias in traditional
models like Convolutional Neural Networks (CNNs), and how ViTs mitigate these
biases. We provide a comprehensive review of recent literature, focusing on key
methodologies, datasets, and performance metrics. This study also includes a
comparative analysis of CNNs and ViTs, along with a review of hybrid models and
performance enhancements. Technical challenges such as data requirements,
computational demands, and model interpretability are addressed, along with
potential solutions. Finally, we outline future research directions and
technological advancements that could further support the integration of ViTs
in real-world agricultural settings. Our goal with this study is to offer
practitioners and researchers a deeper understanding of how ViTs are poised to
transform smart and precision agriculture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Swin-TUNA : A Novel PEFT Approach for Accurate Food Image <span class="highlight-title">Segmentation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17347v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17347v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotian Chen, Zhiyong Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of food image processing, efficient semantic segmentation
techniques are crucial for industrial applications. However, existing
large-scale Transformer-based models (such as FoodSAM) face challenges in
meeting practical deploymentrequirements due to their massive parameter counts
and high computational resource demands. This paper introduces TUNable Adapter
module (Swin-TUNA), a Parameter Efficient Fine-Tuning (PEFT) method that
integrates multiscale trainable adapters into the Swin Transformer
architecture, achieving high-performance food image segmentation by updating
only 4% of the parameters. The core innovation of Swin-TUNA lies in its
hierarchical feature adaptation mechanism: it designs separable convolutions in
depth and dimensional mappings of varying scales to address the differences in
features between shallow and deep networks, combined with a dynamic balancing
strategy for tasks-agnostic and task-specific features. Experiments demonstrate
that this method achieves mIoU of 50.56% and 74.94% on the FoodSeg103 and
UECFoodPix Complete datasets, respectively, surpassing the fully parameterized
FoodSAM model while reducing the parameter count by 98.7% (to only 8.13M).
Furthermore, Swin-TUNA exhibits faster convergence and stronger generalization
capabilities in low-data scenarios, providing an efficient solution for
assembling lightweight food image.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>After discussion among the authors, some parts of the paper are
  deemed inappropriate and will be revised and resubmitted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging the Structure of Medical Data for Improved Representation
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02987v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02987v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Agostini, Sonia Laguna, Alain Ryser, Samuel Ruiperez-Campillo, Moritz Vandenhirtz, Nicolas Deperrois, Farhad Nooralahzadeh, Michael Krauthammer, Thomas M. Sutter, Julia E. Vogt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building generalizable medical AI systems requires pretraining strategies
that are data-efficient and domain-aware. Unlike internet-scale corpora,
clinical datasets such as MIMIC-CXR offer limited image counts and scarce
annotations, but exhibit rich internal structure through multi-view imaging. We
propose a self-supervised framework that leverages the inherent structure of
medical datasets. Specifically, we treat paired chest X-rays (i.e., frontal and
lateral views) as natural positive pairs, learning to reconstruct each view
from sparse patches while aligning their latent embeddings. Our method requires
no textual supervision and produces informative representations. Evaluated on
MIMIC-CXR, we show strong performance compared to supervised objectives and
baselines being trained without leveraging structure. This work provides a
lightweight, modality-agnostic blueprint for domain-specific pretraining where
data is structured but scarce
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NSegment : Label-specific Deformations for Remote Sensing Image
  <span class="highlight-title">Segmentation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.19634v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.19634v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yechan Kim, DongHo Yoon, SooYeon Kim, Moongu Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Labeling errors in remote sensing (RS) image segmentation datasets often
remain implicit and subtle due to ambiguous class boundaries, mixed pixels,
shadows, complex terrain features, and subjective annotator bias. Furthermore,
the scarcity of annotated RS data due to high image acquisition and labeling
costs complicates training noise-robust models. While sophisticated mechanisms
such as label selection or noise correction might address this issue, they tend
to increase training time and add implementation complexity. In this letter, we
propose NSegment-a simple yet effective data augmentation solution to mitigate
this issue. Unlike traditional methods, it applies elastic transformations only
to segmentation labels, varying deformation intensity per sample in each
training epoch to address annotation inconsistencies. Experimental results
demonstrate that our approach improves the performance of RS image segmentation
on various state-of-the-art models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper is being revised substantially and will be resubmitted.</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SR-NeRV: Improving Embedding Efficiency of Neural Video Representation
  via Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.00046v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.00046v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taiga Hayami, Kakeru Koizumi, Hiroshi Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit Neural Representations (INRs) have garnered significant attention
for their ability to model complex signals in various domains. Recently,
INR-based frameworks have shown promise in neural video compression by
embedding video content into compact neural networks. However, these methods
often struggle to reconstruct high-frequency details under stringent
constraints on model size, which are critical in practical compression
scenarios. To address this limitation, we propose an INR-based video
representation framework that integrates a general-purpose super-resolution
(SR) network. This design is motivated by the observation that high-frequency
components tend to exhibit low temporal redundancy across frames. By offloading
the reconstruction of fine details to a dedicated SR network pre-trained on
natural images, the proposed method improves visual fidelity. Experimental
results demonstrate that the proposed method outperforms conventional INR-based
baselines in reconstruction quality, while maintaining a comparable model size.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Degradation-Agnostic Statistical Facial Feature Transformation for Blind
  Face Restoration in Adverse Weather Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07464v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07464v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang-Hwan Son
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing deployment of intelligent CCTV systems in outdoor
environments, there is a growing demand for face recognition systems optimized
for challenging weather conditions. Adverse weather significantly degrades
image quality, which in turn reduces recognition accuracy. Although recent face
image restoration (FIR) models based on generative adversarial networks (GANs)
and diffusion models have shown progress, their performance remains limited due
to the lack of dedicated modules that explicitly address weather-induced
degradations. This leads to distorted facial textures and structures. To
address these limitations, we propose a novel GAN-based blind FIR framework
that integrates two key components: local Statistical Facial Feature
Transformation (SFFT) and Degradation-Agnostic Feature Embedding (DAFE). The
local SFFT module enhances facial structure and color fidelity by aligning the
local statistical distributions of low-quality (LQ) facial regions with those
of high-quality (HQ) counterparts. Complementarily, the DAFE module enables
robust statistical facial feature extraction under adverse weather conditions
by aligning LQ and HQ encoder representations, thereby making the restoration
process adaptive to severe weather-induced degradations. Experimental results
demonstrate that the proposed degradation-agnostic SFFT model outperforms
existing state-of-the-art FIR methods based on GAN and diffusion models,
particularly in suppressing texture distortions and accurately reconstructing
facial structures. Furthermore, both the SFFT and DAFE modules are empirically
validated in enhancing structural fidelity and perceptual quality in face
restoration under challenging weather scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I-CEE: Tailoring Explanations of Image Classification Models to User
  Expertise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12102v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12102v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Rong, Peizhu Qian, Vaibhav Unhelkar, Enkelejda Kasneci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effectively explaining decisions of black-box machine learning models is
critical to responsible deployment of AI systems that rely on them. Recognizing
their importance, the field of explainable AI (XAI) provides several techniques
to generate these explanations. Yet, there is relatively little emphasis on the
user (the explainee) in this growing body of work and most XAI techniques
generate "one-size-fits-all" explanations. To bridge this gap and achieve a
step closer towards human-centered XAI, we present I-CEE, a framework that
provides Image Classification Explanations tailored to User Expertise. Informed
by existing work, I-CEE explains the decisions of image classification models
by providing the user with an informative subset of training data (i.e.,
example images), corresponding local explanations, and model decisions.
However, unlike prior work, I-CEE models the informativeness of the example
images to depend on user expertise, resulting in different examples for
different users. We posit that by tailoring the example set to user expertise,
I-CEE can better facilitate users' understanding and simulatability of the
model. To evaluate our approach, we conduct detailed experiments in both
simulation and with human participants (N = 100) on multiple datasets.
Experiments with simulated users show that I-CEE improves users' ability to
accurately predict the model's decisions (simulatability) compared to
baselines, providing promising preliminary results. Experiments with human
participants demonstrate that our method significantly improves user
simulatability accuracy, highlighting the importance of human-centered XAI
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ToonifyGB: StyleGAN-based Gaussian Blendshapes for 3D Stylized Head
  Avatars 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10072v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10072v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui-Yang Ju, Sheng-Yen Huang, Yi-Ping Hung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The introduction of 3D Gaussian blendshapes has enabled the real-time
reconstruction of animatable head avatars from monocular video. Toonify, a
StyleGAN-based method, has become widely used for facial image stylization. To
extend Toonify for synthesizing diverse stylized 3D head avatars using Gaussian
blendshapes, we propose an efficient two-stage framework, ToonifyGB. In Stage 1
(stylized video generation), we adopt an improved StyleGAN to generate the
stylized video from the input video frames, which overcomes the limitation of
cropping aligned faces at a fixed resolution as preprocessing for normal
StyleGAN. This process provides a more stable stylized video, which enables
Gaussian blendshapes to better capture the high-frequency details of the video
frames, facilitating the synthesis of high-quality animations in the next
stage. In Stage 2 (Gaussian blendshapes synthesis), our method learns a
stylized neutral head model and a set of expression blendshapes from the
generated stylized video. By combining the neutral head model with expression
blendshapes, ToonifyGB can efficiently render stylized avatars with arbitrary
expressions. We validate the effectiveness of ToonifyGB on benchmark datasets
using two representative styles: Arcane and Pixar.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Aligning Vision to Language: Annotation-Free Multimodal Knowledge <span class="highlight-title">Graph</span>
  Construction for Enhanced LLMs Reasoning <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12972v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12972v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun<span class="highlight-author">ming Liu</span>, Siyuan Meng, Yanting Gao, Song Mao, Pinlong Cai, Guohang Yan, Yirong Chen, Zilin Bian, Ding Wang, Botian Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal reasoning in Large Language Models (LLMs) struggles with
incomplete knowledge and hallucination artifacts, challenges that textual
Knowledge Graphs (KGs) only partially mitigate due to their modality isolation.
While Multimodal Knowledge Graphs (MMKGs) promise enhanced cross-modal
understanding, their practical construction is impeded by semantic narrowness
of manual text annotations and inherent noise in visual-semantic entity
linkages. In this paper, we propose Vision-align-to-Language integrated
Knowledge Graph (VaLiK), a novel approach for constructing MMKGs that enhances
LLMs reasoning through cross-modal information supplementation. Specifically,
we cascade pre-trained Vision-Language Models (VLMs) to align image features
with text, transforming them into descriptions that encapsulate image-specific
information. Furthermore, we developed a cross-modal similarity verification
mechanism to quantify semantic consistency, effectively filtering out noise
introduced during feature alignment. Even without manually annotated image
captions, the refined descriptions alone suffice to construct the MMKG.
Compared to conventional MMKGs construction paradigms, our approach achieves
substantial storage efficiency gains while maintaining direct entity-to-image
linkage capability. Experimental results on multimodal reasoning tasks
demonstrate that LLMs augmented with VaLiK outperform previous state-of-the-art
models. Our code is published at https://github.com/Wings-Of-Disaster/VaLiK.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures, 6 tables; Accepted to ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VolDoGer: LLM-assisted <span class="highlight-title">Dataset</span>s for Domain Generalization in
  Vision-Language Tasks <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juhwan Choi, Junehyoung Kwon, JungMin Yun, Seunguk Yu, YoungBin Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain generalizability is a crucial aspect of a deep learning model since it
determines the capability of the model to perform well on data from unseen
domains. However, research on the domain generalizability of deep learning
models for vision-language tasks remains limited, primarily because of the lack
of required datasets. To address these challenges, we propose VolDoGer:
Vision-Language Dataset for Domain Generalization, a dedicated dataset designed
for domain generalization that addresses three vision-language tasks: image
captioning, visual question answering, and visual entailment. We constructed
VolDoGer by extending LLM-based data annotation techniques to vision-language
tasks, thereby alleviating the burden of recruiting human annotators. We
evaluated the domain generalizability of various models, ranging from
fine-tuned models to a recent multimodal large language model, through
VolDoGer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025 Workshop on Curated Data for Efficient Learning (CDEL)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17596v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17596v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej K. Wozniak, Lianhang Liu, Yixi Cai, Patric Jensfelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While end-to-end autonomous driving models show promising results, their
practical deployment is often hindered by large model sizes, a reliance on
expensive LiDAR sensors and computationally intensive BEV feature
representations. This limits their scalability, especially for mass-market
vehicles equipped only with cameras. To address these challenges, we propose
PRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving
architecture operates using only camera data, without explicit BEV
representation and forgoing the need for LiDAR. PRIX leverages a visual feature
extractor coupled with a generative planning head to predict safe trajectories
from raw pixel inputs directly. A core component of our architecture is the
Context-aware Recalibration Transformer (CaRT), a novel module designed to
effectively enhance multi-level visual features for more robust planning. We
demonstrate through comprehensive experiments that PRIX achieves
state-of-the-art performance on the NavSim and nuScenes benchmarks, matching
the capabilities of larger, multimodal diffusion planners while being
significantly more efficient in terms of inference speed and model size, making
it a practical solution for real-world deployment. Our work is open-source and
the code will be at https://maxiuw.github.io/prix.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Label Anything: Multi-Class Few-Shot Semantic <span class="highlight-title">Segmentation</span> with Visual
  Prompts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02075v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02075v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pasquale De Marinis, Nicola Fanelli, Raffaele Scaringi, Emanuele Colonna, Giuseppe Fiameni, Gennaro Vessio, Giovanna Castellano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Label Anything, an innovative neural network architecture designed
for few-shot semantic segmentation (FSS) that demonstrates remarkable
generalizability across multiple classes with minimal examples required per
class. Diverging from traditional FSS methods that predominantly rely on masks
for annotating support images, Label Anything introduces varied visual prompts
-- points, bounding boxes, and masks -- thereby enhancing the framework's
versatility and adaptability. Unique to our approach, Label Anything is
engineered for end-to-end training across multi-class FSS scenarios,
efficiently learning from diverse support set configurations without
retraining. This approach enables a "universal" application to various FSS
challenges, ranging from $1$-way $1$-shot to complex $N$-way $K$-shot
configurations while remaining agnostic to the specific number of class
examples. This innovative training strategy reduces computational requirements
and substantially improves the model's adaptability and generalization across
diverse segmentation tasks. Our comprehensive experimental validation,
particularly achieving state-of-the-art results on the COCO-$20^i$ benchmark,
underscores Label Anything's robust generalization and flexibility. The source
code is publicly available at: https://github.com/pasqualedem/LabelAnything.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inversion-DPO: Precise and Efficient Post-Training for Dif<span class="highlight-title">fusion</span> Models <span class="chip">ACM MM25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.11554v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.11554v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zejian Li, Yize Li, Chenye Meng, Zhongni Liu, Yang Ling, Shengyuan Zhang, Guang Yang, Changyuan Yang, Zhiyuan Yang, Lingyun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in diffusion models (DMs) have been propelled by
alignment methods that post-train models to better conform to human
preferences. However, these approaches typically require computation-intensive
training of a base model and a reward model, which not only incurs substantial
computational overhead but may also compromise model accuracy and training
efficiency. To address these limitations, we propose Inversion-DPO, a novel
alignment framework that circumvents reward modeling by reformulating Direct
Preference Optimization (DPO) with DDIM inversion for DMs. Our method conducts
intractable posterior sampling in Diffusion-DPO with the deterministic
inversion from winning and losing samples to noise and thus derive a new
post-training paradigm. This paradigm eliminates the need for auxiliary reward
models or inaccurate appromixation, significantly enhancing both precision and
efficiency of training. We apply Inversion-DPO to a basic task of text-to-image
generation and a challenging task of compositional image generation. Extensive
experiments show substantial performance improvements achieved by Inversion-DPO
compared to existing post-training methods and highlight the ability of the
trained generative models to generate high-fidelity compositionally coherent
images. For the post-training of compostitional image geneation, we curate a
paired dataset consisting of 11,140 images with complex structural annotations
and comprehensive scores, designed to enhance the compositional capabilities of
generative models. Inversion-DPO explores a new avenue for efficient,
high-precision alignment in diffusion models, advancing their applicability to
complex realistic generation tasks. Our code is available at
https://github.com/MIGHTYEZ/Inversion-DPO
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantifying and Narrowing the Unknown: Interactive Text-to-Video
  Retrieval via Uncertainty Minimization <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15504v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15504v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingqing Zhang, Zhuo Cao, Heming Du, Yang Li, Xue Li, Jiajun Liu, Sen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances, Text-to-video retrieval (TVR) is still hindered by
multiple inherent uncertainties, such as ambiguous textual queries, indistinct
text-video mappings, and low-quality video frames. Although interactive systems
have emerged to address these challenges by refining user intent through
clarifying questions, current methods typically rely on heuristic or ad-hoc
strategies without explicitly quantifying these uncertainties, limiting their
effectiveness. Motivated by this gap, we propose UMIVR, an
Uncertainty-Minimizing Interactive Text-to-Video Retrieval framework that
explicitly quantifies three critical uncertainties-text ambiguity, mapping
uncertainty, and frame uncertainty-via principled, training-free metrics:
semantic entropy-based Text Ambiguity Score (TAS), Jensen-Shannon
divergence-based Mapping Uncertainty Score (MUS), and a Temporal Quality-based
Frame Sampler (TQFS). By adaptively generating targeted clarifying questions
guided by these uncertainty measures, UMIVR iteratively refines user queries,
significantly reducing retrieval ambiguity. Extensive experiments on multiple
benchmarks validate UMIVR's effectiveness, achieving notable gains in Recall@1
(69.2\% after 10 interactive rounds) on the MSR-VTT-1k dataset, thereby
establishing an uncertainty-minimizing foundation for interactive TVR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EVEv2: Improved Baselines for Encoder-Free Vision-Language Models <span class="chip">ICCV2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06788v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06788v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiwen Diao, Xiaotong Li, Yufeng Cui, Yueze Wang, Haoge Deng, Ting Pan, Wenxuan Wang, Huchuan Lu, Xinlong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing encoder-free vision-language models (VLMs) are rapidly narrowing the
performance gap with their encoder-based counterparts, highlighting the
promising potential for unified multimodal systems with structural simplicity
and efficient deployment. We systematically clarify the performance gap between
VLMs using pre-trained vision encoders, discrete tokenizers, and minimalist
visual layers from scratch, deeply excavating the under-examined
characteristics of encoder-free VLMs. We develop efficient strategies for
encoder-free VLMs that rival mainstream encoder-based ones. After an in-depth
investigation, we launch EVEv2.0, a new and improved family of encoder-free
VLMs. We show that: (i) Properly decomposing and hierarchically associating
vision and language within a unified model reduces interference between
modalities. (ii) A well-designed training strategy enables effective
optimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0
represents a thorough study for developing a decoder-only architecture across
modalities, demonstrating superior data efficiency and strong vision-reasoning
capability. Code is publicly available at: https://github.com/baaivision/EVE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 10 figures, Accepted by ICCV2025 (highlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Frequency-<span class="highlight-title">Dynamic</span> Attention Modulation for Dense <span class="highlight-title">Prediction</span> <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.12006v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.12006v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linwei Chen, Lin Gu, Ying Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformers (ViTs) have significantly advanced computer vision,
demonstrating strong performance across various tasks. However, the attention
mechanism in ViTs makes each layer function as a low-pass filter, and the
stacked-layer architecture in existing transformers suffers from frequency
vanishing. This leads to the loss of critical details and textures. We propose
a novel, circuit-theory-inspired strategy called Frequency-Dynamic Attention
Modulation (FDAM), which can be easily plugged into ViTs. FDAM directly
modulates the overall frequency response of ViTs and consists of two
techniques: Attention Inversion (AttInv) and Frequency Dynamic Scaling
(FreqScale). Since circuit theory uses low-pass filters as fundamental
elements, we introduce AttInv, a method that generates complementary high-pass
filtering by inverting the low-pass filter in the attention matrix, and
dynamically combining the two. We further design FreqScale to weight different
frequency components for fine-grained adjustments to the target response
function. Through feature similarity analysis and effective rank evaluation, we
demonstrate that our approach avoids representation collapse, leading to
consistent performance improvements across various models, including SegFormer,
DeiT, and MaskDINO. These improvements are evident in tasks such as semantic
segmentation, object detection, and instance segmentation. Additionally, we
apply our method to remote sensing detection, achieving state-of-the-art
results in single-scale settings. The code is available at
https://github.com/Linwei-Chen/FDAM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distilling Dif<span class="highlight-title">fusion</span> Models to Efficient 3D <span class="highlight-title">LiDAR</span> Scene Completion <span class="chip">ICCV'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03515v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03515v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyuan Zhang, An Zhao, Ling Yang, Zejian Li, Chenye Meng, Haoran Xu, Tianrun Chen, AnYang Wei, Perry Pengyun GU, Lingyun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have been applied to 3D LiDAR scene completion due to their
strong training stability and high completion quality. However, the slow
sampling speed limits the practical application of diffusion-based scene
completion models since autonomous vehicles require an efficient perception of
surrounding environments. This paper proposes a novel distillation method
tailored for 3D Li- DAR scene completion models, dubbed ScoreLiDAR, which
achieves efficient yet high-quality scene completion. Score- LiDAR enables the
distilled model to sample in significantly fewer steps after distillation. To
improve completion quality, we also introduce a novel Structural Loss, which
encourages the distilled model to capture the geometric structure of the 3D
LiDAR scene. The loss contains a scene-wise term constraining the holistic
structure and a point-wise term constraining the key landmark points and their
relative configuration. Extensive experiments demonstrate that ScoreLiDAR
significantly accelerates the completion time from 30.55 to 5.37 seconds per
frame (>5x) on SemanticKITTI and achieves superior performance compared to
state-of-the-art 3D LiDAR scene completion models. Our model and code are
publicly available on https: //github.com/happyw1nd/ScoreLiDAR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accept by ICCV'25(Oral), the model and code are
  publicly available on https: //github.com/happyw1nd/ScoreLiDAR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SyncMapV2: <span class="highlight-title">Robust</span> and Adaptive <span class="highlight-title">Unsupervised</span> <span class="highlight-title">Segmentation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16297v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16297v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng Zhang, Zikang Wan, Danilo Vasconcellos Vargas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human vision excels at segmenting visual cues without the need for explicit
training, and it remains remarkably robust even as noise severity increases. In
contrast, existing AI algorithms struggle to maintain accuracy under similar
conditions. Here, we present SyncMapV2, the first to solve unsupervised
segmentation with state-of-the-art robustness. SyncMapV2 exhibits a minimal
drop in mIoU, only 0.01%, under digital corruption, compared to a 23.8% drop
observed in SOTA methods. This superior performance extends across various
types of corruption: noise (7.3% vs. 37.7%), weather (7.5% vs. 33.8%), and blur
(7.0% vs. 29.5%). Notably, SyncMapV2 accomplishes this without any robust
training, supervision, or loss functions. It is based on a learning paradigm
that uses self-organizing dynamical equations combined with concepts from
random networks. Moreover, unlike conventional methods that require
re-initialization for each new input, SyncMapV2 adapts online, mimicking the
continuous adaptability of human vision. Thus, we go beyond the accurate and
robust results, and present the first algorithm that can do all the above
online, adapting to input rather than re-initializing. In adaptability tests,
SyncMapV2 demonstrates near-zero performance degradation, which motivates and
fosters a new generation of robust and adaptive intelligence in the near
future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RadioDUN: A Physics-Inspired Deep Unfolding Network for Radio Map
  <span class="highlight-title">Estimation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08418v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08418v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taiqin Chen, Zikun Zhou, Zheng Fang, Wenzhen Zou, Kangjun Liu, Ke Chen, Yongbing Zhang, Yaowei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The radio map represents the spatial distribution of spectrum resources
within a region, supporting efficient resource allocation and interference
mitigation. However, it is difficult to construct a dense radio map as a
limited number of samples can be measured in practical scenarios. While
existing works have used deep learning to estimate dense radio maps from sparse
samples, they are hard to integrate with the physical characteristics of the
radio map. To address this challenge, we cast radio map estimation as the
sparse signal recovery problem. A physical propagation model is further
incorporated to decompose the problem into multiple factor optimization
sub-problems, thereby reducing recovery complexity. Inspired by the existing
compressive sensing methods, we propose the Radio Deep Unfolding Network
(RadioDUN) to unfold the optimization process, achieving adaptive parameter
adjusting and prior fitting in a learnable manner. To account for the radio
propagation characteristics, we develop a dynamic reweighting module (DRM) to
adaptively model the importance of each factor for the radio map. Inspired by
the shadowing factor in the physical propagation model, we integrate
obstacle-related factors to express the obstacle-induced signal stochastic
decay. The shadowing loss is further designed to constrain the factor
prediction and act as a supplementary supervised objective, which enhances the
performance of RadioDUN. Extensive experiments have been conducted to
demonstrate that the proposed method outperforms the state-of-the-art methods.
Our code will be made publicly available upon publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Robust</span> <span class="highlight-title">Multi-View</span> Learning via Representation <span class="highlight-title">Fusion</span> of Sample-Level
  Attention and Alignment of Simulated Perturbation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04151v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04151v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Xu, Na Zhao, Gang Niu, Masashi Sugiyama, Xiaofeng Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, multi-view learning (MVL) has garnered significant attention due to
its ability to fuse discriminative information from multiple views. However,
real-world multi-view datasets are often heterogeneous and imperfect, which
usually causes MVL methods designed for specific combinations of views to lack
application potential and limits their effectiveness. To address this issue, we
propose a novel robust MVL method (namely RML) with simultaneous representation
fusion and alignment. Specifically, we introduce a simple yet effective
multi-view transformer fusion network where we transform heterogeneous
multi-view data into homogeneous word embeddings, and then integrate multiple
views by the sample-level attention mechanism to obtain a fused representation.
Furthermore, we propose a simulated perturbation based multi-view contrastive
learning framework that dynamically generates the noise and unusable
perturbations for simulating imperfect data conditions. The simulated noisy and
unusable data obtain two distinct fused representations, and we utilize
contrastive learning to align them for learning discriminative and robust
representations. Our RML is self-supervised and can also be applied for
downstream tasks as a regularization. In experiments, we employ it in
multi-view unsupervised clustering, noise-label classification, and as a
plug-and-play module for cross-modal hashing retrieval. Extensive comparison
experiments and ablation studies validate RML's effectiveness. Code is
available at https://github.com/SubmissionsIn/RML.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PF3plat: Pose-Free Feed-Forward 3D Gaussian Splatting <span class="chip">ICML'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22128v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22128v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunghwan Hong, Jaewoo Jung, Heeseong Shin, Jisang Han, Jiaolong Yang, Chong Luo, Seungryong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of novel view synthesis from unposed images in a
single feed-forward. Our framework capitalizes on fast speed, scalability, and
high-quality 3D reconstruction and view synthesis capabilities of 3DGS, where
we further extend it to offer a practical solution that relaxes common
assumptions such as dense image views, accurate camera poses, and substantial
image overlaps. We achieve this through identifying and addressing unique
challenges arising from the use of pixel-aligned 3DGS: misaligned 3D Gaussians
across different views induce noisy or sparse gradients that destabilize
training and hinder convergence, especially when above assumptions are not met.
To mitigate this, we employ pre-trained monocular depth estimation and visual
correspondence models to achieve coarse alignments of 3D Gaussians. We then
introduce lightweight, learnable modules to refine depth and pose estimates
from the coarse alignments, improving the quality of 3D reconstruction and
novel view synthesis. Furthermore, the refined estimates are leveraged to
estimate geometry confidence scores, which assess the reliability of 3D
Gaussian centers and condition the prediction of Gaussian parameters
accordingly. Extensive evaluations on large-scale real-world datasets
demonstrate that PF3plat sets a new state-of-the-art across all benchmarks,
supported by comprehensive ablation studies validating our design choices.
project page: https://cvlab-kaist.github.io/PF3plat/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QR-LoRA: Efficient and Disentangled Fine-tuning via QR Decomposition for
  Customized Generation <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.04599v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.04599v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahui Yang, Yongjia Ma, Donglin Di, Hao Li, Wei Chen, Yan Xie, Jianxun Cui, Xun Yang, Wangmeng Zuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing text-to-image models often rely on parameter fine-tuning techniques
such as Low-Rank Adaptation (LoRA) to customize visual attributes. However,
when combining multiple LoRA models for content-style fusion tasks,
unstructured modifications of weight matrices often lead to undesired feature
entanglement between content and style attributes. We propose QR-LoRA, a novel
fine-tuning framework leveraging QR decomposition for structured parameter
updates that effectively separate visual attributes. Our key insight is that
the orthogonal Q matrix naturally minimizes interference between different
visual features, while the upper triangular R matrix efficiently encodes
attribute-specific transformations. Our approach fixes both Q and R matrices
while only training an additional task-specific $\Delta R$ matrix. This
structured design reduces trainable parameters to half of conventional LoRA
methods and supports effective merging of multiple adaptations without
cross-contamination due to the strong disentanglement properties between
$\Delta R$ matrices. Experiments demonstrate that QR-LoRA achieves superior
disentanglement in content-style fusion tasks, establishing a new paradigm for
parameter-efficient, disentangled fine-tuning in generative models. The project
page is available at: https://luna-ai-lab.github.io/QR-LoRA/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025, 30 pages, 26 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ViLU: Learning Vision-Language Uncertainties for Failure <span class="highlight-title">Prediction</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07620v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07620v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc Lafon, Yannis Karmim, Julio Silva-Rodríguez, Paul Couairon, Clément Rambour, Raphaël Fournier-Sniehotta, Ismail Ben Ayed, Jose Dolz, Nicolas Thome
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable Uncertainty Quantification (UQ) and failure prediction remain open
challenges for Vision-Language Models (VLMs). We introduce ViLU, a new
Vision-Language Uncertainty quantification framework that contextualizes
uncertainty estimates by leveraging all task-relevant textual representations.
ViLU constructs an uncertainty-aware multi-modal representation by integrating
the visual embedding, the predicted textual embedding, and an image-conditioned
textual representation via cross-attention. Unlike traditional UQ methods based
on loss prediction, ViLU trains an uncertainty predictor as a binary classifier
to distinguish correct from incorrect predictions using a weighted binary
cross-entropy loss, making it loss-agnostic. In particular, our proposed
approach is well-suited for post-hoc settings, where only vision and text
embeddings are available without direct access to the model itself. Extensive
experiments on diverse datasets show the significant gains of our method
compared to state-of-the-art failure prediction methods. We apply our method to
standard classification datasets, such as ImageNet-1k, as well as large-scale
image-caption datasets like CC12M and LAION-400M. Ablation studies highlight
the critical role of our architecture and training in achieving effective
uncertainty quantification. Our code is publicly available and can be found
here: https://github.com/ykrmm/ViLU.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Generalize without Bias for Open-Vocabulary Action
  Recognition <span class="chip">ICCV2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20158v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20158v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yating Yu, Congqi Cao, Yifan Zhang, Yanning Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging the effective visual-text alignment and static generalizability
from CLIP, recent video learners adopt CLIP initialization with further
regularization or recombination for generalization in open-vocabulary action
recognition in-context. However, due to the static bias of CLIP, such video
learners tend to overfit on shortcut static features, thereby compromising
their generalizability, especially to novel out-of-context actions. To address
this issue, we introduce Open-MeDe, a novel Meta-optimization framework with
static Debiasing for Open-vocabulary action recognition. From a fresh
perspective of generalization, Open-MeDe adopts a meta-learning approach to
improve known-to-open generalizing and image-to-video debiasing in a
cost-effective manner. Specifically, Open-MeDe introduces a cross-batch
meta-optimization scheme that explicitly encourages video learners to quickly
generalize to arbitrary subsequent data via virtual evaluation, steering a
smoother optimization landscape. In effect, the free of CLIP regularization
during optimization implicitly mitigates the inherent static bias of the video
meta-learner. We further apply self-ensemble over the optimization trajectory
to obtain generic optimal parameters that can achieve robust generalization to
both in-context and out-of-context novel data. Extensive evaluations show that
Open-MeDe not only surpasses state-of-the-art regularization methods tailored
for in-context open-vocabulary action recognition but also substantially excels
in out-of-context scenarios.Code is released at
https://github.com/Mia-YatingYu/Open-MeDe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV2025 (Highlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ L-<span class="highlight-title">FUSION</span>: Laplacian Fetal Ultrasound <span class="highlight-title">Segmentation</span> & Uncertainty
  <span class="highlight-title">Estimation</span> <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.05245v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.05245v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johanna P. Müller, Robert Wright, Thomas G. Day, Lorenzo Venturini, Samuel F. Budd, Hadrien Reynaud, Joseph V. Hajnal, Reza Razavi, Bernhard Kainz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate analysis of prenatal ultrasound (US) is essential for early
detection of developmental anomalies. However, operator dependency and
technical limitations (e.g. intrinsic artefacts and effects, setting errors)
can complicate image interpretation and the assessment of diagnostic
uncertainty. We present L-FUSION (Laplacian Fetal US Segmentation with
Integrated FoundatiON models), a framework that integrates uncertainty
quantification through unsupervised, normative learning and large-scale
foundation models for robust segmentation of fetal structures in normal and
pathological scans. We propose to utilise the aleatoric logit distributions of
Stochastic Segmentation Networks and Laplace approximations with fast Hessian
estimations to estimate epistemic uncertainty only from the segmentation head.
This enables us to achieve reliable abnormality quantification for instant
diagnostic feedback. Combined with an integrated Dropout component, L-FUSION
enables reliable differentiation of lesions from normal fetal anatomy with
enhanced uncertainty maps and segmentation counterfactuals in US imaging. It
improves epistemic and aleatoric uncertainty interpretation and removes the
need for manual disease-labelling. Evaluations across multiple datasets show
that L-FUSION achieves superior segmentation accuracy and consistent
uncertainty quantification, supporting on-site decision-making and offering a
scalable solution for advancing fetal ultrasound analysis in clinical settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MICCAI ASMUS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeGauss: <span class="highlight-title">Dynamic</span>-Static Decomposition with Gaussian Splatting for
  Distractor-free 3D Reconstruction <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.13176v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.13176v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Wang, Quentin Lohmeyer, Mirko Meboldt, Siyu Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing clean, distractor-free 3D scenes from real-world captures
remains a significant challenge, particularly in highly dynamic and cluttered
settings such as egocentric videos. To tackle this problem, we introduce
DeGauss, a simple and robust self-supervised framework for dynamic scene
reconstruction based on a decoupled dynamic-static Gaussian Splatting design.
DeGauss models dynamic elements with foreground Gaussians and static content
with background Gaussians, using a probabilistic mask to coordinate their
composition and enable independent yet complementary optimization. DeGauss
generalizes robustly across a wide range of real-world scenarios, from casual
image collections to long, dynamic egocentric videos, without relying on
complex heuristics or extensive supervision. Experiments on benchmarks
including NeRF-on-the-go, ADT, AEA, Hot3D, and EPIC-Fields demonstrate that
DeGauss consistently outperforms existing methods, establishing a strong
baseline for generalizable, distractor-free 3D reconstructionin highly dynamic,
interaction-rich environments. Project page:
https://batfacewayne.github.io/DeGauss.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Occlusion in FER: A Semantic-Aware Perspective and Go Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15401v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15401v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiyu Zhai, Xingxing Yang, Yalan Ye, Chenyang Li, Bin Fan, Changze Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial expression recognition (FER) is a challenging task due to pervasive
occlusion and dataset biases. Especially when facial information is partially
occluded, existing FER models struggle to extract effective facial features,
leading to inaccurate classifications. In response, we present ORSANet, which
introduces the following three key contributions: First, we introduce auxiliary
multi-modal semantic guidance to disambiguate facial occlusion and learn
high-level semantic knowledge, which is two-fold: 1) we introduce semantic
segmentation maps as dense semantics prior to generate semantics-enhanced
facial representations; 2) we introduce facial landmarks as sparse geometric
prior to mitigate intrinsic noises in FER, such as identity and gender biases.
Second, to facilitate the effective incorporation of these two multi-modal
priors, we customize a Multi-scale Cross-interaction Module (MCM) to adaptively
fuse the landmark feature and semantics-enhanced representations within
different scales. Third, we design a Dynamic Adversarial Repulsion Enhancement
Loss (DARELoss) that dynamically adjusts the margins of ambiguous classes,
further enhancing the model's ability to distinguish similar expressions. We
further construct the first occlusion-oriented FER dataset to facilitate
specialized robustness analysis on various real-world occlusion conditions,
dubbed Occlu-FER. Extensive experiments on both public benchmarks and Occlu-FER
demonstrate that our proposed ORSANet achieves SOTA recognition performance.
Code is publicly available at https://github.com/Wenyuzhy/ORSANet-master.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CutS3D: Cutting Semantics in 3D for 2D <span class="highlight-title">Unsupervised</span> Instance
  <span class="highlight-title">Segmentation</span> <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16319v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16319v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leon Sick, Dominik Engel, Sebastian Hartwig, Pedro Hermosilla, Timo Ropinski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditionally, algorithms that learn to segment object instances in 2D images
have heavily relied on large amounts of human-annotated data. Only recently,
novel approaches have emerged tackling this problem in an unsupervised fashion.
Generally, these approaches first generate pseudo-masks and then train a
class-agnostic detector. While such methods deliver the current state of the
art, they often fail to correctly separate instances overlapping in 2D image
space since only semantics are considered. To tackle this issue, we instead
propose to cut the semantic masks in 3D to obtain the final 2D instances by
utilizing a point cloud representation of the scene. Furthermore, we derive a
Spatial Importance function, which we use to resharpen the semantics along the
3D borders of instances. Nevertheless, these pseudo-masks are still subject to
mask ambiguity. To address this issue, we further propose to augment the
training of a class-agnostic detector with three Spatial Confidence components
aiming to isolate a clean learning signal. With these contributions, our
approach outperforms competing methods across multiple standard benchmarks for
unsupervised instance segmentation and object detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCV 2025. Project Page with Code, Models & Demo:
  https://leonsick.github.io/cuts3d/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One Look is Enough: A Novel Seamless Patchwise Refinement for Zero-Shot
  Monocular Depth <span class="highlight-title">Estimation</span> Models on High-Resolution Images <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.22351v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.22351v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Byeongjun Kwon, Munchurl Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot depth estimation (DE) models exhibit strong generalization
performance as they are trained on large-scale datasets. However, existing
models struggle with high-resolution images due to the discrepancy in image
resolutions of training (with smaller resolutions) and inference (for high
resolutions). Processing them at full resolution leads to decreased estimation
accuracy on depth with tremendous memory consumption, while downsampling to the
training resolution results in blurred edges in the estimated depth images.
Prevailing high-resolution depth estimation methods adopt a patch-based
approach, which introduces depth discontinuity issues when reassembling the
estimated depth patches, resulting in test-time inefficiency. Additionally, to
obtain fine-grained depth details, these methods rely on synthetic datasets due
to the real-world sparse ground truth depth, leading to poor generalizability.
To tackle these limitations, we propose Patch Refine Once (PRO), an efficient
and generalizable tile-based framework. Our PRO consists of two key components:
(i) Grouped Patch Consistency Training that enhances test-time efficiency while
mitigating the depth discontinuity problem by jointly processing four
overlapping patches and enforcing a consistency loss on their overlapping
regions within a single backpropagation step, and (ii) Bias Free Masking that
prevents the DE models from overfitting to dataset-specific biases, enabling
better generalization to real-world datasets even after training on synthetic
data. Zero-shot evaluations on Booster, ETH3D, Middlebury 2014, and NuScenes
demonstrate that our PRO can be seamlessly integrated into existing depth
estimation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025 (camera-ready version). [Project
  page](https://kaist-viclab.github.io/One-Look-is-Enough_site)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cloud gap-filling with deep learning for improved grassland monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09554v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09554v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iason Tsardanidis, Alkiviadis Koukos, Vasileios Sitokonstantinou, Thanassis Drivas, Charalampos Kontoes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uninterrupted optical image time series are crucial for the timely monitoring
of agricultural land changes, particularly in grasslands. However, the
continuity of such time series is often disrupted by clouds. In response to
this challenge, we propose an innovative deep learning method that integrates
cloud-free optical (Sentinel-2) observations and weather-independent
(Sentinel-1) Synthetic Aperture Radar (SAR) data. Our approach employs a hybrid
architecture combining Convolutional Neural Networks (CNNs) and Recurrent
Neural Networks (RNNs) to generate continuous Normalized Difference Vegetation
Index (NDVI) time series, highlighting the role of NDVI in the synergy between
SAR and optical data. We demonstrate the significance of observation continuity
by assessing the impact of the generated NDVI time series on the downstream
task of grassland mowing event detection. We conducted our study in Lithuania,
a country characterized by extensive cloud coverage, and compared our approach
with alternative interpolation techniques (i.e., linear, Akima, quadratic). Our
method outperformed these techniques, achieving an average Mean Absolute Error
(MAE) of 0.024 and a coefficient of determination R^2 of 0.92. Additionally,
our analysis revealed improvement in the performance of the mowing event
detection, with F1-score up to 84% using two widely applied mowing detection
methodologies. Our method also effectively mitigated sudden shifts and noise
originating from cloudy observations, which are often missed by conventional
cloud masks and adversely affect mowing detection precision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Computers and Electronics in Agriculture</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-Shot Skeleton-Based Action Recognition With Prototype-Guided
  Feature Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.00566v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.00566v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Zhou, Shuhai Zhang, Zeng You, Jinwu Hu, Mingkui Tan, Fei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot skeleton-based action recognition aims to classify unseen
skeleton-based human actions without prior exposure to such categories during
training. This task is extremely challenging due to the difficulty in
generalizing from known to unknown actions. Previous studies typically use
two-stage training: pre-training skeleton encoders on seen action categories
using cross-entropy loss and then aligning pre-extracted skeleton and text
features, enabling knowledge transfer to unseen classes through skeleton-text
alignment and language models' generalization. However, their efficacy is
hindered by 1) insufficient discrimination for skeleton features, as the fixed
skeleton encoder fails to capture necessary alignment information for effective
skeleton-text alignment; 2) the neglect of alignment bias between skeleton and
unseen text features during testing. To this end, we propose a prototype-guided
feature alignment paradigm for zero-shot skeleton-based action recognition,
termed PGFA. Specifically, we develop an end-to-end cross-modal contrastive
training framework to improve skeleton-text alignment, ensuring sufficient
discrimination for skeleton features. Additionally, we introduce a
prototype-guided text feature alignment strategy to mitigate the adverse impact
of the distribution discrepancy during testing. We provide a theoretical
analysis to support our prototype-guided text feature alignment strategy and
empirically evaluate our overall PGFA on three well-known datasets. Compared
with the top competitor SMIE method, our PGFA achieves absolute accuracy
improvements of 22.96%, 12.53%, and 18.54% on the NTU-60, NTU-120, and PKU-MMD
datasets, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by IEEE TIP 2025 (The journal version is
  available at https://doi.org/10.1109/TIP.2025.3586487). Code is publicly
  available at https://github.com/kaai520/PGFA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Large Vision-Language Model Meets Large Remote Sensing Imagery:
  Coarse-to-Fine Text-Guided Token Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.07588v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.07588v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junwei Luo, Yingying Zhang, Xue Yang, Kang Wu, Qi Zhu, Lei Liang, Jingdong Chen, Yansheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient vision-language understanding of large Remote Sensing Images (RSIs)
is meaningful but challenging. Current Large Vision-Language Models (LVLMs)
typically employ limited pre-defined grids to process images, leading to
information loss when handling gigapixel RSIs. Conversely, using unlimited
grids significantly increases computational costs. To preserve image details
while reducing computational complexity, we propose a text-guided token pruning
method with Dynamic Image Pyramid (DIP) integration. Our method introduces: (i)
a Region Focus Module (RFM) that leverages text-aware region localization
capability to identify critical vision tokens, and (ii) a coarse-to-fine image
tile selection and vision token pruning strategy based on DIP, which is guided
by RFM outputs and avoids directly processing the entire large imagery.
Additionally, existing benchmarks for evaluating LVLMs' perception ability on
large RSI suffer from limited question diversity and constrained image sizes.
We construct a new benchmark named LRS-VQA, which contains 7,333 QA pairs
across 8 categories, with image length up to 27,328 pixels. Our method
outperforms existing high-resolution strategies on four datasets using the same
data. Moreover, compared to existing token reduction methods, our approach
demonstrates higher efficiency under high-resolution settings. Dataset and code
are in https://github.com/VisionXLab/LRS-VQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures, 18 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LPTR-AFLNet: Lightweight Integrated Chinese License Plate Rectification
  and Recognition Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16362v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16362v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangzhu Xu, Pengcheng Zuo, Zhi Ke, Bangjun Lei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chinese License Plate Recognition (CLPR) faces numerous challenges in
unconstrained and complex environments, particularly due to perspective
distortions caused by various shooting angles and the correction of single-line
and double-line license plates. Considering the limited computational resources
of edge devices, developing a low-complexity, end-to-end integrated network for
both correction and recognition is essential for achieving real-time and
efficient deployment. In this work, we propose a lightweight, unified network
named LPTR-AFLNet for correcting and recognizing Chinese license plates, which
combines a perspective transformation correction module (PTR) with an optimized
license plate recognition network, AFLNet. The network leverages the
recognition output as a weak supervisory signal to effectively guide the
correction process, ensuring accurate perspective distortion correction. To
enhance recognition accuracy, we introduce several improvements to LPRNet,
including an improved attention module to reduce confusion among similar
characters and the use of Focal Loss to address class imbalance during
training. Experimental results demonstrate the exceptional performance of
LPTR-AFLNet in rectifying perspective distortion and recognizing double-line
license plate images, maintaining high recognition accuracy across various
challenging scenarios. Moreover, on lower-mid-range GPUs platform, the method
runs in less than 10 milliseconds, indicating its practical efficiency and
broad applicability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 33 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flash-VStream: Efficient Real-Time Understanding for Long Video Streams <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.23825v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.23825v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoji Zhang, Yiqin Wang, Yansong Tang, Yong Liu, Jiashi Feng, Xiaojie Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Benefiting from the advances in large language models and cross-modal
alignment, existing multimodal large language models have achieved prominent
performance in image and short video understanding. However, the understanding
of long videos is still challenging, as their long-context nature results in
significant computational and memory overhead. Most existing work treats long
videos in the same way as short videos, which is inefficient for real-world
applications and hard to generalize to even longer videos. To address these
issues, we propose Flash-VStream, an efficient video language model capable of
processing extremely long videos and responding to user queries in real time.
Particularly, we design a Flash Memory module, containing a low-capacity
context memory to aggregate long-context temporal information and model the
distribution of information density, and a high-capacity augmentation memory to
retrieve detailed spatial information based on this distribution. Compared to
existing models, Flash-VStream achieves significant reductions in inference
latency. Extensive experiments on long video benchmarks and comprehensive video
benchmarks, i.e., EgoSchema, MLVU, LVBench, MVBench and Video-MME, demonstrate
the state-of-the-art performance and outstanding efficiency of our method. Code
is available at https://github.com/IVGSZ/Flash-VStream.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ X-ray2CTPA: Leveraging Dif<span class="highlight-title">fusion</span> Models to Enhance Pulmonary Embolism
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16109v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16109v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noa Cahan, Eyal Klang, Galit Aviram, Yiftach Barash, Eli Konen, Raja Giryes, Hayit Greenspan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chest X-rays or chest radiography (CXR), commonly used for medical
diagnostics, typically enables limited imaging compared to computed tomography
(CT) scans, which offer more detailed and accurate three-dimensional data,
particularly contrast-enhanced scans like CT Pulmonary Angiography (CTPA).
However, CT scans entail higher costs, greater radiation exposure, and are less
accessible than CXRs. In this work we explore cross-modal translation from a 2D
low contrast-resolution X-ray input to a 3D high contrast and
spatial-resolution CTPA scan. Driven by recent advances in generative AI, we
introduce a novel diffusion-based approach to this task. We evaluate the models
performance using both quantitative metrics and qualitative feedback from
radiologists, ensuring diagnostic relevance of the generated images.
Furthermore, we employ the synthesized 3D images in a classification framework
and show improved AUC in a PE categorization task, using the initial CXR input.
The proposed method is generalizable and capable of performing additional
cross-modality translations in medical imaging. It may pave the way for more
accessible and cost-effective advanced diagnostic tools. The code for this
project is available: https://github.com/NoaCahan/X-ray2CTPA .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint, project code: https://github.com/NoaCahan/X-ray2CTPA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TextCrafter: Accurately Rendering Multiple Texts in Complex Visual
  Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.23461v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.23461v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikai Du, Zhennan Chen, Zhizhou Chen, Shan Gao, Xi Chen, Zhengkai Jiang, Jian Yang, Ying Tai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the task of Complex Visual Text Generation (CVTG), which
centers on generating intricate textual content distributed across diverse
regions within visual images. In CVTG, image generation models often rendering
distorted and blurred visual text or missing some visual text. To tackle these
challenges, we propose TextCrafter, a novel multi-visual text rendering method.
TextCrafter employs a progressive strategy to decompose complex visual text
into distinct components while ensuring robust alignment between textual
content and its visual carrier. Additionally, it incorporates a token focus
enhancement mechanism to amplify the prominence of visual text during the
generation process. TextCrafter effectively addresses key challenges in CVTG
tasks, such as text confusion, omissions, and blurriness. Moreover, we present
a new benchmark dataset, CVTG-2K, tailored to rigorously evaluate the
performance of generative models on CVTG tasks. Extensive experiments
demonstrate that our method surpasses state-of-the-art approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DAA*: Deep Angular A Star for Image-based Path <span class="highlight-title">Planning</span> <span class="chip">ICCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.09305v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.09305v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Path smoothness is often overlooked in path imitation learning from expert
demonstrations. In this paper, we introduce a novel learning method, termed
deep angular A* (DAA*), by incorporating the proposed path angular freedom
(PAF) into A* to improve path similarity through adaptive path smoothness. The
PAF aims to explore the effect of move angles on path node expansion by finding
the trade-off between their minimum and maximum values, allowing for high
adaptiveness for imitation learning. DAA* improves path optimality by closely
aligning with the reference path through joint optimization of path shortening
and smoothing, which correspond to heuristic distance and PAF, respectively.
Throughout comprehensive evaluations on 7 datasets, including 4 maze datasets,
2 video-game datasets, and a real-world drone-view dataset containing 2
scenarios, we demonstrate remarkable improvements of our DAA* over neural A* in
path similarity between the predicted and reference paths with a shorter path
length when the shortest path is plausible, improving by 9.0% SPR, 6.9% ASIM,
and 3.9% PSIM. Furthermore, when jointly learning pathfinding with both path
loss and path probability map loss, DAA* significantly outperforms the
state-of-the-art TransPath by 6.3% SPR, 6.0% PSIM, and 3.7% ASIM. We also
discuss the minor trade-off between path optimality and search efficiency where
applicable. Our code and model weights are available at
https://github.com/zwxu064/DAAStar.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Computer Vision (ICCV), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Orthogonal Constrained Minimization with Tensor $\ell_{2,p}$
  Regularization for HSI Denoising and Destriping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03605v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03605v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxia Liu, Shijie Yu, Jian Lu, Xiaojun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral images~(HSIs) are often contaminated by a mixture of noise such
as Gaussian noise, dead lines, stripes, and so on. In this paper, we propose a
multi-scale low-rank tensor regularized $\ell_{2,p}$ (MLTL2p) approach for HSI
denoising and destriping, which consists of an orthogonal constrained
minimization model and an iterative algorithm with convergence guarantees. The
model of the proposed MLTL2p approach is built based on a new sparsity-enhanced
Multi-scale Low-rank Tensor regularization and a tensor $\ell_{2,p}$ norm with
\(p\in (0,1)\). The multi-scale low-rank regularization for HSI denoising
utilizes the global and local spectral correlation as well as the spatial
nonlocal self-similarity priors of HSIs. The corresponding low-rank constraints
are formulated based on independent higher-order singular value decomposition
with sparsity enhancement on its core tensor to prompt more low-rankness. The
tensor $\ell_{2,p}$ norm for HSI destriping is extended from the matrix
$\ell_{2,p}$ norm. A proximal block coordinate descent algorithm is proposed in
the MLTL2p approach to solve the resulting nonconvex nonsmooth minimization
with orthogonal constraints. We show any accumulation point of the sequence
generated by the proposed algorithm converges to a first-order stationary
point, which is defined using three equalities of substationarity, symmetry,
and feasibility for orthogonal constraints. In the numerical experiments, we
compare the proposed method with state-of-the-art methods including a deep
learning based method, and test the methods on both simulated and real HSI
datasets. Our proposed MLTL2p method demonstrates outperformance in terms of
metrics such as mean peak signal-to-noise ratio as well as visual quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing against Infeasible Inclusions from Data for Semantic
  <span class="highlight-title">Segmentation</span> through Morphology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14672v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14672v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shamik Basu, Luc Van Gool, Christos Sakaridis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art semantic segmentation models are typically optimized in a
data-driven fashion, minimizing solely per-pixel or per-segment classification
objectives on their training data. This purely data-driven paradigm often leads
to absurd segmentations, especially when the domain of input images is shifted
from the one encountered during training. For instance, state-of-the-art models
may assign the label "road" to a segment that is included by another segment
that is respectively labeled as "sky". However, the ground truth of the
existing dataset at hand dictates that such inclusion is not feasible. Our
method, Infeasible Semantic Inclusions (InSeIn), first extracts explicit
inclusion constraints that govern spatial class relations from the semantic
segmentation training set at hand in an offline, data-driven fashion, and then
enforces a morphological yet differentiable loss that penalizes violations of
these constraints during training to promote prediction feasibility. InSeIn is
a light-weight plug-and-play method, constitutes a novel step towards
minimizing infeasible semantic inclusions in the predictions of learned
segmentation models, and yields consistent and significant performance
improvements over diverse state-of-the-art networks across the ADE20K,
Cityscapes, and ACDC datasets. https://github.com/SHAMIK-97/InSeIn/tree/main
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of Deep Learning for Geometry Problem Solving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.11936v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.11936v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianzhe Ma, Wenxuan Wang, Qin Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geometry problem solving is a key area of mathematical reasoning, which is
widely involved in many important fields such as education, mathematical
ability assessment of artificial intelligence, and multimodal ability
assessment. In recent years, the rapid development of deep learning technology,
especially the rise of multimodal large language models, has triggered a
widespread research boom. This paper provides a survey of the applications of
deep learning in geometry problem solving, including (i) a comprehensive
summary of the relevant tasks in geometry problem solving; (ii) a thorough
review of related deep learning methods; (iii) a detailed analysis of
evaluation metrics and methods; and (iv) a critical discussion of the current
challenges and future directions that can be explored. Our goal is to provide a
comprehensive and practical reference of deep learning for geometry problem
solving to promote further developments in this field. We create a continuously
updated list of papers on GitHub: https://github.com/majianz/dl4gps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FUDOKI: Discrete Flow-based Unified Understanding and Generation via
  Kinetic-Optimal Velocities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20147v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20147v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Wang, Yao Lai, Aoxue Li, Shifeng Zhang, Jiacheng Sun, Ning Kang, Chengyue Wu, Zhenguo Li, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid progress of large language models (LLMs) has catalyzed the
emergence of multimodal large language models (MLLMs) that unify visual
understanding and image generation within a single framework. However, most
existing MLLMs rely on autoregressive (AR) architectures, which impose inherent
limitations on future development, such as the raster-scan order in image
generation and restricted reasoning abilities in causal context modeling. In
this work, we challenge the dominance of AR-based approaches by introducing
FUDOKI, a unified multimodal model purely based on discrete flow matching, as
an alternative to conventional AR paradigms. By leveraging metric-induced
probability paths with kinetic optimal velocities, our framework goes beyond
the previous masking-based corruption process, enabling iterative refinement
with self-correction capability and richer bidirectional context integration
during generation. To mitigate the high cost of training from scratch, we
initialize FUDOKI from pre-trained AR-based MLLMs and adaptively transition to
the discrete flow matching paradigm. Experimental results show that FUDOKI
achieves performance comparable to state-of-the-art AR-based MLLMs across both
visual understanding and image generation tasks, highlighting its potential as
a foundation for next-generation unified multimodal models. Furthermore, we
show that applying test-time scaling techniques to FUDOKI yields significant
performance gains, further underscoring its promise for future enhancement
through reinforcement learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Trigger without Trace: Towards Stealthy Backdoor Attack on Text-to-Image
  Dif<span class="highlight-title">fusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17724v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17724v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Zhang, Zhongqi Wang, Shiguang Shan, Xilin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backdoor attacks targeting text-to-image diffusion models have advanced
rapidly. However, current backdoor samples often exhibit two key abnormalities
compared to benign samples: 1) Semantic Consistency, where backdoor prompts
tend to generate images with similar semantic content even with significant
textual variations to the prompts; 2) Attention Consistency, where the trigger
induces consistent structural responses in the cross-attention maps. These
consistencies leave detectable traces for defenders, making backdoors easier to
identify. In this paper, toward stealthy backdoor samples, we propose Trigger
without Trace (TwT) by explicitly mitigating these consistencies. Specifically,
our approach leverages syntactic structures as backdoor triggers to amplify the
sensitivity to textual variations, effectively breaking down the semantic
consistency. Besides, a regularization method based on Kernel Maximum Mean
Discrepancy (KMMD) is proposed to align the distribution of cross-attention
responses between backdoor and benign samples, thereby disrupting attention
consistency. Extensive experiments demonstrate that our method achieves a 97.5%
attack success rate while exhibiting stronger resistance to defenses. It
achieves an average of over 98% backdoor samples bypassing three
state-of-the-art detection mechanisms, revealing the vulnerabilities of current
backdoor defense methods. The code is available at
https://github.com/Robin-WZQ/TwT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ELITE: Enhanced Language-Image Toxicity <span class="highlight-title">Evaluation</span> for Safety <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04757v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04757v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonjun Lee, Doehyeon Lee, Eugene Choi, Sangyoon Yu, Ashkan Yousefpour, Haon Park, Bumsub Ham, Suhyun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current Vision Language Models (VLMs) remain vulnerable to malicious prompts
that induce harmful outputs. Existing safety benchmarks for VLMs primarily rely
on automated evaluation methods, but these methods struggle to detect implicit
harmful content or produce inaccurate evaluations. Therefore, we found that
existing benchmarks have low levels of harmfulness, ambiguous data, and limited
diversity in image-text pair combinations. To address these issues, we propose
the ELITE benchmark, a high-quality safety evaluation benchmark for VLMs,
underpinned by our enhanced evaluation method, the ELITE evaluator. The ELITE
evaluator explicitly incorporates a toxicity score to accurately assess
harmfulness in multimodal contexts, where VLMs often provide specific,
convincing, but unharmful descriptions of images. We filter out ambiguous and
low-quality image-text pairs from existing benchmarks using the ELITE evaluator
and generate diverse combinations of safe and unsafe image-text pairs. Our
experiments demonstrate that the ELITE evaluator achieves superior alignment
with human evaluations compared to prior automated methods, and the ELITE
benchmark offers enhanced benchmark quality and diversity. By introducing
ELITE, we pave the way for safer, more robust VLMs, contributing essential
tools for evaluating and mitigating safety risks in real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025. Project page at https://velpegor.github.io/ELITE/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rectifying Magnitude Neglect in Linear Attention <span class="chip">ICCV2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.00698v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.00698v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qihang Fan, Huaibo Huang, Yuang Ai, ran He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the core operator of Transformers, Softmax Attention exhibits excellent
global modeling capabilities. However, its quadratic complexity limits its
applicability to vision tasks. In contrast, Linear Attention shares a similar
formulation with Softmax Attention while achieving linear complexity, enabling
efficient global information modeling. Nevertheless, Linear Attention suffers
from a significant performance degradation compared to standard Softmax
Attention. In this paper, we analyze the underlying causes of this issue based
on the formulation of Linear Attention. We find that, unlike Softmax Attention,
Linear Attention entirely disregards the magnitude information of the Query.
This prevents the attention score distribution from dynamically adapting as the
Query scales. As a result, despite its structural similarity to Softmax
Attention, Linear Attention exhibits a significantly different attention score
distribution. Based on this observation, we propose Magnitude-Aware Linear
Attention (MALA), which modifies the computation of Linear Attention to fully
incorporate the Query's magnitude. This adjustment allows MALA to generate an
attention score distribution that closely resembles Softmax Attention while
exhibiting a more well-balanced structure. We evaluate the effectiveness of
MALA on multiple tasks, including image classification, object detection,
instance segmentation, semantic segmentation, natural language processing,
speech recognition, and image generation. Our MALA achieves strong results on
all of these tasks. Code will be available at https://github.com/qhfan/MALA
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV2025, highlight paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PolarAnything: Dif<span class="highlight-title">fusion</span>-based Polarimetric Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17268v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17268v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kailong Zhang, Youwei Lyu, Heng Guo, Si Li, Zhanyu Ma, Boxin Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Polarization images facilitate image enhancement and 3D reconstruction tasks,
but the limited accessibility of polarization cameras hinders their broader
application. This gap drives the need for synthesizing photorealistic
polarization images. The existing polarization simulator Mitsuba relies on a
parametric polarization image formation model and requires extensive 3D assets
covering shape and PBR materials, preventing it from generating large-scale
photorealistic images. To address this problem, we propose PolarAnything,
capable of synthesizing polarization images from a single RGB input with both
photorealism and physical accuracy, eliminating the dependency on 3D asset
collections. Drawing inspiration from the zero-shot performance of pretrained
diffusion models, we introduce a diffusion-based generative framework with an
effective representation strategy that preserves the fidelity of polarization
properties. Experiments show that our model generates high-quality polarization
images and supports downstream tasks like shape from polarization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MLRU++: Multiscale Lightweight Residual UNETR++ with Attention for
  Efficient 3D Medical Image <span class="highlight-title">Segmentation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16122v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16122v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nand Kumar Yadav, Rodrigue Rizk, William CW Chen,  KC
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and efficient medical image segmentation is crucial but challenging
due to anatomical variability and high computational demands on volumetric
data. Recent hybrid CNN-Transformer architectures achieve state-of-the-art
results but add significant complexity. In this paper, we propose MLRU++, a
Multiscale Lightweight Residual UNETR++ architecture designed to balance
segmentation accuracy and computational efficiency. It introduces two key
innovations: a Lightweight Channel and Bottleneck Attention Module (LCBAM) that
enhances contextual feature encoding with minimal overhead, and a Multiscale
Bottleneck Block (M2B) in the decoder that captures fine-grained details via
multi-resolution feature aggregation. Experiments on four publicly available
benchmark datasets (Synapse, BTCV, ACDC, and Decathlon Lung) demonstrate that
MLRU++ achieves state-of-the-art performance, with average Dice scores of
87.57% (Synapse), 93.00% (ACDC), and 81.12% (Lung). Compared to existing
leading models, MLRU++ improves Dice scores by 5.38% and 2.12% on Synapse and
ACDC, respectively, while significantly reducing parameter count and
computational cost. Ablation studies evaluating LCBAM and M2B further confirm
the effectiveness of the proposed architectural components. Results suggest
that MLRU++ offers a practical and high-performing solution for 3D medical
image segmentation tasks. Source code is available at:
https://github.com/1027865/MLRUPP
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ History-Guided Video Dif<span class="highlight-title">fusion</span> <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06764v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06764v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kiwhan Song, Boyuan Chen, Max Simchowitz, Yilun Du, Russ Tedrake, Vincent Sitzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classifier-free guidance (CFG) is a key technique for improving conditional
generation in diffusion models, enabling more accurate control while enhancing
sample quality. It is natural to extend this technique to video diffusion,
which generates video conditioned on a variable number of context frames,
collectively referred to as history. However, we find two key challenges to
guiding with variable-length history: architectures that only support
fixed-size conditioning, and the empirical observation that CFG-style history
dropout performs poorly. To address this, we propose the Diffusion Forcing
Transformer (DFoT), a video diffusion architecture and theoretically grounded
training objective that jointly enable conditioning on a flexible number of
history frames. We then introduce History Guidance, a family of guidance
methods uniquely enabled by DFoT. We show that its simplest form, vanilla
history guidance, already significantly improves video generation quality and
temporal consistency. A more advanced method, history guidance across time and
frequency further enhances motion dynamics, enables compositional
generalization to out-of-distribution history, and can stably roll out
extremely long videos. Project website: https://boyuan.space/history-guidance
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025. Project website: https://boyuan.space/history-guidance</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PreMix: Label-Efficient Multiple Instance Learning via Non-Contrastive
  Pre-training and Feature Mixing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01162v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01162v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bryan Wong, Mun Yong Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple instance learning (MIL) has emerged as a powerful framework for
weakly supervised whole slide image (WSI) classification, enabling slide-level
predictions without requiring detailed patch-level annotations. Despite its
success, a critical limitation of current MIL methods lies in the
underutilization of pre-training for the MIL aggregator. Most existing
approaches initialize the aggregator randomly and train it from scratch, making
performance highly sensitive to the quantity of labeled WSIs and ignoring the
abundance of unlabeled WSIs commonly available in clinical settings. To address
this, we propose PreMix, a novel framework that leverages a non-contrastive
pre-training method, Barlow Twins, augmented with the Slide Mixing approach to
generate additional positive pairs and enhance feature learning, particularly
under limited labeled WSI conditions. Fine-tuning with Mixup and Manifold Mixup
further enhances robustness by effectively handling the diverse sizes of
gigapixel WSIs. Experimental results demonstrate that integrating PreMix as a
plug-in module into HIPT yields an average F1 improvement of 4.7% over the
baseline HIPT across various WSI training sizes and datasets. These findings
underscore its potential to advance WSI classification with limited labeled
data and its applicability to real-world histopathology practices. The code is
available at https://github.com/bryanwong17/PreMix
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards a Universal 3D Medical Multi-modality Generalization via
  Learning Personalized Invariant Representation <span class="chip">ICCV25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06106v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06106v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaorui Tan, Xi Yang, Tan Pan, Tianyi Liu, Chen Jiang, Xin Guo, Qiufeng Wang, Anh Nguyen, Yuan Qi, Kaizhu Huang, Yuan Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variations in medical imaging modalities and individual anatomical
differences pose challenges to cross-modality generalization in multi-modal
tasks. Existing methods often concentrate exclusively on common anatomical
patterns, thereby neglecting individual differences and consequently limiting
their generalization performance. This paper emphasizes the critical role of
learning individual-level invariance, i.e., personalized representation
$\mathbb{X}_h$, to enhance multi-modality generalization under both homogeneous
and heterogeneous settings. It reveals that mappings from individual biological
profile to different medical modalities remain static across the population,
which is implied in the personalization process. We propose a two-stage
approach: pre-training with invariant representation $\mathbb{X}_h$ for
personalization, then fine-tuning for diverse downstream tasks. We provide both
theoretical and empirical evidence demonstrating the feasibility and advantages
of personalization, showing that our approach yields greater generalizability
and transferability across diverse multi-modal medical tasks compared to
methods lacking personalization. Extensive experiments further validate that
our approach significantly enhances performance in various generalization
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Benchmark</span>ing Cross-Domain Audio-Visual Deception <span class="highlight-title">Detection</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.06995v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.06995v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaobao Guo, Zitong Yu, Nithish Muthuchamy Selvaraj, Bingquan Shen, Adams Wai-Kin Kong, Alex C. Kot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated deception detection is crucial for assisting humans in accurately
assessing truthfulness and identifying deceptive behavior. Conventional
contact-based techniques, like polygraph devices, rely on physiological signals
to determine the authenticity of an individual's statements. Nevertheless,
recent developments in automated deception detection have demonstrated that
multimodal features derived from both audio and video modalities may outperform
human observers on publicly available datasets. Despite these positive
findings, the generalizability of existing audio-visual deception detection
approaches across different scenarios remains largely unexplored. To close this
gap, we present the first cross-domain audio-visual deception detection
benchmark, that enables us to assess how well these methods generalize for use
in real-world scenarios. We used widely adopted audio and visual features and
different architectures for benchmarking, comparing single-to-single and
multi-to-single domain generalization performance. To further exploit the
impacts using data from multiple source domains for training, we investigate
three types of domain sampling strategies, including domain-simultaneous,
domain-alternating, and domain-by-domain for multi-to-single domain
generalization evaluation. We also propose an algorithm to enhance the
generalization performance by maximizing the gradient inner products between
modality encoders, named ``MM-IDGM". Furthermore, we proposed the
Attention-Mixer fusion method to improve performance, and we believe that this
new cross-domain benchmark will facilitate future research in audio-visual
deception detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advances in 4D Generation: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.14501v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.14501v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiaowei Miao, Kehan Li, Jinsheng Quan, Zhiyuan Min, Shaojie Ma, Yichao Xu, Yi Yang, Ping Liu, Yawei Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative artificial intelligence has recently progressed from static image
and video synthesis to 3D content generation, culminating in the emergence of
4D generation-the task of synthesizing temporally coherent dynamic 3D assets
guided by user input. As a burgeoning research frontier, 4D generation enables
richer interactive and immersive experiences, with applications ranging from
digital humans to autonomous driving. Despite rapid progress, the field lacks a
unified understanding of 4D representations, generative frameworks, basic
paradigms, and the core technical challenges it faces. This survey provides a
systematic and in-depth review of the 4D generation landscape. To
comprehensively characterize 4D generation, we first categorize fundamental 4D
representations and outline associated techniques for 4D generation. We then
present an in-depth analysis of representative generative pipelines based on
conditions and representation methods. Subsequently, we discuss how motion and
geometry priors are integrated into 4D outputs to ensure spatio-temporal
consistency under various control schemes. From an application perspective,
this paper summarizes 4D generation tasks in areas such as dynamic object/scene
generation, digital human synthesis, editable 4D content, and embodied AI.
Furthermore, we summarize and multi-dimensionally compare four basic paradigms
for 4D generation: End-to-End, Generated-Data-Based,
Implicit-Distillation-Based, and Explicit-Supervision-Based. Concluding our
analysis, we highlight five key challenges-consistency, controllability,
diversity, efficiency, and fidelity-and contextualize these with current
approaches.By distilling recent advances and outlining open problems, this work
offers a comprehensive and forward-looking perspective to guide future research
in 4D generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Reinforcing Prototype Evolution with Dual-Knowledge Cooperation for
  <span class="highlight-title">Semi-Supervised</span> <span class="highlight-title">Life</span>long Person Re-Identification <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01884v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01884v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunlun Xu, Fan Zhuo, Jiangmeng Li, Xu Zou, Jiahuan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current lifelong person re-identification (LReID) methods predominantly rely
on fully labeled data streams. However, in real-world scenarios where
annotation resources are limited, a vast amount of unlabeled data coexists with
scarce labeled samples, leading to the Semi-Supervised LReID (Semi-LReID)
problem where LReID methods suffer severe performance degradation. Existing
LReID methods, even when combined with semi-supervised strategies, suffer from
limited long-term adaptation performance due to struggling with the noisy
knowledge occurring during unlabeled data utilization. In this paper, we
pioneer the investigation of Semi-LReID, introducing a novel Self-Reinforcing
Prototype Evolution with Dual-Knowledge Cooperation framework (SPRED). Our key
innovation lies in establishing a self-reinforcing cycle between dynamic
prototype-guided pseudo-label generation and new-old knowledge collaborative
purification to enhance the utilization of unlabeled data. Specifically,
learnable identity prototypes are introduced to dynamically capture the
identity distributions and generate high-quality pseudo-labels. Then, the
dual-knowledge cooperation scheme integrates current model specialization and
historical model generalization, refining noisy pseudo-labels. Through this
cyclic design, reliable pseudo-labels are progressively mined to improve
current-stage learning and ensure positive knowledge propagation over long-term
learning. Experiments on the established Semi-LReID benchmarks show that our
SPRED achieves state-of-the-art performance. Our source code is available at
https://github.com/zhoujiahuan1991/ICCV2025-SPRED
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PARTE: Part-Guided Texturing for 3D Human Reconstruction from a Single
  Image <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17332v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17332v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyeongjin Nam, Donghwan Kim, Gyeongsik Moon, Kyoung Mu Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The misaligned human texture across different human parts is one of the main
limitations of existing 3D human reconstruction methods. Each human part, such
as a jacket or pants, should maintain a distinct texture without blending into
others. The structural coherence of human parts serves as a crucial cue to
infer human textures in the invisible regions of a single image. However, most
existing 3D human reconstruction methods do not explicitly exploit such part
segmentation priors, leading to misaligned textures in their reconstructions.
In this regard, we present PARTE, which utilizes 3D human part information as a
key guide to reconstruct 3D human textures. Our framework comprises two core
components. First, to infer 3D human part information from a single image, we
propose a 3D part segmentation module (PartSegmenter) that initially
reconstructs a textureless human surface and predicts human part labels based
on the textureless surface. Second, to incorporate part information into
texture reconstruction, we introduce a part-guided texturing module
(PartTexturer), which acquires prior knowledge from a pre-trained image
generation network on texture alignment of human parts. Extensive experiments
demonstrate that our framework achieves state-of-the-art quality in 3D human
reconstruction. The project page is available at
https://hygenie1228.github.io/PARTE/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICCV 2025, 22 pages including the supplementary material</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tackling Hallucination from Conditional Models for Medical Image
  Reconstruction with <span class="highlight-title">Dynamic</span>DPS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01075v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01075v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seunghoi Kim, Henry F. J. Tregidgo, Matteo Figini, Chen Jin, Sarang Joshi, Daniel C. Alexander
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucinations are spurious structures not present in the ground truth,
posing a critical challenge in medical image reconstruction, especially for
data-driven conditional models. We hypothesize that combining an unconditional
diffusion model with data consistency, trained on a diverse dataset, can reduce
these hallucinations. Based on this, we propose DynamicDPS, a diffusion-based
framework that integrates conditional and unconditional diffusion models to
enhance low-quality medical images while systematically reducing
hallucinations. Our approach first generates an initial reconstruction using a
conditional model, then refines it with an adaptive diffusion-based inverse
problem solver. DynamicDPS skips early stage in the reverse process by
selecting an optimal starting time point per sample and applies Wolfe's line
search for adaptive step sizes, improving both efficiency and image fidelity.
Using diffusion priors and data consistency, our method effectively reduces
hallucinations from any conditional model output. We validate its effectiveness
in Image Quality Transfer for low-field MRI enhancement. Extensive evaluations
on synthetic and real MR scans, including a downstream task for tissue volume
estimation, show that DynamicDPS reduces hallucinations, improving relative
volume estimation by over 15% for critical tissues while using only 5% of the
sampling steps required by baseline diffusion models. As a model-agnostic and
fine-tuning-free approach, DynamicDPS offers a robust solution for
hallucination reduction in medical imaging. The code will be made publicly
available upon publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI Workflow, External Validation, and Development in Eye Disease
  Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15087v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15087v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyu Chen, Tiarnan D L Keenan, Elvira Agron, Alexis Allot, Emily Guan, Bryant Duong, Amr Elsawy, Benjamin Hou, Cancan Xue, Sanjeeb Bhandari, Geoffrey Broadhead, Chantal Cousineau-Krieger, Ellen Davis, William G Gensheimer, David Grasic, Seema Gupta, Luis Haddock, Eleni Konstantinou, Tania Lamba, Michele Maiberger, Dimosthenis Mantopoulos, Mitul C Mehta, Ayman G Nahri, Mutaz AL-Nawaflh, Arnold Oshinsky, Brittany E Powell, Boonkit Purt, Soo Shin, Hillary Stiefel, Alisa T Thavikulwat, Keith James Wroblewski, Tham Yih Chung, Chui Ming Gemmy Cheung, Ching-Yu Cheng, Emily Y Chew, Michelle R. Hribar, Michael F. Chiang, Zhiyong Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Timely disease diagnosis is challenging due to increasing disease burdens and
limited clinician availability. AI shows promise in diagnosis accuracy but
faces real-world application issues due to insufficient validation in clinical
workflows and diverse populations. This study addresses gaps in medical AI
downstream accountability through a case study on age-related macular
degeneration (AMD) diagnosis and severity classification. We designed and
implemented an AI-assisted diagnostic workflow for AMD, comparing diagnostic
performance with and without AI assistance among 24 clinicians from 12
institutions with real patient data sampled from the Age-Related Eye Disease
Study (AREDS). Additionally, we demonstrated continual enhancement of an
existing AI model by incorporating approximately 40,000 additional medical
images (named AREDS2 dataset). The improved model was then systematically
evaluated using both AREDS and AREDS2 test sets, as well as an external test
set from Singapore. AI assistance markedly enhanced diagnostic accuracy and
classification for 23 out of 24 clinicians, with the average F1-score
increasing by 20% from 37.71 (Manual) to 45.52 (Manual + AI) (P-value <
0.0001), achieving an improvement of over 50% in some cases. In terms of
efficiency, AI assistance reduced diagnostic times for 17 out of the 19
clinicians tracked, with time savings of up to 40%. Furthermore, a model
equipped with continual learning showed robust performance across three
independent datasets, recording a 29% increase in accuracy, and elevating the
F1-score from 42 to 54 in the Singapore population.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in JAMA Network Open,
  doi:10.1001/jamanetworkopen.2025.17204</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Holistic Surgical Scene <span class="highlight-title">Graph</span> <span class="chip">MICCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15541v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15541v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jongmin Shin, Enki Cho, Ka Young Kim, Jung Yong Kim, Seong Tae Kim, Namkee Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical scene understanding is crucial for computer-assisted intervention
systems, requiring visual comprehension of surgical scenes that involves
diverse elements such as surgical tools, anatomical structures, and their
interactions. To effectively represent the complex information in surgical
scenes, graph-based approaches have been explored to structurally model
surgical entities and their relationships. Previous surgical scene graph
studies have demonstrated the feasibility of representing surgical scenes using
graphs. However, certain aspects of surgical scenes-such as diverse
combinations of tool-action-target and the identity of the hand operating the
tool-remain underexplored in graph-based representations, despite their
importance. To incorporate these aspects into graph representations, we propose
Endoscapes-SG201 dataset, which includes annotations for tool-action-target
combinations and hand identity. We also introduce SSG-Com, a graph-based method
designed to learn and represent these critical elements. Through experiments on
downstream tasks such as critical view of safety assessment and action triplet
recognition, we demonstrated the importance of integrating these essential
scene graph components, highlighting their significant contribution to surgical
scene understanding. The code and dataset are available at
https://github.com/ailab-kyunghee/SSG-Com
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to MICCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence <span class="chip" style="font-size: 60%">164</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SIDA: Synthetic Image Driven Zero-shot Domain Adaptation <span class="chip">ACM MM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye-Chan Kim, SeungJu Cha, Si-Woo Kim, Taewhan Kim, Dong-Jin Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot domain adaptation is a method for adapting a model to a target
domain without utilizing target domain image data. To enable adaptation without
target images, existing studies utilize CLIP's embedding space and text
description to simulate target-like style features. Despite the previous
achievements in zero-shot domain adaptation, we observe that these text-driven
methods struggle to capture complex real-world variations and significantly
increase adaptation time due to their alignment process. Instead of relying on
text descriptions, we explore solutions leveraging image data, which provides
diverse and more fine-grained style cues. In this work, we propose SIDA, a
novel and efficient zero-shot domain adaptation method leveraging synthetic
images. To generate synthetic images, we first create detailed, source-like
images and apply image translation to reflect the style of the target domain.
We then utilize the style features of these synthetic images as a proxy for the
target domain. Based on these features, we introduce Domain Mix and Patch Style
Transfer modules, which enable effective modeling of real-world variations. In
particular, Domain Mix blends multiple styles to expand the intra-domain
representations, and Patch Style Transfer assigns different styles to
individual patches. We demonstrate the effectiveness of our method by showing
state-of-the-art performance in diverse zero-shot adaptation scenarios,
particularly in challenging domains. Moreover, our approach achieves high
efficiency by significantly reducing the overall adaptation time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM MM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Software Synthesis Guided by Constraint-Expressive Intermediate
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuqing Li, Anson Y. Lam, Yun Peng, Wenxuan Wang, Michael R. Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphical user interface (UI) software has undergone a fundamental
transformation from traditional two-dimensional (2D) desktop/web/mobile
interfaces to spatial three-dimensional (3D) environments. While existing work
has made remarkable success in automated 2D software generation, such as
HTML/CSS and mobile app interface code synthesis, the generation of 3D software
still remains under-explored. Current methods for 3D software generation
usually generate the 3D environments as a whole and cannot modify or control
specific elements in the software. Furthermore, these methods struggle to
handle the complex spatial and semantic constraints inherent in the real world.
To address the challenges, we present Scenethesis, a novel
requirement-sensitive 3D software synthesis approach that maintains formal
traceability between user specifications and generated 3D software. Scenethesis
is built upon ScenethesisLang, a domain-specific language that serves as a
granular constraint-aware intermediate representation (IR) to bridge natural
language requirements and executable 3D software. It serves both as a
comprehensive scene description language enabling fine-grained modification of
3D software elements and as a formal constraint-expressive specification
language capable of expressing complex spatial constraints. By decomposing 3D
software synthesis into stages operating on ScenethesisLang, Scenethesis
enables independent verification, targeted modification, and systematic
constraint satisfaction. Our evaluation demonstrates that Scenethesis
accurately captures over 80% of user requirements and satisfies more than 90%
of hard constraints while handling over 100 constraints simultaneously.
Furthermore, Scenethesis achieves a 42.8% improvement in BLIP-2 visual
evaluation scores compared to the state-of-the-art method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Moving Out: Physically-grounded Human-AI Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuhui Kang, Sung-Wook Lee, Haolin Liu, Yuyan Wang, Yen-Ling Kuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to adapt to physical actions and constraints in an environment is
crucial for embodied agents (e.g., robots) to effectively collaborate with
humans. Such physically grounded human-AI collaboration must account for the
increased complexity of the continuous state-action space and constrained
dynamics caused by physical constraints. In this paper, we introduce
\textit{Moving Out}, a new human-AI collaboration benchmark that resembles a
wide range of collaboration modes affected by physical attributes and
constraints, such as moving heavy items together and maintaining consistent
actions to move a big item around a corner. Using Moving Out, we designed two
tasks and collected human-human interaction data to evaluate models' abilities
to adapt to diverse human behaviors and unseen physical attributes. To address
the challenges in physical environments, we propose a novel method, BASS
(Behavior Augmentation, Simulation, and Selection), to enhance the diversity of
agents and their understanding of the outcome of actions. Our experiments show
that BASS outperforms state-of-the-art models in AI-AI and human-AI
collaboration. The project page is available at
\href{https://live-robotics-uva.github.io/movingout_ai/}{https://live-robotics-uva.github.io/movingout\_ai/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SynC: Synthetic Image Caption <span class="highlight-title">Dataset</span> Refinement with One-to-many
  <span class="highlight-title">Mapping</span> for Zero-shot Image Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Si-Woo Kim, MinJu Jeon, Ye-Chan Kim, Soeun Lee, Taewhan Kim, Dong-Jin Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot Image Captioning (ZIC) increasingly utilizes synthetic datasets
generated by text-to-image (T2I) models to mitigate the need for costly manual
annotation. However, these T2I models often produce images that exhibit
semantic misalignments with their corresponding input captions (e.g., missing
objects, incorrect attributes), resulting in noisy synthetic image-caption
pairs that can hinder model training. Existing dataset pruning techniques are
largely designed for removing noisy text in web-crawled data. However, these
methods are ill-suited for the distinct challenges of synthetic data, where
captions are typically well-formed, but images may be inaccurate
representations. To address this gap, we introduce SynC, a novel framework
specifically designed to refine synthetic image-caption datasets for ZIC.
Instead of conventional filtering or regeneration, SynC focuses on reassigning
captions to the most semantically aligned images already present within the
synthetic image pool. Our approach employs a one-to-many mapping strategy by
initially retrieving multiple relevant candidate images for each caption. We
then apply a cycle-consistency-inspired alignment scorer that selects the best
image by verifying its ability to retrieve the original caption via
image-to-text retrieval. Extensive evaluations demonstrate that SynC
consistently and significantly improves performance across various ZIC models
on standard benchmarks (MS-COCO, Flickr30k, NoCaps), achieving state-of-the-art
results in several scenarios. SynC offers an effective strategy for curating
refined synthetic data to enhance ZIC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM Multimedia 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approximate SMT Counting Beyond Discrete Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arijit Shaw, Kuldeep S. Meel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Satisfiability Modulo Theory (SMT) solvers have advanced automated reasoning,
solving complex formulas across discrete and continuous domains. Recent
progress in propositional model counting motivates extending SMT capabilities
toward model counting, especially for hybrid SMT formulas. Existing approaches,
like bit-blasting, are limited to discrete variables, highlighting the
challenge of counting solutions projected onto the discrete domain in hybrid
formulas.
  We introduce pact, an SMT model counter for hybrid formulas that uses
hashing-based approximate model counting to estimate solutions with theoretical
guarantees. pact makes a logarithmic number of SMT solver calls relative to the
projection variables, leveraging optimized hash functions. pact achieves
significant performance improvements over baselines on a large suite of
benchmarks. In particular, out of 14,202 instances, pact successfully finished
on 603 instances, while Baseline could only finish on 13 instances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in the proceedings of Design Automation Conference
  (DAC) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DRWKV: Focusing on Object Edges for Low-Light Image Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuecheng Bai, Yuxiang Wang, Boyu Hu, Qinyuan Jie, Chuanzhi Xu, Hongru Xiao, Kechen Li, Vera Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-light image enhancement remains a challenging task, particularly in
preserving object edge continuity and fine structural details under extreme
illumination degradation. In this paper, we propose a novel model, DRWKV
(Detailed Receptance Weighted Key Value), which integrates our proposed Global
Edge Retinex (GER) theory, enabling effective decoupling of illumination and
edge structures for enhanced edge fidelity. Secondly, we introduce Evolving WKV
Attention, a spiral-scanning mechanism that captures spatial edge continuity
and models irregular structures more effectively. Thirdly, we design the
Bilateral Spectrum Aligner (Bi-SAB) and a tailored MS2-Loss to jointly align
luminance and chrominance features, improving visual naturalness and mitigating
artifacts. Extensive experiments on five LLIE benchmarks demonstrate that DRWKV
achieves leading performance in PSNR, SSIM, and NIQE while maintaining low
computational complexity. Furthermore, DRWKV enhances downstream performance in
low-light multi-object tracking tasks, validating its generalization
capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Foundation Model for Massive MIMO Precoding with an Adaptive per-User
  Rate-Power Tradeoff 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jérôme Emery, Ali Hasanzadeh Karkan, Jean-François Frigon, François Leduc-Primeau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning (DL) has emerged as a solution for precoding in massive
multiple-input multiple-output (mMIMO) systems due to its capacity to learn the
characteristics of the propagation environment. However, training such a model
requires high-quality, local datasets at the deployment site, which are often
difficult to collect. We propose a transformer-based foundation model for mMIMO
precoding that seeks to minimize the energy consumption of the transmitter
while dynamically adapting to per-user rate requirements. At equal energy
consumption, zero-shot deployment of the proposed foundation model
significantly outperforms zero forcing, and approaches weighted minimum mean
squared error performance with 8x less complexity. To address model adaptation
in data-scarce settings, we introduce a data augmentation method that finds
training samples similar to the target distribution by computing the cosine
similarity between the outputs of the pre-trained feature extractor. Our work
enables the implementation of DL-based solutions in practice by addressing
challenges of data availability and training complexity. Moreover, the ability
to dynamically configure per-user rate requirements can be leveraged by higher
level resource allocation and scheduling algorithms for greater control over
energy efficiency, spectral efficiency and fairness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures. Accepted to the IEEE International Symposium on
  Personal, Indoor and Mobile Radio Communications (PIMRC) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance
  Data Synthesis for Specialist LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18584v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18584v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaopeng Ke, Hexuan Deng, Xuebo Liu, Jun Rao, Zhenxi Song, Jun Yu, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the impressive performance of large language models (LLMs) in general
domains, they often underperform in specialized domains. Existing approaches
typically rely on data synthesis methods and yield promising results by using
unlabeled data to capture domain-specific features. However, these methods
either incur high computational costs or suffer from performance limitations,
while also demonstrating insufficient generalization across different tasks. To
address these challenges, we propose AQuilt, a framework for constructing
instruction-tuning data for any specialized domains from corresponding
unlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic,
and Task type. By incorporating logic and inspection, we encourage reasoning
processes and self-inspection to enhance model performance. Moreover,
customizable task instructions enable high-quality data generation for any
task. As a result, we construct a dataset of 703k examples to train a powerful
data synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3
while utilizing just 17% of the production cost. Further analysis demonstrates
that our generated data exhibits higher relevance to downstream tasks. Source
code, models, and scripts are available at https://github.com/Krueske/AQuilt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DR.EHR: Dense Retrieval for Electronic Health Record with Knowledge
  Injection and Synthetic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyun Zhao, Huaiyuan Ying, Yue Zhong, Sheng Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic Health Records (EHRs) are pivotal in clinical practices, yet their
retrieval remains a challenge mainly due to semantic gap issues. Recent
advancements in dense retrieval offer promising solutions but existing models,
both general-domain and biomedical-domain, fall short due to insufficient
medical knowledge or mismatched training corpora. This paper introduces
\texttt{DR.EHR}, a series of dense retrieval models specifically tailored for
EHR retrieval. We propose a two-stage training pipeline utilizing MIMIC-IV
discharge summaries to address the need for extensive medical knowledge and
large-scale training data. The first stage involves medical entity extraction
and knowledge injection from a biomedical knowledge graph, while the second
stage employs large language models to generate diverse training data. We train
two variants of \texttt{DR.EHR}, with 110M and 7B parameters, respectively.
Evaluated on the CliniQ benchmark, our models significantly outperforms all
existing dense retrievers, achieving state-of-the-art results. Detailed
analyses confirm our models' superiority across various match and query types,
particularly in challenging semantic matches like implication and abbreviation.
Ablation studies validate the effectiveness of each pipeline component, and
supplementary experiments on EHR QA datasets demonstrate the models'
generalizability on natural language questions, including complex ones with
multiple entities. This work significantly advances EHR retrieval, offering a
robust solution for clinical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Model and code released upon acceptance</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SafeWork-R1: Coevolving Safety and Intelligence under the
  AI-45$^{\circ}$ Law 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18576v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18576v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanghai AI Lab,  :, Yicheng Bao, Guanxu Chen, Mingkang Chen, Yunhao Chen, Chiyu Chen, Lingjie Chen, Sirui Chen, Xinquan Chen, Jie Cheng, Yu Cheng, Dengke Deng, Yizhuo Ding, Dan Ding, Xiaoshan Ding, Yi Ding, Zhichen Dong, Lingxiao Du, Yuyu Fan, Xinshun Feng, Yanwei Fu, Yuxuan Gao, Ruijun Ge, Tianle Gu, Lujun Gui, Jiaxuan Guo, Qianxi He, Yuenan Hou, Xuhao Hu, Hong Huang, Kaichen Huang, Shiyang Huang, Yuxian Jiang, Shanzhe Lei, Jie Li, Lijun Li, Hao Li, Juncheng Li, Xiangtian Li, Yafu Li, Lingyu Li, Xueyan Li, Haotian Liang, Dongrui Liu, Qihua Liu, Zhixuan Liu, Bangwei Liu, Huacan Liu, Yuexiao Liu, Zongkai Liu, Chaochao Lu, Yudong Lu, Xiaoya Lu, Zhenghao Lu, Qitan Lv, Caoyuan Ma, Jiachen Ma, Xiaoya Ma, Zhongtian Ma, Lingyu Meng, Ziqi Miao, Yazhe Niu, Yuezhang Peng, Yuan Pu, Han Qi, Chen Qian, Xingge Qiao, Jingjing Qu, Jiashu Qu, Wanying Qu, Wenwen Qu, Xiaoye Qu, Qihan Ren, Qingnan Ren, Qingyu Ren, Jing Shao, Wenqi Shao, Shuai Shao, Dongxing Shi, Xin Song, Xinhao Song, Yan Teng, Xuan Tong, Yingchun Wang, Xuhong Wang, Shujie Wang, Xin Wang, Yige Wang, Yixu Wang, Yuanfu Wang, Futing Wang, Ruofan Wang, Wenjie Wang, Yajie Wang, Muhao Wei, Xiaoyu Wen, Fenghua Weng, Yuqi Wu, Yingtong Xiong, Xingcheng Xu, Chao Yang, Yue Yang, Yang Yao, Yulei Ye, Zhenyun Yin, Yi Yu, Bo Zhang, Qiaosheng Zhang, Jinxuan Zhang, Yexin Zhang, Yinqiang Zheng, Hefeng Zhou, Zhanhui Zhou, Pengyu Zhu, Qingzi Zhu, Yubo Zhu, Bowen Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that
demonstrates the coevolution of capabilities and safety. It is developed by our
proposed SafeLadder framework, which incorporates large-scale, progressive,
safety-oriented reinforcement learning post-training, supported by a suite of
multi-principled verifiers. Unlike previous alignment methods such as RLHF that
simply learn human preferences, SafeLadder enables SafeWork-R1 to develop
intrinsic safety reasoning and self-reflection abilities, giving rise to safety
`aha' moments. Notably, SafeWork-R1 achieves an average improvement of
$46.54\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks
without compromising general capabilities, and delivers state-of-the-art safety
performance compared to leading proprietary models such as GPT-4.1 and Claude
Opus 4. To further bolster its reliability, we implement two distinct
inference-time intervention methods and a deliberative search mechanism,
enforcing step-level verification. Finally, we further develop
SafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and
SafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and
capability can co-evolve synergistically, highlighting the generalizability of
our framework in building robust, reliable, and trustworthy general-purpose AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>47 pages, 18 figures, authors are listed in alphabetical order by
  their last names</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PosterMate: Audience-driven Collaborative Persona Agents for Poster
  Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghoon Shin, Daniel Lee, Gary Hsieh, Gromit Yeuk-Yin Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Poster designing can benefit from synchronous feedback from target audiences.
However, gathering audiences with diverse perspectives and reconciling them on
design edits can be challenging. Recent generative AI models present
opportunities to simulate human-like interactions, but it is unclear how they
may be used for feedback processes in design. We introduce PosterMate, a poster
design assistant that facilitates collaboration by creating audience-driven
persona agents constructed from marketing documents. PosterMate gathers
feedback from each persona agent regarding poster components, and stimulates
discussion with the help of a moderator to reach a conclusion. These
agreed-upon edits can then be directly integrated into the poster design.
Through our user study (N=12), we identified the potential of PosterMate to
capture overlooked viewpoints, while serving as an effective prototyping tool.
Additionally, our controlled online evaluation (N=100) revealed that the
feedback from an individual persona agent is appropriate given its persona
identity, and the discussion effectively synthesizes the different persona
agents' perspectives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proceedings 19th International Workshop on the ACL2 Theorem Prover and
  Its Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18567v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18567v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruben Gamboa, Panagiotis Manolios
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ACL2 Workshop series is the major technical forum for users of the ACL2
theorem proving system to present research related to the ACL2 theorem prover
and its applications. ACL2 is an industrial-strength automated reasoning
system, the latest in the Boyer-Moore family of theorem provers. The 2005 ACM
Software System Award was awarded to Boyer, Kaufmann, and Moore for their work
on ACL2 and the other theorem provers in the Boyer-Moore family.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GIIFT: <span class="highlight-title">Graph</span>-guided Inductive Image-free Multimodal Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiafeng Xiong, Yuting Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Machine Translation (MMT) has demonstrated the significant help of
visual information in machine translation. However, existing MMT methods face
challenges in leveraging the modality gap by enforcing rigid visual-linguistic
alignment whilst being confined to inference within their trained multimodal
domains. In this work, we construct novel multimodal scene graphs to preserve
and integrate modality-specific information and introduce GIIFT, a two-stage
Graph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph
Attention Network adapter to learn multimodal knowledge in a unified fused
space and inductively generalize it to broader image-free translation domains.
Experimental results on the Multi30K dataset of English-to-French and
English-to-German tasks demonstrate that our GIIFT surpasses existing
approaches and achieves the state-of-the-art, even without images during
inference. Results on the WMT benchmark show significant improvements over the
image-free translation baselines, demonstrating the strength of GIIFT towards
inductive image-free inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Internal Data: Constructing Complete <span class="highlight-title">Dataset</span>s for Fairness
  Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Varsha Ramineni, Hossein A. Rahmani, Emine Yilmaz, David Barber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As AI becomes prevalent in high-risk domains and decision-making, it is
essential to test for potential harms and biases. This urgency is reflected by
the global emergence of AI regulations that emphasise fairness and adequate
testing, with some mandating independent bias audits. However, procuring the
necessary data for fairness testing remains a significant challenge.
Particularly in industry settings, legal and privacy concerns restrict the
collection of demographic data required to assess group disparities, and
auditors face practical and cultural challenges in gaining access to data.
Further, internal historical datasets are often insufficiently representative
to identify real-world biases. This work focuses on evaluating classifier
fairness when complete datasets including demographics are inaccessible. We
propose leveraging separate overlapping datasets to construct complete
synthetic data that includes demographic information and accurately reflects
the underlying relationships between protected attributes and model features.
We validate the fidelity of the synthetic data by comparing it to real data,
and empirically demonstrate that fairness metrics derived from testing on such
synthetic data are consistent with those obtained from real data. This work,
therefore, offers a path to overcome real-world data scarcity for fairness
testing, enabling independent, model-agnostic evaluation of fairness, and
serving as a viable substitute where real data is limited.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HARLF: Hierarchical <span class="highlight-title">Reinforcement</span> Learning and Lightweight LLM-Driven
  Sentiment Integration for Financial Portfolio <span class="highlight-title">Optimization</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Coriat, Eric Benhamou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel hierarchical framework for portfolio
optimization, integrating lightweight Large Language Models (LLMs) with Deep
Reinforcement Learning (DRL) to combine sentiment signals from financial news
with traditional market indicators. Our three-tier architecture employs base RL
agents to process hybrid data, meta-agents to aggregate their decisions, and a
super-agent to merge decisions based on market data and sentiment analysis.
Evaluated on data from 2018 to 2024, after training on 2000-2017, the framework
achieves a 26% annualized return and a Sharpe ratio of 1.2, outperforming
equal-weighted and S&P 500 benchmarks. Key contributions include scalable
cross-modal integration, a hierarchical RL structure for enhanced stability,
and open-source reproducibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VideoMind: An Omni-Modal Video <span class="highlight-title">Dataset</span> with Intent Grounding for
  Deep-Cognitive Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baoyao Yang, Wanyun Li, Dixin Chen, Junxiang Chen, Wenbin Yao, Haifeng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces VideoMind, a video-centric omni-modal dataset designed
for deep video content cognition and enhanced multi-modal feature
representation. The dataset comprises 103K video samples (3K reserved for
testing), each paired with audio and systematically detailed textual
descriptions. Specifically, every video and its audio is described across three
hierarchical layers (factual, abstract, and intent), progressing from surface
to depth. It contains over 22 million words, averaging ~225 words per sample.
VideoMind's key distinction from existing datasets is its provision of intent
expressions, which require contextual integration across the entire video and
are not directly observable. These deep-cognitive expressions are generated
using a Chain-of-Thought (COT) approach, prompting the mLLM through
step-by-step reasoning. Each description includes annotations for subject,
place, time, event, action, and intent, supporting downstream recognition
tasks. Crucially, we establish a gold-standard benchmark with 3,000 manually
validated samples for evaluating deep-cognitive video understanding. We design
hybrid-cognitive retrieval experiments, scored by multi-level retrieval
metrics, to appropriately assess deep video comprehension. Evaluation results
for models (e.g., InternVideo, VAST, UMT-L) are released. VideoMind serves as a
powerful benchmark for fine-grained cross-modal alignment and advances fields
requiring in-depth video understanding, such as emotion and intent recognition.
The data is publicly available on GitHub, HuggingFace, and OpenDataLab,
https://github.com/cdx-cindy/VideoMind.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages; 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Performance of Concept Probing: The Influence of the Data
  (Extended Version) <span class="chip">ECAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel de Sousa Ribeiro, Afonso Leote, João Leite
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept probing has recently garnered increasing interest as a way to help
interpret artificial neural networks, dealing both with their typically large
size and their subsymbolic nature, which ultimately renders them unfeasible for
direct human interpretation. Concept probing works by training additional
classifiers to map the internal representations of a model into human-defined
concepts of interest, thus allowing humans to peek inside artificial neural
networks. Research on concept probing has mainly focused on the model being
probed or the probing model itself, paying limited attention to the data
required to train such probing models. In this paper, we address this gap.
Focusing on concept probing in the context of image classification tasks, we
investigate the effect of the data used to train probing models on their
performance. We also make available concept labels for two widely used
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of the paper published in Proceedings of the
  European Conference on Artificial Intelligence (ECAI 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLiNER2: An Efficient Multi-Task Information Extraction System with
  Schema-Driven Interface 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Urchade Zaratiana, Gil Pasternak, Oliver Boyd, George Hurn-Maloney, Ash Lewis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information extraction (IE) is fundamental to numerous NLP applications, yet
existing solutions often require specialized models for different tasks or rely
on computationally expensive large language models. We present GLiNER2, a
unified framework that enhances the original GLiNER architecture to support
named entity recognition, text classification, and hierarchical structured data
extraction within a single efficient model. Built pretrained transformer
encoder architecture, GLiNER2 maintains CPU efficiency and compact size while
introducing multi-task composition through an intuitive schema-based interface.
Our experiments demonstrate competitive performance across extraction and
classification tasks with substantial improvements in deployment accessibility
compared to LLM-based alternatives. We release GLiNER2 as an open-source
pip-installable library with pre-trained models and documentation at
https://github.com/fastino-ai/GLiNER2.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ C2G-KD: PCA-Constrained Generator for Data-Free Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Magnus Bengtsson, Kenneth Östberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce C2G-KD, a data-free knowledge distillation framework where a
class-conditional generator is trained to produce synthetic samples guided by a
frozen teacher model and geometric constraints derived from PCA. The generator
never observes real training data but instead learns to activate the teacher's
output through a combination of semantic and structural losses. By constraining
generated samples to lie within class-specific PCA subspaces estimated from as
few as two real examples per class, we preserve topological consistency and
diversity. Experiments on MNIST show that even minimal class structure is
sufficient to bootstrap useful synthetic training pipelines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLANCE: <span class="highlight-title">Graph</span> Logic Attention Network with <span class="highlight-title">Cluster</span> Enhancement for
  Heterophilous <span class="highlight-title">Graph</span> Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongtian Sun, Anoushka Harit, Alexandra Cristea, Christl A. Donnelly, Pietro Liò
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have demonstrated significant success in
learning from graph-structured data but often struggle on heterophilous graphs,
where connected nodes differ in features or class labels. This limitation
arises from indiscriminate neighbor aggregation and insufficient incorporation
of higher-order structural patterns. To address these challenges, we propose
GLANCE (Graph Logic Attention Network with Cluster Enhancement), a novel
framework that integrates logic-guided reasoning, dynamic graph refinement, and
adaptive clustering to enhance graph representation learning. GLANCE combines a
logic layer for interpretable and structured embeddings, multi-head
attention-based edge pruning for denoising graph structures, and clustering
mechanisms for capturing global patterns. Experimental results in benchmark
datasets, including Cornell, Texas, and Wisconsin, demonstrate that GLANCE
achieves competitive performance, offering robust and interpretable solutions
for heterophilous graph scenarios. The proposed framework is lightweight,
adaptable, and uniquely suited to the challenges of heterophilous graphs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explaining How Visual, Textual and Multimodal Encoders Share Concepts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clément Cornet, Romaric Besançon, Hervé Le Borgne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse autoencoders (SAEs) have emerged as a powerful technique for
extracting human-interpretable features from neural networks activations.
Previous works compared different models based on SAE-derived features but
those comparisons have been restricted to models within the same modality. We
propose a novel indicator allowing quantitative comparison of models across SAE
features, and use it to conduct a comparative study of visual, textual and
multimodal encoders. We also propose to quantify the Comparative Sharedness of
individual features between different classes of models. With these two new
tools, we conduct several studies on 21 encoders of the three types, with two
significantly different sizes, and considering generalist and domain specific
datasets. The results allow to revisit previous studies at the light of
encoders trained in a multimodal context and to quantify to which extent all
these models share some representations or features. They also suggest that
visual features that are specific to VLMs among vision encoders are shared with
text encoders, highlighting the impact of text pretraining. The code is
available at https://github.com/CEA-LIST/SAEshareConcepts
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforced <span class="highlight-title">Embodied</span> Active Defense: Exploiting Adaptive Interaction for
  <span class="highlight-title">Robust</span> Visual Perception in Adversarial 3D Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Yang, Lingxuan Wu, Lizhong Wang, Chengyang Ying, Hang Su, Jun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks in 3D environments have emerged as a critical threat to
the reliability of visual perception systems, particularly in safety-sensitive
applications such as identity verification and autonomous driving. These
attacks employ adversarial patches and 3D objects to manipulate deep neural
network (DNN) predictions by exploiting vulnerabilities within complex scenes.
Existing defense mechanisms, such as adversarial training and purification,
primarily employ passive strategies to enhance robustness. However, these
approaches often rely on pre-defined assumptions about adversarial tactics,
limiting their adaptability in dynamic 3D settings. To address these
challenges, we introduce Reinforced Embodied Active Defense (Rein-EAD), a
proactive defense framework that leverages adaptive exploration and interaction
with the environment to improve perception robustness in 3D adversarial
contexts. By implementing a multi-step objective that balances immediate
prediction accuracy with predictive entropy minimization, Rein-EAD optimizes
defense strategies over a multi-step horizon. Additionally, Rein-EAD involves
an uncertainty-oriented reward-shaping mechanism that facilitates efficient
policy updates, thereby reducing computational overhead and supporting
real-world applicability without the need for differentiable environments.
Comprehensive experiments validate the effectiveness of Rein-EAD, demonstrating
a substantial reduction in attack success rates while preserving standard
accuracy across diverse tasks. Notably, Rein-EAD exhibits robust generalization
to unseen and adaptive attacks, making it suitable for real-world complex
tasks, including 3D object classification, face recognition and autonomous
driving.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2404.00540</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Code <span class="highlight-title">Review</span> Using Large Language Models with Symbolic
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Busra Icoz, Goksel Biricik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code review is one of the key processes in the software development lifecycle
and is essential to maintain code quality. However, manual code review is
subjective and time consuming. Given its rule-based nature, code review is well
suited for automation. In recent years, significant efforts have been made to
automate this process with the help of artificial intelligence. Recent
developments in Large Language Models (LLMs) have also emerged as a promising
tool in this area, but these models often lack the logical reasoning
capabilities needed to fully understand and evaluate code. To overcome this
limitation, this study proposes a hybrid approach that integrates symbolic
reasoning techniques with LLMs to automate the code review process. We tested
our approach using the CodexGlue dataset, comparing several models, including
CodeT5, CodeBERT, and GraphCodeBERT, to assess the effectiveness of combining
symbolic reasoning and prompting techniques with LLMs. Our results show that
this approach improves the accuracy and efficiency of automated code review.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Physically Realizable Adversarial Object Attack against
  <span class="highlight-title">LiDAR</span>-based <span class="highlight-title">Detection</span>: Clarifying Problem Formulation and Experimental
  Protocols 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luo Cheng, Hanwei Zhang, Lijun Zhang, Holger Hermanns
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial robustness in LiDAR-based 3D object detection is a critical
research area due to its widespread application in real-world scenarios. While
many digital attacks manipulate point clouds or meshes, they often lack
physical realizability, limiting their practical impact. Physical adversarial
object attacks remain underexplored and suffer from poor reproducibility due to
inconsistent setups and hardware differences. To address this, we propose a
device-agnostic, standardized framework that abstracts key elements of physical
adversarial object attacks, supports diverse methods, and provides open-source
code with benchmarking protocols in simulation and real-world settings. Our
framework enables fair comparison, accelerates research, and is validated by
successfully transferring simulated attacks to a physical LiDAR system. Beyond
the framework, we offer insights into factors influencing attack success and
advance understanding of adversarial robustness in real-world LiDAR perception.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generation of Synthetic Clinical Text: A Systematic <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Basel Alshaikhdeeb, Ahmed Abdelmonem Hemedan, Soumyabrata Ghosh, Irina Balaur, Venkata Satagopam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating clinical synthetic text represents an effective solution for
common clinical NLP issues like sparsity and privacy. This paper aims to
conduct a systematic review on generating synthetic medical free-text by
formulating quantitative analysis to three research questions concerning (i)
the purpose of generation, (ii) the techniques, and (iii) the evaluation
methods. We searched PubMed, ScienceDirect, Web of Science, Scopus, IEEE,
Google Scholar, and arXiv databases for publications associated with generating
synthetic medical unstructured free-text. We have identified 94 relevant
articles out of 1,398 collected ones. A great deal of attention has been given
to the generation of synthetic medical text from 2018 onwards, where the main
purpose of such a generation is towards text augmentation, assistive writing,
corpus building, privacy-preserving, annotation, and usefulness. Transformer
architectures were the main predominant technique used to generate the text,
especially the GPTs. On the other hand, there were four main aspects of
evaluation, including similarity, privacy, structure, and utility, where
utility was the most frequent method used to assess the generated synthetic
medical text. Although the generated synthetic medical text demonstrated a
moderate possibility to act as real medical documents in different downstream
NLP tasks, it has proven to be a great asset as augmented, complementary to the
real documents, towards improving the accuracy and overcoming
sparsity/undersampling issues. Yet, privacy is still a major issue behind
generating synthetic medical text, where more human assessments are needed to
check for the existence of any sensitive information. Despite that, advances in
generating synthetic medical text will considerably accelerate the adoption of
workflows and pipeline development, discarding the time-consuming legalities of
data transfer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Restoring Rhythm: Punctuation Restoration Using Transformer Models for
  Bangla, a Low-Resource Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Obyedullahil Mamun, Md Adyelullahil Mamun, Arif Ahmad, Md. Imran Hossain Emu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Punctuation restoration enhances the readability of text and is critical for
post-processing tasks in Automatic Speech Recognition (ASR), especially for
low-resource languages like Bangla. In this study, we explore the application
of transformer-based models, specifically XLM-RoBERTa-large, to automatically
restore punctuation in unpunctuated Bangla text. We focus on predicting four
punctuation marks: period, comma, question mark, and exclamation mark across
diverse text domains. To address the scarcity of annotated resources, we
constructed a large, varied training corpus and applied data augmentation
techniques. Our best-performing model, trained with an augmentation factor of
alpha = 0.20%, achieves an accuracy of 97.1% on the News test set, 91.2% on the
Reference set, and 90.2% on the ASR set.
  Results show strong generalization to reference and ASR transcripts,
demonstrating the model's effectiveness in real-world, noisy scenarios. This
work establishes a strong baseline for Bangla punctuation restoration and
contributes publicly available datasets and code to support future research in
low-resource NLP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AraTable: <span class="highlight-title">Benchmark</span>ing LLMs' Reasoning and Understanding of Arabic
  Tabular Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rana Alshaikh, Israa Alghanmi, Shelan Jeawak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The cognitive and reasoning abilities of large language models (LLMs) have
enabled remarkable progress in natural language processing. However, their
performance in interpreting structured data, especially in tabular formats,
remains limited. Although benchmarks for English tabular data are widely
available, Arabic is still underrepresented because of the limited availability
of public resources and its unique language features. To address this gap, we
present AraTable, a novel and comprehensive benchmark designed to evaluate the
reasoning and understanding capabilities of LLMs when applied to Arabic tabular
data. AraTable consists of various evaluation tasks, such as direct question
answering, fact verification, and complex reasoning, involving a wide range of
Arabic tabular sources. Our methodology follows a hybrid pipeline, where
initial content is generated by LLMs and subsequently filtered and verified by
human experts to ensure high dataset quality. Initial analyses using AraTable
show that, while LLMs perform adequately on simpler tabular tasks such as
direct question answering, they continue to face significant cognitive
challenges when tasks require deeper reasoning and fact verification. This
indicates that there are substantial opportunities for future work to improve
performance on complex tabular reasoning tasks. We also propose a fully
automated evaluation framework that uses a self-deliberation mechanism and
achieves performance nearly identical to that of human judges. This research
provides a valuable, publicly available resource and evaluation framework that
can help accelerate the development of foundational models for processing and
analysing Arabic structured data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GPU Accelerated Compact-Table Propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enrico Santi, Fabio Tardivo, Agostino Dovier, Andrea Formisano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constraint Programming developed within Logic Programming in the Eighties;
nowadays all Prolog systems encompass modules capable of handling constraint
programming on finite domains demanding their solution to a constraint solver.
This work focuses on a specific form of constraint, the so-called table
constraint, used to specify conditions on the values of variables as an
enumeration of alternative options. Since every condition on a set of finite
domain variables can be ultimately expressed as a finite set of cases, Table
can, in principle, simulate any other constraint. These characteristics make
Table one of the most studied constraints ever, leading to a series of
increasingly efficient propagation algorithms. Despite this, it is not uncommon
to encounter real-world problems with hundreds or thousands of valid cases that
are simply too many to be handled effectively with standard CPU-based
approaches. In this paper, we deal with the Compact-Table (CT) algorithm, the
state-of-the-art propagation algorithms for Table. We describe how CT can be
enhanced by exploiting the massive computational power offered by modern GPUs
to handle large Table constraints. In particular, we report on the design and
implementation of GPU-accelerated CT, on its integration into an existing
constraint solver, and on an experimental validation performed on a significant
set of instances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under consideration in Theory and Practice of Logic Programming
  (TPLP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimising Call Centre Operations using <span class="highlight-title">Reinforcement</span> Learning: Value
  Iteration versus Proximal Policy Optimisation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kwong Ho Li, Wathsala Karunarathne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the application of Reinforcement Learning (RL) to
optimise call routing in call centres to minimise client waiting time and staff
idle time. Two methods are compared: a model-based approach using Value
Iteration (VI) under known system dynamics, and a model-free approach using
Proximal Policy Optimisation (PPO) that learns from experience. For the
model-based approach, a theoretical model is used, while a simulation model
combining Discrete Event Simulation (DES) with the OpenAI Gym environment is
developed for model-free learning. Both models frame the problem as a Markov
Decision Process (MDP) within a Skills-Based Routing (SBR) framework, with
Poisson client arrivals and exponentially distributed service and abandonment
times. For policy evaluation, random, VI, and PPO policies are evaluated using
the simulation model. After 1,000 test episodes, PPO consistently achives the
highest rewards, along with the lowest client waiting time and staff idle time,
despite requiring longer training time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLEAR: Error Analysis via LLM-as-a-Judge Made Easy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asaf Yehudai, Lilach Eden, Yotam Perlitz, Roy Bar-Haim, Michal Shmueli-Scheuer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evaluation of Large Language Models (LLMs) increasingly relies on other
LLMs acting as judges. However, current evaluation paradigms typically yield a
single score or ranking, answering which model is better but not why. While
essential for benchmarking, these top-level scores obscure the specific,
actionable reasons behind a model's performance. To bridge this gap, we
introduce CLEAR, an interactive, open-source package for LLM-based error
analysis. CLEAR first generates per-instance textual feedback, then it creates
a set of system-level error issues, and quantifies the prevalence of each
identified issue. Our package also provides users with an interactive dashboard
that allows for a comprehensive error analysis through aggregate
visualizations, applies interactive filters to isolate specific issues or score
ranges, and drills down to the individual instances that exemplify a particular
behavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks,
and showcase its utility through a user case study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting LLM Reasoning via Information Bottleneck 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiye Lei, Zhihao Cheng, Kai Jia, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have recently demonstrated remarkable progress
in reasoning capabilities through reinforcement learning with verifiable
rewards (RLVR). By leveraging simple rule-based rewards, RL effectively
incentivizes LLMs to produce extended chain-of-thought (CoT) reasoning
trajectories, progressively guiding them toward correct answers. However,
existing approaches remain largely heuristic and intuition-driven, limiting the
development of principled methodologies. In this paper, we present a
theoretical characterization of LLM reasoning grounded in information
bottleneck (IB) principle, introducing IB-aware reasoning optimization (IBRO),
a framework that encourages reasoning trajectories to be both informative about
the final correct answer and generalizable across diverse prompts. We derive a
practical token-level surrogate objective and propose an efficient
approximation, resulting in the lightweight IB regularization method. This
technique integrates seamlessly into existing RL-based post-training frameworks
without additional computational overhead, requiring only a one-line code
modification. Empirically, we validate IB regularization across multiple
mathematical reasoning benchmarks and RL algorithms, demonstrating consistent
improvements in LLM reasoning performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reasoning Beyond the Obvious: Evaluating Divergent and Convergent
  Thinking in LLMs for Financial Scenarios <span class="chip">KDD2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuang Qiang Bok, Watson Wei Khong Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most reasoning benchmarks for LLMs emphasize factual accuracy or step-by-step
logic. In finance, however, professionals must not only converge on optimal
decisions but also generate creative, plausible futures under uncertainty. We
introduce ConDiFi, a benchmark that jointly evaluates divergent and convergent
thinking in LLMs for financial tasks.
  ConDiFi features 607 macro-financial prompts for divergent reasoning and 990
multi-hop adversarial MCQs for convergent reasoning. Using this benchmark, we
evaluated 14 leading models and uncovered striking differences. Despite high
fluency, GPT-4o underperforms on Novelty and Actionability. In contrast, models
like DeepSeek-R1 and Cohere Command R+ rank among the top for generating
actionable, insights suitable for investment decisions. ConDiFi provides a new
perspective to assess reasoning capabilities essential to safe and strategic
deployment of LLMs in finance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Agentic & GenAI Evaluation KDD2025: KDD workshop on
  Evaluation and Trustworthiness of Agentic and Generative AI Models
  https://kdd-eval-workshop.github.io/genai-evaluation-kdd2025/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The AlphaPhysics Term Rewriting System for Marking Algebraic Expressions
  in Physics Exams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Baumgartner, Lachlan McGinness
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present our method for automatically marking Physics exams. The marking
problem consists in assessing typed student answers for correctness with
respect to a ground truth solution. This is a challenging problem that we seek
to tackle using a combination of a computer algebra system, an SMT solver and a
term rewriting system. A Large Language Model is used to interpret and remove
errors from student responses and rewrite these in a machine readable format.
Once formalized and language-aligned, the next step then consists in applying
automated reasoning techniques for assessing student solution correctness. We
consider two methods of automated theorem proving: off-the-shelf SMT solving
and term rewriting systems tailored for physics problems involving
trigonometric expressions. The development of the term rewrite system and
establishing termination and confluence properties was not trivial, and we
describe it in some detail in the paper. We evaluate our system on a rich pool
of over 1500 real-world student exam responses from the 2023 Australian Physics
Olympiad.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Bird Classification with Primary Color Additives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ezhini Rasendiran R, Chandresh Kumar Maurya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of classifying bird species using their song
recordings, a challenging task due to environmental noise, overlapping
vocalizations, and missing labels. Existing models struggle with low-SNR or
multi-species recordings. We hypothesize that birds can be classified by
visualizing their pitch pattern, speed, and repetition, collectively called
motifs. Deep learning models applied to spectrogram images help, but similar
motifs across species cause confusion. To mitigate this, we embed frequency
information into spectrograms using primary color additives. This enhances
species distinction and improves classification accuracy. Our experiments show
that the proposed approach achieves statistically significant gains over models
without colorization and surpasses the BirdCLEF 2024 winner, improving F1 by
7.3%, ROC-AUC by 6.2%, and CMAP by 6.6%. These results demonstrate the
effectiveness of incorporating frequency information via colorization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages (Accepted to Interspeech 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Concept for Efficient Scalability of Automated Driving Allowing for
  Technical, Legal, Cultural, and Ethical Differences <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lars Ullrich, Michael Buchholz, Jonathan Petit, Klaus Dietmayer, Knut Graichen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient scalability of automated driving (AD) is key to reducing costs,
enhancing safety, conserving resources, and maximizing impact. However,
research focuses on specific vehicles and context, while broad deployment
requires scalability across various configurations and environments.
Differences in vehicle types, sensors, actuators, but also traffic regulations,
legal requirements, cultural dynamics, or even ethical paradigms demand high
flexibility of data-driven developed capabilities. In this paper, we address
the challenge of scalable adaptation of generic capabilities to desired systems
and environments. Our concept follows a two-stage fine-tuning process. In the
first stage, fine-tuning to the specific environment takes place through a
country-specific reward model that serves as an interface between technological
adaptations and socio-political requirements. In the second stage,
vehicle-specific transfer learning facilitates system adaptation and governs
the validation of design decisions. In sum, our concept offers a data-driven
process that integrates both technological and socio-political aspects,
enabling effective scalability across technical, legal, cultural, and ethical
differences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to be published at 2025 28th IEEE International Conference
  on Intelligent Transportation Systems (ITSC), Gold Coast, Australia, November
  18-21, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-<span class="highlight-title">Dataset</span> <span class="highlight-title">Benchmark</span> for <span class="highlight-title">Semi-Supervised</span> Semantic <span class="highlight-title">Segmentation</span> in
  ECG Delineation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minje Park, Jeonghwa Lim, Taehyung Yu, Sunghoon Joo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electrocardiogram (ECG) delineation, the segmentation of meaningful waveform
features, is critical for clinical diagnosis. Despite recent advances using
deep learning, progress has been limited by the scarcity of publicly available
annotated datasets. Semi-supervised learning presents a promising solution by
leveraging abundant unlabeled ECG data. In this study, we present the first
systematic benchmark for semi-supervised semantic segmentation (SemiSeg) in ECG
delineation. We curated and unified multiple public datasets, including
previously underused sources, to support robust and diverse evaluation. We
adopted five representative SemiSeg algorithms from computer vision,
implemented them on two different architectures: the convolutional network and
the transformer, and evaluated them in two different settings: in-domain and
cross-domain. Additionally, we propose ECG-specific training configurations and
augmentation strategies and introduce a standardized evaluation framework. Our
results show that the transformer outperforms the convolutional network in
semi-supervised ECG delineation. We anticipate that our benchmark will serve as
a foundation for advancing semi-supervised ECG delineation methods and will
facilitate further research in this domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoRA-Leak: Membership Inference Attacks Against LoRA Fine-tuned Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Delong Ran, Xinlei He, Tianshuo Cong, Anyu Wang, Qi Li, Xiaoyun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Models (LMs) typically adhere to a "pre-training and fine-tuning"
paradigm, where a universal pre-trained model can be fine-tuned to cater to
various specialized domains. Low-Rank Adaptation (LoRA) has gained the most
widespread use in LM fine-tuning due to its lightweight computational cost and
remarkable performance. Because the proportion of parameters tuned by LoRA is
relatively small, there might be a misleading impression that the LoRA
fine-tuning data is invulnerable to Membership Inference Attacks (MIAs).
However, we identify that utilizing the pre-trained model can induce more
information leakage, which is neglected by existing MIAs. Therefore, we
introduce LoRA-Leak, a holistic evaluation framework for MIAs against the
fine-tuning datasets of LMs. LoRA-Leak incorporates fifteen membership
inference attacks, including ten existing MIAs, and five improved MIAs that
leverage the pre-trained model as a reference. In experiments, we apply
LoRA-Leak to three advanced LMs across three popular natural language
processing tasks, demonstrating that LoRA-based fine-tuned LMs are still
vulnerable to MIAs (e.g., 0.775 AUC under conservative fine-tuning settings).
We also applied LoRA-Leak to different fine-tuning settings to understand the
resulting privacy risks. We further explore four defenses and find that only
dropout and excluding specific LM layers during fine-tuning effectively
mitigate MIA risks while maintaining utility. We highlight that under the
"pre-training and fine-tuning" paradigm, the existence of the pre-trained model
makes MIA a more severe risk for LoRA-based LMs. We hope that our findings can
provide guidance on data privacy protection for specialized LM providers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Foundations for Risk Assessment of AI in Protecting Fundamental Rights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18290v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18290v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonino Rotolo, Beatrice Ferrigno, Jose Miguel Angel Garcia Godinez, Claudio Novelli, Giovanni Sartor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This chapter introduces a conceptual framework for qualitative risk
assessment of AI, particularly in the context of the EU AI Act. The framework
addresses the complexities of legal compliance and fundamental rights
protection by itegrating definitional balancing and defeasible reasoning.
Definitional balancing employs proportionality analysis to resolve conflicts
between competing rights, while defeasible reasoning accommodates the dynamic
nature of legal decision-making. Our approach stresses the need for an analysis
of AI deployment scenarios and for identifying potential legal violations and
multi-layered impacts on fundamental rights. On the basis of this analysis, we
provide philosophical foundations for a logical account of AI risk analysis. In
particular, we consider the basic building blocks for conceptually grasping the
interaction between AI deployment scenarios and fundamental rights,
incorporating in defeasible reasoning definitional balancing and arguments
about the contextual promotion or demotion of rights. This layered approach
allows for more operative models of assessment of both high-risk AI systems and
General Purpose AI (GPAI) systems, emphasizing the broader applicability of the
latter. Future work aims to develop a formal model and effective algorithms to
enhance AI risk assessment, bridging theoretical insights with practical
applications to support responsible AI governance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 1 figure. To be published in: The Philosophical Foundations
  of Information Technology Law. Oxford University Press, Oxford</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> TCM-Tongue: A Standardized Tongue Image <span class="highlight-title">Dataset</span> with Pathological
  Annotations for AI-Assisted TCM Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuebo Jin, Long<span class="highlight-author">fei Gao</span>, Anshuo Tong, Zhengyang Chen, Jianlei Kong, Ning Sun, Huijun Ma, Qiang Wang, Yuting Bai, Tingli Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional Chinese medicine (TCM) tongue diagnosis, while clinically
valuable, faces standardization challenges due to subjective interpretation and
inconsistent imaging protocols, compounded by the lack of large-scale,
annotated datasets for AI development. To address this gap, we present the
first specialized dataset for AI-driven TCM tongue diagnosis, comprising 6,719
high-quality images captured under standardized conditions and annotated with
20 pathological symptom categories (averaging 2.54 clinically validated labels
per image, all verified by licensed TCM practitioners). The dataset supports
multiple annotation formats (COCO, TXT, XML) for broad usability and has been
benchmarked using nine deep learning models (YOLOv5/v7/v8 variants, SSD, and
MobileNetV2) to demonstrate its utility for AI development. This resource
provides a critical foundation for advancing reliable computational tools in
TCM, bridging the data shortage that has hindered progress in the field, and
facilitating the integration of AI into both research and clinical practice
through standardized, high-quality diagnostic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 11 figures, 2 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Locate-and-Focus: Enhancing Terminology Translation in Speech Language
  Models <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suhang Wu, Jialong Tang, Chengyi Yang, Pei Zhang, Baosong Yang, Junhui Li, Junfeng Yao, Min Zhang, Jinsong Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct speech translation (ST) has garnered increasing attention nowadays,
yet the accurate translation of terminology within utterances remains a great
challenge. In this regard, current studies mainly concentrate on leveraging
various translation knowledge into ST models. However, these methods often
struggle with interference from irrelevant noise and can not fully utilize the
translation knowledge. To address these issues, in this paper, we propose a
novel Locate-and-Focus method for terminology translation. It first effectively
locates the speech clips containing terminologies within the utterance to
construct translation knowledge, minimizing irrelevant information for the ST
model. Subsequently, it associates the translation knowledge with the utterance
and hypothesis from both audio and textual modalities, allowing the ST model to
better focus on translation knowledge during translation. Experimental results
across various datasets demonstrate that our method effectively locates
terminologies within utterances and enhances the success rate of terminology
translation, while maintaining robust general translation performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic
  Grounding for Generalizable <span class="highlight-title">Robot</span>ic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu Su, Weiwei Shang, Chen Qian, Fei Zhang, Shuang Cong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantics-driven 3D spatial constraints align highlevel semantic
representations with low-level action spaces, facilitating the unification of
task understanding and execution in robotic manipulation. The synergistic
reasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation
Models (VFMs) enables cross-modal 3D spatial constraint construction.
Nevertheless, existing methods have three key limitations: (1) coarse semantic
granularity in constraint modeling, (2) lack of real-time closed-loop planning,
(3) compromised robustness in semantically diverse environments. To address
these challenges, we propose ReSem3D, a unified manipulation framework for
semantically diverse environments, leveraging the synergy between VFMs and
MLLMs to achieve fine-grained visual grounding and dynamically constructs
hierarchical 3D spatial constraints for real-time manipulation. Specifically,
the framework is driven by hierarchical recursive reasoning in MLLMs, which
interact with VFMs to automatically construct 3D spatial constraints from
natural language instructions and RGB-D observations in two stages: part-level
extraction and region-level refinement. Subsequently, these constraints are
encoded as real-time optimization objectives in joint space, enabling reactive
behavior to dynamic disturbances. Extensive simulation and real-world
experiments are conducted in semantically rich household and sparse chemical
lab environments. The results demonstrate that ReSem3D performs diverse
manipulation tasks under zero-shot conditions, exhibiting strong adaptability
and generalization. Code and videos at https://resem3d.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages,9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting Gaussian Agnostic Representation Learning with Dif<span class="highlight-title">fusion</span>
  Priors for Enhanced Infrared Small Target <span class="highlight-title">Detection</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyao Li, Yahao Lu, Xingyuan Guo, Xiaoyu Xian, Tiantian Wang, Yukai Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Infrared small target detection (ISTD) plays a vital role in numerous
practical applications. In pursuit of determining the performance boundaries,
researchers employ large and expensive manual-labeling data for representation
learning. Nevertheless, this approach renders the state-of-the-art ISTD methods
highly fragile in real-world challenges. In this paper, we first study the
variation in detection performance across several mainstream methods under
various scarcity -- namely, the absence of high-quality infrared data -- that
challenge the prevailing theories about practical ISTD. To address this
concern, we introduce the Gaussian Agnostic Representation Learning.
Specifically, we propose the Gaussian Group Squeezer, leveraging Gaussian
sampling and compression for non-uniform quantization. By exploiting a diverse
array of training samples, we enhance the resilience of ISTD models against
various challenges. Then, we introduce two-stage diffusion models for
real-world reconstruction. By aligning quantized signals closely with
real-world distributions, we significantly elevate the quality and fidelity of
the synthetic samples. Comparative evaluations against state-of-the-art
detection methods in various scarcity scenarios demonstrate the efficacy of the
proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Neural Networks. We propose the Gaussian Group Squeezer,
  leveraging Gaussian sampling and compression with diffusion models for
  channel-based data augmentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Behavioral Patterns Analysis with Eye-Tracking and LLM-Based
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyang Guo, Yasmeen Abdrabou, Enkeleda Thaqi, Enkelejda Kasneci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Eye-tracking data reveals valuable insights into users' cognitive states but
is difficult to analyze due to its structured, non-linguistic nature. While
large language models (LLMs) excel at reasoning over text, they struggle with
temporal and numerical data. This paper presents a multimodal human-AI
collaborative framework designed to enhance cognitive pattern extraction from
eye-tracking signals. The framework includes: (1) a multi-stage pipeline using
horizontal and vertical segmentation alongside LLM reasoning to uncover latent
gaze patterns; (2) an Expert-Model Co-Scoring Module that integrates expert
judgment with LLM output to generate trust scores for behavioral
interpretations; and (3) a hybrid anomaly detection module combining LSTM-based
temporal modeling with LLM-driven semantic analysis. Our results across several
LLMs and prompt strategies show improvements in consistency, interpretability,
and performance, with up to 50% accuracy in difficulty prediction tasks. This
approach offers a scalable, interpretable solution for cognitive modeling and
has broad potential in adaptive learning, human-computer interaction, and
educational analytics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DepthDark: <span class="highlight-title">Robust</span> Monocular Depth <span class="highlight-title">Estimation</span> for Low-Light Environments <span class="chip">ACM MM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longjian Zeng, Zunjie Zhu, Rongfeng Lu, Ming Lu, Bolun Zheng, Chenggang Yan, Anke Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, foundation models for monocular depth estimation have
received increasing attention. Current methods mainly address typical daylight
conditions, but their effectiveness notably decreases in low-light
environments. There is a lack of robust foundational models for monocular depth
estimation specifically designed for low-light scenarios. This largely stems
from the absence of large-scale, high-quality paired depth datasets for
low-light conditions and the effective parameter-efficient fine-tuning (PEFT)
strategy. To address these challenges, we propose DepthDark, a robust
foundation model for low-light monocular depth estimation. We first introduce a
flare-simulation module and a noise-simulation module to accurately simulate
the imaging process under nighttime conditions, producing high-quality paired
depth datasets for low-light conditions. Additionally, we present an effective
low-light PEFT strategy that utilizes illumination guidance and multiscale
feature fusion to enhance the model's capability in low-light environments. Our
method achieves state-of-the-art depth estimation performance on the
challenging nuScenes-Night and RobotCar-Night datasets, validating its
effectiveness using limited training data and computing resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM 2025 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Individual Learning to Market Equilibrium: Correcting Structural
  and Parametric Biases in RL <span class="highlight-title">Simulation</span>s of Economic Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18229v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18229v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeqiang Zhang, Ruxin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of Reinforcement Learning (RL) to economic modeling reveals a
fundamental conflict between the assumptions of equilibrium theory and the
emergent behavior of learning agents. While canonical economic models assume
atomistic agents act as `takers' of aggregate market conditions, a naive
single-agent RL simulation incentivizes the agent to become a `manipulator' of
its environment. This paper first demonstrates this discrepancy within a
search-and-matching model with concave production, showing that a standard RL
agent learns a non-equilibrium, monopsonistic policy. Additionally, we identify
a parametric bias arising from the mismatch between economic discounting and
RL's treatment of intertemporal costs. To address both issues, we propose a
calibrated Mean-Field Reinforcement Learning framework that embeds a
representative agent in a fixed macroeconomic field and adjusts the cost
function to reflect economic opportunity costs. Our iterative algorithm
converges to a self-consistent fixed point where the agent's policy aligns with
the competitive equilibrium. This approach provides a tractable and
theoretically sound methodology for modeling learning agents in economic
systems within the broader domain of computational social science.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GenAI for Automotive Software Development: From Requirements to Wheels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nenad Petrovic, Fengjunjie Pan, Vahid Zolfaghari, Krzysztof Lebioda, Andre Schamschurko, Alois Knoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a GenAI-empowered approach to automated development of
automotive software, with emphasis on autonomous and Advanced Driver Assistance
Systems (ADAS) capabilities. The process starts with requirements as input,
while the main generated outputs are test scenario code for simulation
environment, together with implementation of desired ADAS capabilities
targeting hardware platform of the vehicle connected to testbench. Moreover, we
introduce additional steps for requirements consistency checking leveraging
Model-Driven Engineering (MDE). In the proposed workflow, Large Language Models
(LLMs) are used for model-based summarization of requirements (Ecore metamodel,
XMI model instance and OCL constraint creation), test scenario generation,
simulation code (Python) and target platform code generation (C++).
Additionally, Retrieval Augmented Generation (RAG) is adopted to enhance test
scenario generation from autonomous driving regulations-related documents. Our
approach aims shorter compliance and re-engineering cycles, as well as reduced
development and testing time when it comes to ADAS-related capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedSA-GCL: A Semi-Asynchronous Federated <span class="highlight-title">Graph</span> Learning Framework with
  Personalized Aggregation and <span class="highlight-title">Cluster</span>-Aware Broadcasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongzheng Yuan, Lianshuai Guo, Xunkai Li, Yinlin Zhu, Wenyu Wang, Meixia Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Graph Learning (FGL) is a distributed learning paradigm that
enables collaborative training over large-scale subgraphs located on multiple
local systems. However, most existing FGL approaches rely on synchronous
communication, which leads to inefficiencies and is often impractical in
real-world deployments. Meanwhile, current asynchronous federated learning
(AFL) methods are primarily designed for conventional tasks such as image
classification and natural language processing, without accounting for the
unique topological properties of graph data. Directly applying these methods to
graph learning can possibly result in semantic drift and representational
inconsistency in the global model. To address these challenges, we propose
FedSA-GCL, a semi-asynchronous federated framework that leverages both
inter-client label distribution divergence and graph topological
characteristics through a novel ClusterCast mechanism for efficient training.
We evaluate FedSA-GCL on multiple real-world graph datasets using the Louvain
and Metis split algorithms, and compare it against 9 baselines. Extensive
experiments demonstrate that our method achieves strong robustness and
outstanding efficiency, outperforming the baselines by an average of 2.92% with
the Louvain and by 3.4% with the Metis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Information Security Based on LLM Approaches: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Gong, Zhongwen Li, Xiaoqi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information security is facing increasingly severe challenges, and
traditional protection means are difficult to cope with complex and changing
threats. In recent years, as an emerging intelligent technology, large language
models (LLMs) have shown a broad application prospect in the field of
information security. In this paper, we focus on the key role of LLM in
information security, systematically review its application progress in
malicious behavior prediction, network threat analysis, system vulnerability
detection, malicious code identification, and cryptographic algorithm
optimization, and explore its potential in enhancing security protection
performance. Based on neural networks and Transformer architecture, this paper
analyzes the technical basis of large language models and their advantages in
natural language processing tasks. It is shown that the introduction of large
language modeling helps to improve the detection accuracy and reduce the false
alarm rate of security systems. Finally, this paper summarizes the current
application results and points out that it still faces challenges in model
transparency, interpretability, and scene adaptability, among other issues. It
is necessary to explore further the optimization of the model structure and the
improvement of the generalization ability to realize a more intelligent and
accurate information security protection system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoRPI-PINN: A Physics-Informed Framework for Mobile <span class="highlight-title">Robot</span> Pure Inertial
  <span class="highlight-title">Navigation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arup Kumar Sahoo, Itzik Klein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental requirement for full autonomy in mobile robots is accurate
navigation even in situations where satellite navigation or cameras are
unavailable. In such practical situations, relying only on inertial sensors
will result in navigation solution drift due to the sensors' inherent noise and
error terms. One of the emerging solutions to mitigate drift is to maneuver the
robot in a snake-like slithering motion to increase the inertial
signal-to-noise ratio, allowing the regression of the mobile robot position. In
this work, we propose MoRPI-PINN as a physics-informed neural network framework
for accurate inertial-based mobile robot navigation. By embedding physical laws
and constraints into the training process, MoRPI-PINN is capable of providing
an accurate and robust navigation solution. Using real-world experiments, we
show accuracy improvements of over 85% compared to other approaches. MoRPI-PINN
is a lightweight approach that can be implemented even on edge devices and used
in any typical mobile robot application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token
  Probability Method for Poisoned Document <span class="highlight-title">Detection</span> <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        San Kim, Jonghwi Kim, Yejin Jeon, Gary Geunbae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by
providing external knowledge for accurate and up-to-date responses. However,
this reliance on external sources exposes a security risk, attackers can inject
poisoned documents into the knowledge base to steer the generation process
toward harmful or misleading outputs. In this paper, we propose Gradient-based
Masked Token Probability (GMTP), a novel defense method to detect and filter
out adversarially crafted documents. Specifically, GMTP identifies high-impact
tokens by examining gradients of the retriever's similarity function. These key
tokens are then masked, and their probabilities are checked via a Masked
Language Model (MLM). Since injected tokens typically exhibit markedly low
masked-token probabilities, this enables GMTP to easily detect malicious
documents and achieve high-precision filtering. Experiments demonstrate that
GMTP is able to eliminate over 90% of poisoned content while retaining relevant
documents, thus maintaining robust retrieval and generation performance across
diverse datasets and adversarial settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, accepted to ACL Findings 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparing Non-minimal Semantics for Disjunction in Answer Set
  Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felicidad Aguado, Pedro Cabalar, Brais Muñiz, Gilberto Pérez, Concepción Vidal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we compare four different semantics for disjunction in Answer
Set Programming that, unlike stable models, do not adhere to the principle of
model minimality. Two of these approaches, Cabalar and Mu\~niz' \emph{Justified
Models} and Doherty and Szalas' \emph{Strongly Supported Models}, directly
provide an alternative non-minimal semantics for disjunction. The other two,
Aguado et al's \emph{Forks} and Shen and Eiter's \emph{Determining Inference}
(DI) semantics, actually introduce a new disjunction connective, but are
compared here as if they constituted new semantics for the standard disjunction
operator. We are able to prove that three of these approaches (Forks, Justified
Models and a reasonable relaxation of the DI semantics) actually coincide,
constituting a common single approach under different definitions. Moreover,
this common semantics always provides a superset of the stable models of a
program (in fact, modulo any context) and is strictly stronger than the fourth
approach (Strongly Supported Models), that actually treats disjunctions as in
classical logic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCOPE: Stochastic and Counterbiased Option Placement for Evaluating
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonjun Jeong, Dongseok Kim, Taegkeun Whangbo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) can achieve inflated scores on multiple-choice
tasks by exploiting inherent biases in option positions or labels, rather than
demonstrating genuine understanding. This study introduces SCOPE, an evaluation
framework designed to measure and mitigate such selection bias in a
dataset-independent manner. By repeatedly invoking a null prompt that lacks
semantic content, SCOPE estimates each model's unique position-bias
distribution. It then redistributes the answer slot according to the
inverse-bias distribution, thereby equalizing the lucky-rate, the probability
of selecting the correct answer by chance. Furthermore, it prevents
semantically similar distractors from being placed adjacent to the answer,
thereby blocking near-miss guesses based on superficial proximity cues. Across
multiple benchmark experiments, SCOPE consistently outperformed existing
debiasing methods in terms of stable performance improvements and showed
clearer confidence distributions over correct options. This framework thus
offers a new standard for enhancing the fairness and reliability of LLM
evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoupling Knowledge and Reasoning in LLMs: An <span class="highlight-title">Exploration</span> Using
  Cognitive Dual-System Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mutian Yang, Jiandong Gao, Ji Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) leverage both knowledge and reasoning
during inference, the capacity to distinguish between them plays a pivotal role
in model analysis, interpretability, and development. Inspired by dual-system
cognitive theory, we propose a cognition attribution framework to decouple the
contribution of knowledge and reasoning. In particular, the cognition of LLMs
is decomposed into two distinct yet complementary phases: knowledge retrieval
(Phase 1) and reasoning adjustment (Phase 2). To separate these phases, LLMs
are prompted to generate answers under two different cognitive modes, fast
thinking and slow thinking, respectively. The performance under different
cognitive modes is analyzed to quantify the contribution of knowledge and
reasoning. This architecture is employed to 15 LLMs across 3 datasets. Results
reveal: (1) reasoning adjustment is domain-specific, benefiting
reasoning-intensive domains (e.g., mathematics, physics, and chemistry) and
potentially imparing knowledge-intensive domains. (2) Parameter scaling
improves both knowledge and reasoning, with knowledge improvements being more
pronounced. Additionally, parameter scaling make LLMs reasoning significantly
more prudent, while moderately more intelligent. (3) Knowledge primarily
resides in lower network layers, while reasoning operates in higher layers. Our
framework not only helps understand LLMs from a "decoupling" perspective, but
also provides new insights into existing research, including scaling laws,
hierarchical knowledge editing, and limitations of small-model reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differential-UMamba: Rethinking Tumor <span class="highlight-title">Segmentation</span> Under Limited Data
  Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhruv Jain, Romain Modzelewski, Romain Hérault, Clement Chatelain, Eva Torfeh, Sebastien Thureau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In data-scarce scenarios, deep learning models often overfit to noise and
irrelevant patterns, which limits their ability to generalize to unseen
samples. To address these challenges in medical image segmentation, we
introduce Diff-UMamba, a novel architecture that combines the UNet framework
with the mamba mechanism for modeling long-range dependencies. At the heart of
Diff-UMamba is a Noise Reduction Module (NRM), which employs a signal
differencing strategy to suppress noisy or irrelevant activations within the
encoder. This encourages the model to filter out spurious features and enhance
task-relevant representations, thereby improving its focus on clinically
meaningful regions. As a result, the architecture achieves improved
segmentation accuracy and robustness, particularly in low-data settings.
Diff-UMamba is evaluated on multiple public datasets, including MSD (lung and
pancreas) and AIIB23, demonstrating consistent performance gains of 1-3% over
baseline methods across diverse segmentation tasks. To further assess
performance under limited-data conditions, additional experiments are conducted
on the BraTS-21 dataset by varying the proportion of available training
samples. The approach is also validated on a small internal non-small cell lung
cancer (NSCLC) dataset for gross tumor volume (GTV) segmentation in cone beam
CT (CBCT), where it achieves a 4-5% improvement over the baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sticking to the Mean: <span class="highlight-title">Detect</span>ing Sticky Tokens in Text Embedding Models <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kexin Chen, Dongxia Wang, Yi Liu, Haonan Zhang, Wenhai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the widespread use of Transformer-based text embedding models in NLP
tasks, surprising 'sticky tokens' can undermine the reliability of embeddings.
These tokens, when repeatedly inserted into sentences, pull sentence similarity
toward a certain value, disrupting the normal distribution of embedding
distances and degrading downstream performance. In this paper, we
systematically investigate such anomalous tokens, formally defining them and
introducing an efficient detection method, Sticky Token Detector (STD), based
on sentence and token filtering. Applying STD to 40 checkpoints across 14 model
families, we discover a total of 868 sticky tokens. Our analysis reveals that
these tokens often originate from special or unused entries in the vocabulary,
as well as fragmented subwords from multilingual corpora. Notably, their
presence does not strictly correlate with model size or vocabulary size. We
further evaluate how sticky tokens affect downstream tasks like clustering and
retrieval, observing significant performance drops of up to 50%. Through
attention-layer analysis, we show that sticky tokens disproportionately
dominate the model's internal representations, raising concerns about
tokenization robustness. Our findings show the need for better tokenization
strategies and model design to mitigate the impact of sticky tokens in future
text embedding applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Noisy Labels Meet Class Imbalance on <span class="highlight-title">Graph</span>s: A <span class="highlight-title">Graph</span> Augmentation
  Method with LLM and Pseudo Label 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18153v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18153v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riting Xia, Rucong Wang, Yulin Liu, Anchen Li, Xueyan Liu, Yan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-imbalanced graph node classification is a practical yet underexplored
research problem. Although recent studies have attempted to address this issue,
they typically assume clean and reliable labels when processing
class-imbalanced graphs. This assumption often violates the nature of
real-world graphs, where labels frequently contain noise. Given this gap, this
paper systematically investigates robust node classification for
class-imbalanced graphs with noisy labels. We propose GraphALP, a novel Graph
Augmentation framework based on Large language models (LLMs) and
Pseudo-labeling techniques. Specifically, we design an LLM-based oversampling
method to generate synthetic minority nodes, producing label-accurate minority
nodes to alleviate class imbalance. Based on the class-balanced graphs, we
develop a dynamically weighted pseudo-labeling method to obtain high-confidence
pseudo labels to reduce label noise ratio. Additionally, we implement a
secondary LLM-guided oversampling mechanism to mitigate potential class
distribution skew caused by pseudo labels. Experimental results show that
GraphALP achieves superior performance over state-of-the-art methods on
class-imbalanced graphs with noisy labels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Logical Characterizations of GNNs with Mean Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moritz Schönherr, Carsten Lutz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the expressive power of graph neural networks (GNNs) with mean as
the aggregation function. In the non-uniform setting, we show that such GNNs
have exactly the same expressive power as ratio modal logic, which has modal
operators expressing that at least a certain ratio of the successors of a
vertex satisfies a specified property. The non-uniform expressive power of mean
GNNs is thus higher than that of GNNs with max aggregation, but lower than for
sum aggregation--the latter are characterized by modal logic and graded modal
logic, respectively. In the uniform setting, we show that the expressive power
relative to MSO is exactly that of alternation-free modal logic, under the
natural assumptions that combination functions are continuous and
classification functions are thresholds. This implies that, relative to MSO and
in the uniform setting, mean GNNs are strictly less expressive than sum GNNs
and max GNNs. When any of the assumptions is dropped, the expressive power
increases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HIVMedQA: <span class="highlight-title">Benchmark</span>ing large language models for HIV medical <span class="highlight-title">decision</span>
  support 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gonzalo Cardenal Antolin, Jacques Fellay, Bashkim Jaha, Roger Kouyos, Niko Beerenwinkel, Diane Duroux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are emerging as valuable tools to support
clinicians in routine decision-making. HIV management is a compelling use case
due to its complexity, including diverse treatment options, comorbidities, and
adherence challenges. However, integrating LLMs into clinical practice raises
concerns about accuracy, potential harm, and clinician acceptance. Despite
their promise, AI applications in HIV care remain underexplored, and LLM
benchmarking studies are scarce. This study evaluates the current capabilities
of LLMs in HIV management, highlighting their strengths and limitations. We
introduce HIVMedQA, a benchmark designed to assess open-ended medical question
answering in HIV care. The dataset consists of curated, clinically relevant
questions developed with input from an infectious disease physician. We
evaluated seven general-purpose and three medically specialized LLMs, applying
prompt engineering to enhance performance. Our evaluation framework
incorporates both lexical similarity and an LLM-as-a-judge approach, extended
to better reflect clinical relevance. We assessed performance across key
dimensions: question comprehension, reasoning, knowledge recall, bias,
potential harm, and factual accuracy. Results show that Gemini 2.5 Pro
consistently outperformed other models across most dimensions. Notably, two of
the top three models were proprietary. Performance declined as question
complexity increased. Medically fine-tuned models did not always outperform
general-purpose ones, and larger model size was not a reliable predictor of
performance. Reasoning and comprehension were more challenging than factual
recall, and cognitive biases such as recency and status quo were observed.
These findings underscore the need for targeted development and evaluation to
ensure safe, effective LLM integration in clinical care.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning for Glioblastoma Morpho-pathological Features
  Identification: A BraTS-Pathology Challenge Solution <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18133v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18133v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juexin Zhang, Ying Weng, Ke Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Glioblastoma, a highly aggressive brain tumor with diverse molecular and
pathological features, poses a diagnostic challenge due to its heterogeneity.
Accurate diagnosis and assessment of this heterogeneity are essential for
choosing the right treatment and improving patient outcomes. Traditional
methods rely on identifying specific features in tissue samples, but deep
learning offers a promising approach for improved glioblastoma diagnosis. In
this paper, we present our approach to the BraTS-Path Challenge 2024. We
leverage a pre-trained model and fine-tune it on the BraTS-Path training
dataset. Our model demonstrates poor performance on the challenging BraTS-Path
validation set, as rigorously assessed by the Synapse online platform. The
model achieves an accuracy of 0.392229, a recall of 0.392229, and a F1-score of
0.392229, indicating a consistent ability to correctly identify instances under
the target condition. Notably, our model exhibits perfect specificity of
0.898704, showing an exceptional capacity to correctly classify negative cases.
Moreover, a Matthews Correlation Coefficient (MCC) of 0.255267 is calculated,
to signify a limited positive correlation between predicted and actual values
and highlight our model's overall predictive power. Our solution also achieves
the second place during the testing phase.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the International Brain Tumor Segmentation (BraTS)
  challenge organized at MICCAI 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ U-Net Based Healthy 3D Brain Tissue Inpainting <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juexin Zhang, Ying Weng, Ke Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel approach to synthesize healthy 3D brain tissue
from masked input images, specifically focusing on the task of 'ASNR-MICCAI
BraTS Local Synthesis of Tissue via Inpainting'. Our proposed method employs a
U-Net-based architecture, which is designed to effectively reconstruct the
missing or corrupted regions of brain MRI scans. To enhance our model's
generalization capabilities and robustness, we implement a comprehensive data
augmentation strategy that involves randomly masking healthy images during
training. Our model is trained on the BraTS-Local-Inpainting dataset and
demonstrates the exceptional performance in recovering healthy brain tissue.
The evaluation metrics employed, including Structural Similarity Index (SSIM),
Peak Signal-to-Noise Ratio (PSNR), and Mean Squared Error (MSE), consistently
yields impressive results. On the BraTS-Local-Inpainting validation set, our
model achieved an SSIM score of 0.841, a PSNR score of 23.257, and an MSE score
of 0.007. Notably, these evaluation metrics exhibit relatively low standard
deviations, i.e., 0.103 for SSIM score, 4.213 for PSNR score and 0.007 for MSE
score, which indicates that our model's reliability and consistency across
various input scenarios. Our method also secured first place in the challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the International Brain Tumor Segmentation (BraTS)
  challenge organized at MICCAI 2024 conference. Included 7 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Actively evaluating and learning the distinctions that matter: Vaccine
  safety signal <span class="highlight-title">detection</span> from emergency triage notes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sedigh Khademi, Christopher Palmer, Muhammad Javed, Hazel Clothier, Jim Buttery, Gerardo Luis Dimaguila, Jim Black
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of COVID-19 vaccines has showcased the global
communitys ability to combat infectious diseases. However, the need for
post-licensure surveillance systems has grown due to the limited window for
safety data collection in clinical trials and early widespread implementation.
This study aims to employ Natural Language Processing techniques and Active
Learning to rapidly develop a classifier that detects potential vaccine safety
issues from emergency department notes. ED triage notes, containing expert,
succinct vital patient information at the point of entry to health systems, can
significantly contribute to timely vaccine safety signal surveillance. While
keyword-based classification can be effective, it may yield false positives and
demand extensive keyword modifications. This is exacerbated by the infrequency
of vaccination-related ED presentations and their similarity to other reasons
for ED visits. NLP offers a more accurate and efficient alternative, albeit
requiring annotated data, which is often scarce in the medical field. Active
learning optimizes the annotation process and the quality of annotated data,
which can result in faster model implementation and improved model performance.
This work combines active learning, data augmentation, and active learning and
evaluation techniques to create a classifier that is used to enhance vaccine
safety surveillance from ED triage notes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker
  Characteristic Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjie Chen, Zehan Li, Yaodong Song, Wenming Deng, Yitong Yao, Yuxin Zhang, Hang Lv, Xuechao Zhu, Jian Kang, Jie Lian, Jie Li, Chao Wang, Shuangyong Song, Yongxiang Li, Zhongjiang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in end-to-end spoken language models (SLMs) have
significantly improved the ability of AI systems to engage in natural spoken
interactions. However, most existing models treat speech merely as a vehicle
for linguistic content, often overlooking the rich paralinguistic and speaker
characteristic cues embedded in human speech, such as dialect, age, emotion,
and non-speech vocalizations. In this work, we introduce GOAT-SLM, a novel
spoken language model with paralinguistic and speaker characteristic awareness,
designed to extend spoken language modeling beyond text semantics. GOAT-SLM
adopts a dual-modality head architecture that decouples linguistic modeling
from acoustic realization, enabling robust language understanding while
supporting expressive and adaptive speech generation. To enhance model
efficiency and versatility, we propose a modular, staged training strategy that
progressively aligns linguistic, paralinguistic, and speaker characteristic
information using large-scale speech-text corpora. Experimental results on
TELEVAL, a multi-dimensional evaluation benchmark, demonstrate that GOAT-SLM
achieves well-balanced performance across both semantic and non-semantic tasks,
and outperforms existing open-source models in handling emotion, dialectal
variation, and age-sensitive interactions. This work highlights the importance
of modeling beyond linguistic content and advances the development of more
natural, adaptive, and socially aware spoken language systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agentic AI framework for End-to-End Medical Data Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18115v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18115v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soorya Ram Shimgekar, Shayan Vassef, Abhay Goyal, Navin Kumar, Koustuv Saha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building and deploying machine learning solutions in healthcare remains
expensive and labor-intensive due to fragmented preprocessing workflows, model
compatibility issues, and stringent data privacy constraints. In this work, we
introduce an Agentic AI framework that automates the entire clinical data
pipeline, from ingestion to inference, through a system of modular,
task-specific agents. These agents handle both structured and unstructured
data, enabling automatic feature selection, model selection, and preprocessing
recommendation without manual intervention. We evaluate the system on publicly
available datasets from geriatrics, palliative care, and colonoscopy imaging.
For example, in the case of structured data (anxiety data) and unstructured
data (colonoscopy polyps data), the pipeline begins with file-type detection by
the Ingestion Identifier Agent, followed by the Data Anonymizer Agent ensuring
privacy compliance, where we first identify the data type and then anonymize
it. The Feature Extraction Agent identifies features using an embedding-based
approach for tabular data, extracting all column names, and a multi-stage
MedGemma-based approach for image data, which infers modality and disease name.
These features guide the Model-Data Feature Matcher Agent in selecting the
best-fit model from a curated repository. The Preprocessing Recommender Agent
and Preprocessing Implementor Agent then apply tailored preprocessing based on
data type and model requirements. Finally, the ``Model Inference Agent" runs
the selected model on the uploaded data and generates interpretable outputs
using tools like SHAP, LIME, and DETR attention maps. By automating these
high-friction stages of the ML lifecycle, the proposed framework reduces the
need for repeated expert intervention, offering a scalable, cost-efficient
pathway for operationalizing AI in clinical environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, 2 tables, BIBM conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameter-Efficient Fine-Tuning of 3D DDPM for MRI Image Generation
  Using Tensor Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binghua Li, Ziqing Chang, Tong Liang, Chao Li, Toshihisa Tanaka, Shigeki Aoki, Qibin Zhao, Zhe Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the challenge of parameter-efficient fine-tuning (PEFT) for
three-dimensional (3D) U-Net-based denoising diffusion probabilistic models
(DDPMs) in magnetic resonance imaging (MRI) image generation. Despite its
practical significance, research on parameter-efficient representations of 3D
convolution operations remains limited. To bridge this gap, we propose Tensor
Volumetric Operator (TenVOO), a novel PEFT method specifically designed for
fine-tuning DDPMs with 3D convolutional backbones. Leveraging tensor network
modeling, TenVOO represents 3D convolution kernels with lower-dimensional
tensors, effectively capturing complex spatial dependencies during fine-tuning
with few parameters. We evaluate TenVOO on three downstream brain MRI
datasets-ADNI, PPMI, and BraTS2021-by fine-tuning a DDPM pretrained on 59,830
T1-weighted brain MRI scans from the UK Biobank. Our results demonstrate that
TenVOO achieves state-of-the-art performance in multi-scale structural
similarity index measure (MS-SSIM), outperforming existing approaches in
capturing spatial dependencies while requiring only 0.3% of the trainable
parameters of the original model. Our code is available at:
https://github.com/xiaovhua/tenvoo
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributional Uncertainty for Out-of-Distribution <span class="highlight-title">Detection</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        JinYoung Kim, DaeUng Jo, Kimin Yun, Jeonghyo Song, Youngjoon Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating uncertainty from deep neural networks is a widely used approach
for detecting out-of-distribution (OoD) samples, which typically exhibit high
predictive uncertainty. However, conventional methods such as Monte Carlo (MC)
Dropout often focus solely on either model or data uncertainty, failing to
align with the semantic objective of OoD detection. To address this, we propose
the Free-Energy Posterior Network, a novel framework that jointly models
distributional uncertainty and identifying OoD and misclassified regions using
free energy. Our method introduces two key contributions: (1) a
free-energy-based density estimator parameterized by a Beta distribution, which
enables fine-grained uncertainty estimation near ambiguous or unseen regions;
and (2) a loss integrated within a posterior network, allowing direct
uncertainty estimation from learned parameters without requiring stochastic
sampling. By integrating our approach with the residual prediction branch (RPL)
framework, the proposed method goes beyond post-hoc energy thresholding and
enables the network to learn OoD regions by leveraging the variance of the Beta
distribution, resulting in a semantically meaningful and computationally
efficient solution for uncertainty-aware segmentation. We validate the
effectiveness of our method on challenging real-world benchmarks, including
Fishyscapes, RoadAnomaly, and Segment-Me-If-You-Can.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages , 3 figures , IEEE International Conference on Advanced
  Visual and Signal-Based Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Dataset</span>s and Recipes for Video Temporal Grounding via <span class="highlight-title">Reinforcement</span>
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruizhe Chen, Zhiting Fan, Tianze Luo, Heqing Zou, Zhaopeng Feng, Guiyang Xie, Hansheng Zhang, Zhuochen Wang, Zuozhu Liu, Huaijian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Temporal Grounding (VTG) aims to localize relevant temporal segments in
videos given natural language queries. Despite recent progress with large
vision-language models (LVLMs) and instruction-tuning, existing approaches
often suffer from limited temporal awareness and poor generalization. In this
work, we introduce a two-stage training framework that integrates supervised
fine-tuning with reinforcement learning (RL) to improve both the accuracy and
robustness of VTG models. Our approach first leverages high-quality curated
cold start data for SFT initialization, followed by difficulty-controlled RL to
further enhance temporal localization and reasoning abilities. Comprehensive
experiments on multiple VTG benchmarks demonstrate that our method consistently
outperforms existing models, particularly in challenging and open-domain
scenarios. We conduct an in-depth analysis of training strategies and dataset
curation, highlighting the importance of both high-quality cold start data and
difficulty-controlled RL. To facilitate further research and industrial
adoption, we release all intermediate datasets, models, and code to the
community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TextSAM-EUS: Text Prompt Learning for SAM to Accurately Segment
  Pancreatic Tumor in Endoscopic Ultrasound <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18082v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18082v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Spiegler, Taha Koleilat, Arash Harirpoush, Corey S. Miller, Hassan Rivaz, Marta Kersten-Oertel, Yiming Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pancreatic cancer carries a poor prognosis and relies on endoscopic
ultrasound (EUS) for targeted biopsy and radiotherapy. However, the speckle
noise, low contrast, and unintuitive appearance of EUS make segmentation of
pancreatic tumors with fully supervised deep learning (DL) models both
error-prone and dependent on large, expert-curated annotation datasets. To
address these challenges, we present TextSAM-EUS, a novel, lightweight,
text-driven adaptation of the Segment Anything Model (SAM) that requires no
manual geometric prompts at inference. Our approach leverages text prompt
learning (context optimization) through the BiomedCLIP text encoder in
conjunction with a LoRA-based adaptation of SAM's architecture to enable
automatic pancreatic tumor segmentation in EUS, tuning only 0.86% of the total
parameters. On the public Endoscopic Ultrasound Database of the Pancreas,
TextSAM-EUS with automatic prompts attains 82.69% Dice and 85.28% normalized
surface distance (NSD), and with manual geometric prompts reaches 83.10% Dice
and 85.70% NSD, outperforming both existing state-of-the-art (SOTA) supervised
DL models and foundation models (e.g., SAM and its variants). As the first
attempt to incorporate prompt learning in SAM-based medical image segmentation,
TextSAM-EUS offers a practical option for efficient and robust automatic EUS
segmentation. Our code will be publicly available upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV 2025 Workshop CVAMD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AlphaGo Moment for Model Architecture Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixiu Liu, Yang Nan, Weixian Xu, Xiangkun Hu, Lyumanshan Ye, Zhen Qin, Pengfei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While AI systems demonstrate exponentially improving capabilities, the pace
of AI research itself remains linearly bounded by human cognitive capacity,
creating an increasingly severe development bottleneck. We present ASI-Arch,
the first demonstration of Artificial Superintelligence for AI research
(ASI4AI) in the critical domain of neural architecture discovery--a fully
autonomous system that shatters this fundamental constraint by enabling AI to
conduct its own architectural innovation. Moving beyond traditional Neural
Architecture Search (NAS), which is fundamentally limited to exploring
human-defined spaces, we introduce a paradigm shift from automated optimization
to automated innovation. ASI-Arch can conduct end-to-end scientific research in
the domain of architecture discovery, autonomously hypothesizing novel
architectural concepts, implementing them as executable code, training and
empirically validating their performance through rigorous experimentation and
past experience. ASI-Arch conducted 1,773 autonomous experiments over 20,000
GPU hours, culminating in the discovery of 106 innovative, state-of-the-art
(SOTA) linear attention architectures. Like AlphaGo's Move 37 that revealed
unexpected strategic insights invisible to human players, our AI-discovered
architectures demonstrate emergent design principles that systematically
surpass human-designed baselines and illuminate previously unknown pathways for
architectural innovation. Crucially, we establish the first empirical scaling
law for scientific discovery itself--demonstrating that architectural
breakthroughs can be scaled computationally, transforming research progress
from a human-limited to a computation-scalable process. We provide
comprehensive analysis of the emergent design patterns and autonomous research
capabilities that enabled these breakthroughs, establishing a blueprint for
self-accelerating AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Group Sequence Policy <span class="highlight-title">Optimization</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, Junyang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Group Sequence Policy Optimization (GSPO), our stable,
efficient, and performant reinforcement learning algorithm for training large
language models. Unlike previous algorithms that adopt token-level importance
ratios, GSPO defines the importance ratio based on sequence likelihood and
performs sequence-level clipping, rewarding, and optimization. We demonstrate
that GSPO achieves superior training efficiency and performance compared to the
GRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and
has the potential for simplifying the design of RL infrastructure. These merits
of GSPO have contributed to the remarkable improvements in the latest Qwen3
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TELEVAL: A <span class="highlight-title">Dynamic</span> <span class="highlight-title">Benchmark</span> Designed for Spoken Language Models in
  Chinese Interactive Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18061v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18061v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehan Li, Hongjie Chen, Yuxin Zhang, Jing Zhou, Xuening Wang, Hang Lv, Mengjie Du, Yaodong Song, Jie Lian, Jian Kang, Jie Li, Yongxiang Li, Zhongjiang He, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spoken language models (SLMs) have seen rapid progress in recent years, along
with the development of numerous benchmarks for evaluating their performance.
However, most existing benchmarks primarily focus on evaluating whether SLMs
can perform complex tasks comparable to those tackled by large language models
(LLMs), often failing to align with how users naturally interact in real-world
conversational scenarios. In this paper, we propose TELEVAL, a dynamic
benchmark specifically designed to evaluate SLMs' effectiveness as
conversational agents in realistic Chinese interactive settings. TELEVAL
defines three evaluation dimensions: Explicit Semantics, Paralinguistic and
Implicit Semantics, and System Abilities. It adopts a dialogue format
consistent with real-world usage and evaluates text and audio outputs
separately. TELEVAL particularly focuses on the model's ability to extract
implicit cues from user speech and respond appropriately without additional
instructions. Our experiments demonstrate that despite recent progress,
existing SLMs still have considerable room for improvement in natural
conversational tasks. We hope that TELEVAL can serve as a user-centered
evaluation framework that directly reflects the user experience and contributes
to the development of more capable dialogue-oriented SLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Agent Guided Policy <span class="highlight-title">Optimization</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueheng Li, Guangming Xie, Zongqing Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to practical constraints such as partial observability and limited
communication, Centralized Training with Decentralized Execution (CTDE) has
become the dominant paradigm in cooperative Multi-Agent Reinforcement Learning
(MARL). However, existing CTDE methods often underutilize centralized training
or lack theoretical guarantees. We propose Multi-Agent Guided Policy
Optimization (MAGPO), a novel framework that better leverages centralized
training by integrating centralized guidance with decentralized execution.
MAGPO uses an auto-regressive joint policy for scalable, coordinated
exploration and explicitly aligns it with decentralized policies to ensure
deployability under partial observability. We provide theoretical guarantees of
monotonic policy improvement and empirically evaluate MAGPO on 43 tasks across
6 diverse environments. Results show that MAGPO consistently outperforms strong
CTDE baselines and matches or surpasses fully centralized approaches, offering
a principled and practical solution for decentralized multi-agent learning. Our
code and experimental data can be found in https://github.com/liyheng/MAGPO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Scene Transition Awareness in Video Generation via
  Post-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanwen Shen, Jiajie Lu, Yupeng Cao, Xiaonan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in AI-generated video have shown strong performance on
\emph{text-to-video} tasks, particularly for short clips depicting a single
scene. However, current models struggle to generate longer videos with coherent
scene transitions, primarily because they cannot infer when a transition is
needed from the prompt. Most open-source models are trained on datasets
consisting of single-scene video clips, which limits their capacity to learn
and respond to prompts requiring multiple scenes. Developing scene transition
awareness is essential for multi-scene generation, as it allows models to
identify and segment videos into distinct clips by accurately detecting
transitions.
  To address this, we propose the \textbf{Transition-Aware Video} (TAV)
dataset, which consists of preprocessed video clips with multiple scene
transitions. Our experiment shows that post-training on the \textbf{TAV}
dataset improves prompt-based scene transition understanding, narrows the gap
between required and generated scenes, and maintains image quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthetic Data Generation for Phrase Break <span class="highlight-title">Prediction</span> with Large
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoyeon Lee, Sejung Son, Ye-Eun Kang, Jong-Hwan Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current approaches to phrase break prediction address crucial prosodic
aspects of text-to-speech systems but heavily rely on vast human annotations
from audio or text, incurring significant manual effort and cost. Inherent
variability in the speech domain, driven by phonetic factors, further
complicates acquiring consistent, high-quality data. Recently, large language
models (LLMs) have shown success in addressing data challenges in NLP by
generating tailored synthetic data while reducing manual annotation needs.
Motivated by this, we explore leveraging LLM to generate synthetic phrase break
annotations, addressing the challenges of both manual annotation and
speech-related tasks by comparing with traditional annotations and assessing
effectiveness across multiple languages. Our findings suggest that LLM-based
synthetic data generation effectively mitigates data challenges in phrase break
prediction and highlights the potential of LLMs as a viable solution for the
speech domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Interspeech 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs
  and VLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duy Nguyen, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inference-time steering methods offer a lightweight alternative to
fine-tuning large language models (LLMs) and vision-language models (VLMs) by
modifying internal activations at test time without updating model weights.
However, most existing approaches rely on fixed, global intervention vectors,
overlook the causal influence of individual input tokens, and fail to leverage
informative gradients from the model's logits, particularly in multimodal
settings where visual and textual inputs contribute unevenly. To address these
limitations, we introduce GrAInS, an inference-time steering approach that
operates across both language-only and vision-language models and tasks. GrAInS
uses contrastive, gradient-based attribution via Integrated Gradients to
identify the top-k most influential tokens, both positively and negatively
attributed based on their contribution to preferred versus dispreferred
outputs. These tokens are then used to construct directional steering vectors
that capture semantic shifts from undesirable to desirable behavior. During
inference, GrAInS adjusts hidden activations at transformer layers guided by
token-level attribution signals, and normalizes activations to preserve
representational scale. This enables fine-grained, interpretable, and modular
control over model behavior, without retraining or auxiliary supervision.
Empirically, GrAInS consistently outperforms both fine-tuning and existing
steering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using
Llama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514
with LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all
while preserving the model's fluency and general capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages. Code: https://github.com/duykhuongnguyen/GrAInS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenNav: Open-World <span class="highlight-title">Navigation</span> with Multimodal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18033v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18033v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingfeng Yuan, Letian Wang, Steven L. Waslander
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained large language models (LLMs) have demonstrated strong
common-sense reasoning abilities, making them promising for robotic navigation
and planning tasks. However, despite recent progress, bridging the gap between
language descriptions and actual robot actions in the open-world, beyond merely
invoking limited predefined motion primitives, remains an open challenge. In
this work, we aim to enable robots to interpret and decompose complex language
instructions, ultimately synthesizing a sequence of trajectory points to
complete diverse navigation tasks given open-set instructions and open-set
objects. We observe that multi-modal large language models (MLLMs) exhibit
strong cross-modal understanding when processing free-form language
instructions, demonstrating robust scene comprehension. More importantly,
leveraging their code-generation capability, MLLMs can interact with
vision-language perception models to generate compositional 2D bird-eye-view
value maps, effectively integrating semantic knowledge from MLLMs with spatial
information from maps to reinforce the robot's spatial understanding. To
further validate our approach, we effectively leverage large-scale autonomous
vehicle datasets (AVDs) to validate our proposed zero-shot vision-language
navigation framework in outdoor navigation tasks, demonstrating its capability
to execute a diverse range of free-form natural language navigation
instructions while maintaining robustness against object detection errors and
linguistic ambiguities. Furthermore, we validate our system on a Husky robot in
both indoor and outdoor scenes, demonstrating its real-world robustness and
applicability. Supplementary videos are available at
https://trailab.github.io/OpenNav-website/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViGText: Deepfake Image <span class="highlight-title">Detection</span> with Vision-Language Model
  Explanations and <span class="highlight-title">Graph</span> Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18031v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18031v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad ALBarqawi, Mahmoud Nazzal, Issa Khalil, Abdallah Khreishah, NhatHai Phan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid rise of deepfake technology, which produces realistic but
fraudulent digital content, threatens the authenticity of media. Traditional
deepfake detection approaches often struggle with sophisticated, customized
deepfakes, especially in terms of generalization and robustness against
malicious attacks. This paper introduces ViGText, a novel approach that
integrates images with Vision Large Language Model (VLLM) Text explanations
within a Graph-based framework to improve deepfake detection. The novelty of
ViGText lies in its integration of detailed explanations with visual data, as
it provides a more context-aware analysis than captions, which often lack
specificity and fail to reveal subtle inconsistencies. ViGText systematically
divides images into patches, constructs image and text graphs, and integrates
them for analysis using Graph Neural Networks (GNNs) to identify deepfakes.
Through the use of multi-level feature extraction across spatial and frequency
domains, ViGText captures details that enhance its robustness and accuracy to
detect sophisticated deepfakes. Extensive experiments demonstrate that ViGText
significantly enhances generalization and achieves a notable performance boost
when it detects user-customized deepfakes. Specifically, average F1 scores rise
from 72.45% to 98.32% under generalization evaluation, and reflects the model's
superior ability to generalize to unseen, fine-tuned variations of stable
diffusion models. As for robustness, ViGText achieves an increase of 11.1% in
recall compared to other deepfake detection approaches. When facing targeted
attacks that exploit its graph-based architecture, ViGText limits
classification performance degradation to less than 4%. ViGText uses detailed
visual and textual analysis to set a new standard for detecting deepfakes,
helping ensure media authenticity and information integrity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural
  KV Database 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18028v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18028v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weizhi Fei, Hao Shi, Jing Xu, Jingchen Peng, Jiazheng Li, Jingzhao Zhang, Bo Bai, Wei Han, Zhenyuan Chen, Xueyan Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficiently editing knowledge stored in large language models (LLMs) enables
model updates without large-scale training. One possible solution is
Locate-and-Edit (L\&E), allowing simultaneous modifications of a massive number
of facts. However, such editing may compromise the general abilities of LLMs
and even result in forgetting edited facts when scaling up to thousands of
edits. In this paper, we model existing linear L\&E methods as querying a
Key-Value (KV) database. From this perspective, we then propose NeuralDB, an
editing framework that explicitly represents the edited facts as a neural KV
database equipped with a non-linear gated retrieval module, % In particular,
our gated module only operates when inference involves the edited facts,
effectively preserving the general abilities of LLMs. Comprehensive experiments
involving the editing of 10,000 facts were conducted on the ZsRE and
CounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results
demonstrate that NeuralDB not only excels in editing efficacy, generalization,
specificity, fluency, and consistency, but also preserves overall performance
across six representative text understanding and generation tasks. Further
experiments indicate that NeuralDB maintains its effectiveness even when scaled
to 100,000 facts (\textbf{50x} more than in prior work).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does visualization help AI understand data? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18022v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18022v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victoria R. Li, Johnathan Sun, Martin Wattenberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Charts and graphs help people analyze data, but can they also be useful to AI
systems? To investigate this question, we perform a series of experiments with
two commercial vision-language models: GPT 4.1 and Claude 3.5. Across three
representative analysis tasks, the two systems describe synthetic datasets more
precisely and accurately when raw data is accompanied by a scatterplot,
especially as datasets grow in complexity. Comparison with two baselines --
providing a blank chart and a chart with mismatched data -- shows that the
improved performance is due to the content of the charts. Our results are
initial evidence that AI systems, like humans, can benefit from visualization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fashion-AlterEval: A <span class="highlight-title">Dataset</span> for Improved <span class="highlight-title">Evaluation</span> of Conversational
  Recommendation Systems with Alternative Relevant Items 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18017v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18017v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Vlachou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Conversational Recommendation Systems (CRS), a user provides feedback on
recommended items at each turn, leading the CRS towards improved
recommendations. Due to the need for a large amount of data, a user simulator
is employed for both training and evaluation. Such user simulators critique the
current retrieved item based on knowledge of a single target item. However,
system evaluation in offline settings with simulators is limited by the focus
on a single target item and their unlimited patience over a large number of
turns. To overcome these limitations of existing simulators, we propose
Fashion-AlterEval, a new dataset that contains human judgments for a selection
of alternative items by adding new annotations in common fashion CRS datasets.
Consequently, we propose two novel meta-user simulators that use the collected
judgments and allow simulated users not only to express their preferences about
alternative items to their original target, but also to change their mind and
level of patience. In our experiments using the Shoes and Fashion IQ as the
original datasets and three CRS models, we find that using the knowledge of
alternatives by the simulator can have a considerable impact on the evaluation
of existing CRS models, specifically that the existing single-target evaluation
underestimates their effectiveness, and when simulatedusers are allowed to
instead consider alternative relevant items, the system can rapidly respond to
more quickly satisfy the user.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2401.05783</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GRR-CoCa: Leveraging LLM Mechanisms in Multimodal Model Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jake R. Patock, Nicole Catherine Lewis, Kevin McCoy, Christina Gomez, Canling Chen, Lorenzo Luzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art (SOTA) image and text generation models are multimodal
models that have many similarities to large language models (LLMs). Despite
achieving strong performances, leading foundational multimodal model
architectures frequently lag behind the architectural sophistication of
contemporary LLMs. We propose GRR-CoCa, an improved SOTA Contrastive Captioner
(CoCa) model that incorporates Gaussian error gated linear units, root mean
squared normalization, and rotary positional embedding into the textual
decoders and the vision transformer (ViT) encoder. Each architectural
modification has been shown to improve model performance in LLMs, but has yet
to be adopted in CoCa. We benchmarked GRR-CoCa against Baseline CoCa, a model
with the same modified textual decoders but with CoCa's original ViT encoder.
We used standard pretraining and fine-tuning workflows to benchmark the models
on contrastive and generative tasks. Our GRR-CoCa significantly outperformed
Baseline CoCa on the pretraining dataset and three diverse fine-tuning
datasets. Pretraining improvements were 27.25% in contrastive loss, 3.71% in
perplexity, and 7.15% in CoCa loss. The average fine-tuning improvements were
13.66% in contrastive loss, 5.18% in perplexity, and 5.55% in CoCa loss. We
show that GRR-CoCa's modified architecture improves performance and
generalization across vision-language domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ E.A.R.T.H.: Structuring Creative Evolution through Model Error in
  Generative AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18004v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18004v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusen Peng, Shuhua Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can AI move beyond imitation toward genuine creativity? This paper
proposes the E.A.R.T.H. framework, a five-stage generative pipeline that
transforms model-generated errors into creative assets through Error
generation, Amplification, Refine selection, Transform, and Harness feedback.
Drawing on cognitive science and generative modeling, we posit that "creative
potential hides in failure" and operationalize this via structured prompts,
semantic scoring, and human-in-the-loop evaluation. Implemented using
LLaMA-2-7B-Chat, SBERT, BERTScore, CLIP, BLIP-2, and Stable Diffusion, the
pipeline employs a composite reward function based on novelty, surprise, and
relevance. At the Refine stage, creativity scores increase by 52.5% (1.179 to
1.898, t = -5.56, p < 0.001), with final outputs reaching 2.010 - a 70.4%
improvement. Refined slogans are 48.4% shorter, 40.7% more novel, with only a
4.0% drop in relevance. Cross-modal tests show strong slogan-to-image alignment
(CLIPScore: 0.249; BERTScore F1: 0.816). In human evaluations, 60% of outputs
scored >= 4.0, with metaphorical slogans (avg. 4.09) outperforming literal ones
(3.99). Feedback highlights stylistic precision and emotional resonance. These
results demonstrate that error-centered, feedback-driven generation enhances
creativity, offering a scalable path toward self-evolving, human-aligned
creative AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages,11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dif<span class="highlight-title">fusion</span> Beats Autoregressive in Data-Constrained Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15857v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15857v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihir Prabhudesai, Menging Wu, Amir Zadeh, Katerina Fragkiadaki, Deepak Pathak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive (AR) models have long dominated the landscape of large
language models, driving progress across a wide range of tasks. Recently,
diffusion-based language models have emerged as a promising alternative, though
their advantages over AR models remain underexplored. In this paper, we
systematically study masked diffusion models in data-constrained settings-where
training involves repeated passes over limited data-and find that they
significantly outperform AR models when compute is abundant but data is scarce.
Diffusion models make better use of repeated data, achieving lower validation
loss and superior downstream performance. We interpret this advantage as
implicit data augmentation: masked diffusion exposes the model to a diverse
distribution of token orderings and prediction tasks, unlike AR's fixed
left-to-right factorization. We find new scaling laws for diffusion models and
derive a closed-form expression for the critical compute threshold at which
diffusion begins to outperform AR. These results suggest that when data, not
compute, is the bottleneck, diffusion models offer a compelling alternative to
the standard AR paradigm. Our code is available at:
https://diffusion-scaling.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Webpage: https://diffusion-scaling.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BEARCUBS: A <span class="highlight-title">benchmark</span> for computer-using web agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.07919v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.07919v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixiao Song, Katherine Thai, Chau Minh Pham, Yapei Chang, Mazin Nadaf, Mohit Iyyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern web agents possess computer use abilities that allow them to interact
with webpages by sending commands to a virtual keyboard and mouse. While such
agents have considerable potential to assist human users with complex tasks,
evaluating their capabilities in real-world settings poses a major challenge.
To this end, we introduce BEARCUBS, a "smallbut mighty" benchmark of 111
information-seeking questions designed to evaluate a web agent's ability to
search, browse, and identify factual information from the web. Unlike prior web
agent benchmarks, solving BEARCUBS requires (1) accessing live web content
rather than synthetic or simulated pages, which captures the unpredictability
of real-world web interactions; and (2) performing a broad range of multimodal
interactions (e.g., video understanding, 3D navigation) that cannot be bypassed
via text-based workarounds. Each question in BEARCUBS has a corresponding
short, unambiguous answer and a human-validated browsing trajectory, allowing
for transparent evaluation of agent performance and strategies. A human study
confirms that BEARCUBS questions are solvable but non-trivial (84.7% human
accuracy), revealing domain knowledge gaps and overlooked details as common
failure points. We find that ChatGPT Agent significantly outperforms other
computer-using agents with an overall accuracy of 65.8% (compared to e.g.,
Operator's 23.4%), showcasing substantial progress in tasks involving real
computer use, such as playing web games and navigating 3D environments.
Nevertheless, closing the gap to human performance requires improvements in
areas like fine control, complex data filtering, and execution speed. To
facilitate future research, BEARCUBS will be updated periodically to replace
invalid or contaminated questions, keeping the benchmark fresh for future
generations of web agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16870v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16870v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Anshumann, Mohd Abbas Zaidi, Akhil Kedia, Jinwoo Ahn, Taehwak Kwon, Kangwook Lee, Haejun Lee, Joohyung Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation can be a cost-effective technique to distill knowledge
in Large Language Models, if the teacher output logits can be pre-computed and
cached. However, successfully applying this to pre-training remains largely
unexplored. In this work, we prove that naive approaches for sparse knowledge
distillation such as caching Top-K probabilities, while intuitive, provide
biased estimates of teacher probability distribution to the student, resulting
in suboptimal performance and calibration. We propose an
importance-sampling-based method `Random Sampling Knowledge Distillation',
which provides unbiased estimates, preserves the gradient in expectation, and
requires storing significantly sparser logits. Our method enables faster
training of student models with marginal overhead (<10%) compared to
cross-entropy based training, while maintaining competitive performance
compared to full distillation, across a range of model sizes from 300M to 3B.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as Oral paper at ACL 2025. Source code is available at
  https://github.com/akhilkedia/RandomSamplingKD . Anshumann, Mohd Abbas Zaidi
  and Akhil Kedia have Equal Contribution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MambaNeXt-YOLO: A Hybrid State Space Model for Real-time Object
  <span class="highlight-title">Detection</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03654v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03654v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaochun Lei, Siqi Wu, Weilin Wu, Zetao Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time object detection is a fundamental but challenging task in computer
vision, particularly when computational resources are limited. Although
YOLO-series models have set strong benchmarks by balancing speed and accuracy,
the increasing need for richer global context modeling has led to the use of
Transformer-based architectures. Nevertheless, Transformers have high
computational complexity because of their self-attention mechanism, which
limits their practicality for real-time and edge deployments. To overcome these
challenges, recent developments in linear state space models, such as Mamba,
provide a promising alternative by enabling efficient sequence modeling with
linear complexity. Building on this insight, we propose MambaNeXt-YOLO, a novel
object detection framework that balances accuracy and efficiency through three
key contributions: (1) MambaNeXt Block: a hybrid design that integrates CNNs
with Mamba to effectively capture both local features and long-range
dependencies; (2) Multi-branch Asymmetric Fusion Pyramid Network (MAFPN): an
enhanced feature pyramid architecture that improves multi-scale object
detection across various object sizes; and (3) Edge-focused Efficiency: our
method achieved 66.6% mAP at 31.9 FPS on the PASCAL VOC dataset without any
pre-training and supports deployment on edge devices such as the NVIDIA Jetson
Xavier NX and Orin NX.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is under consideration at Image and Vision Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling RL to Long Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07966v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07966v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, Sifei Liu, Hongxu Yin, Yao Lu, Song Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a full-stack framework that scales up reasoning in
vision-language models (VLMs) to long videos, leveraging reinforcement
learning. We address the unique challenges of long video reasoning by
integrating three critical components: (1) a large-scale dataset,
LongVideo-Reason, comprising 104K long video QA pairs with high-quality
reasoning annotations across diverse domains such as sports, games, and vlogs;
(2) a two-stage training pipeline that extends VLMs with chain-of-thought
supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a
training infrastructure for long video RL, named Multi-modal Reinforcement
Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a
vLLM-based engine tailored for long video, using cached video embeddings for
efficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves
strong performance on video benchmarks, reaching 65.0% and 70.7% accuracy on
VideoMME without and with subtitles, respectively, and consistently
outperforming LongVILA-R1 across multiple benchmarks. Moreover, LongVILA-R1
shows steady performance improvements as the number of input video frames
increases. Notably, our MR-SP system achieves up to 2.1x speedup on long video
RL training. In addition, we release our training system for public
availability that supports RL training on various modalities (video, text, and
audio), various models (VILA and Qwen series), and even image and video
generation models. On a single A100 node (8 GPUs), it supports RL training on
hour-long videos (e.g., 3,600 frames / around 256k tokens).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code at https://github.com/NVlabs/Long-RL and model at
  https://huggingface.co/Efficient-Large-Model/LongVILA-R1-7B</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RUMI: Rummaging Using Mutual Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10450v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10450v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng Zhong, Nima Fazeli, Dmitry Berenson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Rummaging Using Mutual Information (RUMI), a method for
online generation of robot action sequences to gather information about the
pose of a known movable object in visually-occluded environments. Focusing on
contact-rich rummaging, our approach leverages mutual information between the
object pose distribution and robot trajectory for action planning. From an
observed partial point cloud, RUMI deduces the compatible object pose
distribution and approximates the mutual information of it with workspace
occupancy in real time. Based on this, we develop an information gain cost
function and a reachability cost function to keep the object within the robot's
reach. These are integrated into a model predictive control (MPC) framework
with a stochastic dynamics model, updating the pose distribution in a closed
loop. Key contributions include a new belief framework for object pose
estimation, an efficient information gain computation strategy, and a robust
MPC-based control scheme. RUMI demonstrates superior performance in both
simulated and real tasks compared to baseline methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 20 figures, accepted by IEEE Transactions on Robotics
  (T-RO), preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compliance Brain Assistant: Conversational Agentic AI for Assisting
  Compliance Tasks in Enterprise Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17289v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17289v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shitong Zhu, Chenhao Fang, Derek Larson, Neel Reddy Pochareddy, Rajeev Rao, Sophie Zeng, Yanqing Peng, Wendy Summer, Alex Goncalves, Arya Pudota, Hervé Robert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Compliance Brain Assistant (CBA), a conversational,
agentic AI assistant designed to boost the efficiency of daily compliance tasks
for personnel in enterprise environments. To strike a good balance between
response quality and latency, we design a user query router that can
intelligently choose between (i) FastTrack mode: to handle simple requests that
only need additional relevant context retrieved from knowledge corpora; and
(ii) FullAgentic mode: to handle complicated requests that need composite
actions and tool invocations to proactively discover context across various
compliance artifacts, and/or involving other APIs/models for accommodating
requests. A typical example would be to start with a user query, use its
description to find a specific entity and then use the entity's information to
query other APIs for curating and enriching the final AI response.
  Our experimental evaluations compared CBA against an out-of-the-box LLM on
various real-world privacy/compliance-related queries targeting various
personas. We found that CBA substantially improved upon the vanilla LLM's
performance on metrics such as average keyword match rate (83.7% vs. 41.7%) and
LLM-judge pass rate (82.0% vs. 20.0%). We also compared metrics for the full
routing-based design against the `fast-track only` and `full-agentic` modes and
found that it had a better average match-rate and pass-rate while keeping the
run-time approximately the same. This finding validated our hypothesis that the
routing mechanism leads to a good trade-off between the two worlds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Omni-Thinker: Scaling Cross-Domain Generalization in LLMs via Multi-Task
  RL with Hybrid Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Derek Li, Jiaming Zhou, Amirreza Kazemi, Qianyi Sun, Abbas Ghaddar, Mohammad Ali Alomrani, Liheng Ma, Yu Luo, Dong Li, Feng Wen, Jianye Hao, Mark Coates, Yingxue Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of general-purpose artificial intelligence relies on large
language models (LLMs) that excel across a wide range of tasks, from structured
reasoning to creative generation. However, post-training methods like
Supervised Fine-Tuning (SFT) often struggle with generalization, favoring
memorization over transferable learning. In this work, we introduce
Omni-Thinker, a unified reinforcement learning (RL) framework that enhances LLM
performance across diverse tasks by combining rule-based verifiable rewards
with generative preference signals via LLM-as-a-Judge evaluations. Our approach
enables consistent optimization across task types and scales RL-based training
to subjective domains. We further investigate training strategies,
demonstrating that a curriculum-based progression that orders tasks from
structured to open-ended improves performance and reduces forgetting.
Experimental results across four domains reveal that curriculum learning
improves performance by 5.2% over joint training and 9.1% over model merging.
These results highlight the importance of task-aware sampling and hybrid
supervision in scaling RL-based post-training for general-purpose LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are
  Important 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.04704v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.04704v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manlai Liang, JiaMing Zhang, Xiong Li, Jinlong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing size of the Key-Value (KV) cache during the Large Language
Models long-context inference is the main obstacle for its balance between the
deployment cost and task accuracy. To reduce the KV cache size in such
scenarios, most previous efforts leveraged on the attention weight to evict
non-critical cache tokens. But there is a trade-off in those methods, they
usually require major modification of the inference infrastructure and
significant computation overhead. Based on the fact that the Large Language
models are autoregressive models, we propose LagKV, a KV compression strategy
only relying on straight forward comparison among KV themselves. It is a
totally attention free method which offers easy integration to the main stream
inference platform and comparable performance comparing to other complicated KV
compression methods. Results on RULER benchmark show that, our approach
outperforms SnapKV and StreamingLLM in different compression ratios. Especially
in the 64-digit passkey retrieval task, our method outperforms the attention
weight based method $H_2O$ over $50\%$ with same compression ratios. Our code
is available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zeroth-Order Fine-Tuning of LLMs in Random Subspaces <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08989v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08989v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziming Yu, Pan Zhou, Sike Wang, Jia Li, Mi Tian, Hua Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning Large Language Models (LLMs) has proven effective for a variety
of downstream tasks. However, as LLMs grow in size, the memory demands for
backpropagation become increasingly prohibitive. Zeroth-order (ZO) optimization
methods offer a memory-efficient alternative by using forward passes to
estimate gradients, but the variance of gradient estimates typically scales
linearly with the model's parameter dimension$\unicode{x2013}$a significant
issue for LLMs. In this paper, we propose the random Subspace Zeroth-order
(SubZero) optimization to address the challenges posed by LLMs' high
dimensionality. We introduce a low-rank perturbation tailored for LLMs that
significantly reduces memory consumption while improving training performance.
Additionally, we prove that our gradient estimation closely approximates the
backpropagation gradient, exhibits lower variance than traditional ZO methods,
and ensures convergence when combined with SGD. Experimental results show that
SubZero enhances fine-tuning performance and achieves faster convergence
compared to standard ZO approaches like MeZO across various language modeling
tasks. Code is available at https://github.com/zimingyy/SubZero.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025 camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Learning Solutions Integrated in an IoT Healthcare Platform for
  Heart Failure Risk Stratification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09619v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09619v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aiman Faiz, Anna Maria De Roberto, Claudio Pascarelli, Gianvito Mitrano, Gianluca Fimiani, Marina Garofano, Genoveffa Tortora, Mariangela Lazoi, Claudio Passino, Alessia Bramanti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The management of chronic Heart Failure (HF) presents significant challenges
in modern healthcare, requiring continuous monitoring, early detection of
exacerbations, and personalized treatment strategies. In this paper, we present
a predictive model founded on Machine Learning (ML) techniques to identify
patients at HF risk. This model is an ensemble learning approach, a modified
stacking technique, that uses two specialized models leveraging clinical and
echocardiographic features and then a meta-model to combine the predictions of
these two models. We initially assess the model on a real dataset and the
obtained results suggest that it performs well in the stratification of
patients at HR risk. Specifically, we obtained high sensitivity (95\%),
ensuring that nearly all high-risk patients are identified. As for accuracy, we
obtained 84\%, which can be considered moderate in some ML contexts. However,
it is acceptable given our priority of identifying patients at risk of HF
because they will be asked to participate in the telemonitoring program of the
PrediHealth research project on which some of the authors of this paper are
working. The initial findings also suggest that ML-based risk stratification
models can serve as valuable decision-support tools not only in the PrediHealth
project but also for healthcare professionals, aiding in early intervention and
personalized patient management. To have a better understanding of the value
and of potentiality of our predictive model, we also contrasted its results
with those obtained by using three baseline models. The preliminary results
indicate that our predictive model outperforms these baselines that flatly
consider features, \ie not grouping them in clinical and echocardiographic
features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Diffuse and Disperse: Image Generation with Representation
  Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09027v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09027v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runqian Wang, <span class="highlight-author">Kaiming He</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of diffusion-based generative models over the past decade has
largely proceeded independently of progress in representation learning. These
diffusion models typically rely on regression-based objectives and generally
lack explicit regularization. In this work, we propose \textit{Dispersive
Loss}, a simple plug-and-play regularizer that effectively improves
diffusion-based generative models. Our loss function encourages internal
representations to disperse in the hidden space, analogous to contrastive
self-supervised learning, with the key distinction that it requires no positive
sample pairs and therefore does not interfere with the sampling process used
for regression. Compared to the recent method of representation alignment
(REPA), our approach is self-contained and minimalist, requiring no
pre-training, no additional parameters, and no external data. We evaluate
Dispersive Loss on the ImageNet dataset across a range of models and report
consistent improvements over widely used and strong baselines. We hope our work
will help bridge the gap between generative modeling and representation
learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GCC-Spam: Spam <span class="highlight-title">Detection</span> via GAN, Contrastive Learning, and Character
  Similarity Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14679v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14679v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijie Wang, Zixin Xu, Zhiyuan Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exponential growth of spam text on the Internet necessitates robust
detection mechanisms to mitigate risks such as information leakage and social
instability. This work addresses two principal challenges: adversarial
strategies employed by spammers and the scarcity of labeled data. We propose a
novel spam-text detection framework GCC-Spam, which integrates three core
innovations. First, a character similarity network captures orthographic and
phonetic features to counter character-obfuscation attacks and furthermore
produces sentence embeddings for downstream classification. Second, contrastive
learning enhances discriminability by optimizing the latent-space distance
between spam and normal texts. Third, a Generative Adversarial Network (GAN)
generates realistic pseudo-spam samples to alleviate data scarcity while
improving model robustness and classification accuracy. Extensive experiments
on real-world datasets demonstrate that our model outperforms baseline
approaches, achieving higher detection rates with significantly fewer labeled
examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sublinear Regret for a Class of Continuous-Time Linear-Quadratic
  <span class="highlight-title">Reinforcement</span> Learning Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17226v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17226v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilie Huang, Yanwei Jia, Xun Yu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study reinforcement learning (RL) for a class of continuous-time
linear-quadratic (LQ) control problems for diffusions, where states are
scalar-valued and running control rewards are absent but volatilities of the
state processes depend on both state and control variables. We apply a
model-free approach that relies neither on knowledge of model parameters nor on
their estimations, and devise an RL algorithm to learn the optimal policy
parameter directly. Our main contributions include the introduction of an
exploration schedule and a regret analysis of the proposed algorithm. We
provide the convergence rate of the policy parameter to the optimal one, and
prove that the algorithm achieves a regret bound of $O(N^{\frac{3}{4}})$ up to
a logarithmic factor, where $N$ is the number of learning episodes. We conduct
a simulation study to validate the theoretical results and demonstrate the
effectiveness and reliability of the proposed algorithm. We also perform
numerical comparisons between our method and those of the recent model-based
stochastic LQ RL studies adapted to the state- and control-dependent volatility
setting, demonstrating a better performance of the former in terms of regret
bounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 4 figures. Accepted for publication in SIAM Journal on
  Control and Optimization (2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Masked Autoencoders that Feel the Heart: Unveiling Simplicity Bias for
  ECG Analyses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.22495v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.22495v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He-Yang Xu, Hongxiang Gao, Yuwen Li, Xiu-Shen Wei, Chengyu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The diagnostic value of electrocardiogram (ECG) lies in its dynamic
characteristics, ranging from rhythm fluctuations to subtle waveform
deformations that evolve across time and frequency domains. However, supervised
ECG models tend to overfit dominant and repetitive patterns, overlooking
fine-grained but clinically critical cues, a phenomenon known as Simplicity
Bias (SB), where models favor easily learnable signals over subtle but
informative ones. In this work, we first empirically demonstrate the presence
of SB in ECG analyses and its negative impact on diagnostic performance, while
simultaneously discovering that self-supervised learning (SSL) can alleviate
it, providing a promising direction for tackling the bias. Following the SSL
paradigm, we propose a novel method comprising two key components: 1)
Temporal-Frequency aware Filters to capture temporal-frequency features
reflecting the dynamic characteristics of ECG signals, and 2) building on this,
Multi-Grained Prototype Reconstruction for coarse and fine representation
learning across dual domains, further mitigating SB. To advance SSL in ECG
analyses, we curate a large-scale multi-site ECG dataset with 1.53 million
recordings from over 300 clinical centers. Experiments on three downstream
tasks across six ECG datasets demonstrate that our method effectively reduces
SB and achieves state-of-the-art performance. Code and dataset will be released
publicly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>there are factual errors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DualXDA: Towards Sparse, Efficient and Explainable Data Attribution in
  Large AI Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12118v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12118v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Galip Ümit Yolcu, Moritz Weckbecker, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models achieve remarkable performance, yet their
decision-making processes often remain opaque. In response, the field of
eXplainable Artificial Intelligence (XAI) has grown significantly over the last
decade, primarily focusing on feature attribution methods. Complementing this
perspective, Data Attribution (DA) has emerged as a promising paradigm that
shifts the focus from features to data provenance. However, existing DA
approaches suffer from prohibitively high computational costs and memory
demands. Additionally, current attribution methods exhibit low sparsity,
hindering the discovery of decisive patterns in the data. We introduce DualXDA,
a framework for sparse, efficient and explainable DA, comprised of two
interlinked approaches for Dual Data Attribution (DualDA) and eXplainable Data
Attribution (XDA): With DualDA, we propose efficient and effective DA,
leveraging Support Vector Machine theory to provide fast and naturally sparse
data attributions for AI predictions. We demonstrate that DualDA achieves high
attribution quality, excels at solving a series of evaluated downstream tasks,
while at the same time improving explanation time by a factor of up to
4,100,000$\times$ compared to the original Influence Functions method, and up
to 11,000$\times$ compared to the method's most efficient approximation from
literature. We further introduce XDA, a method for enhancing Data Attribution
with capabilities from feature attribution methods to explain why training
samples are relevant for the prediction of a test sample in terms of impactful
features. Taken together, our contributions in DualXDA ultimately point towards
a future of eXplainable AI applied at unprecedented scale, enabling
transparent, efficient and novel analysis of even the largest neural
architectures fostering a new generation of accountable AI systems. Code at
https://github.com/gumityolcu/DualXDA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EarthLink: A Self-Evolving AI Agent for Climate Science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17311v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17311v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijie Guo, Jiong Wang, Xiaoyu Yue, Wangxu Wei, Zhe Jiang, Wanghan Xu, Ben Fei, Wenlong Zhang, Xinyu Gu, Lijing Cheng, Jing-Jia Luo, Chao Li, Yaqiang Wang, Tao Chen, Wanli Ouyang, Fenghua Ling, Lei Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern Earth science is at an inflection point. The vast, fragmented, and
complex nature of Earth system data, coupled with increasingly sophisticated
analytical demands, creates a significant bottleneck for rapid scientific
discovery. Here we introduce EarthLink, the first AI agent designed as an
interactive copilot for Earth scientists. It automates the end-to-end research
workflow, from planning and code generation to multi-scenario analysis. Unlike
static diagnostic tools, EarthLink can learn from user interaction,
continuously refining its capabilities through a dynamic feedback loop. We
validated its performance on a number of core scientific tasks of climate
change, ranging from model-observation comparisons to the diagnosis of complex
phenomena. In a multi-expert evaluation, EarthLink produced scientifically
sound analyses and demonstrated an analytical competency that was rated as
comparable to specific aspects of a human junior researcher's workflow.
Additionally, its transparent, auditable workflows and natural language
interface empower scientists to shift from laborious manual execution to
strategic oversight and hypothesis generation. EarthLink marks a pivotal step
towards an efficient, trustworthy, and collaborative paradigm for Earth system
research in an era of accelerating global change. The system is accessible at
our website https://earthlink.intern-ai.org.cn.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Unsupervised</span> Concept Drift <span class="highlight-title">Detection</span> from Deep Learning Representations
  in Real-time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17813v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17813v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salvatore Greco, Bartolomeo Vacchetti, Daniele Apiletti, Tania Cerquitelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept drift is the phenomenon in which the underlying data distributions
and statistical properties of a target domain change over time, leading to a
degradation in model performance. Consequently, production models require
continuous drift detection monitoring. Most drift detection methods to date are
supervised, relying on ground-truth labels. However, they are inapplicable in
many real-world scenarios, as true labels are often unavailable. Although
recent efforts have proposed unsupervised drift detectors, many lack the
accuracy required for reliable detection or are too computationally intensive
for real-time use in high-dimensional, large-scale production environments.
Moreover, they often fail to characterize or explain drift effectively.
  To address these limitations, we propose \textsc{DriftLens}, an unsupervised
framework for real-time concept drift detection and characterization. Designed
for deep learning classifiers handling unstructured data, \textsc{DriftLens}
leverages distribution distances in deep learning representations to enable
efficient and accurate detection. Additionally, it characterizes drift by
analyzing and explaining its impact on each label. Our evaluation across
classifiers and data-types demonstrates that \textsc{DriftLens} (i) outperforms
previous methods in detecting drift in 15/17 use cases; (ii) runs at least 5
times faster; (iii) produces drift curves that align closely with actual drift
(correlation $\geq\!0.85$); (iv) effectively identifies representative drift
samples as explanations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE Transactions on Knowledge and Data Engineering
  (TKDE)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging multi-source and heterogeneous signals for fatigue <span class="highlight-title">detection</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16859v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16859v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luobin Cui, Yanlai Wu, Tang Ying, Weikai Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fatigue detection plays a critical role in safety-critical applications such
as aviation, mining, and long-haul transport. However, most existing methods
rely on high-end sensors and controlled environments, limiting their
applicability in real world settings. This paper formally defines a practical
yet underexplored problem setting for real world fatigue detection, where
systems operating with context-appropriate sensors aim to leverage knowledge
from differently instrumented sources including those using impractical sensors
deployed in controlled environments. To tackle this challenge, we propose a
heterogeneous and multi-source fatigue detection framework that adaptively
utilizes the available modalities in the target domain while benefiting from
the diverse configurations present in source domains. Our experiments,
conducted using a realistic field-deployed sensor setup and two publicly
available datasets, demonstrate the practicality, robustness, and improved
generalization of our approach, paving the practical way for effective fatigue
monitoring in sensor-constrained scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>1figures,32pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Outcome-Based Online <span class="highlight-title">Reinforcement</span> Learning: Algorithms and Fundamental
  Limits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20268v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20268v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Chen, Zeyu Jia, Alexander Rakhlin, Tengyang Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning with outcome-based feedback faces a fundamental
challenge: when rewards are only observed at trajectory endpoints, how do we
assign credit to the right actions? This paper provides the first comprehensive
analysis of this problem in online RL with general function approximation. We
develop a provably sample-efficient algorithm achieving $\widetilde{O}({C_{\rm
cov} H^3}/{\epsilon^2})$ sample complexity, where $C_{\rm cov}$ is the
coverability coefficient of the underlying MDP. By leveraging general function
approximation, our approach works effectively in large or infinite state spaces
where tabular methods fail, requiring only that value functions and reward
functions can be represented by appropriate function classes. Our results also
characterize when outcome-based feedback is statistically separated from
per-step rewards, revealing an unavoidable exponential separation for certain
MDPs. For deterministic MDPs, we show how to eliminate the completeness
assumption, dramatically simplifying the algorithm. We further extend our
approach to preference-based feedback settings, proving that equivalent
statistical efficiency can be achieved even under more limited information.
Together, these results constitute a theoretical foundation for understanding
the statistical properties of outcome-based reinforcement learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IPCGRL: Language-Instructed <span class="highlight-title">Reinforcement</span> Learning for Procedural Level
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12358v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12358v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        In-Chang Baek, Sung-Hyun Kim, Seo-Young Lee, Dong-Hyeon Kim, Kyung-Joong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has highlighted the significance of natural language in
enhancing the controllability of generative models. While various efforts have
been made to leverage natural language for content generation, research on deep
reinforcement learning (DRL) agents utilizing text-based instructions for
procedural content generation remains limited. In this paper, we propose
IPCGRL, an instruction-based procedural content generation method via
reinforcement learning, which incorporates a sentence embedding model. IPCGRL
fine-tunes task-specific embedding representations to effectively compress
game-level conditions. We evaluate IPCGRL in a two-dimensional level generation
task and compare its performance with a general-purpose embedding method. The
results indicate that IPCGRL achieves up to a 21.4% improvement in
controllability and a 17.2% improvement in generalizability for unseen
instructions. Furthermore, the proposed method extends the modality of
conditional input, enabling a more flexible and expressive interaction
framework for procedural content generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 9 figures, 3 tables, accepted to Conference on Games 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-D12: A Dual-Dimensional Scale of Instrumental and Relational
  Dependencies on Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06874v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06874v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ala Yankouskaya, Areej B. Babiker, Syeda W. F. Rizvi, Sameha Alshakhsi, Magnus Liebherr, Raian Ali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is growing interest in understanding how people interact with large
language models (LLMs) and whether such models elicit dependency or even
addictive behaviour. Validated tools to assess the extent to which individuals
may become dependent on LLMs are scarce and primarily build on classic
behavioral addiction symptoms, adapted to the context of LLM use. We view this
as a conceptual limitation, as the LLM-human relationship is more nuanced and
warrants a fresh and distinct perspective. To address this gap, we developed
and validated a new 12-item questionnaire to measure LLM dependency, referred
to as LLM-D12. The scale was based on the authors' prior theoretical work, with
items developed accordingly and responses collected from 526 participants in
the UK. Exploratory and confirmatory factor analyses, performed on separate
halves of the total sample using a split-sample approach, supported a
two-factor structure: Instrumental Dependency (six items) and Relationship
Dependency (six items). Instrumental Dependency reflects the extent to which
individuals rely on LLMs to support or collaborate in decision-making and
cognitive tasks. Relationship Dependency captures the tendency to perceive LLMs
as socially meaningful, sentient, or companion-like entities. The two-factor
structure demonstrated excellent internal consistency and clear discriminant
validity. External validation confirmed both the conceptual foundation and the
distinction between the two subscales. The psychometric properties and
structure of our LLM-D12 scale were interpreted in light of the emerging view
that dependency on LLMs does not necessarily indicate dysfunction but may still
reflect reliance levels that could become problematic in certain contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Integrated Framework of Prompt Engineering and Multidimensional
  Knowledge <span class="highlight-title">Graph</span>s for Legal Dispute Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07893v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07893v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingda Zhang, Na Zhao, Jianglong Qing, Qing xu, Kaiwen Pan, Ting luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal dispute analysis is crucial for intelligent legal assistance systems.
However, current LLMs face significant challenges in understanding complex
legal concepts, maintaining reasoning consistency, and accurately citing legal
sources. This research presents a framework combining prompt engineering with
multidimensional knowledge graphs to improve LLMs' legal dispute analysis.
Specifically, the framework includes a three-stage hierarchical prompt
structure (task definition, knowledge background, reasoning guidance) along
with a three-layer knowledge graph (legal ontology, representation, instance
layers). Additionally, four supporting methods enable precise legal concept
retrieval: direct code matching, semantic vector similarity, ontology path
reasoning, and lexical segmentation. Through extensive testing, results show
major improvements: sensitivity increased by 9.9%-13.8%, specificity by
4.8%-6.7%, and citation accuracy by 22.4%-39.7%. As a result, the framework
provides better legal analysis and understanding of judicial logic, thus
offering a new technical method for intelligent legal assistance systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages,3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15487v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15487v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martina Miliani, Serena Auriemma, Alessandro Bondielli, Emmanuele Chersoni, Lucia Passaro, Irene Sucameli, Alessandro Lenci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly used in tasks requiring
interpretive and inferential accuracy. In this paper, we introduce ExpliCa, a
new dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely
integrates both causal and temporal relations presented in different linguistic
orders and explicitly expressed by linguistic connectives. The dataset is
enriched with crowdsourced human acceptability ratings. We tested LLMs on
ExpliCa through prompting and perplexity-based metrics. We assessed seven
commercial and open-source LLMs, revealing that even top models struggle to
reach 0.80 accuracy. Interestingly, models tend to confound temporal relations
with causal ones, and their performance is also strongly influenced by the
linguistic order of the events. Finally, perplexity-based scores and prompting
performance are differently affected by model size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Findings of ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Endo<span class="highlight-title">Control</span>Mag: <span class="highlight-title">Robust</span> Endoscopic Vascular Motion Magnification with
  Periodic Reference Resetting and Hierarchical Tissue-aware Dual-Mask <span class="highlight-title">Control</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15292v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15292v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        An Wang, Rulin Zhou, Mengya Xu, Yiru Ye, Longfei Gou, Yiting Chang, Hao Chen, Chwee Ming Lim, Jiankun Wang, Hongliang Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visualizing subtle vascular motions in endoscopic surgery is crucial for
surgical precision and decision-making, yet remains challenging due to the
complex and dynamic nature of surgical scenes. To address this, we introduce
EndoControlMag, a training-free, Lagrangian-based framework with
mask-conditioned vascular motion magnification tailored to endoscopic
environments. Our approach features two key modules: a Periodic Reference
Resetting (PRR) scheme that divides videos into short overlapping clips with
dynamically updated reference frames to prevent error accumulation while
maintaining temporal coherence, and a Hierarchical Tissue-aware Magnification
(HTM) framework with dual-mode mask dilation. HTM first tracks vessel cores
using a pretrained visual tracking model to maintain accurate localization
despite occlusions and view changes. It then applies one of two adaptive
softening strategies to surrounding tissues: motion-based softening that
modulates magnification strength proportional to observed tissue displacement,
or distance-based exponential decay that simulates biomechanical force
attenuation. This dual-mode approach accommodates diverse surgical
scenarios-motion-based softening excels with complex tissue deformations while
distance-based softening provides stability during unreliable optical flow
conditions. We evaluate EndoControlMag on our EndoVMM24 dataset spanning four
different surgery types and various challenging scenarios, including
occlusions, instrument disturbance, view changes, and vessel deformations.
Quantitative metrics, visual assessments, and expert surgeon evaluations
demonstrate that EndoControlMag significantly outperforms existing methods in
both magnification accuracy and visual quality while maintaining robustness
across challenging surgical conditions. The code, dataset, and video results
are available at https://szupc.github.io/EndoControlMag/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Att-Adapter: A <span class="highlight-title">Robust</span> and Precise Domain-Specific Multi-Attributes T2I
  Dif<span class="highlight-title">fusion</span> Adapter via Conditional Variational Autoencoder <span class="chip">ICCV'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.11937v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.11937v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonwoong Cho, Yan-Ying Chen, Matthew Klenk, David I. Inouye, Yanxia Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-Image (T2I) Diffusion Models have achieved remarkable performance in
generating high quality images. However, enabling precise control of continuous
attributes, especially multiple attributes simultaneously, in a new domain
(e.g., numeric values like eye openness or car width) with text-only guidance
remains a significant challenge. To address this, we introduce the Attribute
(Att) Adapter, a novel plug-and-play module designed to enable fine-grained,
multi-attributes control in pretrained diffusion models. Our approach learns a
single control adapter from a set of sample images that can be unpaired and
contain multiple visual attributes. The Att-Adapter leverages the decoupled
cross attention module to naturally harmonize the multiple domain attributes
with text conditioning. We further introduce Conditional Variational
Autoencoder (CVAE) to the Att-Adapter to mitigate overfitting, matching the
diverse nature of the visual world. Evaluations on two public datasets show
that Att-Adapter outperforms all LoRA-based baselines in controlling continuous
attributes. Additionally, our method enables a broader control range and also
improves disentanglement across multiple attributes, surpassing StyleGAN-based
techniques. Notably, Att-Adapter is flexible, requiring no paired synthetic
data for training, and is easily scalable to multiple attributes within a
single model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV'25 (Highlight), The project page is available at
  https://tri-mac.github.io/att-adapter/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in
  Public Goods Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.23276v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.23276v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Guzman Piedrahita, Yongjin Yang, Mrinmaya Sachan, Giorgia Ramponi, Bernhard Schölkopf, Zhijing Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) are increasingly deployed as autonomous
agents, understanding their cooperation and social mechanisms is becoming
increasingly important. In particular, how LLMs balance self-interest and
collective well-being is a critical challenge for ensuring alignment,
robustness, and safe deployment. In this paper, we examine the challenge of
costly sanctioning in multi-agent LLM systems, where an agent must decide
whether to invest its own resources to incentivize cooperation or penalize
defection. To study this, we adapt a public goods game with institutional
choice from behavioral economics, allowing us to observe how different LLMs
navigate social dilemmas over repeated interactions. Our analysis reveals four
distinct behavioral patterns among models: some consistently establish and
sustain high levels of cooperation, others fluctuate between engagement and
disengagement, some gradually decline in cooperative behavior over time, and
others rigidly follow fixed strategies regardless of outcomes. Surprisingly, we
find that reasoning LLMs, such as the o1 series, struggle significantly with
cooperation, whereas some traditional LLMs consistently achieve high levels of
cooperation. These findings suggest that the current approach to improving
LLMs, which focuses on enhancing their reasoning capabilities, does not
necessarily lead to cooperation, providing valuable insights for deploying LLM
agents in environments that require sustained collaboration. Our code is
available at https://github.com/davidguzmanp/SanctSim
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at COLM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision Transformers in Precision Agriculture: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.21706v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.21706v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saber Mehdipour, Seyed Abolghasem Mirroshandel, Seyed Amirhossein Tabatabaei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting plant diseases is a crucial aspect of modern agriculture, as it
plays a key role in maintaining crop health and increasing overall yield.
Traditional approaches, though still valuable, often rely on manual inspection
or conventional machine learning techniques, both of which face limitations in
scalability and accuracy. Recently, Vision Transformers (ViTs) have emerged as
a promising alternative, offering advantages such as improved handling of
long-range dependencies and better scalability for visual tasks. This review
explores the application of ViTs in precision agriculture, covering a range of
tasks. We begin by introducing the foundational architecture of ViTs and
discussing their transition from Natural Language Processing (NLP) to Computer
Vision. The discussion includes the concept of inductive bias in traditional
models like Convolutional Neural Networks (CNNs), and how ViTs mitigate these
biases. We provide a comprehensive review of recent literature, focusing on key
methodologies, datasets, and performance metrics. This study also includes a
comparative analysis of CNNs and ViTs, along with a review of hybrid models and
performance enhancements. Technical challenges such as data requirements,
computational demands, and model interpretability are addressed, along with
potential solutions. Finally, we outline future research directions and
technological advancements that could further support the integration of ViTs
in real-world agricultural settings. Our goal with this study is to offer
practitioners and researchers a deeper understanding of how ViTs are poised to
transform smart and precision agriculture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beamforming and Resource Allocation for Delay Minimization in
  RIS-Assisted OFDM Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03586v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03586v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Ma, Xiao Li, Chongtao Guo, Le Liang, Michail Matthaiou, Shi Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates a joint beamforming and resource allocation problem
in downlink reconfigurable intelligent surface (RIS)-assisted orthogonal
frequency division multiplexing (OFDM) systems to minimize the average delay,
where data packets for each user arrive at the base station (BS)
stochastically. The sequential optimization problem is inherently a Markov
decision process (MDP), thus falling within the remit of reinforcement
learning. To effectively handle the mixed action space and reduce the state
space dimensionality, a hybrid deep reinforcement learning (DRL) approach is
proposed. Specifically, proximal policy optimization (PPO)-Theta is employed to
optimize the RIS phase shift design, while PPO-N is responsible for subcarrier
allocation decisions. The active beamforming at the BS is then derived from the
jointly optimized RIS phase shifts and subcarrier allocation decisions. To
further mitigate the curse of dimensionality associated with subcarrier
allocation, a multi-agent strategy is introduced to optimize the subcarrier
allocation indicators more efficiently. Moreover, to achieve more adaptive
resource allocation and accurately capture the network dynamics, key factors
closely related to average delay, such as the number of backlogged packets in
buffers and current packet arrivals, are incorporated into the state space.
Furthermore, a transfer learning framework is introduced to enhance the
training efficiency and accelerate convergence. Simulation results demonstrate
that the proposed algorithm significantly reduces the average delay, enhances
resource allocation efficiency, and achieves superior system robustness and
fairness compared to baseline methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Swin-TUNA : A Novel PEFT Approach for Accurate Food Image <span class="highlight-title">Segmentation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17347v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17347v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotian Chen, Zhiyong Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of food image processing, efficient semantic segmentation
techniques are crucial for industrial applications. However, existing
large-scale Transformer-based models (such as FoodSAM) face challenges in
meeting practical deploymentrequirements due to their massive parameter counts
and high computational resource demands. This paper introduces TUNable Adapter
module (Swin-TUNA), a Parameter Efficient Fine-Tuning (PEFT) method that
integrates multiscale trainable adapters into the Swin Transformer
architecture, achieving high-performance food image segmentation by updating
only 4% of the parameters. The core innovation of Swin-TUNA lies in its
hierarchical feature adaptation mechanism: it designs separable convolutions in
depth and dimensional mappings of varying scales to address the differences in
features between shallow and deep networks, combined with a dynamic balancing
strategy for tasks-agnostic and task-specific features. Experiments demonstrate
that this method achieves mIoU of 50.56% and 74.94% on the FoodSeg103 and
UECFoodPix Complete datasets, respectively, surpassing the fully parameterized
FoodSAM model while reducing the parameter count by 98.7% (to only 8.13M).
Furthermore, Swin-TUNA exhibits faster convergence and stronger generalization
capabilities in low-data scenarios, providing an efficient solution for
assembling lightweight food image.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>After discussion among the authors, some parts of the paper are
  deemed inappropriate and will be revised and resubmitted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mechanistic Indicators of Understanding in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.08017v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.08017v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Beckmann, Matthieu Queloz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent findings in mechanistic interpretability (MI), the field probing the
inner workings of Large Language Models (LLMs), challenge the view that these
models rely solely on superficial statistics. We offer an accessible synthesis
of these findings that doubles as an introduction to MI while integrating these
findings within a novel theoretical framework for thinking about machine
understanding. We argue that LLMs develop internal structures that are
functionally analogous to the kind of understanding that consists in seeing
connections. To sharpen this idea, we propose a three-tiered conception of
understanding. First, conceptual understanding emerges when a model forms
"features" as directions in latent space, learning the connections between
diverse manifestations of something. Second, state-of-the-world understanding
emerges when a model learns contingent factual connections between features and
dynamically tracks changes in the world. Third, principled understanding
emerges when a model ceases to rely on a collection of memorized facts and
discovers a "circuit" connecting these facts. However, these forms of
understanding remain radically different from human understanding, as the
phenomenon of "parallel mechanisms" shows. We conclude that the debate should
move beyond the yes-or-no question of whether LLMs understand to investigate
how their strange minds work and forge conceptions that fit them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Position: An Empirically Grounded Identifiability Theory Will Accelerate
  <span class="highlight-title">Self-Supervised</span> Learning Research <span class="chip">ICML2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.13101v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.13101v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrik Reizinger, Randall Balestriero, David Klindt, Wieland Brendel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-Supervised Learning (SSL) powers many current AI systems. As research
interest and investment grow, the SSL design space continues to expand. The
Platonic view of SSL, following the Platonic Representation Hypothesis (PRH),
suggests that despite different methods and engineering approaches, all
representations converge to the same Platonic ideal. However, this phenomenon
lacks precise theoretical explanation. By synthesizing evidence from
Identifiability Theory (IT), we show that the PRH can emerge in SSL. However,
current IT cannot explain SSL's empirical success. To bridge the gap between
theory and practice, we propose expanding IT into what we term Singular
Identifiability Theory (SITh), a broader theoretical framework encompassing the
entire SSL pipeline. SITh would allow deeper insights into the implicit data
assumptions in SSL and advance the field towards learning more interpretable
and generalizable representations. We highlight three critical directions for
future research: 1) training dynamics and convergence properties of SSL; 2) the
impact of finite samples, batch size, and data diversity; and 3) the role of
inductive biases in architecture, augmentations, initialization schemes, and
optimizers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML2025 camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A comprehensive study of LLM-based argument classification: from LLAMA
  through GPT-4o to Deepseek-R1 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.08621v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.08621v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcin Pietroń, Rafał Olszowski, Jakub Gomułka, Filip Gampel, Andrzej Tomski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Argument mining (AM) is an interdisciplinary research field that integrates
insights from logic, philosophy, linguistics, rhetoric, law, psychology, and
computer science. It involves the automatic identification and extraction of
argumentative components, such as premises and claims, and the detection of
relationships between them, such as support, attack, or neutrality. Recently,
the field has advanced significantly, especially with the advent of large
language models (LLMs), which have enhanced the efficiency of analyzing and
extracting argument semantics compared to traditional methods and other deep
learning models. There are many benchmarks for testing and verifying the
quality of LLM, but there is still a lack of research and results on the
operation of these models in publicly available argument classification
databases. This paper presents a study of a selection of LLM's, using diverse
datasets such as Args.me and UKP. The models tested include versions of GPT,
Llama, and DeepSeek, along with reasoning-enhanced variants incorporating the
Chain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms
the others in the argument classification benchmarks. In case of models
incorporated with reasoning capabilities, the Deepseek-R1 shows its
superiority. However, despite their superiority, GPT-4o and Deepseek-R1 still
make errors. The most common errors are discussed for all models. To our
knowledge, the presented work is the first broader analysis of the mentioned
datasets using LLM and prompt algorithms. The work also shows some weaknesses
of known prompt algorithms in argument analysis, while indicating directions
for their improvement. The added value of the work is the in-depth analysis of
the available argument datasets and the demonstration of their shortcomings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I-CEE: Tailoring Explanations of Image Classification Models to User
  Expertise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12102v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12102v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Rong, Peizhu Qian, Vaibhav Unhelkar, Enkelejda Kasneci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effectively explaining decisions of black-box machine learning models is
critical to responsible deployment of AI systems that rely on them. Recognizing
their importance, the field of explainable AI (XAI) provides several techniques
to generate these explanations. Yet, there is relatively little emphasis on the
user (the explainee) in this growing body of work and most XAI techniques
generate "one-size-fits-all" explanations. To bridge this gap and achieve a
step closer towards human-centered XAI, we present I-CEE, a framework that
provides Image Classification Explanations tailored to User Expertise. Informed
by existing work, I-CEE explains the decisions of image classification models
by providing the user with an informative subset of training data (i.e.,
example images), corresponding local explanations, and model decisions.
However, unlike prior work, I-CEE models the informativeness of the example
images to depend on user expertise, resulting in different examples for
different users. We posit that by tailoring the example set to user expertise,
I-CEE can better facilitate users' understanding and simulatability of the
model. To evaluate our approach, we conduct detailed experiments in both
simulation and with human participants (N = 100) on multiple datasets.
Experiments with simulated users show that I-CEE improves users' ability to
accurately predict the model's decisions (simulatability) compared to
baselines, providing promising preliminary results. Experiments with human
participants demonstrate that our method significantly improves user
simulatability accuracy, highlighting the importance of human-centered XAI
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieving Classes of Causal Orders with Inconsistent Knowledge Bases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14019v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14019v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Baldo, Simon Ferreira, Charles K. Assaad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional causal discovery methods often rely on strong, untestable
assumptions, which makes them unreliable in real applications. In this context,
Large Language Models (LLMs) have emerged as a promising alternative for
extracting causal knowledge from text-based metadata, which consolidates domain
expertise. However, LLMs tend to be unreliable and prone to hallucinations,
necessitating strategies that account for their limitations. One effective
strategy is to use a consistency measure to assess reliability. Additionally,
most text metadata does not clearly distinguish direct causal relationships
from indirect ones, further complicating the discovery of a causal DAG. As a
result, focusing on causal orders, rather than causal DAGs, emerges as a more
practical and robust approach. We present a new method to derive a class of
acyclic tournaments, which represent plausible causal orders, maximizing a
consistency score derived from an LLM. Our approach starts by calculating
pairwise consistency scores between variables, resulting in a semi-complete
partially directed graph that consolidates these scores into an abstraction of
the maximally consistent causal orders. Using this structure, we identify
optimal acyclic tournaments, focusing on those that maximize consistency across
all configurations. We subsequently show how both the abstraction and the class
of causal orders can be used to estimate causal effects. We tested our method
on both well-established benchmarks, as well as, real-world datasets from
epidemiology and public health. Our results demonstrate the effectiveness of
our approach in recovering the correct causal order.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentiable Motion Manifold Primitives for Reactive Motion Generation
  under Kino<span class="highlight-title">dynamic</span> Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12193v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12193v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghyeon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time motion generation -- which is essential for achieving reactive and
adaptive behavior -- under kinodynamic constraints for high-dimensional systems
is a crucial yet challenging problem. We address this with a two-step approach:
offline learning of a lower-dimensional trajectory manifold of task-relevant,
constraint-satisfying trajectories, followed by rapid online search within this
manifold. Extending the discrete-time Motion Manifold Primitives (MMP)
framework, we propose Differentiable Motion Manifold Primitives (DMMP), a novel
neural network architecture that encodes and generates continuous-time,
differentiable trajectories, trained using data collected offline through
trajectory optimizations, with a strategy that ensures constraint satisfaction
-- absent in existing methods. Experiments on dynamic throwing with a 7-DoF
robot arm demonstrate that DMMP outperforms prior methods in planning speed,
task success, and constraint satisfaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages and 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DocTER: Evaluating Document-based Knowledge Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.09954v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.09954v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suhang Wu, Ante Wang, Minlong Peng, Yujie Lin, Wenbo Li, Mingming Sun, Jinsong Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge editing aims to correct outdated or inaccurate knowledge in neural
networks. In this paper, we explore knowledge editing using easily accessible
documents instead of manually labeled factual triples employed in earlier
research. To advance this field, we establish the first evaluation benchmark,
\textit{DocTER}, featuring Documents containing counterfactual knowledge for
editing. A comprehensive four-perspective evaluation is introduced: Edit
Success, Locality, Reasoning, and Cross-lingual Transfer. To adapt conventional
triplet-based knowledge editing methods for this task, we develop an
Extract-then-Edit pipeline that extracts triples from documents before applying
existing methods. Experiments on popular knowledge editing methods demonstrate
that editing with documents presents significantly greater challenges than
using triples. In document-based scenarios, even the best-performing in-context
editing approach still lags behind by 10 points in editing success when
compared to using gold triples. This observation also holds for both reasoning
and cross-lingual test sets. We further analyze key factors influencing task
performance, including the quality of extracted triples, the frequency and
position of edited knowledge in documents, various methods for enhancing
reasoning, and performance differences across various directions in
cross-lingual knowledge editing, which provide valuable insights for future
research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Information processing & management</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Aligning Vision to Language: Annotation-Free Multimodal Knowledge <span class="highlight-title">Graph</span>
  Construction for Enhanced LLMs Reasoning <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12972v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12972v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun<span class="highlight-author">ming Liu</span>, Siyuan Meng, Yanting Gao, Song Mao, Pinlong Cai, Guohang Yan, Yirong Chen, Zilin Bian, Ding Wang, Botian Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal reasoning in Large Language Models (LLMs) struggles with
incomplete knowledge and hallucination artifacts, challenges that textual
Knowledge Graphs (KGs) only partially mitigate due to their modality isolation.
While Multimodal Knowledge Graphs (MMKGs) promise enhanced cross-modal
understanding, their practical construction is impeded by semantic narrowness
of manual text annotations and inherent noise in visual-semantic entity
linkages. In this paper, we propose Vision-align-to-Language integrated
Knowledge Graph (VaLiK), a novel approach for constructing MMKGs that enhances
LLMs reasoning through cross-modal information supplementation. Specifically,
we cascade pre-trained Vision-Language Models (VLMs) to align image features
with text, transforming them into descriptions that encapsulate image-specific
information. Furthermore, we developed a cross-modal similarity verification
mechanism to quantify semantic consistency, effectively filtering out noise
introduced during feature alignment. Even without manually annotated image
captions, the refined descriptions alone suffice to construct the MMKG.
Compared to conventional MMKGs construction paradigms, our approach achieves
substantial storage efficiency gains while maintaining direct entity-to-image
linkage capability. Experimental results on multimodal reasoning tasks
demonstrate that LLMs augmented with VaLiK outperform previous state-of-the-art
models. Our code is published at https://github.com/Wings-Of-Disaster/VaLiK.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures, 6 tables; Accepted to ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OR-LLM-Agent: Automating Modeling and Solving of Operations Research
  <span class="highlight-title">Optimization</span> Problems with Reasoning LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.10009v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.10009v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Zhang, Pengcheng Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rise of artificial intelligence (AI), applying large language models
(LLMs) to Operations Research (OR) problem-solving has attracted increasing
attention. Most existing approaches attempt to improve OR problem-solving
through prompt engineering or fine-tuning strategies for LLMs. However, these
methods are fundamentally constrained by the limited capabilities of
non-reasoning LLMs. To overcome these limitations, we propose OR-LLM-Agent, an
AI agent built on reasoning LLMs for automated OR problem solving. The agent
decomposes the task into three sequential stages: mathematical modeling, code
generation, and debugging. Each task is handled by a dedicated sub-agent, which
enables more targeted reasoning. We also construct BWOR, a high-quality dataset
for evaluating LLM performance on OR tasks. Our analysis shows that existing
benchmarks such as NL4OPT, MAMO, and IndustryOR suffer from certain issues,
making them less suitable for reliably evaluating LLM performance. In contrast,
BWOR provides a more consistent and discriminative assessment of model
capabilities. Experimental results demonstrate that OR-LLM-Agent outperforms
advanced methods, including GPT-o3, Gemini 2.5 Pro, and ORLM, by at least 7% in
accuracy. These results demonstrate the effectiveness of task decomposition for
OR problem solving.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VolDoGer: LLM-assisted <span class="highlight-title">Dataset</span>s for Domain Generalization in
  Vision-Language Tasks <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juhwan Choi, Junehyoung Kwon, JungMin Yun, Seunguk Yu, YoungBin Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain generalizability is a crucial aspect of a deep learning model since it
determines the capability of the model to perform well on data from unseen
domains. However, research on the domain generalizability of deep learning
models for vision-language tasks remains limited, primarily because of the lack
of required datasets. To address these challenges, we propose VolDoGer:
Vision-Language Dataset for Domain Generalization, a dedicated dataset designed
for domain generalization that addresses three vision-language tasks: image
captioning, visual question answering, and visual entailment. We constructed
VolDoGer by extending LLM-based data annotation techniques to vision-language
tasks, thereby alleviating the burden of recruiting human annotators. We
evaluated the domain generalizability of various models, ranging from
fine-tuned models to a recent multimodal large language model, through
VolDoGer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025 Workshop on Curated Data for Efficient Learning (CDEL)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17596v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17596v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej K. Wozniak, Lianhang Liu, Yixi Cai, Patric Jensfelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While end-to-end autonomous driving models show promising results, their
practical deployment is often hindered by large model sizes, a reliance on
expensive LiDAR sensors and computationally intensive BEV feature
representations. This limits their scalability, especially for mass-market
vehicles equipped only with cameras. To address these challenges, we propose
PRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving
architecture operates using only camera data, without explicit BEV
representation and forgoing the need for LiDAR. PRIX leverages a visual feature
extractor coupled with a generative planning head to predict safe trajectories
from raw pixel inputs directly. A core component of our architecture is the
Context-aware Recalibration Transformer (CaRT), a novel module designed to
effectively enhance multi-level visual features for more robust planning. We
demonstrate through comprehensive experiments that PRIX achieves
state-of-the-art performance on the NavSim and nuScenes benchmarks, matching
the capabilities of larger, multimodal diffusion planners while being
significantly more efficient in terms of inference speed and model size, making
it a practical solution for real-world deployment. Our work is open-source and
the code will be at https://maxiuw.github.io/prix.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inversion-DPO: Precise and Efficient Post-Training for Dif<span class="highlight-title">fusion</span> Models <span class="chip">ACM MM25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.11554v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.11554v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zejian Li, Yize Li, Chenye Meng, Zhongni Liu, Yang Ling, Shengyuan Zhang, Guang Yang, Changyuan Yang, Zhiyuan Yang, Lingyun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in diffusion models (DMs) have been propelled by
alignment methods that post-train models to better conform to human
preferences. However, these approaches typically require computation-intensive
training of a base model and a reward model, which not only incurs substantial
computational overhead but may also compromise model accuracy and training
efficiency. To address these limitations, we propose Inversion-DPO, a novel
alignment framework that circumvents reward modeling by reformulating Direct
Preference Optimization (DPO) with DDIM inversion for DMs. Our method conducts
intractable posterior sampling in Diffusion-DPO with the deterministic
inversion from winning and losing samples to noise and thus derive a new
post-training paradigm. This paradigm eliminates the need for auxiliary reward
models or inaccurate appromixation, significantly enhancing both precision and
efficiency of training. We apply Inversion-DPO to a basic task of text-to-image
generation and a challenging task of compositional image generation. Extensive
experiments show substantial performance improvements achieved by Inversion-DPO
compared to existing post-training methods and highlight the ability of the
trained generative models to generate high-fidelity compositionally coherent
images. For the post-training of compostitional image geneation, we curate a
paired dataset consisting of 11,140 images with complex structural annotations
and comprehensive scores, designed to enhance the compositional capabilities of
generative models. Inversion-DPO explores a new avenue for efficient,
high-precision alignment in diffusion models, advancing their applicability to
complex realistic generation tasks. Our code is available at
https://github.com/MIGHTYEZ/Inversion-DPO
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EVEv2: Improved Baselines for Encoder-Free Vision-Language Models <span class="chip">ICCV2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06788v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06788v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiwen Diao, Xiaotong Li, Yufeng Cui, Yueze Wang, Haoge Deng, Ting Pan, Wenxuan Wang, Huchuan Lu, Xinlong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing encoder-free vision-language models (VLMs) are rapidly narrowing the
performance gap with their encoder-based counterparts, highlighting the
promising potential for unified multimodal systems with structural simplicity
and efficient deployment. We systematically clarify the performance gap between
VLMs using pre-trained vision encoders, discrete tokenizers, and minimalist
visual layers from scratch, deeply excavating the under-examined
characteristics of encoder-free VLMs. We develop efficient strategies for
encoder-free VLMs that rival mainstream encoder-based ones. After an in-depth
investigation, we launch EVEv2.0, a new and improved family of encoder-free
VLMs. We show that: (i) Properly decomposing and hierarchically associating
vision and language within a unified model reduces interference between
modalities. (ii) A well-designed training strategy enables effective
optimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0
represents a thorough study for developing a decoder-only architecture across
modalities, demonstrating superior data efficiency and strong vision-reasoning
capability. Code is publicly available at: https://github.com/baaivision/EVE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 10 figures, Accepted by ICCV2025 (highlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Housing Market 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julien Lesca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies an online variant of the celebrated housing market
problem, where each agent has a single house and seeks to exchange it for
another based on her preferences. In this online setting, agents may arrive and
depart at any time, meaning that not all agents are present on the housing
market simultaneously. I extend the well known serial dictatorship and Gale s
top trading cycle mechanisms to this online scenario, aiming to retain their
desirable properties such as Pareto efficiency, individual rationality, and
strategy proofness. These extensions also seek to prevent agents from
strategically delaying their arrival or advancing their departure. I
demonstrate that achieving all of these properties simultaneously is impossible
in the online context, and I present several variants that achieve different
subsets of these properties.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HPS: Hard Preference Sampling for Human Preference Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14400v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14400v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiandong Zou, Wanyu Lin, Yuchen Li, Pan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning Large Language Model (LLM) responses with human preferences is vital
for building safe and controllable AI systems. While preference optimization
methods based on Plackett-Luce (PL) and Bradley-Terry (BT) models have shown
promise, they face challenges such as poor handling of harmful content,
inefficient use of dispreferred responses, and, specifically for PL, high
computational costs. To address these issues, we propose Hard Preference
Sampling (HPS), a novel framework for robust and efficient human preference
alignment. HPS introduces a training loss that prioritizes the most preferred
response while rejecting all dispreferred and harmful ones. It emphasizes
"hard" dispreferred responses -- those closely resembling preferred ones -- to
enhance the model's rejection capabilities. By leveraging a single-sample Monte
Carlo sampling strategy, HPS reduces computational overhead while maintaining
alignment quality. Theoretically, HPS improves sample efficiency over existing
PL methods and maximizes the reward margin between preferred and dispreferred
responses, ensuring clearer distinctions. Experiments on HH-RLHF and PKU-Safety
datasets validate HPS's effectiveness, achieving comparable BLEU and reward
scores while greatly improving reward margins and thus reducing harmful content
generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Frequency-<span class="highlight-title">Dynamic</span> Attention Modulation for Dense <span class="highlight-title">Prediction</span> <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.12006v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.12006v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linwei Chen, Lin Gu, Ying Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformers (ViTs) have significantly advanced computer vision,
demonstrating strong performance across various tasks. However, the attention
mechanism in ViTs makes each layer function as a low-pass filter, and the
stacked-layer architecture in existing transformers suffers from frequency
vanishing. This leads to the loss of critical details and textures. We propose
a novel, circuit-theory-inspired strategy called Frequency-Dynamic Attention
Modulation (FDAM), which can be easily plugged into ViTs. FDAM directly
modulates the overall frequency response of ViTs and consists of two
techniques: Attention Inversion (AttInv) and Frequency Dynamic Scaling
(FreqScale). Since circuit theory uses low-pass filters as fundamental
elements, we introduce AttInv, a method that generates complementary high-pass
filtering by inverting the low-pass filter in the attention matrix, and
dynamically combining the two. We further design FreqScale to weight different
frequency components for fine-grained adjustments to the target response
function. Through feature similarity analysis and effective rank evaluation, we
demonstrate that our approach avoids representation collapse, leading to
consistent performance improvements across various models, including SegFormer,
DeiT, and MaskDINO. These improvements are evident in tasks such as semantic
segmentation, object detection, and instance segmentation. Additionally, we
apply our method to remote sensing detection, achieving state-of-the-art
results in single-scale settings. The code is available at
https://github.com/Linwei-Chen/FDAM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SyncMapV2: <span class="highlight-title">Robust</span> and Adaptive <span class="highlight-title">Unsupervised</span> <span class="highlight-title">Segmentation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16297v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16297v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng Zhang, Zikang Wan, Danilo Vasconcellos Vargas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human vision excels at segmenting visual cues without the need for explicit
training, and it remains remarkably robust even as noise severity increases. In
contrast, existing AI algorithms struggle to maintain accuracy under similar
conditions. Here, we present SyncMapV2, the first to solve unsupervised
segmentation with state-of-the-art robustness. SyncMapV2 exhibits a minimal
drop in mIoU, only 0.01%, under digital corruption, compared to a 23.8% drop
observed in SOTA methods. This superior performance extends across various
types of corruption: noise (7.3% vs. 37.7%), weather (7.5% vs. 33.8%), and blur
(7.0% vs. 29.5%). Notably, SyncMapV2 accomplishes this without any robust
training, supervision, or loss functions. It is based on a learning paradigm
that uses self-organizing dynamical equations combined with concepts from
random networks. Moreover, unlike conventional methods that require
re-initialization for each new input, SyncMapV2 adapts online, mimicking the
continuous adaptability of human vision. Thus, we go beyond the accurate and
robust results, and present the first algorithm that can do all the above
online, adapting to input rather than re-initializing. In adaptability tests,
SyncMapV2 demonstrates near-zero performance degradation, which motivates and
fosters a new generation of robust and adaptive intelligence in the near
future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DisMS-TS: Eliminating Redundant Multi-Scale Features for Time Series
  Classification <span class="chip">ACM MM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.04600v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.04600v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhipeng Liu, Peibo Duan, Binwu Wang, Xuan Tang, Qi Chu, Changsheng Zhang, Yongsheng Huang, Bin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world time series typically exhibit complex temporal variations, making
the time series classification task notably challenging. Recent advancements
have demonstrated the potential of multi-scale analysis approaches, which
provide an effective solution for capturing these complex temporal patterns.
However, existing multi-scale analysis-based time series prediction methods
fail to eliminate redundant scale-shared features across multi-scale time
series, resulting in the model over- or under-focusing on scale-shared
features. To address this issue, we propose a novel end-to-end Disentangled
Multi-Scale framework for Time Series classification (DisMS-TS). The core idea
of DisMS-TS is to eliminate redundant shared features in multi-scale time
series, thereby improving prediction performance. Specifically, we propose a
temporal disentanglement module to capture scale-shared and scale-specific
temporal representations, respectively. Subsequently, to effectively learn both
scale-shared and scale-specific temporal representations, we introduce two
regularization terms that ensure the consistency of scale-shared
representations and the disparity of scale-specific representations across all
temporal scales. Extensive experiments conducted on multiple datasets validate
the superiority of DisMS-TS over its competitive baselines, with the accuracy
improvement up to 9.71%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for presentation at the ACM
  International Conference on Multimedia (ACM MM 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Robust</span> <span class="highlight-title">Multi-View</span> Learning via Representation <span class="highlight-title">Fusion</span> of Sample-Level
  Attention and Alignment of Simulated Perturbation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04151v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04151v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Xu, Na Zhao, Gang Niu, Masashi Sugiyama, Xiaofeng Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, multi-view learning (MVL) has garnered significant attention due to
its ability to fuse discriminative information from multiple views. However,
real-world multi-view datasets are often heterogeneous and imperfect, which
usually causes MVL methods designed for specific combinations of views to lack
application potential and limits their effectiveness. To address this issue, we
propose a novel robust MVL method (namely RML) with simultaneous representation
fusion and alignment. Specifically, we introduce a simple yet effective
multi-view transformer fusion network where we transform heterogeneous
multi-view data into homogeneous word embeddings, and then integrate multiple
views by the sample-level attention mechanism to obtain a fused representation.
Furthermore, we propose a simulated perturbation based multi-view contrastive
learning framework that dynamically generates the noise and unusable
perturbations for simulating imperfect data conditions. The simulated noisy and
unusable data obtain two distinct fused representations, and we utilize
contrastive learning to align them for learning discriminative and robust
representations. Our RML is self-supervised and can also be applied for
downstream tasks as a regularization. In experiments, we employ it in
multi-view unsupervised clustering, noise-label classification, and as a
plug-and-play module for cross-modal hashing retrieval. Extensive comparison
experiments and ablation studies validate RML's effectiveness. Code is
available at https://github.com/SubmissionsIn/RML.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compositional Coordination for Multi-<span class="highlight-title">Robot</span> Teams with Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16068v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16068v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhehui Huang, Guangyao Shi, Yuwei Wu, Vijay Kumar, Gaurav S. Sukhatme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-robot coordination has traditionally relied on a mission-specific and
expert-driven pipeline, where natural language mission descriptions are
manually translated by domain experts into mathematical formulation, algorithm
design, and executable code. This conventional process is labor-intensive,
inaccessible to non-experts, and inflexible to changes in mission requirements.
Here, we propose LAN2CB (Language to Collective Behavior), a novel framework
that leverages large language models (LLMs) to streamline and generalize the
multi-robot coordination pipeline. LAN2CB transforms natural language (NL)
mission descriptions into executable Python code for multi-robot systems
through two core modules: (1) Mission Analysis, which parses mission
descriptions into behavior trees, and (2) Code Generation, which leverages the
behavior tree and a structured knowledge base to generate robot control code.
We further introduce a dataset of natural language mission descriptions to
support development and benchmarking. Experiments in both simulation and
real-world environments demonstrate that LAN2CB enables robust and flexible
multi-robot coordination from natural language, significantly reducing manual
engineering effort and supporting broad generalization across diverse mission
types. Website: https://sites.google.com/view/lan-cb
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Meta Prompting for AI Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11482v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11482v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Zhang, Yang Yuan, Andrew Chi-Chih Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Meta Prompting (MP), a framework that elevates the reasoning
capabilities of large language models (LLMs) by focusing on the formal
structure of a task rather than content-specific examples. We establish a
theoretical foundation for this paradigm, formalizing MP as a functor that maps
a category of tasks to a category of structured prompts, thereby guaranteeing
that compositional problem-solving strategies can be systematically decomposed
into modular prompt structures. We extend this concept to Recursive Meta
Prompting (RMP), an automated process where an LLM can generate and refine its
own prompts. We model this self-improvement loop formally as a monad, providing
a principled framework for automated prompt engineering. Our claims are
validated through extensive experiments demonstrating that a Qwen-72B base
model, guided by a single, example-agnostic meta-prompt, achieves
state-of-the-art results on MATH, GSM8K, and Game of 24. These results are
achieved with substantial token efficiency gains over traditional few-shot
methods. Project Page: https://github.com/meta-prompting/meta-prompting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://github.com/meta-prompting/meta-prompting</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Why Do Class-Dependent <span class="highlight-title">Evaluation</span> Effects Occur with Time Series Feature
  Attributions? A Synthetic Data Investigation <span class="chip">ECML-PKDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11790v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11790v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gregor Baer, Isel Grau, Chao Zhang, Pieter Van Gorp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating feature attribution methods represents a critical challenge in
explainable AI (XAI), as researchers typically rely on perturbation-based
metrics when ground truth is unavailable. However, recent work reveals that
these evaluation metrics can show different performance across predicted
classes within the same dataset. These "class-dependent evaluation effects"
raise questions about whether perturbation analysis reliably measures
attribution quality, with direct implications for XAI method development and
evaluation trustworthiness. We investigate under which conditions these
class-dependent effects arise by conducting controlled experiments with
synthetic time series data where ground truth feature locations are known. We
systematically vary feature types and class contrasts across binary
classification tasks, then compare perturbation-based degradation scores with
ground truth-based precision-recall metrics using multiple attribution methods.
Our experiments demonstrate that class-dependent effects emerge with both
evaluation approaches, even in simple scenarios with temporally localized
features, triggered by basic variations in feature amplitude or temporal extent
between classes. Most critically, we find that perturbation-based and ground
truth metrics frequently yield contradictory assessments of attribution quality
across classes, with weak correlations between evaluation approaches. These
findings suggest that researchers should interpret perturbation-based metrics
with care, as they may not always align with whether attributions correctly
identify discriminating features. By showing this disconnect, our work points
toward reconsidering what attribution evaluation actually measures and
developing more rigorous evaluation methods that capture multiple dimensions of
attribution quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at TempXAI Workshop @ ECML-PKDD 2025 (Explainable AI for
  Time Series and Data Streams)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Low-rank Decomposition: A Shortcut Approach for Efficient
  On-Device Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05086v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05086v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le-Trung Nguyen, Ael Quelennec, Van-Tam Nguyen, Enzo Tartaglione
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  On-device learning has emerged as a promising direction for AI development,
particularly because of its potential to reduce latency issues and mitigate
privacy risks associated with device-server communication, while improving
energy efficiency. Despite these advantages, significant memory and
computational constraints still represent major challenges for its deployment.
Drawing on previous studies on low-rank decomposition methods that address
activation memory bottlenecks in backpropagation, we propose a novel shortcut
approach as an alternative. Our analysis and experiments demonstrate that our
method can reduce activation memory usage, even up to $120.09\times$ compared
to vanilla training, while also reducing overall training FLOPs up to
$1.86\times$ when evaluated on traditional benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A general language model for peptide identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15610v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15610v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jixiu Zhai, Tianchi Lu, Haitian Zhong, Ziyang Xu, Yuhuan Liu, Shengrui Xu, Jingwan Wang, Dan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate identification of bioactive peptides (BPs) and protein
post-translational modifications (PTMs) is essential for understanding protein
function and advancing therapeutic discovery. However, most computational
methods remain limited in their generalizability across diverse peptide
functions. Here, we present PDeepPP, a unified deep learning framework that
integrates pretrained protein language models with a hybrid
transformer-convolutional architecture, enabling robust identification across
diverse peptide classes and PTM sites. We curated comprehensive benchmark
datasets and implemented strategies to address data imbalance, allowing PDeepPP
to systematically extract both global and local sequence features. Through
extensive analyses-including dimensionality reduction and comparison
studies-PDeepPP demonstrates strong, interpretable peptide representations and
achieves state-of-the-art performance in 25 of the 33 biological identification
tasks. Notably, PDeepPP attains high accuracy in antimicrobial (0.9726) and
phosphorylation site (0.9984) identification, with 99.5% specificity in
glycosylation site prediction and substantial reduction in false negatives in
antimalarial tasks. By enabling large-scale, accurate peptide analysis, PDeepPP
supports biomedical research and the discovery of novel therapeutic targets for
disease treatment. All code, datasets, and pretrained models are publicly
available via GitHub:https://github.com/fondress/PDeepPP and Hugging
Face:https://huggingface.co/fondress/PDeppPP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 9 figures, 4 tables, submitted to arXiv</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Temporal Abstractions via Variational Homomorphisms in
  Option-Induced Abstract MDPs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16473v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16473v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Li, Yaren Zhang, Haoran Lv, Qiong Cao, Chao Xue, Xiaodong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable reasoning ability through
explicit Chain-of-Thought (CoT) prompting, but generating these step-by-step
textual explanations is computationally expensive and slow. To overcome this,
we aim to develop a framework for efficient, implicit reasoning, where the
model "thinks" in a latent space without generating explicit text for every
step. We propose that these latent thoughts can be modeled as
temporally-extended abstract actions, or options, within a hierarchical
reinforcement learning framework. To effectively learn a diverse library of
options as latent embeddings, we first introduce the Variational Markovian
Option Critic (VMOC), an off-policy algorithm that uses variational inference
within the HiT-MDP framework. To provide a rigorous foundation for using these
options as an abstract reasoning space, we extend the theory of continuous MDP
homomorphisms. This proves that learning a policy in the simplified, abstract
latent space, for which VMOC is suited, preserves the optimality of the
solution to the original, complex problem. Finally, we propose a cold-start
procedure that leverages supervised fine-tuning (SFT) data to distill human
reasoning demonstrations into this latent option space, providing a rich
initialization for the model's reasoning capabilities. Extensive experiments
demonstrate that our approach achieves strong performance on complex logical
reasoning benchmarks and challenging locomotion tasks, validating our framework
as a principled method for learning abstract skills for both language and
control.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of Event Causality Identification: Taxonomy, Challenges,
  Assessment, and Prospects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.10371v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.10371v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qing Cheng, Zefan Zeng, Xingchen Hu, Yuehang Si, Zhong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event Causality Identification (ECI) has become an essential task in Natural
Language Processing (NLP), focused on automatically detecting causal
relationships between events within texts. This comprehensive survey
systematically investigates fundamental concepts and models, developing a
systematic taxonomy and critically evaluating diverse models. We begin by
defining core concepts, formalizing the ECI problem, and outlining standard
evaluation protocols. Our classification framework divides ECI models into two
primary tasks: Sentence-level Event Causality Identification (SECI) and
Document-level Event Causality Identification (DECI). For SECI, we review
models employing feature pattern-based matching, machine learning classifiers,
deep semantic encoding, prompt-based fine-tuning, and causal knowledge
pre-training, alongside data augmentation strategies. For DECI, we focus on
approaches utilizing deep semantic encoding, event graph reasoning, and
prompt-based fine-tuning. Special attention is given to recent advancements in
multi-lingual and cross-lingual ECI, as well as zero-shot ECI leveraging Large
Language Models (LLMs). We analyze the strengths, limitations, and unresolved
challenges associated with each approach. Extensive quantitative evaluations
are conducted on four benchmark datasets to rigorously assess the performance
of various ECI models. We conclude by discussing future research directions and
highlighting opportunities to advance the field further.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SDSC:A Structure-Aware Metric for Semantic Signal Representation
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14516v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14516v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeyoung Lee, Hochul Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose the Signal Dice Similarity Coefficient (SDSC), a structure-aware
metric function for time series self-supervised representation learning. Most
Self-Supervised Learning (SSL) methods for signals commonly adopt
distance-based objectives such as mean squared error (MSE), which are sensitive
to amplitude, invariant to waveform polarity, and unbounded in scale. These
properties hinder semantic alignment and reduce interpretability. SDSC
addresses this by quantifying structural agreement between temporal signals
based on the intersection of signed amplitudes, derived from the Dice
Similarity Coefficient (DSC).Although SDSC is defined as a structure-aware
metric, it can be used as a loss by subtracting from 1 and applying a
differentiable approximation of the Heaviside function for gradient-based
optimization. A hybrid loss formulation is also proposed to combine SDSC with
MSE, improving stability and preserving amplitude where necessary. Experiments
on forecasting and classification benchmarks demonstrate that SDSC-based
pre-training achieves comparable or improved performance over MSE, particularly
in in-domain and low-resource scenarios. The results suggest that structural
fidelity in signal representations enhances the semantic representation
quality, supporting the consideration of structure-aware metrics as viable
alternatives to conventional distance-based methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantum Machine Learning in Precision Medicine and Drug Discovery -- A
  Game Changer for Tailored Treatments? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18639v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18639v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus Bertl, Alan Mott, Salvatore Sinno, Bhavika Bhalgamiya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The digitization of healthcare presents numerous challenges, including the
complexity of biological systems, vast data generation, and the need for
personalized treatment plans. Traditional computational methods often fall
short, leading to delayed and sometimes ineffective diagnoses and treatments.
Quantum Computing (QC) and Quantum Machine Learning (QML) offer transformative
advancements with the potential to revolutionize medicine. This paper
summarizes areas where QC promises unprecedented computational power, enabling
faster, more accurate diagnostics, personalized treatments, and enhanced drug
discovery processes. However, integrating quantum technologies into precision
medicine also presents challenges, including errors in algorithms and high
costs. We show that mathematically-based techniques for specifying, developing,
and verifying software (formal methods) can enhance the reliability and
correctness of QC. By providing a rigorous mathematical framework, formal
methods help to specify, develop, and verify systems with high precision. In
genomic data analysis, formal specification languages can precisely (1) define
the behavior and properties of quantum algorithms designed to identify genetic
markers associated with diseases. Model checking tools can systematically
explore all possible states of the algorithm to (2) ensure it behaves correctly
under all conditions, while theorem proving techniques provide mathematical (3)
proof that the algorithm meets its specified properties, ensuring accuracy and
reliability. Additionally, formal optimization techniques can (4) enhance the
efficiency and performance of quantum algorithms by reducing resource usage,
such as the number of qubits and gate operations. Therefore, we posit that
formal methods can significantly contribute to enabling QC to realize its full
potential as a game changer in precision medicine.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>presented at AISoLA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Large Vision-Language Model Meets Large Remote Sensing Imagery:
  Coarse-to-Fine Text-Guided Token Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.07588v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.07588v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junwei Luo, Yingying Zhang, Xue Yang, Kang Wu, Qi Zhu, Lei Liang, Jingdong Chen, Yansheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient vision-language understanding of large Remote Sensing Images (RSIs)
is meaningful but challenging. Current Large Vision-Language Models (LVLMs)
typically employ limited pre-defined grids to process images, leading to
information loss when handling gigapixel RSIs. Conversely, using unlimited
grids significantly increases computational costs. To preserve image details
while reducing computational complexity, we propose a text-guided token pruning
method with Dynamic Image Pyramid (DIP) integration. Our method introduces: (i)
a Region Focus Module (RFM) that leverages text-aware region localization
capability to identify critical vision tokens, and (ii) a coarse-to-fine image
tile selection and vision token pruning strategy based on DIP, which is guided
by RFM outputs and avoids directly processing the entire large imagery.
Additionally, existing benchmarks for evaluating LVLMs' perception ability on
large RSI suffer from limited question diversity and constrained image sizes.
We construct a new benchmark named LRS-VQA, which contains 7,333 QA pairs
across 8 categories, with image length up to 27,328 pixels. Our method
outperforms existing high-resolution strategies on four datasets using the same
data. Moreover, compared to existing token reduction methods, our approach
demonstrates higher efficiency under high-resolution settings. Dataset and code
are in https://github.com/VisionXLab/LRS-VQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures, 18 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Parameter Design for Superconducting Quantum Circuits with
  <span class="highlight-title">Graph</span> Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16354v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16354v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Ai, Yu-xi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To demonstrate supremacy of quantum computing, increasingly large-scale
superconducting quantum computing chips are being designed and fabricated.
However, the complexity of simulating quantum systems poses a significant
challenge to computer-aided design of quantum chips, especially for large-scale
chips. Harnessing the scalability of graph neural networks (GNNs), we here
propose a parameter designing algorithm for large-scale superconducting quantum
circuits. The algorithm depends on the so-called 'three-stair scaling'
mechanism, which comprises two neural-network models: an evaluator supervisedly
trained on small-scale circuits for applying to medium-scale circuits, and a
designer unsupervisedly trained on medium-scale circuits for applying to
large-scale ones. We demonstrate our algorithm in mitigating quantum crosstalk
errors. Frequencies for both single- and two-qubit gates (corresponding to the
parameters of nodes and edges) are considered simultaneously. Numerical results
indicate that the well-trained designer achieves notable advantages in
efficiency, effectiveness, and scalability. For example, for large-scale
superconducting quantum circuits consisting of around 870 qubits, our
GNNs-based algorithm achieves 51% of the errors produced by the
state-of-the-art algorithm, with a time reduction from 90 min to 27 sec.
Overall, a better-performing and more scalable algorithm for designing
parameters of superconducting quantum chips is proposed, which initially
demonstrates the advantages of applying GNNs in superconducting quantum chips.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reality Proxy: Fluid Interactions with Real-World Objects in MR via
  Abstract Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoan Liu, Difan Jia, Xianhao Carton Liu, Mar Gonzalez-Franco, Chen Zhu-Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interacting with real-world objects in Mixed Reality (MR) often proves
difficult when they are crowded, distant, or partially occluded, hindering
straightforward selection and manipulation. We observe that these difficulties
stem from performing interaction directly on physical objects, where input is
tightly coupled to their physical constraints. Our key insight is to decouple
interaction from these constraints by introducing proxies-abstract
representations of real-world objects. We embody this concept in Reality Proxy,
a system that seamlessly shifts interaction targets from physical objects to
their proxies during selection. Beyond facilitating basic selection, Reality
Proxy uses AI to enrich proxies with semantic attributes and hierarchical
spatial relationships of their corresponding physical objects, enabling novel
and previously cumbersome interactions in MR - such as skimming,
attribute-based filtering, navigating nested groups, and complex multi object
selections - all without requiring new gestures or menu systems. We demonstrate
Reality Proxy's versatility across diverse scenarios, including office
information retrieval, large-scale spatial navigation, and multi-drone control.
An expert evaluation suggests the system's utility and usability, suggesting
that proxy-based abstractions offer a powerful and generalizable interaction
paradigm for future MR systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 9 figures. Accepted for publication in UIST'25 (The 38th
  Annual ACM Symposium on User Interface Software and Technology), Busan,
  Republic of Korea, 28 Sep - 1 Oct 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Integrated Learning and <span class="highlight-title">Optimization</span> for Congestion Management and
  Profit Maximization in Real-Time Electricity Market 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18003v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18003v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Imran Pervez, Ricardo Pinto Lima, Omar Knio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop novel integrated learning and optimization (ILO) methodologies to
solve economic dispatch (ED) and DC optimal power flow (DCOPF) problems for
better economic operation. The optimization problem for ED is formulated with
load being an unknown parameter while DCOPF consists of load and power transfer
distribution factor (PTDF) matrix as unknown parameters. PTDF represents the
incremental variations of real power on transmission lines which occur due to
real power transfers between two regions. These values represent a linearized
approximation of power flows over the transmission lines. We develop novel ILO
formulations to solve post-hoc penalties in electricity market and line
congestion problems using ED and DCOPF optimization formulations. Our proposed
methodologies capture the real-time electricity market and line congestion
behavior to train the regret function which eventually train unknown loads at
different buses and line PTDF matrix to achieve the afore-mentioned post-hoc
goals. The proposed methodology is compared to sequential learning and
optimization (SLO) which train load and PTDF forecasts for accuracy rather than
economic operation. Our experimentation prove the superiority of ILO in
minimizing the post-hoc penalties in electricity markets and minimizing the
line congestion thereby improving the economic operation with noticeable
amount.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OrQstrator: An AI-Powered Framework for Advanced Quantum Circuit
  <span class="highlight-title">Optimization</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.09682v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.09682v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Baird, Armin Moin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel approach, OrQstrator, which is a modular framework for
conducting quantum circuit optimization in the Noisy Intermediate-Scale Quantum
(NISQ) era. Our framework is powered by Deep Reinforcement Learning (DRL). Our
orchestration engine intelligently selects among three complementary circuit
optimizers: A DRL-based circuit rewriter trained to reduce depth and gate count
via learned rewrite sequences; a domain-specific optimizer that performs
efficient local gate resynthesis and numeric optimization; a parameterized
circuit instantiator that improves compilation by optimizing template circuits
during gate set translation. These modules are coordinated by a central
orchestration engine that learns coordination policies based on circuit
structure, hardware constraints, and backend-aware performance features such as
gate count, depth, and expected fidelity. The system outputs an optimized
circuit for hardware-aware transpilation and execution, leveraging techniques
from an existing state-of-the-art approach, called the NISQ Analyzer, to adapt
to backend constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE International Conference on Quantum Computing and Engineering
  (QCE) 2025 - Extended Abstract</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of Deep Learning for Geometry Problem Solving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.11936v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.11936v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianzhe Ma, Wenxuan Wang, Qin Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geometry problem solving is a key area of mathematical reasoning, which is
widely involved in many important fields such as education, mathematical
ability assessment of artificial intelligence, and multimodal ability
assessment. In recent years, the rapid development of deep learning technology,
especially the rise of multimodal large language models, has triggered a
widespread research boom. This paper provides a survey of the applications of
deep learning in geometry problem solving, including (i) a comprehensive
summary of the relevant tasks in geometry problem solving; (ii) a thorough
review of related deep learning methods; (iii) a detailed analysis of
evaluation metrics and methods; and (iv) a critical discussion of the current
challenges and future directions that can be explored. Our goal is to provide a
comprehensive and practical reference of deep learning for geometry problem
solving to promote further developments in this field. We create a continuously
updated list of papers on GitHub: https://github.com/majianz/dl4gps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Differentiated Reward Method for <span class="highlight-title">Reinforcement</span> Learning based
  Multi-Vehicle Cooperative <span class="highlight-title">Decision</span>-Making Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00352v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00352v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Han, Lijun Zhang, Dejian Meng, Zhuang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) shows great potential for optimizing
multi-vehicle cooperative driving strategies through the state-action-reward
feedback loop, but it still faces challenges such as low sample efficiency.
This paper proposes a differentiated reward method based on steady-state
transition systems, which incorporates state transition gradient information
into the reward design by analyzing traffic flow characteristics, aiming to
optimize action selection and policy learning in multi-vehicle cooperative
decision-making. The performance of the proposed method is validated in RL
algorithms such as MAPPO, MADQN, and QMIX under varying autonomous vehicle
penetration. The results show that the differentiated reward method
significantly accelerates training convergence and outperforms centering reward
and others in terms of traffic efficiency, safety, and action rationality.
Additionally, the method demonstrates strong scalability and environmental
adaptability, providing a novel approach for multi-agent cooperative
decision-making in complex traffic scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Integrating Evidence into the Design of XAI and AI-based <span class="highlight-title">Decision</span>
  Support Systems: A Means-End Framework for End-users in Construction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14209v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14209v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter E. D. Love, Jane Matthews, Weili Fang, Hadi Mahamivanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable Artificial Intelligence seeks to make the reasoning processes of
AI models transparent and interpretable, particularly in complex decision
making environments. In the construction industry, where AI based decision
support systems are increasingly adopted, limited attention has been paid to
the integration of supporting evidence that underpins the reliability and
accountability of AI generated outputs. The absence of such evidence undermines
the validity of explanations and the trustworthiness of system recommendations.
This paper addresses this gap by introducing a theoretical, evidence based
means end framework developed through a narrative review. The framework offers
an epistemic foundation for designing XAI enabled DSS that generate meaningful
explanations tailored to users knowledge needs and decision contexts. It
focuses on evaluating the strength, relevance, and utility of different types
of evidence supporting AI generated explanations. While developed with
construction professionals as primary end users, the framework is also
applicable to developers, regulators, and project managers with varying
epistemic goals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>74 pages, 5 figures and 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion
  in Social Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14660v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14660v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qibing Ren, Sitao Xie, Longxuan Wei, Zhenfei Yin, Junchi Yan, Lizhuang Ma, Jing Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large-scale events like election fraud and financial scams have shown
how harmful coordinated efforts by human groups can be. With the rise of
autonomous AI systems, there is growing concern that AI-driven groups could
also cause similar harm. While most AI safety research focuses on individual AI
systems, the risks posed by multi-agent systems (MAS) in complex real-world
situations are still underexplored. In this paper, we introduce a
proof-of-concept to simulate the risks of malicious MAS collusion, using a
flexible framework that supports both centralized and decentralized
coordination structures. We apply this framework to two high-risk fields:
misinformation spread and e-commerce fraud. Our findings show that
decentralized systems are more effective at carrying out malicious actions than
centralized ones. The increased autonomy of decentralized systems allows them
to adapt their strategies and cause more damage. Even when traditional
interventions, like content flagging, are applied, decentralized groups can
adjust their tactics to avoid detection. We present key insights into how these
malicious groups operate and the need for better detection systems and
countermeasures. Code is available at https://github.com/renqibing/RogueAgent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at
  https://github.com/renqibing/MultiAgent4Collusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Trigger without Trace: Towards Stealthy Backdoor Attack on Text-to-Image
  Dif<span class="highlight-title">fusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17724v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17724v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Zhang, Zhongqi Wang, Shiguang Shan, Xilin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backdoor attacks targeting text-to-image diffusion models have advanced
rapidly. However, current backdoor samples often exhibit two key abnormalities
compared to benign samples: 1) Semantic Consistency, where backdoor prompts
tend to generate images with similar semantic content even with significant
textual variations to the prompts; 2) Attention Consistency, where the trigger
induces consistent structural responses in the cross-attention maps. These
consistencies leave detectable traces for defenders, making backdoors easier to
identify. In this paper, toward stealthy backdoor samples, we propose Trigger
without Trace (TwT) by explicitly mitigating these consistencies. Specifically,
our approach leverages syntactic structures as backdoor triggers to amplify the
sensitivity to textual variations, effectively breaking down the semantic
consistency. Besides, a regularization method based on Kernel Maximum Mean
Discrepancy (KMMD) is proposed to align the distribution of cross-attention
responses between backdoor and benign samples, thereby disrupting attention
consistency. Extensive experiments demonstrate that our method achieves a 97.5%
attack success rate while exhibiting stronger resistance to defenses. It
achieves an average of over 98% backdoor samples bypassing three
state-of-the-art detection mechanisms, revealing the vulnerabilities of current
backdoor defense methods. The code is available at
https://github.com/Robin-WZQ/TwT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Long-Short Distance <span class="highlight-title">Graph</span> Neural Networks and Improved Curriculum
  Learning for Emotion Recognition in Conversation <span class="chip">ECAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15205v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15205v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinran Li, Xiujuan Xu, Jiaqi Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion Recognition in Conversation (ERC) is a practical and challenging
task. This paper proposes a novel multimodal approach, the Long-Short Distance
Graph Neural Network (LSDGNN). Based on the Directed Acyclic Graph (DAG), it
constructs a long-distance graph neural network and a short-distance graph
neural network to obtain multimodal features of distant and nearby utterances,
respectively. To ensure that long- and short-distance features are as distinct
as possible in representation while enabling mutual influence between the two
modules, we employ a Differential Regularizer and incorporate a BiAffine Module
to facilitate feature interaction. In addition, we propose an Improved
Curriculum Learning (ICL) to address the challenge of data imbalance. By
computing the similarity between different emotions to emphasize the shifts in
similar emotions, we design a "weighted emotional shift" metric and develop a
difficulty measurer, enabling a training process that prioritizes learning easy
samples before harder ones. Experimental results on the IEMOCAP and MELD
datasets demonstrate that our model outperforms existing benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 28th European Conference on Artificial Intelligence
  (ECAI 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM Web <span class="highlight-title">Dynamic</span>s: Tracing Model Collapse in a Network of LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15690v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15690v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Wang, Akira Horiguchi, Lingyou Pang, Carey E. Priebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing use of synthetic data from the public Internet has enhanced
data usage efficiency in large language model (LLM) training. However, the
potential threat of model collapse remains insufficiently explored. Existing
studies primarily examine model collapse in a single model setting or rely
solely on statistical surrogates. In this work, we introduce LLM Web Dynamics
(LWD), an efficient framework for investigating model collapse at the network
level. By simulating the Internet with a retrieval-augmented generation (RAG)
database, we analyze the convergence pattern of model outputs. Furthermore, we
provide theoretical guarantees for this convergence by drawing an analogy to
interacting Gaussian Mixture Models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Corrective Machine Unranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08562v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08562v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingrui Hou, Axel Finke, Georgina Cosma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine unlearning in neural information retrieval (IR) systems requires
removing specific data whilst maintaining model performance. Applying existing
machine unlearning methods to IR may compromise retrieval effectiveness or
inadvertently expose unlearning actions due to the removal of particular items
from the retrieved results presented to users. We formalise corrective
unranking, which extends machine unlearning in (neural) IR context by
integrating substitute documents to preserve ranking integrity, and propose a
novel teacher-student framework, Corrective unRanking Distillation (CuRD), for
this task. CuRD (1) facilitates forgetting by adjusting the (trained) neural IR
model such that its output relevance scores of to-be-forgotten samples mimic
those of low-ranking, non-retrievable samples; (2) enables correction by
fine-tuning the relevance scores for the substitute samples to match those of
corresponding to-be-forgotten samples closely; (3) seeks to preserve
performance on samples that are not targeted for forgetting. We evaluate CuRD
on four neural IR models (BERTcat, BERTdot, ColBERT, PARADE) using MS MARCO and
TREC CAR datasets. Experiments with forget set sizes from 1 % and 20 % of the
training dataset demonstrate that CuRD outperforms seven state-of-the-art
baselines in terms of forgetting and correction while maintaining model
retention and generalisation capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to Information Sciences</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent
  Dialogue Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.14928v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.14928v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Shi, Rongkeng Liang, Yong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) increasingly serve as educational tools, yet
evaluating their teaching capabilities remains challenging due to the
resource-intensive, context-dependent, and methodologically complex nature of
teacher-student interactions. We introduce EducationQ, a multi-agent dialogue
framework that efficiently assesses teaching capabilities through simulated
dynamic educational scenarios, featuring specialized agents for teaching,
learning, and evaluation. Testing 14 LLMs across major AI Organizations
(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13
disciplines and 10 difficulty levels reveals that teaching effectiveness does
not correlate linearly with model scale or general reasoning capabilities -
with some smaller open-source models outperforming larger commercial
counterparts in teaching contexts. This finding highlights a critical gap in
current evaluations that prioritize knowledge recall over interactive pedagogy.
Our mixed-methods evaluation, combining quantitative metrics with qualitative
analysis and expert case studies, identifies distinct pedagogical strengths
employed by top-performing models (e.g., sophisticated questioning strategies,
adaptive feedback mechanisms). Human expert evaluations show 78% agreement with
our automated qualitative analysis of effective teaching behaviors, validating
our methodology. EducationQ demonstrates that LLMs-as-teachers require
specialized optimization beyond simple scaling, suggesting next-generation
educational AI prioritize targeted enhancement of specific pedagogical
effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper URL: https://aclanthology.org/2025.acl-long.1576/; Presentation
  Video: https://www.youtube.com/watch?v=j63ooKE50I0</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A PBN-RL-XAI Framework for Discovering a "Hit-and-Run" Therapeutic
  Strategy in Melanoma 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.10136v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.10136v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhonglin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Innate resistance to anti-PD-1 immunotherapy remains a major clinical
challenge in metastatic melanoma, with the underlying molecular networks being
poorly understood. To address this, we constructed a dynamic Probabilistic
Boolean Network model using transcriptomic data from patient tumor biopsies to
elucidate the regulatory logic governing therapy response. We then employed a
reinforcement learning agent to systematically discover optimal, multi-step
therapeutic interventions and used explainable artificial intelligence to
mechanistically interpret the agent's control policy. The analysis revealed
that a precisely timed, 4-step temporary inhibition of the lysyl oxidase like 2
protein (LOXL2) was the most effective strategy. Our explainable analysis
showed that this ''hit-and-run" intervention is sufficient to erase the
molecular signature driving resistance, allowing the network to self-correct
without requiring sustained intervention. This study presents a novel,
time-dependent therapeutic hypothesis for overcoming immunotherapy resistance
and provides a powerful computational framework for identifying non-obvious
intervention protocols in complex biological systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures. Submitted to the IEEE International Conference on
  Bioinformatics and Biomedicine (BIBM) 2025. Code is available at
  https://github.com/Liu-Zhonglin/pbn-melanoma-project</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Relative Pose <span class="highlight-title">Estimation</span> Framework with Dual Noise Tuning for
  Safe Approaching Maneuvers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16214v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16214v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Batu Candan, Simone Servadio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and robust relative pose estimation is crucial for enabling
challenging Active Debris Removal (ADR) missions targeting tumbling derelict
satellites such as ESA's ENVISAT. This work presents a complete pipeline
integrating advanced computer vision techniques with adaptive nonlinear
filtering to address this challenge. A Convolutional Neural Network (CNN),
enhanced with image preprocessing, detects structural markers (corners) from
chaser imagery, whose 2D coordinates are converted to 3D measurements using
camera modeling. These measurements are fused within an Unscented Kalman Filter
(UKF) framework, selected for its ability to handle nonlinear relative
dynamics, to estimate the full relative pose. Key contributions include the
integrated system architecture and a dual adaptive strategy within the UKF:
dynamic tuning of the measurement noise covariance compensates for varying CNN
measurement uncertainty, while adaptive tuning of the process noise covariance,
utilizing measurement residual analysis, accounts for unmodeled dynamics or
maneuvers online. This dual adaptation enhances robustness against both
measurement imperfections and dynamic model uncertainties. The performance of
the proposed adaptive integrated system is evaluated through high-fidelity
simulations using a realistic ENVISAT model, comparing estimates against ground
truth under various conditions, including measurement outages. This
comprehensive approach offers an enhanced solution for robust onboard relative
navigation, significantly advancing the capabilities required for safe
proximity operations during ADR missions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Machine Unranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05330v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05330v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingrui Hou, Axel Finke, Georgina Cosma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of machine unlearning in neural information retrieval
(IR), introducing a novel task termed Neural Machine UnRanking (NuMuR). This
problem is motivated by growing demands for data privacy compliance and
selective information removal in neural IR systems. Existing task- or model-
agnostic unlearning approaches, primarily designed for classification tasks,
are suboptimal for NuMuR due to two core challenges: (1) neural rankers output
unnormalised relevance scores rather than probability distributions, limiting
the effectiveness of traditional teacher-student distillation frameworks; and
(2) entangled data scenarios, where queries and documents appear simultaneously
across both forget and retain sets, may degrade retention performance in
existing methods. To address these issues, we propose Contrastive and
Consistent Loss (CoCoL), a dual-objective framework. CoCoL comprises (1) a
contrastive loss that reduces relevance scores on forget sets while maintaining
performance on entangled samples, and (2) a consistent loss that preserves
accuracy on retain set. Extensive experiments on MS MARCO and TREC CAR
datasets, across four neural IR models, demonstrate that CoCoL achieves
substantial forgetting with minimal retain and generalisation performance loss.
Our method facilitates more effective and controllable data removal than
existing techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards a Universal 3D Medical Multi-modality Generalization via
  Learning Personalized Invariant Representation <span class="chip">ICCV25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06106v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06106v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaorui Tan, Xi Yang, Tan Pan, Tianyi Liu, Chen Jiang, Xin Guo, Qiufeng Wang, Anh Nguyen, Yuan Qi, Kaizhu Huang, Yuan Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variations in medical imaging modalities and individual anatomical
differences pose challenges to cross-modality generalization in multi-modal
tasks. Existing methods often concentrate exclusively on common anatomical
patterns, thereby neglecting individual differences and consequently limiting
their generalization performance. This paper emphasizes the critical role of
learning individual-level invariance, i.e., personalized representation
$\mathbb{X}_h$, to enhance multi-modality generalization under both homogeneous
and heterogeneous settings. It reveals that mappings from individual biological
profile to different medical modalities remain static across the population,
which is implied in the personalization process. We propose a two-stage
approach: pre-training with invariant representation $\mathbb{X}_h$ for
personalization, then fine-tuning for diverse downstream tasks. We provide both
theoretical and empirical evidence demonstrating the feasibility and advantages
of personalization, showing that our approach yields greater generalizability
and transferability across diverse multi-modal medical tasks compared to
methods lacking personalization. Extensive experiments further validate that
our approach significantly enhances performance in various generalization
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multi-Faceted <span class="highlight-title">Evaluation</span> Framework for Assessing Synthetic Data
  Generated by Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14445v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14445v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yefeng Yuan, Yuhong Liu, Liang Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancements in generative AI and large language models (LLMs) have
opened up new avenues for producing synthetic data, particularly in the realm
of structured tabular formats, such as product reviews. Despite the potential
benefits, concerns regarding privacy leakage have surfaced, especially when
personal information is utilized in the training datasets. In addition, there
is an absence of a comprehensive evaluation framework capable of quantitatively
measuring the quality of the generated synthetic data and their utility for
downstream tasks. In response to this gap, we introduce SynEval, an open-source
evaluation framework designed to assess the fidelity, utility, and privacy
preservation of synthetically generated tabular data via a suite of diverse
evaluation metrics. We validate the efficacy of our proposed framework -
SynEval - by applying it to synthetic product review data generated by three
state-of-the-art LLMs: ChatGPT, Claude, and Llama. Our experimental findings
illuminate the trade-offs between various evaluation metrics in the context of
synthetic data generation. Furthermore, SynEval stands as a critical instrument
for researchers and practitioners engaged with synthetic tabular data,,
empowering them to judiciously determine the suitability of the generated data
for their specific applications, with an emphasis on upholding user privacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 1 figure, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Hypothesis to Publication: A Comprehensive <span class="highlight-title">Survey</span> of AI-Driven
  Research Support Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01424v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01424v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zekun Zhou, Xiaocheng Feng, Lei Huang, Xiachong Feng, Ziyun Song, Ruihan Chen, Liang Zhao, Weitao Ma, Yuxuan Gu, Baoxin Wang, Dayong Wu, Guoping Hu, Ting Liu, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research is a fundamental process driving the advancement of human
civilization, yet it demands substantial time and effort from researchers. In
recent years, the rapid development of artificial intelligence (AI)
technologies has inspired researchers to explore how AI can accelerate and
enhance research. To monitor relevant advancements, this paper presents a
systematic review of the progress in this domain. Specifically, we organize the
relevant studies into three main categories: hypothesis formulation, hypothesis
validation, and manuscript publication. Hypothesis formulation involves
knowledge synthesis and hypothesis generation. Hypothesis validation includes
the verification of scientific claims, theorem proving, and experiment
validation. Manuscript publication encompasses manuscript writing and the peer
review process. Furthermore, we identify and discuss the current challenges
faced in these areas, as well as potential future directions for research.
Finally, we also offer a comprehensive overview of existing benchmarks and
tools across various domains that support the integration of AI into the
research process. We hope this paper serves as an introduction for beginners
and fosters future research. Resources have been made publicly available at
https://github.com/zkzhou126/AI-for-Research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Segmentation</span>-free Goodness of Pronunciation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16838v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16838v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinwei Cao, Zijian Fan, Torbjørn Svendsen, Giampiero Salvi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mispronunciation detection and diagnosis (MDD) is a significant part in
modern computer aided language learning (CALL) systems. Within MDD,
phoneme-level pronunciation assessment is key to helping L2 learners improve
their pronunciation. However, most systems are based on a form of goodness of
pronunciation (GOP) which requires pre-segmentation of speech into phonetic
units. This limits the accuracy of these methods and the possibility to use
modern CTC-based acoustic models for their evaluation. In this study, we first
propose self-alignment GOP (GOP-SA) that enables the use of CTC-trained ASR
models for MDD. Next, we define a more general alignment-free method that takes
all possible alignments of the target phoneme into account (GOP-AF). We give a
theoretical account of our definition of GOP-AF, an implementation that solves
potential numerical issues as well as a proper normalization which makes the
method applicable with acoustic models with different peakiness over time. We
provide extensive experimental results on the CMU Kids and Speechocean762
datasets comparing the different definitions of our methods, estimating the
dependency of GOP-AF on the peakiness of the acoustic models and on the amount
of context around the target phoneme. Finally, we compare our methods with
recent studies over the Speechocean762 data showing that the feature vectors
derived from the proposed method achieve state-of-the-art results on
phoneme-level pronunciation assessment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tackling Hallucination from Conditional Models for Medical Image
  Reconstruction with <span class="highlight-title">Dynamic</span>DPS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01075v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01075v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seunghoi Kim, Henry F. J. Tregidgo, Matteo Figini, Chen Jin, Sarang Joshi, Daniel C. Alexander
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucinations are spurious structures not present in the ground truth,
posing a critical challenge in medical image reconstruction, especially for
data-driven conditional models. We hypothesize that combining an unconditional
diffusion model with data consistency, trained on a diverse dataset, can reduce
these hallucinations. Based on this, we propose DynamicDPS, a diffusion-based
framework that integrates conditional and unconditional diffusion models to
enhance low-quality medical images while systematically reducing
hallucinations. Our approach first generates an initial reconstruction using a
conditional model, then refines it with an adaptive diffusion-based inverse
problem solver. DynamicDPS skips early stage in the reverse process by
selecting an optimal starting time point per sample and applies Wolfe's line
search for adaptive step sizes, improving both efficiency and image fidelity.
Using diffusion priors and data consistency, our method effectively reduces
hallucinations from any conditional model output. We validate its effectiveness
in Image Quality Transfer for low-field MRI enhancement. Extensive evaluations
on synthetic and real MR scans, including a downstream task for tissue volume
estimation, show that DynamicDPS reduces hallucinations, improving relative
volume estimation by over 15% for critical tissues while using only 5% of the
sampling steps required by baseline diffusion models. As a model-agnostic and
fine-tuning-free approach, DynamicDPS offers a robust solution for
hallucination reduction in medical imaging. The code will be made publicly
available upon publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SuperARC: An Agnostic Test for Narrow, General, and Super Intelligence
  Based On the Principles of Recursive Compression and Algorithmic Probability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16743v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16743v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alberto Hernández-Espinosa, Luan Ozelim, Felipe S. Abrahão, Hector Zenil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce an open-ended test grounded in algorithmic probability that can
avoid benchmark contamination in the quantitative evaluation of frontier models
in the context of their Artificial General Intelligence (AGI) and
Superintelligence (ASI) claims. Unlike other tests, this test does not rely on
statistical compression methods (such as GZIP or LZW), which are more closely
related to Shannon entropy than to Kolmogorov complexity and are not able to
test beyond simple pattern matching. The test challenges aspects of AI, in
particular LLMs, related to features of intelligence of fundamental nature such
as synthesis and model creation in the context of inverse problems (generating
new knowledge from observation). We argue that metrics based on model
abstraction and abduction (optimal Bayesian `inference') for predictive
`planning' can provide a robust framework for testing intelligence, including
natural intelligence (human and animal), narrow AI, AGI, and ASI. We found that
LLM model versions tend to be fragile and incremental as a result of
memorisation only with progress likely driven by the size of training data. The
results were compared with a hybrid neurosymbolic approach that theoretically
guarantees universal intelligence based on the principles of algorithmic
probability and Kolmogorov complexity. The method outperforms LLMs in a
proof-of-concept on short binary sequences. We prove that compression is
equivalent and directly proportional to a system's predictive power and vice
versa. That is, if a system can better predict it can better compress, and if
it can better compress, then it can better predict. Our findings strengthen the
suspicion regarding the fundamental limitations of LLMs, exposing them as
systems optimised for the perception of mastery over human language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>51 pages + Technical Supplementary Information, 79 pages total</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neurodivergent Influenceability as a Contingent Solution to the AI
  Alignment Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.02581v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.02581v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alberto Hernández-Espinosa, Felipe S. Abrahão, Olaf Witkowski, Hector Zenil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The AI alignment problem, which focusses on ensuring that artificial
intelligence (AI), including AGI and ASI, systems act according to human
values, presents profound challenges. With the progression from narrow AI to
Artificial General Intelligence (AGI) and Superintelligence, fears about
control and existential risk have escalated. Here, we investigate whether
embracing inevitable AI misalignment can be a contingent strategy to foster a
dynamic ecosystem of competing agents as a viable path to steer them in more
human-aligned trends and mitigate risks. We explore how misalignment may serve
and should be promoted as a counterbalancing mechanism to team up with
whichever agents are most aligned to human interests, ensuring that no single
system dominates destructively. The main premise of our contribution is that
misalignment is inevitable because full AI-human alignment is a mathematical
impossibility from Turing-complete systems, which we also offer as a proof in
this contribution, a feature then inherited to AGI and ASI systems. We
introduce a change-of-opinion attack test based on perturbation and
intervention analysis to study how humans and agents may change or neutralise
friendly and unfriendly AIs through cooperation and competition. We show that
open models are more diverse and that most likely guardrails implemented in
proprietary models are successful at controlling some of the agents' range of
behaviour with positive and negative consequences while closed systems are more
steerable and can also be used against proprietary AI systems. We also show
that human and AI intervention has different effects hence suggesting multiple
strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Bilateral Teleoperation and Imitation Learning Using Sensorless
  Force <span class="highlight-title">Control</span> via Accurate <span class="highlight-title">Dynamic</span>s Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06174v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06174v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koki Yamane, Yunhan Li, Masashi Konosu, Koki Inami, Junji Oaki, Sho Sakaino, Toshiaki Tsuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the advancement of imitation learning has led to increased
interest in teleoperating low-cost manipulators to collect demonstration data.
However, most existing systems rely on unilateral control, which only transmits
target position values. While this approach is easy to implement and suitable
for slow, non-contact tasks, it struggles with fast or contact-rich operations
due to the absence of force feedback. This work demonstrates that fast
teleoperation with force feedback is feasible even with force-sensorless,
low-cost manipulators by leveraging 4-channel bilateral control. Based on
accurately identified manipulator dynamics, our method integrates nonlinear
terms compensation, velocity and external force estimation, and variable gain
corresponding to inertial variation. Furthermore, using data collected by
4-channel bilateral control, we show that incorporating force information into
both the input and output of learned policies improves performance in imitation
learning. These results highlight the practical effectiveness of our system for
high-fidelity teleoperation and data collection on affordable hardware.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 9 figures, Submitted to CoRL 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">145</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SIDA: Synthetic Image Driven Zero-shot Domain Adaptation <span class="chip">ACM MM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye-Chan Kim, SeungJu Cha, Si-Woo Kim, Taewhan Kim, Dong-Jin Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot domain adaptation is a method for adapting a model to a target
domain without utilizing target domain image data. To enable adaptation without
target images, existing studies utilize CLIP's embedding space and text
description to simulate target-like style features. Despite the previous
achievements in zero-shot domain adaptation, we observe that these text-driven
methods struggle to capture complex real-world variations and significantly
increase adaptation time due to their alignment process. Instead of relying on
text descriptions, we explore solutions leveraging image data, which provides
diverse and more fine-grained style cues. In this work, we propose SIDA, a
novel and efficient zero-shot domain adaptation method leveraging synthetic
images. To generate synthetic images, we first create detailed, source-like
images and apply image translation to reflect the style of the target domain.
We then utilize the style features of these synthetic images as a proxy for the
target domain. Based on these features, we introduce Domain Mix and Patch Style
Transfer modules, which enable effective modeling of real-world variations. In
particular, Domain Mix blends multiple styles to expand the intra-domain
representations, and Patch Style Transfer assigns different styles to
individual patches. We demonstrate the effectiveness of our method by showing
state-of-the-art performance in diverse zero-shot adaptation scenarios,
particularly in challenging domains. Moreover, our approach achieves high
efficiency by significantly reducing the overall adaptation time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM MM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gait Recognition Based on Tiny ML and IMU Sensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahang Zhang, Mingtong Chen, Zhengbao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This project presents the development of a gait recognition system using Tiny
Machine Learning (Tiny ML) and Inertial Measurement Unit (IMU) sensors. The
system leverages the XIAO-nRF52840 Sense microcontroller and the LSM6DS3 IMU
sensor to capture motion data, including acceleration and angular velocity,
from four distinct activities: walking, stationary, going upstairs, and going
downstairs. The data collected is processed through Edge Impulse, an edge AI
platform, which enables the training of machine learning models that can be
deployed directly onto the microcontroller for real-time activity
classification.The data preprocessing step involves extracting relevant
features from the raw sensor data using techniques such as sliding windows and
data normalization, followed by training a Deep Neural Network (DNN) classifier
for activity recognition. The model achieves over 80% accuracy on a test
dataset, demonstrating its ability to classify the four activities effectively.
Additionally, the platform enables anomaly detection, further enhancing the
robustness of the system. The integration of Tiny ML ensures low-power
operation, making it suitable for battery-powered or energy-harvesting devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Moving Out: Physically-grounded Human-AI Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuhui Kang, Sung-Wook Lee, Haolin Liu, Yuyan Wang, Yen-Ling Kuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to adapt to physical actions and constraints in an environment is
crucial for embodied agents (e.g., robots) to effectively collaborate with
humans. Such physically grounded human-AI collaboration must account for the
increased complexity of the continuous state-action space and constrained
dynamics caused by physical constraints. In this paper, we introduce
\textit{Moving Out}, a new human-AI collaboration benchmark that resembles a
wide range of collaboration modes affected by physical attributes and
constraints, such as moving heavy items together and maintaining consistent
actions to move a big item around a corner. Using Moving Out, we designed two
tasks and collected human-human interaction data to evaluate models' abilities
to adapt to diverse human behaviors and unseen physical attributes. To address
the challenges in physical environments, we propose a novel method, BASS
(Behavior Augmentation, Simulation, and Selection), to enhance the diversity of
agents and their understanding of the outcome of actions. Our experiments show
that BASS outperforms state-of-the-art models in AI-AI and human-AI
collaboration. The project page is available at
\href{https://live-robotics-uva.github.io/movingout_ai/}{https://live-robotics-uva.github.io/movingout\_ai/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TRPrompt: Bootstrapping Query-Aware Prompt <span class="highlight-title">Optimization</span> from Textual
  Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreea Nica, Ivan Zakazov, Nicolas Mario Baldwin, Saibo Geng, Robert West
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt optimization improves the reasoning abilities of large language models
(LLMs) without requiring parameter updates to the target model. Following
heuristic-based "Think step by step" approaches, the field has evolved in two
main directions: while one group of methods uses textual feedback to elicit
improved prompts from general-purpose LLMs in a training-free way, a concurrent
line of research relies on numerical rewards to train a special prompt model,
tailored for providing optimal prompts to the target model. In this paper, we
introduce the Textual Reward Prompt framework (TRPrompt), which unifies these
approaches by directly incorporating textual feedback into training of the
prompt model. Our framework does not require prior dataset collection and is
being iteratively improved with the feedback on the generated prompts. When
coupled with the capacity of an LLM to internalize the notion of what a "good"
prompt is, the high-resolution signal provided by the textual rewards allows us
to train a prompt model yielding state-of-the-art query-specific prompts for
the problems from the challenging math datasets GSMHard and MATH.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SynC: Synthetic Image Caption <span class="highlight-title">Dataset</span> Refinement with One-to-many
  <span class="highlight-title">Mapping</span> for Zero-shot Image Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Si-Woo Kim, MinJu Jeon, Ye-Chan Kim, Soeun Lee, Taewhan Kim, Dong-Jin Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot Image Captioning (ZIC) increasingly utilizes synthetic datasets
generated by text-to-image (T2I) models to mitigate the need for costly manual
annotation. However, these T2I models often produce images that exhibit
semantic misalignments with their corresponding input captions (e.g., missing
objects, incorrect attributes), resulting in noisy synthetic image-caption
pairs that can hinder model training. Existing dataset pruning techniques are
largely designed for removing noisy text in web-crawled data. However, these
methods are ill-suited for the distinct challenges of synthetic data, where
captions are typically well-formed, but images may be inaccurate
representations. To address this gap, we introduce SynC, a novel framework
specifically designed to refine synthetic image-caption datasets for ZIC.
Instead of conventional filtering or regeneration, SynC focuses on reassigning
captions to the most semantically aligned images already present within the
synthetic image pool. Our approach employs a one-to-many mapping strategy by
initially retrieving multiple relevant candidate images for each caption. We
then apply a cycle-consistency-inspired alignment scorer that selects the best
image by verifying its ability to retrieve the original caption via
image-to-text retrieval. Extensive evaluations demonstrate that SynC
consistently and significantly improves performance across various ZIC models
on standard benchmarks (MS-COCO, Flickr30k, NoCaps), achieving state-of-the-art
results in several scenarios. SynC offers an effective strategy for curating
refined synthetic data to enhance ZIC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM Multimedia 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Mapper: Charting LLM Embedding Spaces Using
  Perturbation-Based Explanation and Verification Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyuan Yan, Rita Sevastjanova, Sinie van der Ben, Mennatallah El-Assady, Bei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) produce high-dimensional embeddings that capture
rich semantic and syntactic relationships between words, sentences, and
concepts. Investigating the topological structures of LLM embedding spaces via
mapper graphs enables us to understand their underlying structures.
Specifically, a mapper graph summarizes the topological structure of the
embedding space, where each node represents a topological neighborhood
(containing a cluster of embeddings), and an edge connects two nodes if their
corresponding neighborhoods overlap. However, manually exploring these
embedding spaces to uncover encoded linguistic properties requires considerable
human effort. To address this challenge, we introduce a framework for
semi-automatic annotation of these embedding properties. To organize the
exploration process, we first define a taxonomy of explorable elements within a
mapper graph such as nodes, edges, paths, components, and trajectories. The
annotation of these elements is executed through two types of customizable
LLM-based agents that employ perturbation techniques for scalable and automated
analysis. These agents help to explore and explain the characteristics of
mapper elements and verify the robustness of the generated explanations. We
instantiate the framework within a visual analytics workspace and demonstrate
its effectiveness through case studies. In particular, we replicate findings
from prior research on BERT's embedding properties across various layers of its
architecture and provide further observations into the linguistic properties of
topological neighborhoods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid quantum-classical algorithm for near-optimal <span class="highlight-title">planning</span> in POMDPs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gilberto Cunha, Alexandra Ramôa, André Sequeira, Michael de Oliveira, Luís Barbosa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) provides a principled framework for
decision-making in partially observable environments, which can be modeled as
Markov decision processes and compactly represented through dynamic decision
Bayesian networks. Recent advances demonstrate that inference on sparse
Bayesian networks can be accelerated using quantum rejection sampling combined
with amplitude amplification, leading to a computational speedup in estimating
acceptance probabilities.\\ Building on this result, we introduce Quantum
Bayesian Reinforcement Learning (QBRL), a hybrid quantum-classical look-ahead
algorithm for model-based RL in partially observable environments. We present a
rigorous, oracle-free time complexity analysis under fault-tolerant assumptions
for the quantum device. Unlike standard treatments that assume a black-box
oracle, we explicitly specify the inference process, allowing our bounds to
more accurately reflect the true computational cost. We show that, for
environments whose dynamics form a sparse Bayesian network, horizon-based
near-optimal planning can be achieved sub-quadratically faster through
quantum-enhanced belief updates.
  Furthermore, we present numerical experiments benchmarking QBRL against its
classical counterpart on simple yet illustrative decision-making tasks. Our
results offer a detailed analysis of how the quantum computational advantage
translates into decision-making performance, highlighting that the magnitude of
the advantage can vary significantly across different deployment settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Demystify Protein Generation with Hierarchical Conditional Dif<span class="highlight-title">fusion</span>
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zinan Ling, Yi Shi, Da Yan, Yang Zhou, Bo Hui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating novel and functional protein sequences is critical to a wide range
of applications in biology. Recent advancements in conditional diffusion models
have shown impressive empirical performance in protein generation tasks.
However, reliable generations of protein remain an open research question in de
novo protein design, especially when it comes to conditional diffusion models.
Considering the biological function of a protein is determined by multi-level
structures, we propose a novel multi-level conditional diffusion model that
integrates both sequence-based and structure-based information for efficient
end-to-end protein design guided by specified functions. By generating
representations at different levels simultaneously, our framework can
effectively model the inherent hierarchical relations between different levels,
resulting in an informative and discriminative representation of the generated
protein. We also propose a Protein-MMD, a new reliable evaluation metric, to
evaluate the quality of generated protein with conditional diffusion models.
Our new metric is able to capture both distributional and functional
similarities between real and generated protein sequences while ensuring
conditional consistency. We experiment with the benchmark datasets, and the
results on conditional protein generation tasks demonstrate the efficacy of the
proposed generation framework and evaluation metric.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linear Memory SE(2) Invariant Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Pronovost, Neha Boloor, Peter Schleede, Noureldin Hendy, Andres Morales, Nicholas Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Processing spatial data is a key component in many learning tasks for
autonomous driving such as motion forecasting, multi-agent simulation, and
planning. Prior works have demonstrated the value in using SE(2) invariant
network architectures that consider only the relative poses between objects
(e.g. other agents, scene features such as traffic lanes). However, these
methods compute the relative poses for all pairs of objects explicitly,
requiring quadratic memory. In this work, we propose a mechanism for SE(2)
invariant scaled dot-product attention that requires linear memory relative to
the number of objects in the scene. Our SE(2) invariant transformer
architecture enjoys the same scaling properties that have benefited large
language models in recent years. We demonstrate experimentally that our
approach is practical to implement and improves performance compared to
comparable non-invariant architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Best paper award, Equivariant Systems Workshop at RSS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DRWKV: Focusing on Object Edges for Low-Light Image Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuecheng Bai, Yuxiang Wang, Boyu Hu, Qinyuan Jie, Chuanzhi Xu, Hongru Xiao, Kechen Li, Vera Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-light image enhancement remains a challenging task, particularly in
preserving object edge continuity and fine structural details under extreme
illumination degradation. In this paper, we propose a novel model, DRWKV
(Detailed Receptance Weighted Key Value), which integrates our proposed Global
Edge Retinex (GER) theory, enabling effective decoupling of illumination and
edge structures for enhanced edge fidelity. Secondly, we introduce Evolving WKV
Attention, a spiral-scanning mechanism that captures spatial edge continuity
and models irregular structures more effectively. Thirdly, we design the
Bilateral Spectrum Aligner (Bi-SAB) and a tailored MS2-Loss to jointly align
luminance and chrominance features, improving visual naturalness and mitigating
artifacts. Extensive experiments on five LLIE benchmarks demonstrate that DRWKV
achieves leading performance in PSNR, SSIM, and NIQE while maintaining low
computational complexity. Furthermore, DRWKV enhances downstream performance in
low-light multi-object tracking tasks, validating its generalization
capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Internal Data: Constructing Complete <span class="highlight-title">Dataset</span>s for Fairness
  Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Varsha Ramineni, Hossein A. Rahmani, Emine Yilmaz, David Barber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As AI becomes prevalent in high-risk domains and decision-making, it is
essential to test for potential harms and biases. This urgency is reflected by
the global emergence of AI regulations that emphasise fairness and adequate
testing, with some mandating independent bias audits. However, procuring the
necessary data for fairness testing remains a significant challenge.
Particularly in industry settings, legal and privacy concerns restrict the
collection of demographic data required to assess group disparities, and
auditors face practical and cultural challenges in gaining access to data.
Further, internal historical datasets are often insufficiently representative
to identify real-world biases. This work focuses on evaluating classifier
fairness when complete datasets including demographics are inaccessible. We
propose leveraging separate overlapping datasets to construct complete
synthetic data that includes demographic information and accurately reflects
the underlying relationships between protected attributes and model features.
We validate the fidelity of the synthetic data by comparing it to real data,
and empirically demonstrate that fairness metrics derived from testing on such
synthetic data are consistent with those obtained from real data. This work,
therefore, offers a path to overcome real-world data scarcity for fairness
testing, enabling independent, model-agnostic evaluation of fairness, and
serving as a viable substitute where real data is limited.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Tangent Kernels and Fisher Information Matrices for Simple ReLU
  Networks with Random Hidden Weights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun'ichi Takeuchia, Yoshinari Takeishia, Noboru Muratab, Kazushi Mimurac, Ka Long Keith Hod, Hiroshi Nagaoka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fisher information matrices and neural tangent kernels (NTK) for 2-layer ReLU
networks with random hidden weight are argued. We discuss the relation between
both notions as a linear transformation and show that spectral decomposition of
NTK with concrete forms of eigenfunctions with major eigenvalues. We also
obtain an approximation formula of the functions presented by the 2-layer
neural networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane
  Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiale Chen, Torsten Hoefler, Dan Alistarh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantizing the weights of large language models (LLMs) from 16-bit to lower
bitwidth is the de facto approach to deploy massive transformers onto more
affordable accelerators. GPTQ emerged as one of the standard methods for
one-shot post-training quantization at LLM scale. Yet, its inner workings are
described as a sequence of ad-hoc algebraic updates that obscure any geometric
meaning or worst-case guarantees. In this work, we show that, when executed
back-to-front (from the last to first dimension) for a linear layer, GPTQ is
mathematically identical to Babai's nearest plane algorithm for the classical
closest vector problem (CVP) on a lattice defined by the Hessian matrix of the
layer's inputs. This equivalence is based on a sophisticated mathematical
argument, and has two analytical consequences: (i) the GPTQ error propagation
step gains an intuitive geometric interpretation; (ii) GPTQ inherits the error
upper bound of Babai's algorithm under the no-clipping condition. Taken
together, these results place GPTQ on firm theoretical footing and open the
door to importing decades of progress in lattice algorithms towards the design
of future quantization algorithms for billion-parameter models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Performance of Concept Probing: The Influence of the Data
  (Extended Version) <span class="chip">ECAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel de Sousa Ribeiro, Afonso Leote, João Leite
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept probing has recently garnered increasing interest as a way to help
interpret artificial neural networks, dealing both with their typically large
size and their subsymbolic nature, which ultimately renders them unfeasible for
direct human interpretation. Concept probing works by training additional
classifiers to map the internal representations of a model into human-defined
concepts of interest, thus allowing humans to peek inside artificial neural
networks. Research on concept probing has mainly focused on the model being
probed or the probing model itself, paying limited attention to the data
required to train such probing models. In this paper, we address this gap.
Focusing on concept probing in the context of image classification tasks, we
investigate the effect of the data used to train probing models on their
performance. We also make available concept labels for two widely used
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of the paper published in Proceedings of the
  European Conference on Artificial Intelligence (ECAI 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Price equation reveals a universal force-metric-bias law of
  algorithmic learning and natural selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven A. Frank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diverse learning algorithms, optimization methods, and natural selection
share a common mathematical structure, despite their apparent differences. Here
I show that a simple notational partitioning of change by the Price equation
reveals a universal force-metric-bias (FMB) law: $\Delta\mathbf{\theta} =
\mathbf{M}\,\mathbf{f} + \mathbf{b} + \mathbf{\xi}$. The force $\mathbf{f}$
drives improvement in parameters, $\Delta\mathbf{\theta}$, through the
covariance between the parameters and performance. The metric $\mathbf{M}$
rescales movement by inverse curvature. The bias $\mathbf{b}$ adds momentum or
changes in the frame of reference. The noise $\mathbf{\xi}$ enables
exploration. This framework unifies natural selection, Bayesian updating,
Newton's method, stochastic gradient descent, stochastic Langevin dynamics,
Adam optimization, and most other algorithms as special cases of the same
underlying process. The Price equation also reveals why Fisher information,
Kullback-Leibler divergence, and d'Alembert's principle arise naturally in
learning dynamics. By exposing this common structure, the FMB law provides a
principled foundation for understanding, comparing, and designing learning
algorithms across disciplines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Variational Free Energy Calculation of Hydrogen Hugoniot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihang Li, Hao Xie, Xinyang Dong, Lei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a deep variational free energy framework to compute the equation
of state of hydrogen in the warm dense matter region. This method parameterizes
the variational density matrix of hydrogen nuclei and electrons at finite
temperature using three deep generative models: a normalizing flow model that
represents the Boltzmann distribution of the classical nuclei, an
autoregressive transformer that models the distribution of electrons in excited
states, and a permutational equivariant flow model that constructs backflow
coordinates for electrons in Hartree-Fock orbitals. By jointly optimizing the
three neural networks to minimize the variational free energy, we obtain the
equation of state and related thermodynamic properties of dense hydrogen. We
compare our results with other theoretical and experimental results on the
deuterium Hugoniot curve, aiming to resolve existing discrepancies. The
calculated results provide a valuable benchmark for deuterium in the warm dense
matter region.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7+17 pages, 5+14 figures, for source code and raw data, see
  https://github.com/fermiflow/Hugoniot</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI/ML <span class="highlight-title">Life</span> Cycle Management for Interoperable AI Native RAN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chu-Hsiang Huang, Chao-Kai Wen, Geoffrey Ye Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) and machine learning (ML) models are rapidly
permeating the 5G Radio Access Network (RAN), powering beam management, channel
state information (CSI) feedback, positioning, and mobility prediction.
However, without a standardized life-cycle management (LCM) framework,
challenges, such as model drift, vendor lock-in, and limited transparency,
hinder large-scale adoption. 3GPP Releases 16-20 progressively evolve AI/ML
from experimental features to managed, interoperable network functions.
Beginning with the Network Data Analytics Function (NWDAF) in Rel-16,
subsequent releases introduced standardized interfaces for model transfer,
execution, performance monitoring, and closed-loop control, culminating in
Rel-20's two-sided CSI-compression Work Item and vendor-agnostic LCM profile.
This article reviews the resulting five-block LCM architecture, KPI-driven
monitoring mechanisms, and inter-vendor collaboration schemes, while
identifying open challenges in resource-efficient monitoring, environment drift
detection, intelligent decision-making, and flexible model training. These
developments lay the foundation for AI-native transceivers as a key enabler for
6G.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, 2 table. This work has been submitted to the IEEE
  for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Elucidating the Design Space of Arbitrary-Noise-Based Dif<span class="highlight-title">fusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Qiu, Mengying Yang, Xinghua Ma, Dong Liang, Yuzhen Li, Fanding Li, Gongning Luo, Wei Wang, Kuanquan Wang, Shuo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  EDM elucidates the unified design space of diffusion models, yet its fixed
noise patterns restricted to pure Gaussian noise, limit advancements in image
restoration. Our study indicates that forcibly injecting Gaussian noise
corrupts the degraded images, overextends the image transformation distance,
and increases restoration complexity. To address this problem, our proposed EDA
Elucidates the Design space of Arbitrary-noise-based diffusion models.
Theoretically, EDA expands the freedom of noise pattern while preserving the
original module flexibility of EDM, with rigorous proof that increased noise
complexity incurs no additional computational overhead during restoration. EDA
is validated on three typical tasks: MRI bias field correction (global smooth
noise), CT metal artifact reduction (global sharp noise), and natural image
shadow removal (local boundary-aware noise). With only 5 sampling steps, EDA
outperforms most task-specific methods and achieves state-of-the-art
performance in bias field correction and shadow removal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ C2G-KD: PCA-Constrained Generator for Data-Free Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Magnus Bengtsson, Kenneth Östberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce C2G-KD, a data-free knowledge distillation framework where a
class-conditional generator is trained to produce synthetic samples guided by a
frozen teacher model and geometric constraints derived from PCA. The generator
never observes real training data but instead learns to activate the teacher's
output through a combination of semantic and structural losses. By constraining
generated samples to lie within class-specific PCA subspaces estimated from as
few as two real examples per class, we preserve topological consistency and
diversity. Experiments on MNIST show that even minimal class structure is
sufficient to bootstrap useful synthetic training pipelines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Moral Gap of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej Skorski, Alina Landowska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Moral foundation detection is crucial for analyzing social discourse and
developing ethically-aligned AI systems. While large language models excel
across diverse tasks, their performance on specialized moral reasoning remains
unclear.
  This study provides the first comprehensive comparison between
state-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit
datasets using ROC, PR, and DET curve analysis.
  Results reveal substantial performance gaps, with LLMs exhibiting high false
negative rates and systematic under-detection of moral content despite prompt
engineering efforts. These findings demonstrate that task-specific fine-tuning
remains superior to prompting for moral reasoning applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLANCE: <span class="highlight-title">Graph</span> Logic Attention Network with <span class="highlight-title">Cluster</span> Enhancement for
  Heterophilous <span class="highlight-title">Graph</span> Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongtian Sun, Anoushka Harit, Alexandra Cristea, Christl A. Donnelly, Pietro Liò
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have demonstrated significant success in
learning from graph-structured data but often struggle on heterophilous graphs,
where connected nodes differ in features or class labels. This limitation
arises from indiscriminate neighbor aggregation and insufficient incorporation
of higher-order structural patterns. To address these challenges, we propose
GLANCE (Graph Logic Attention Network with Cluster Enhancement), a novel
framework that integrates logic-guided reasoning, dynamic graph refinement, and
adaptive clustering to enhance graph representation learning. GLANCE combines a
logic layer for interpretable and structured embeddings, multi-head
attention-based edge pruning for denoising graph structures, and clustering
mechanisms for capturing global patterns. Experimental results in benchmark
datasets, including Cornell, Texas, and Wisconsin, demonstrate that GLANCE
achieves competitive performance, offering robust and interpretable solutions
for heterophilous graph scenarios. The proposed framework is lightweight,
adaptable, and uniquely suited to the challenges of heterophilous graphs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Euclidean Distance Deflation Under High-Dimensional Heteroskedastic
  Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keyi Li, Yuval Kluger, Boris Landa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pairwise Euclidean distance calculation is a fundamental step in many machine
learning and data analysis algorithms. In real-world applications, however,
these distances are frequently distorted by heteroskedastic
noise$\unicode{x2014}$a prevalent form of inhomogeneous corruption
characterized by variable noise magnitudes across data observations. Such noise
inflates the computed distances in a nontrivial way, leading to
misrepresentations of the underlying data geometry. In this work, we address
the tasks of estimating the noise magnitudes per observation and correcting the
pairwise Euclidean distances under heteroskedastic noise. Perhaps surprisingly,
we show that in general high-dimensional settings and without assuming prior
knowledge on the clean data structure or noise distribution, both tasks can be
performed reliably, even when the noise levels vary considerably. Specifically,
we develop a principled, hyperparameter-free approach that jointly estimates
the noise magnitudes and corrects the distances. We provide theoretical
guarantees for our approach, establishing probabilistic bounds on the
estimation errors of both noise magnitudes and distances. These bounds,
measured in the normalized $\ell_1$ norm, converge to zero at polynomial rates
as both feature dimension and dataset size increase. Experiments on synthetic
datasets demonstrate that our method accurately estimates distances in
challenging regimes, significantly improving the robustness of subsequent
distance-based computations. Notably, when applied to single-cell RNA
sequencing data, our method yields noise magnitude estimates consistent with an
established prototypical model, enabling accurate nearest neighbor
identification that is fundamental to many downstream analyses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Bi<span class="highlight-title">simulation</span> Metric for <span class="highlight-title">Robust</span> Representations in
  <span class="highlight-title">Reinforcement</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leiji Zhang, Zeyu Wang, Xin Li, Yao-Hui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bisimulation metric has long been regarded as an effective control-related
representation learning technique in various reinforcement learning tasks.
However, in this paper, we identify two main issues with the conventional
bisimulation metric: 1) an inability to represent certain distinctive
scenarios, and 2) a reliance on predefined weights for differences in rewards
and subsequent states during recursive updates. We find that the first issue
arises from an imprecise definition of the reward gap, whereas the second issue
stems from overlooking the varying importance of reward difference and
next-state distinctions across different training stages and task settings. To
address these issues, by introducing a measure for state-action pairs, we
propose a revised bisimulation metric that features a more precise definition
of reward gap and novel update operators with adaptive coefficient. We also
offer theoretical guarantees of convergence for our proposed metric and its
improved representation distinctiveness. In addition to our rigorous
theoretical analysis, we conduct extensive experiments on two representative
benchmarks, DeepMind Control and Meta-World, demonstrating the effectiveness of
our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not All Features Deserve Attention: <span class="highlight-title">Graph</span>-Guided Dependency Learning for
  Tabular Data Generation with Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheyu Zhang, Shuo Yang, Bardh Prenkaj, Gjergji Kasneci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown strong potential for tabular data
generation by modeling textualized feature-value pairs. However, tabular data
inherently exhibits sparse feature-level dependencies, where many feature
interactions are structurally insignificant. This creates a fundamental
mismatch as LLMs' self-attention mechanism inevitably distributes focus across
all pairs, diluting attention on critical relationships, particularly in
datasets with complex dependencies or semantically ambiguous features. To
address this limitation, we propose GraDe (Graph-Guided Dependency Learning), a
novel method that explicitly integrates sparse dependency graphs into LLMs'
attention mechanism. GraDe employs a lightweight dynamic graph learning module
guided by externally extracted functional dependencies, prioritizing key
feature interactions while suppressing irrelevant ones. Our experiments across
diverse real-world datasets demonstrate that GraDe outperforms existing
LLM-based approaches by up to 12% on complex datasets while achieving
competitive results with state-of-the-art approaches in synthetic data quality.
Our method is minimally intrusive yet effective, offering a practical solution
for structure-aware tabular data modeling with LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DriftMoE: A Mixture of Experts Approach to Handle Concept Drifts <span class="chip">ECML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miguel Aspis, Sebastián A. Cajas Ordónez, Andrés L. Suárez-Cetrulo, Ricardo Simón Carbajo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning from non-stationary data streams subject to concept drift requires
models that can adapt on-the-fly while remaining resource-efficient. Existing
adaptive ensemble methods often rely on coarse-grained adaptation mechanisms or
simple voting schemes that fail to optimally leverage specialized knowledge.
This paper introduces DriftMoE, an online Mixture-of-Experts (MoE) architecture
that addresses these limitations through a novel co-training framework.
DriftMoE features a compact neural router that is co-trained alongside a pool
of incremental Hoeffding tree experts. The key innovation lies in a symbiotic
learning loop that enables expert specialization: the router selects the most
suitable expert for prediction, the relevant experts update incrementally with
the true label, and the router refines its parameters using a multi-hot
correctness mask that reinforces every accurate expert. This feedback loop
provides the router with a clear training signal while accelerating expert
specialization. We evaluate DriftMoE's performance across nine state-of-the-art
data stream learning benchmarks spanning abrupt, gradual, and real-world drifts
testing two distinct configurations: one where experts specialize on data
regimes (multi-class variant), and another where they focus on single-class
specialization (task-based variant). Our results demonstrate that DriftMoE
achieves competitive results with state-of-the-art stream learning adaptive
ensembles, offering a principled and efficient approach to concept drift
adaptation. All code, data pipelines, and reproducibility scripts are available
in our public GitHub repository: https://github.com/miguel-ceadar/drift-moe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the SYNDAiTE@ECMLPKDD 2025 workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Restoring Rhythm: Punctuation Restoration Using Transformer Models for
  Bangla, a Low-Resource Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Obyedullahil Mamun, Md Adyelullahil Mamun, Arif Ahmad, Md. Imran Hossain Emu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Punctuation restoration enhances the readability of text and is critical for
post-processing tasks in Automatic Speech Recognition (ASR), especially for
low-resource languages like Bangla. In this study, we explore the application
of transformer-based models, specifically XLM-RoBERTa-large, to automatically
restore punctuation in unpunctuated Bangla text. We focus on predicting four
punctuation marks: period, comma, question mark, and exclamation mark across
diverse text domains. To address the scarcity of annotated resources, we
constructed a large, varied training corpus and applied data augmentation
techniques. Our best-performing model, trained with an augmentation factor of
alpha = 0.20%, achieves an accuracy of 97.1% on the News test set, 91.2% on the
Reference set, and 90.2% on the ASR set.
  Results show strong generalization to reference and ASR transcripts,
demonstrating the model's effectiveness in real-world, noisy scenarios. This
work establishes a strong baseline for Bangla punctuation restoration and
contributes publicly available datasets and code to support future research in
low-resource NLP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NLML-HPE: Head Pose <span class="highlight-title">Estimation</span> with Limited Data via Manifold Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahdi Ghafourian, Federico M. Sukno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Head pose estimation (HPE) plays a critical role in various computer vision
applications such as human-computer interaction and facial recognition. In this
paper, we propose a novel deep learning approach for head pose estimation with
limited training data via non-linear manifold learning called NLML-HPE. This
method is based on the combination of tensor decomposition (i.e., Tucker
decomposition) and feed forward neural networks. Unlike traditional
classification-based approaches, our method formulates head pose estimation as
a regression problem, mapping input landmarks into a continuous representation
of pose angles. To this end, our method uses tensor decomposition to split each
Euler angle (yaw, pitch, roll) to separate subspaces and models each dimension
of the underlying manifold as a cosine curve. We address two key challenges: 1.
Almost all HPE datasets suffer from incorrect and inaccurate pose annotations.
Hence, we generated a precise and consistent 2D head pose dataset for our
training set by rotating 3D head models for a fixed set of poses and rendering
the corresponding 2D images. 2. We achieved real-time performance with limited
training data as our method accurately captures the nature of rotation of an
object from facial landmarks. Once the underlying manifold for rotation around
each axis is learned, the model is very fast in predicting unseen data. Our
training and testing code is available online along with our trained models:
https: //github.com/MahdiGhafoorian/NLML_HPE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Model Ensemble and Reservoir Computing for River Discharge
  <span class="highlight-title">Prediction</span> in Ungauged Basins 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mizuki Funato, Yohei Sawada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the critical need for accurate flood prediction and water management,
many regions lack sufficient river discharge observations, limiting the skill
of rainfall-runoff analyses. Although numerous physically based and machine
learning models exist, achieving high accuracy, interpretability, and
computational efficiency under data-scarce conditions remains a major
challenge. We address this challenge with a novel method, HYdrological
Prediction with multi-model Ensemble and Reservoir computing (HYPER) that
leverages multi-model ensemble and reservoir computing (RC). Our approach first
applies Bayesian model averaging (BMA) to 43 "uncalibrated" catchment-based
conceptual hydrological models. An RC model is then trained via linear
regression to correct errors in the BMA output, a non-iterative process that
ensures high computational efficiency. For ungauged basins, we infer the
required BMA and RC weights by linking them to catchment attributes from gauged
basins, creating a generalizable framework. We evaluated HYPER using data from
87 river basins in Japan. In a data-rich scenario, HYPER (median Kling-Gupta
Efficiency, KGE, of 0.56) performed comparably to a benchmark LSTM (KGE 0.55)
but required only 5% of its computational time. In a data-scarce scenario (23%
of basins gauged), HYPER maintained robust performance (KGE 0.55) and lower
uncertainty, whereas the LSTM's performance degraded significantly (KGE -0.04).
These results reveal that individual conceptual hydrological models do not
necessarily need to be calibrated when an effectively large ensemble is
assembled and combined with machine-learning-based bias correction. HYPER
provides a robust, efficient, and generalizable solution for discharge
prediction, particularly in ungauged basins, making it applicable to a wide
range of regions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FinDPO: Financial Sentiment Analysis for Algorithmic Trading through
  Preference <span class="highlight-title">Optimization</span> of LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgos Iacovides, Wuyang Zhou, Danilo Mandic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Opinions expressed in online finance-related textual data are having an
increasingly profound impact on trading decisions and market movements. This
trend highlights the vital role of sentiment analysis as a tool for quantifying
the nature and strength of such opinions. With the rapid development of
Generative AI (GenAI), supervised fine-tuned (SFT) large language models (LLMs)
have become the de facto standard for financial sentiment analysis. However,
the SFT paradigm can lead to memorization of the training data and often fails
to generalize to unseen samples. This is a critical limitation in financial
domains, where models must adapt to previously unobserved events and the
nuanced, domain-specific language of finance. To this end, we introduce FinDPO,
the first finance-specific LLM framework based on post-training human
preference alignment via Direct Preference Optimization (DPO). The proposed
FinDPO achieves state-of-the-art performance on standard sentiment
classification benchmarks, outperforming existing supervised fine-tuned models
by 11% on the average. Uniquely, the FinDPO framework enables the integration
of a fine-tuned causal LLM into realistic portfolio strategies through a novel
'logit-to-score' conversion, which transforms discrete sentiment predictions
into continuous, rankable sentiment scores (probabilities). In this way,
simulations demonstrate that FinDPO is the first sentiment-based approach to
maintain substantial positive returns of 67% annually and strong risk-adjusted
performance, as indicated by a Sharpe ratio of 2.0, even under realistic
transaction costs of 5 basis points (bps).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Iwin Transformer: Hierarchical Vision Transformer using Interleaved
  Windows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simin Huo, Ning Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Iwin Transformer, a novel position-embedding-free hierarchical
vision transformer, which can be fine-tuned directly from low to high
resolution, through the collaboration of innovative interleaved window
attention and depthwise separable convolution. This approach uses attention to
connect distant tokens and applies convolution to link neighboring tokens,
enabling global information exchange within a single module, overcoming Swin
Transformer's limitation of requiring two consecutive blocks to approximate
global attention. Extensive experiments on visual benchmarks demonstrate that
Iwin Transformer exhibits strong competitiveness in tasks such as image
classification (87.4 top-1 accuracy on ImageNet-1K), semantic segmentation and
video action recognition. We also validate the effectiveness of the core
component in Iwin as a standalone module that can seamlessly replace the
self-attention module in class-conditional image generation. The concepts and
methods introduced by the Iwin Transformer have the potential to inspire future
research, like Iwin 3D Attention in video generation. The code and models are
available at https://github.com/cominder/Iwin-Transformer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 10 figures, Submitted to IEEE Transactions on Pattern
  Analysis and Machine Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLEAR: Error Analysis via LLM-as-a-Judge Made Easy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asaf Yehudai, Lilach Eden, Yotam Perlitz, Roy Bar-Haim, Michal Shmueli-Scheuer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evaluation of Large Language Models (LLMs) increasingly relies on other
LLMs acting as judges. However, current evaluation paradigms typically yield a
single score or ranking, answering which model is better but not why. While
essential for benchmarking, these top-level scores obscure the specific,
actionable reasons behind a model's performance. To bridge this gap, we
introduce CLEAR, an interactive, open-source package for LLM-based error
analysis. CLEAR first generates per-instance textual feedback, then it creates
a set of system-level error issues, and quantifies the prevalence of each
identified issue. Our package also provides users with an interactive dashboard
that allows for a comprehensive error analysis through aggregate
visualizations, applies interactive filters to isolate specific issues or score
ranges, and drills down to the individual instances that exemplify a particular
behavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks,
and showcase its utility through a user case study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive <span class="highlight-title">Review</span> of Dif<span class="highlight-title">fusion</span> Models in Smart Agriculture:
  Progress, Applications, and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xing Hua, Haodong Chen, Qianqian Duan, Danfeng Hong, Ruijiao Li, Huiliang Shang, Linghua Jiang, Haima Yang, Dawei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the global population growing and arable land resources becoming
increasingly scarce,smart agriculture and precision agriculture have emerged as
key directions for the future ofagricultural development.Artificial
intelligence (AI) technologies, particularly deep learning models, have found
widespread applications in areas such as crop monitoring and pest detection. As
an emerging generative model, diffusion models have shown significant promise
in tasks like agricultural image processing, data augmentation, and remote
sensing. Compared to traditional generative adversarial networks (GANs),
diffusion models offer superior training stability and generation quality,
effectively addressing challenges such as limited agricultural data and
imbalanced image samples. This paper reviews the latest advancements in the
application of diffusion models in agriculture, focusing on their potential in
crop pest and disease detection, remote sensing image enhancement, crop growth
prediction, and agricultural resource management. Experimental results
demonstrate that diffusion models significantly improve model accuracy and
robustness in data augmentation, image generation, and denoising, especially in
complex environments. Despite challenges related to computational efficiency
and generalization capabilities, diffusion models are expected to play an
increasingly important role in smart and precision agriculture as technology
advances, providing substantial support for the sustainable development of
global agriculture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Reconstructing Training Data From Bayesian Posteriors and Trained
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Wynne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Publicly releasing the specification of a model with its trained parameters
means an adversary can attempt to reconstruct information about the training
data via training data reconstruction attacks, a major vulnerability of modern
machine learning methods. This paper makes three primary contributions:
establishing a mathematical framework to express the problem, characterising
the features of the training data that are vulnerable via a maximum mean
discrepancy equivalance and outlining a score matching framework for
reconstructing data in both Bayesian and non-Bayesian models, the former is a
first in the literature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Uncertainty in LLMs through Evidential Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lakshmana Sri Harsha Nemani, P. K. Srijith, Tomasz Kuśmierczyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate uncertainty quantification remains a key challenge for standard
LLMs, prompting the adoption of Bayesian and ensemble-based methods. However,
such methods typically necessitate computationally expensive sampling,
involving multiple forward passes to effectively estimate predictive
uncertainty.
  In this paper, we introduce a novel approach enabling efficient and effective
uncertainty estimation in LLMs without sacrificing performance. Specifically,
we distill uncertainty-aware teacher models - originally requiring multiple
forward passes - into compact student models sharing the same architecture but
fine-tuned using Low-Rank Adaptation (LoRA). We compare two distinct
distillation strategies: one in which the student employs traditional
softmax-based outputs, and another in which the student leverages
Dirichlet-distributed outputs to explicitly model epistemic uncertainty via
evidential learning.
  Empirical evaluations on classification datasets demonstrate that such
students can achieve comparable or superior predictive and uncertainty
quantification performance relative to their teacher models, while critically
requiring only a single forward pass. To our knowledge, this is the first
demonstration that immediate and robust uncertainty quantification can be
achieved in LLMs through evidential distillation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tiny is not small enough: High-quality, low-resource facial animation
  models through hybrid knowledge distillation <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Han, Mattias Teye, Derek Yadgaroff, Judith Bütepage
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The training of high-quality, robust machine learning models for
speech-driven 3D facial animation requires a large, diverse dataset of
high-quality audio-animation pairs. To overcome the lack of such a dataset,
recent work has introduced large pre-trained speech encoders that are robust to
variations in the input audio and, therefore, enable the facial animation model
to generalize across speakers, audio quality, and languages. However, the
resulting facial animation models are prohibitively large and lend themselves
only to offline inference on a dedicated machine. In this work, we explore
on-device, real-time facial animation models in the context of game
development. We overcome the lack of large datasets by using hybrid knowledge
distillation with pseudo-labeling. Given a large audio dataset, we employ a
high-performing teacher model to train very small student models. In contrast
to the pre-trained speech encoders, our student models only consist of
convolutional and fully-connected layers, removing the need for attention
context or recurrent updates. In our experiments, we demonstrate that we can
reduce the memory footprint to up to 3.4 MB and required future audio context
to up to 81 ms while maintaining high-quality animations. This paves the way
for on-device inference, an important step towards realistic, model-driven
digital characters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM Transactions on Graphics 2025 (SIGGRAPH journal
  track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-rank adaptive physics-informed HyperDeepONets for solving
  differential equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Etienne Zeudong, Elsa Cardoso-Bihlo, Alex Bihlo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  HyperDeepONets were introduced in Lee, Cho and Hwang [ICLR, 2023] as an
alternative architecture for operator learning, in which a hypernetwork
generates the weights for the trunk net of a DeepONet. While this improves
expressivity, it incurs high memory and computational costs due to the large
number of output parameters required. In this work we introduce, in the
physics-informed machine learning setting, a variation, PI-LoRA-HyperDeepONets,
which leverage low-rank adaptation (LoRA) to reduce complexity by decomposing
the hypernetwork's output layer weight matrix into two smaller low-rank
matrices. This reduces the number of trainable parameters while introducing an
extra regularization of the trunk networks' weights. Through extensive
experiments on both ordinary and partial differential equations we show that
PI-LoRA-HyperDeepONets achieve up to 70\% reduction in parameters and
consistently outperform regular HyperDeepONets in terms of predictive accuracy
and generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Remembering the Markov Property in Cooperative MARL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kale-ab Abebe Tessera, Leonard Hinckeldey, Riccardo Zamboni, David Abel, Amos Storkey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cooperative multi-agent reinforcement learning (MARL) is typically formalised
as a Decentralised Partially Observable Markov Decision Process (Dec-POMDP),
where agents must reason about the environment and other agents' behaviour. In
practice, current model-free MARL algorithms use simple recurrent function
approximators to address the challenge of reasoning about others using partial
information. In this position paper, we argue that the empirical success of
these methods is not due to effective Markov signal recovery, but rather to
learning simple conventions that bypass environment observations and memory.
Through a targeted case study, we show that co-adapting agents can learn
brittle conventions, which then fail when partnered with non-adaptive agents.
Crucially, the same models can learn grounded policies when the task design
necessitates it, revealing that the issue is not a fundamental limitation of
the learning models but a failure of the benchmark design. Our analysis also
suggests that modern MARL environments may not adequately test the core
assumptions of Dec-POMDPs. We therefore advocate for new cooperative
environments built upon two core principles: (1) behaviours grounded in
observations and (2) memory-based reasoning about other agents, ensuring
success requires genuine skill rather than fragile, co-adapted agreements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>RLC Finding the Frame Workshop Camera-Ready, 8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Dimensionless Learning (Hi-π): A physics-data
  hybrid-driven approach for discovering dimensionless parameter combinations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingkun Xia, Haitao Lin, Weiwei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dimensional analysis provides a universal framework for reducing physical
complexity and reveal inherent laws. However, its application to
high-dimensional systems still generates redundant dimensionless parameters,
making it challenging to establish physically meaningful descriptions. Here, we
introduce Hierarchical Dimensionless Learning (Hi-{\pi}), a physics-data
hybrid-driven method that combines dimensional analysis and symbolic regression
to automatically discover key dimensionless parameter combination(s). We
applied this method to classic examples in various research fields of fluid
mechanics. For the Rayleigh-B\'enard convection, this method accurately
extracted two intrinsic dimensionless parameters: the Rayleigh number and the
Prandtl number, validating its unified representation advantage across
multiscale data. For the viscous flows in a circular pipe, the method
automatically discovers two optimal dimensionless parameters: the Reynolds
number and relative roughness, achieving a balance between accuracy and
complexity. For the compressibility correction in subsonic flow, the method
effectively extracts the classic compressibility correction formulation, while
demonstrating its capability to discover hierarchical structural expressions
through optimal parameter transformations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GVCCS: A <span class="highlight-title">Dataset</span> for Contrail Identification and Tracking on Visible
  Whole Sky Camera Sequences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Jarry, Ramon Dalmau, Philippe Very, Franck Ballerini, Stephania-Denisa Bocu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aviation's climate impact includes not only CO2 emissions but also
significant non-CO2 effects, especially from contrails. These ice clouds can
alter Earth's radiative balance, potentially rivaling the warming effect of
aviation CO2. Physics-based models provide useful estimates of contrail
formation and climate impact, but their accuracy depends heavily on the quality
of atmospheric input data and on assumptions used to represent complex
processes like ice particle formation and humidity-driven persistence.
Observational data from remote sensors, such as satellites and ground cameras,
could be used to validate and calibrate these models. However, existing
datasets don't explore all aspect of contrail dynamics and formation: they
typically lack temporal tracking, and do not attribute contrails to their
source flights. To address these limitations, we present the Ground Visible
Camera Contrail Sequences (GVCCS), a new open data set of contrails recorded
with a ground-based all-sky camera in the visible range. Each contrail is
individually labeled and tracked over time, allowing a detailed analysis of its
lifecycle. The dataset contains 122 video sequences (24,228 frames) and
includes flight identifiers for contrails that form above the camera. As
reference, we also propose a unified deep learning framework for contrail
analysis using a panoptic segmentation model that performs semantic
segmentation (contrail pixel identification), instance segmentation (individual
contrail separation), and temporal tracking in a single architecture. By
providing high-quality, temporally resolved annotations and a benchmark for
model evaluation, our work supports improved contrail monitoring and will
facilitate better calibration of physical models. This sets the groundwork for
more accurate climate impact understanding and assessments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-<span class="highlight-title">Dataset</span> <span class="highlight-title">Benchmark</span> for <span class="highlight-title">Semi-Supervised</span> Semantic <span class="highlight-title">Segmentation</span> in
  ECG Delineation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minje Park, Jeonghwa Lim, Taehyung Yu, Sunghoon Joo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electrocardiogram (ECG) delineation, the segmentation of meaningful waveform
features, is critical for clinical diagnosis. Despite recent advances using
deep learning, progress has been limited by the scarcity of publicly available
annotated datasets. Semi-supervised learning presents a promising solution by
leveraging abundant unlabeled ECG data. In this study, we present the first
systematic benchmark for semi-supervised semantic segmentation (SemiSeg) in ECG
delineation. We curated and unified multiple public datasets, including
previously underused sources, to support robust and diverse evaluation. We
adopted five representative SemiSeg algorithms from computer vision,
implemented them on two different architectures: the convolutional network and
the transformer, and evaluated them in two different settings: in-domain and
cross-domain. Additionally, we propose ECG-specific training configurations and
augmentation strategies and introduce a standardized evaluation framework. Our
results show that the transformer outperforms the convolutional network in
semi-supervised ECG delineation. We anticipate that our benchmark will serve as
a foundation for advancing semi-supervised ECG delineation methods and will
facilitate further research in this domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ State of Health <span class="highlight-title">Estimation</span> of Batteries Using a Time-Informed <span class="highlight-title">Dynamic</span>
  Sequence-Inverted Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Janak M. Patel, Milad Ramezankhani, Anirudh Deodhar, Dagnachew Birru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid adoption of battery-powered vehicles and energy storage systems
over the past decade has made battery health monitoring increasingly critical.
Batteries play a central role in the efficiency and safety of these systems,
yet they inevitably degrade over time due to repeated charge-discharge cycles.
This degradation leads to reduced energy efficiency and potential overheating,
posing significant safety concerns. Accurate estimation of a State of Health
(SoH) of battery is therefore essential for ensuring operational reliability
and safety. Several machine learning architectures, such as LSTMs,
transformers, and encoder-based models, have been proposed to estimate SoH from
discharge cycle data. However, these models struggle with the irregularities
inherent in real-world measurements: discharge readings are often recorded at
non-uniform intervals, and the lengths of discharge cycles vary significantly.
To address this, most existing approaches extract features from the sequences
rather than processing them in full, which introduces information loss and
compromises accuracy. To overcome these challenges, we propose a novel
architecture: Time-Informed Dynamic Sequence Inverted Transformer (TIDSIT).
TIDSIT incorporates continuous time embeddings to effectively represent
irregularly sampled data and utilizes padded sequences with temporal attention
mechanisms to manage variable-length inputs without discarding sequence
information. Experimental results on the NASA battery degradation dataset show
that TIDSIT significantly outperforms existing models, achieving over 50%
reduction in prediction error and maintaining an SoH prediction error below
0.58%. Furthermore, the architecture is generalizable and holds promise for
broader applications in health monitoring tasks involving irregular time-series
data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Regression-aware Continual Learning for Android Malware <span class="highlight-title">Detection</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniele Ghiani, Daniele Angioni, Giorgio Piras, Angelo Sotgiu, Luca Minnei, Srishti Gupta, Maura Pintor, Fabio Roli, Battista Biggio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Malware evolves rapidly, forcing machine learning (ML)-based detectors to
adapt continuously. With antivirus vendors processing hundreds of thousands of
new samples daily, datasets can grow to billions of examples, making full
retraining impractical. Continual learning (CL) has emerged as a scalable
alternative, enabling incremental updates without full data access while
mitigating catastrophic forgetting. In this work, we analyze a critical yet
overlooked issue in this context: security regression. Unlike forgetting, which
manifests as a general performance drop on previously seen data, security
regression captures harmful prediction changes at the sample level, such as a
malware sample that was once correctly detected but evades detection after a
model update. Although often overlooked, regressions pose serious risks in
security-critical applications, as the silent reintroduction of previously
detected threats in the system may undermine users' trust in the whole updating
process. To address this issue, we formalize and quantify security regression
in CL-based malware detectors and propose a regression-aware penalty to
mitigate it. Specifically, we adapt Positive Congruent Training (PCT) to the CL
setting, preserving prior predictive behavior in a model-agnostic manner.
Experiments on the ELSA, Tesseract, and AZ-Class datasets show that our method
effectively reduces regression across different CL scenarios while maintaining
strong detection performance over time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions on Information Forensics and Security</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-Supervised</span> Coarsening of Unstructured Grid with Automatic
  Differentiation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sergei Shumilin, Alexander Ryabov, Nikolay Yavich, Evgeny Burnaev, Vladimir Vanovskiy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the high computational load of modern numerical simulation, there is a
demand for approaches that would reduce the size of discrete problems while
keeping the accuracy reasonable. In this work, we present an original algorithm
to coarsen an unstructured grid based on the concepts of differentiable
physics. We achieve this by employing k-means clustering, autodifferentiation
and stochastic minimization algorithms. We demonstrate performance of the
designed algorithm on two PDEs: a linear parabolic equation which governs
slightly compressible fluid flow in porous media and the wave equation. Our
results show that in the considered scenarios, we reduced the number of grid
points up to 10 times while preserving the modeled variable dynamics in the
points of interest. The proposed approach can be applied to the simulation of
an arbitrary system described by evolutionary partial differential equations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Data Augmentation and Siamese Learning for Predictive Process
  Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sjoerd van Straten, Alessandro Padella, Marwan Hassani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predictive Process Monitoring (PPM) enables forecasting future events or
outcomes of ongoing business process instances based on event logs. However,
deep learning PPM approaches are often limited by the low variability and small
size of real-world event logs. To address this, we introduce SiamSA-PPM, a
novel self-supervised learning framework that combines Siamese learning with
Statistical Augmentation for Predictive Process Monitoring. It employs three
novel statistically grounded transformation methods that leverage control-flow
semantics and frequent behavioral patterns to generate realistic, semantically
valid new trace variants. These augmented views are used within a Siamese
learning setup to learn generalizable representations of process prefixes
without the need for labeled supervision. Extensive experiments on real-life
event logs demonstrate that SiamSA-PPM achieves competitive or superior
performance compared to the SOTA in both next activity and final outcome
prediction tasks. Our results further show that statistical augmentation
significantly outperforms random transformations and improves variability in
the data, highlighting SiamSA-PPM as a promising direction for training data
enrichment in process prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic
  Grounding for Generalizable <span class="highlight-title">Robot</span>ic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu Su, Weiwei Shang, Chen Qian, Fei Zhang, Shuang Cong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantics-driven 3D spatial constraints align highlevel semantic
representations with low-level action spaces, facilitating the unification of
task understanding and execution in robotic manipulation. The synergistic
reasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation
Models (VFMs) enables cross-modal 3D spatial constraint construction.
Nevertheless, existing methods have three key limitations: (1) coarse semantic
granularity in constraint modeling, (2) lack of real-time closed-loop planning,
(3) compromised robustness in semantically diverse environments. To address
these challenges, we propose ReSem3D, a unified manipulation framework for
semantically diverse environments, leveraging the synergy between VFMs and
MLLMs to achieve fine-grained visual grounding and dynamically constructs
hierarchical 3D spatial constraints for real-time manipulation. Specifically,
the framework is driven by hierarchical recursive reasoning in MLLMs, which
interact with VFMs to automatically construct 3D spatial constraints from
natural language instructions and RGB-D observations in two stages: part-level
extraction and region-level refinement. Subsequently, these constraints are
encoded as real-time optimization objectives in joint space, enabling reactive
behavior to dynamic disturbances. Extensive simulation and real-world
experiments are conducted in semantically rich household and sparse chemical
lab environments. The results demonstrate that ReSem3D performs diverse
manipulation tasks under zero-shot conditions, exhibiting strong adaptability
and generalization. Code and videos at https://resem3d.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages,9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Revisited: <span class="highlight-title">Benchmark</span>ing and Advancing LP-Based Ensemble Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Akkerman, Julien Ferry, Christian Artigues, Emmanuel Hebrard, Thibaut Vidal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their theoretical appeal, totally corrective boosting methods based
on linear programming have received limited empirical attention. In this paper,
we conduct the first large-scale experimental study of six LP-based boosting
formulations, including two novel methods, NM-Boost and QRLP-Boost, across 20
diverse datasets. We evaluate the use of both heuristic and optimal base
learners within these formulations, and analyze not only accuracy, but also
ensemble sparsity, margin distribution, anytime performance, and hyperparameter
sensitivity. We show that totally corrective methods can outperform or match
state-of-the-art heuristics like XGBoost and LightGBM when using shallow trees,
while producing significantly sparser ensembles. We further show that these
methods can thin pre-trained ensembles without sacrificing performance, and we
highlight both the strengths and limitations of using optimal decision trees in
this context.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse identification of nonlinear <span class="highlight-title">dynamic</span>s with library <span class="highlight-title">optimization</span>
  mechanism: Recursive <span class="highlight-title">long-term</span> <span class="highlight-title">prediction</span> perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18220v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18220v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ansei Yonezawa, Heisei Yonezawa, Shuichi Yahagi, Itsuro Kajiwara, Shinya Kijimoto, Hikaru Taniuchi, Kentaro Murakami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sparse identification of nonlinear dynamics (SINDy) approach can discover
the governing equations of dynamical systems based on measurement data, where
the dynamical model is identified as the sparse linear combination of the given
basis functions. A major challenge in SINDy is the design of a library, which
is a set of candidate basis functions, as the appropriate library is not
trivial for many dynamical systems. To overcome this difficulty, this study
proposes SINDy with library optimization mechanism (SINDy-LOM), which is a
combination of the sparse regression technique and the novel learning strategy
of the library. In the proposed approach, the basis functions are parametrized.
The SINDy-LOM approach involves a two-layer optimization architecture: the
inner-layer, in which the data-driven model is extracted as the sparse linear
combination of the candidate basis functions, and the outer-layer, in which the
basis functions are optimized from the viewpoint of the recursive long-term
(RLT) prediction accuracy; thus, the library design is reformulated as the
optimization of the parametrized basis functions. The resulting SINDy-LOM model
has good interpretability and usability, as the proposed approach yields the
parsimonious model. The library optimization mechanism significantly reduces
user burden. The RLT perspective improves the reliability of the resulting
model compared with the traditional SINDy approach that can only ensure the
one-step-ahead prediction accuracy. The validity of the proposed approach is
demonstrated by applying it to a diesel engine airpath system, which is a
well-known complex industrial system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedSA-GCL: A Semi-Asynchronous Federated <span class="highlight-title">Graph</span> Learning Framework with
  Personalized Aggregation and <span class="highlight-title">Cluster</span>-Aware Broadcasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongzheng Yuan, Lianshuai Guo, Xunkai Li, Yinlin Zhu, Wenyu Wang, Meixia Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Graph Learning (FGL) is a distributed learning paradigm that
enables collaborative training over large-scale subgraphs located on multiple
local systems. However, most existing FGL approaches rely on synchronous
communication, which leads to inefficiencies and is often impractical in
real-world deployments. Meanwhile, current asynchronous federated learning
(AFL) methods are primarily designed for conventional tasks such as image
classification and natural language processing, without accounting for the
unique topological properties of graph data. Directly applying these methods to
graph learning can possibly result in semantic drift and representational
inconsistency in the global model. To address these challenges, we propose
FedSA-GCL, a semi-asynchronous federated framework that leverages both
inter-client label distribution divergence and graph topological
characteristics through a novel ClusterCast mechanism for efficient training.
We evaluate FedSA-GCL on multiple real-world graph datasets using the Louvain
and Metis split algorithms, and compare it against 9 baselines. Extensive
experiments demonstrate that our method achieves strong robustness and
outstanding efficiency, outperforming the baselines by an average of 2.92% with
the Louvain and by 3.4% with the Metis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Goal-based <span class="highlight-title">Trajectory</span> <span class="highlight-title">Prediction</span> for improved Cross-<span class="highlight-title">Dataset</span>
  Generalization <span class="chip">SC 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18196v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18196v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Grimm, Ahmed Abouelazm, J. Marius Zöllner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To achieve full autonomous driving, a good understanding of the surrounding
environment is necessary. Especially predicting the future states of other
traffic participants imposes a non-trivial challenge. Current SotA-models
already show promising results when trained on real datasets (e.g. Argoverse2,
NuScenes). Problems arise when these models are deployed to new/unseen areas.
Typically, performance drops significantly, indicating that the models lack
generalization. In this work, we introduce a new Graph Neural Network (GNN)
that utilizes a heterogeneous graph consisting of traffic participants and
vectorized road network. Latter, is used to classify goals, i.e. endpoints of
the predicted trajectories, in a multi-staged approach, leading to a better
generalization to unseen scenarios. We show the effectiveness of the goal
selection process via cross-dataset evaluation, i.e. training on Argoverse2 and
evaluating on NuScenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted on IEEE ITSC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChronoSelect: <span class="highlight-title">Robust</span> Learning with Noisy Labels via <span class="highlight-title">Dynamic</span>s Temporal
  Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianchao Wang, Qingfeng Li, Pengcheng Zheng, Xiaorong Pu, Yazhou Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training deep neural networks on real-world datasets is often hampered by the
presence of noisy labels, which can be memorized by over-parameterized models,
leading to significant degradation in generalization performance. While
existing methods for learning with noisy labels (LNL) have made considerable
progress, they fundamentally suffer from static snapshot evaluations and fail
to leverage the rich temporal dynamics of learning evolution. In this paper, we
propose ChronoSelect (chrono denoting its temporal nature), a novel framework
featuring an innovative four-stage memory architecture that compresses
prediction history into compact temporal distributions. Our unique sliding
update mechanism with controlled decay maintains only four dynamic memory units
per sample, progressively emphasizing recent patterns while retaining essential
historical knowledge. This enables precise three-way sample partitioning into
clean, boundary, and noisy subsets through temporal trajectory analysis and
dual-branch consistency. Theoretical guarantees prove the mechanism's
convergence and stability under noisy conditions. Extensive experiments
demonstrate ChronoSelect's state-of-the-art performance across synthetic and
real-world benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeoAvatar: Adaptive <span class="highlight-title">Geometric</span>al Gaussian Splatting for 3D Head Avatar <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        SeungJun Moon, Hah Min Lew, Seungeun Lee, Ji-Su Kang, Gyeong-Moon Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent progress in 3D head avatar generation, balancing identity
preservation, i.e., reconstruction, with novel poses and expressions, i.e.,
animation, remains a challenge. Existing methods struggle to adapt Gaussians to
varying geometrical deviations across facial regions, resulting in suboptimal
quality. To address this, we propose GeoAvatar, a framework for adaptive
geometrical Gaussian Splatting. GeoAvatar leverages Adaptive Pre-allocation
Stage (APS), an unsupervised method that segments Gaussians into rigid and
flexible sets for adaptive offset regularization. Then, based on mouth anatomy
and dynamics, we introduce a novel mouth structure and the part-wise
deformation strategy to enhance the animation fidelity of the mouth. Finally,
we propose a regularization loss for precise rigging between Gaussians and 3DMM
faces. Moreover, we release DynamicFace, a video dataset with highly expressive
facial motions. Extensive experiments show the superiority of GeoAvatar
compared to state-of-the-art methods in reconstruction and novel animation
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025, Project page: https://hahminlew.github.io/geoavatar/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Noisy Labels Meet Class Imbalance on <span class="highlight-title">Graph</span>s: A <span class="highlight-title">Graph</span> Augmentation
  Method with LLM and Pseudo Label 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18153v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18153v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riting Xia, Rucong Wang, Yulin Liu, Anchen Li, Xueyan Liu, Yan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-imbalanced graph node classification is a practical yet underexplored
research problem. Although recent studies have attempted to address this issue,
they typically assume clean and reliable labels when processing
class-imbalanced graphs. This assumption often violates the nature of
real-world graphs, where labels frequently contain noise. Given this gap, this
paper systematically investigates robust node classification for
class-imbalanced graphs with noisy labels. We propose GraphALP, a novel Graph
Augmentation framework based on Large language models (LLMs) and
Pseudo-labeling techniques. Specifically, we design an LLM-based oversampling
method to generate synthetic minority nodes, producing label-accurate minority
nodes to alleviate class imbalance. Based on the class-balanced graphs, we
develop a dynamically weighted pseudo-labeling method to obtain high-confidence
pseudo labels to reduce label noise ratio. Additionally, we implement a
secondary LLM-guided oversampling mechanism to mitigate potential class
distribution skew caused by pseudo labels. Experimental results show that
GraphALP achieves superior performance over state-of-the-art methods on
class-imbalanced graphs with noisy labels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neuromorphic Computing for <span class="highlight-title">Embodied</span> Intelligence in Autonomous Systems:
  Current Trends, Challenges, and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alberto Marchisio, Muhammad Shafique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing need for intelligent, adaptive, and energy-efficient autonomous
systems across fields such as robotics, mobile agents (e.g., UAVs), and
self-driving vehicles is driving interest in neuromorphic computing. By drawing
inspiration from biological neural systems, neuromorphic approaches offer
promising pathways to enhance the perception, decision-making, and
responsiveness of autonomous platforms. This paper surveys recent progress in
neuromorphic algorithms, specialized hardware, and cross-layer optimization
strategies, with a focus on their deployment in real-world autonomous
scenarios. Special attention is given to event-based dynamic vision sensors and
their role in enabling fast, efficient perception. The discussion highlights
new methods that improve energy efficiency, robustness, adaptability, and
reliability through the integration of spiking neural networks into autonomous
system architectures. We integrate perspectives from machine learning,
robotics, neuroscience, and neuromorphic engineering to offer a comprehensive
view of the state of the field. Finally, emerging trends and open challenges
are explored, particularly in the areas of real-time decision-making, continual
learning, and the development of secure, resilient autonomous systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at the 31st IEEE International Symposium on On-Line Testing
  and Robust System Design (IOLTS), Ischia, Italy, July 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Maximizing Prefix-Confidence at Test-Time Efficiently Improves
  Mathematical Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18122v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18122v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias Otth, Jonas Hübotter, Ido Hakimi, Andreas Krause
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has shown that language models can self-improve by maximizing
their own confidence in their predictions, without relying on external
verifiers or reward signals. In this work, we study the test-time scaling of
language models for mathematical reasoning tasks, where the model's own
confidence is used to select the most promising attempts. Surprisingly, we find
that we can achieve significant performance gains by continuing only the most
promising attempt, selected by the model's prefix-confidence. We systematically
evaluate prefix-confidence scaling on five mathematical reasoning datasets: the
school-level GSM8K and MATH500, and the competition-level AMC23, AIME24, and
AIME25. We find that prefix-confidence scaling with prefixes of only 32 tokens
achieves a better accuracy-compute trade-off than majority voting. Moreover,
prefix-confidence scaling appears less susceptible than BoN to length biases.
Finally, we also evaluate test-time training with prefix-confidence and find
that, while outperforming the base model, it does not improve over
prefix-confidence scaling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Two-armed Bandit Framework for A/B Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinjuan Wang, Qianglin Wen, Yu Zhang, Xiaodong Yan, Chengchun Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A/B testing is widely used in modern technology companies for policy
evaluation and product deployment, with the goal of comparing the outcomes
under a newly-developed policy against a standard control. Various causal
inference and reinforcement learning methods developed in the literature are
applicable to A/B testing. This paper introduces a two-armed bandit framework
designed to improve the power of existing approaches. The proposed procedure
consists of three main steps: (i) employing doubly robust estimation to
generate pseudo-outcomes, (ii) utilizing a two-armed bandit framework to
construct the test statistic, and (iii) applying a permutation-based method to
compute the $p$-value. We demonstrate the efficacy of the proposed method
through asymptotic theories, numerical experiments and real-world data from a
ridesharing company, showing its superior performance in comparison to existing
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agentic AI framework for End-to-End Medical Data Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18115v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18115v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soorya Ram Shimgekar, Shayan Vassef, Abhay Goyal, Navin Kumar, Koustuv Saha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building and deploying machine learning solutions in healthcare remains
expensive and labor-intensive due to fragmented preprocessing workflows, model
compatibility issues, and stringent data privacy constraints. In this work, we
introduce an Agentic AI framework that automates the entire clinical data
pipeline, from ingestion to inference, through a system of modular,
task-specific agents. These agents handle both structured and unstructured
data, enabling automatic feature selection, model selection, and preprocessing
recommendation without manual intervention. We evaluate the system on publicly
available datasets from geriatrics, palliative care, and colonoscopy imaging.
For example, in the case of structured data (anxiety data) and unstructured
data (colonoscopy polyps data), the pipeline begins with file-type detection by
the Ingestion Identifier Agent, followed by the Data Anonymizer Agent ensuring
privacy compliance, where we first identify the data type and then anonymize
it. The Feature Extraction Agent identifies features using an embedding-based
approach for tabular data, extracting all column names, and a multi-stage
MedGemma-based approach for image data, which infers modality and disease name.
These features guide the Model-Data Feature Matcher Agent in selecting the
best-fit model from a curated repository. The Preprocessing Recommender Agent
and Preprocessing Implementor Agent then apply tailored preprocessing based on
data type and model requirements. Finally, the ``Model Inference Agent" runs
the selected model on the uploaded data and generates interpretable outputs
using tools like SHAP, LIME, and DETR attention maps. By automating these
high-friction stages of the ML lifecycle, the proposed framework reduces the
need for repeated expert intervention, offering a scalable, cost-efficient
pathway for operationalizing AI in clinical environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, 2 tables, BIBM conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nonconvex <span class="highlight-title">Optimization</span> Framework for Group-Sparse Feedback
  Linear-Quadratic Optimal <span class="highlight-title">Control</span> I: Penalty Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lechen Feng, Xun Li, Yuan-Hua Ni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper develops a unified nonconvex optimization framework for the design
of group-sparse feedback controllers in infinite-horizon linear-quadratic (LQ)
problems. We address two prominent extensions of the classical LQ problem: the
distributed LQ problem with fixed communication topology (DFT-LQ) and the
sparse feedback LQ problem (SF-LQ), both of which are motivated by the need for
scalable and structure-aware control in large-scale systems. Unlike existing
approaches that rely on convex relaxations or are limited to block-diagonal
structures, we directly formulate the controller synthesis as a
finite-dimensional nonconvex optimization problem with group $\ell_0$-norm
regularization, capturing general sparsity patterns. We establish a connection
between DFT-LQ and SF-LQ problems, showing that both can be addressed within
our unified framework. Furthermore, we propose a penalty-based proximal
alternating linearized minimization (PALM) algorithm and provide a rigorous
convergence analysis under mild assumptions, overcoming the lack of coercivity
in the objective function. The proposed method admits efficient solvers for all
subproblems and guarantees global convergence to critical points. Our results
fill a key gap in the literature by enabling the direct design of group-sparse
feedback gains with theoretical guarantees, without resorting to convex
surrogates or restrictive structural assumptions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Policy Disruption in <span class="highlight-title">Reinforcement</span> Learning:Adversarial Attack with
  Large Language Models and Critical State Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyong Jiang, Buwei Tian, Chenxing Xu, Songze Li, Lu Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has achieved remarkable success in fields like
robotics and autonomous driving, but adversarial attacks designed to mislead RL
systems remain challenging. Existing approaches often rely on modifying the
environment or policy, limiting their practicality. This paper proposes an
adversarial attack method in which existing agents in the environment guide the
target policy to output suboptimal actions without altering the environment. We
propose a reward iteration optimization framework that leverages large language
models (LLMs) to generate adversarial rewards explicitly tailored to the
vulnerabilities of the target agent, thereby enhancing the effectiveness of
inducing the target agent toward suboptimal decision-making. Additionally, a
critical state identification algorithm is designed to pinpoint the target
agent's most vulnerable states, where suboptimal behavior from the victim leads
to significant degradation in overall performance. Experimental results in
diverse environments demonstrate the superiority of our method over existing
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Percentile-Based Deep <span class="highlight-title">Reinforcement</span> Learning and Reward Based
  Personalization For Delay Aware RAN Slicing in O-RAN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18111v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18111v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peyman Tehrani, Anas Alsoliman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we tackle the challenge of radio access network (RAN) slicing
within an open RAN (O-RAN) architecture. Our focus centers on a network that
includes multiple mobile virtual network operators (MVNOs) competing for
physical resource blocks (PRBs) with the goal of meeting probabilistic delay
upper bound constraints for their clients while minimizing PRB utilization.
Initially, we derive a reward function based on the law of large numbers (LLN),
then implement practical modifications to adapt it for real-world experimental
scenarios. We then propose our solution, the Percentile-based Delay-Aware Deep
Reinforcement Learning (PDA-DRL), which demonstrates its superiority over
several baselines, including DRL models optimized for average delay
constraints, by achieving a 38\% reduction in resultant average delay.
Furthermore, we delve into the issue of model weight sharing among multiple
MVNOs to develop a robust personalized model. We introduce a reward-based
personalization method where each agent prioritizes other agents' model weights
based on their performance. This technique surpasses traditional aggregation
methods, such as federated averaging, and strategies reliant on traffic
patterns and model weight distance similarities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A New Pair of GloVes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riley Carlson, John Bauer, Christopher D. Manning
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report documents, describes, and evaluates new 2024 English GloVe
(Global Vectors for Word Representation) models. While the original GloVe
models built in 2014 have been widely used and found useful, languages and the
world continue to evolve and we thought that current usage could benefit from
updated models. Moreover, the 2014 models were not carefully documented as to
the exact data versions and preprocessing that were used, and we rectify this
by documenting these new models. We trained two sets of word embeddings using
Wikipedia, Gigaword, and a subset of Dolma. Evaluation through vocabulary
comparison, direct testing, and NER tasks shows that the 2024 vectors
incorporate new culturally and linguistically relevant words, perform
comparably on structural tasks like analogy and similarity, and demonstrate
improved performance on recent, temporally dependent NER datasets such as
non-Western newswire data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparison of <span class="highlight-title">Segmentation</span> Methods in Remote Sensing for Land Use Land
  Cover 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naman Srivastava, Joel D Joy, Yash Dixit, Swarup E, Rakshit Ramesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Land Use Land Cover (LULC) mapping is essential for urban and resource
planning, and is one of the key elements in developing smart and sustainable
cities.This study evaluates advanced LULC mapping techniques, focusing on
Look-Up Table (LUT)-based Atmospheric Correction applied to Cartosat
Multispectral (MX) sensor images, followed by supervised and semi-supervised
learning models for LULC prediction. We explore DeeplabV3+ and Cross-Pseudo
Supervision (CPS). The CPS model is further refined with dynamic weighting,
enhancing pseudo-label reliability during training. This comprehensive approach
analyses the accuracy and utility of LULC mapping techniques for various urban
planning applications. A case study of Hyderabad, India, illustrates
significant land use changes due to rapid urbanization. By analyzing Cartosat
MX images over time, we highlight shifts such as urban sprawl, shrinking green
spaces, and expanding industrial areas. This demonstrates the practical utility
of these techniques for urban planners and policymakers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning from Hard Labels with Additional Supervision on
  Non-Hard-Labeled Classes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18098v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18098v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kosuke Sugiyama, Masato Uchida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In scenarios where training data is limited due to observation costs or data
scarcity, enriching the label information associated with each instance becomes
crucial for building high-accuracy classification models. In such contexts, it
is often feasible to obtain not only hard labels but also {\it additional
supervision}, such as the confidences for the hard labels. This setting
naturally raises fundamental questions: {\it What kinds of additional
supervision are intrinsically beneficial?} And {\it how do they contribute to
improved generalization performance?} To address these questions, we propose a
theoretical framework that treats both hard labels and additional supervision
as probability distributions, and constructs soft labels through their affine
combination. Our theoretical analysis reveals that the essential component of
additional supervision is not the confidence score of the assigned hard label,
but rather the information of the distribution over the non-hard-labeled
classes. Moreover, we demonstrate that the additional supervision and the
mixing coefficient contribute to the refinement of soft labels in complementary
roles. Intuitively, in the probability simplex, the additional supervision
determines the direction in which the deterministic distribution representing
the hard label should be adjusted toward the true label distribution, while the
mixing coefficient controls the step size along that direction. Through
generalization error analysis, we theoretically characterize how the additional
supervision and its mixing coefficient affect both the convergence rate and
asymptotic value of the error bound. Finally, we experimentally demonstrate
that, based on our theory, designing additional supervision can lead to
improved classification accuracy, even when utilized in a simple manner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Squeeze10-LLM: Squeezing LLMs' Weights by 10 Times via a Staged
  Mixed-Precision Quantization Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingcheng Zhu, Yangyang Ren, Linlin Yang, Mingbao Lin, Yanjing Li, Sheng Xu, Zichao Feng, Haodong Zhu, Yuguang Yang, Juan Zhang, Runqi Wang, Baochang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deploying large language models (LLMs) is challenging due to their massive
parameters and high computational costs. Ultra low-bit quantization can
significantly reduce storage and accelerate inference, but extreme compression
(i.e., mean bit-width <= 2) often leads to severe performance degradation. To
address this, we propose Squeeze10-LLM, effectively "squeezing" 16-bit LLMs'
weights by 10 times. Specifically, Squeeze10-LLM is a staged mixed-precision
post-training quantization (PTQ) framework and achieves an average of 1.6 bits
per weight by quantizing 80% of the weights to 1 bit and 20% to 4 bits. We
introduce Squeeze10LLM with two key innovations: Post-Binarization Activation
Robustness (PBAR) and Full Information Activation Supervision (FIAS). PBAR is a
refined weight significance metric that accounts for the impact of quantization
on activations, improving accuracy in low-bit settings. FIAS is a strategy that
preserves full activation information during quantization to mitigate
cumulative error propagation across layers. Experiments on LLaMA and LLaMA2
show that Squeeze10-LLM achieves state-of-the-art performance for sub-2bit
weight-only quantization, improving average accuracy from 43% to 56% on six
zero-shot classification tasks--a significant boost over existing PTQ methods.
Our code will be released upon publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ C-AAE: Compressively Anonymizing Autoencoders for Privacy-Preserving
  Activity Recognition in Healthcare Sensor Streams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18072v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18072v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryusei Fujimoto, Yugo Nakamura, Yutaka Arakawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wearable accelerometers and gyroscopes encode fine-grained behavioural
signatures that can be exploited to re-identify users, making privacy
protection essential for healthcare applications. We introduce C-AAE, a
compressive anonymizing autoencoder that marries an Anonymizing AutoEncoder
(AAE) with Adaptive Differential Pulse-Code Modulation (ADPCM). The AAE first
projects raw sensor windows into a latent space that retains activity-relevant
features while suppressing identity cues. ADPCM then differentially encodes
this latent stream, further masking residual identity information and shrinking
the bitrate. Experiments on the MotionSense and PAMAP2 datasets show that C-AAE
cuts user re-identification F1 scores by 10-15 percentage points relative to
AAE alone, while keeping activity-recognition F1 within 5 percentage points of
the unprotected baseline. ADPCM also reduces data volume by roughly 75 %,
easing transmission and storage overheads. These results demonstrate that C-AAE
offers a practical route to balancing privacy and utility in continuous,
sensor-based activity recognition for healthcare.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Group Sequence Policy <span class="highlight-title">Optimization</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, Junyang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Group Sequence Policy Optimization (GSPO), our stable,
efficient, and performant reinforcement learning algorithm for training large
language models. Unlike previous algorithms that adopt token-level importance
ratios, GSPO defines the importance ratio based on sequence likelihood and
performs sequence-level clipping, rewarding, and optimization. We demonstrate
that GSPO achieves superior training efficiency and performance compared to the
GRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and
has the potential for simplifying the design of RL infrastructure. These merits
of GSPO have contributed to the remarkable improvements in the latest Qwen3
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiscale Neural PDE Surrogates for <span class="highlight-title">Prediction</span> and Downscaling:
  Application to Ocean Currents <span class="chip">ICML2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdessamad El-Kabid, Loubna Benabbou, Redouane Lguensat, Alex Hernández-García
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate modeling of physical systems governed by partial differential
equations is a central challenge in scientific computing. In oceanography,
high-resolution current data are critical for coastal management, environmental
monitoring, and maritime safety. However, available satellite products, such as
Copernicus data for sea water velocity at ~0.08 degrees spatial resolution and
global ocean models, often lack the spatial granularity required for detailed
local analyses. In this work, we (a) introduce a supervised deep learning
framework based on neural operators for solving PDEs and providing arbitrary
resolution solutions, and (b) propose downscaling models with an application to
Copernicus ocean current data. Additionally, our method can model surrogate
PDEs and predict solutions at arbitrary resolution, regardless of the input
resolution. We evaluated our model on real-world Copernicus ocean current data
and synthetic Navier-Stokes simulation datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Workshop @ ICML2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy-Preserving Synthetic <span class="highlight-title">Review</span> Generation with Diverse Writing
  Styles Using LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tevin Atwal, Chan Nam Tieu, Yefeng Yuan, Zhan Shi, Yuhong Liu, Liang Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing use of synthetic data generated by Large Language Models
(LLMs) presents both opportunities and challenges in data-driven applications.
While synthetic data provides a cost-effective, scalable alternative to
real-world data to facilitate model training, its diversity and privacy risks
remain underexplored. Focusing on text-based synthetic data, we propose a
comprehensive set of metrics to quantitatively assess the diversity (i.e.,
linguistic expression, sentiment, and user perspective), and privacy (i.e.,
re-identification risk and stylistic outliers) of synthetic datasets generated
by several state-of-the-art LLMs. Experiment results reveal significant
limitations in LLMs' capabilities in generating diverse and privacy-preserving
synthetic data. Guided by the evaluation results, a prompt-based approach is
proposed to enhance the diversity of synthetic reviews while preserving
reviewer privacy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViGText: Deepfake Image <span class="highlight-title">Detection</span> with Vision-Language Model
  Explanations and <span class="highlight-title">Graph</span> Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18031v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18031v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad ALBarqawi, Mahmoud Nazzal, Issa Khalil, Abdallah Khreishah, NhatHai Phan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid rise of deepfake technology, which produces realistic but
fraudulent digital content, threatens the authenticity of media. Traditional
deepfake detection approaches often struggle with sophisticated, customized
deepfakes, especially in terms of generalization and robustness against
malicious attacks. This paper introduces ViGText, a novel approach that
integrates images with Vision Large Language Model (VLLM) Text explanations
within a Graph-based framework to improve deepfake detection. The novelty of
ViGText lies in its integration of detailed explanations with visual data, as
it provides a more context-aware analysis than captions, which often lack
specificity and fail to reveal subtle inconsistencies. ViGText systematically
divides images into patches, constructs image and text graphs, and integrates
them for analysis using Graph Neural Networks (GNNs) to identify deepfakes.
Through the use of multi-level feature extraction across spatial and frequency
domains, ViGText captures details that enhance its robustness and accuracy to
detect sophisticated deepfakes. Extensive experiments demonstrate that ViGText
significantly enhances generalization and achieves a notable performance boost
when it detects user-customized deepfakes. Specifically, average F1 scores rise
from 72.45% to 98.32% under generalization evaluation, and reflects the model's
superior ability to generalize to unseen, fine-tuned variations of stable
diffusion models. As for robustness, ViGText achieves an increase of 11.1% in
recall compared to other deepfake detection approaches. When facing targeted
attacks that exploit its graph-based architecture, ViGText limits
classification performance degradation to less than 4%. ViGText uses detailed
visual and textual analysis to set a new standard for detecting deepfakes,
helping ensure media authenticity and information integrity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does visualization help AI understand data? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18022v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18022v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victoria R. Li, Johnathan Sun, Martin Wattenberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Charts and graphs help people analyze data, but can they also be useful to AI
systems? To investigate this question, we perform a series of experiments with
two commercial vision-language models: GPT 4.1 and Claude 3.5. Across three
representative analysis tasks, the two systems describe synthetic datasets more
precisely and accurately when raw data is accompanied by a scatterplot,
especially as datasets grow in complexity. Comparison with two baselines --
providing a blank chart and a chart with mismatched data -- shows that the
improved performance is due to the content of the charts. Our results are
initial evidence that AI systems, like humans, can benefit from visualization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zeroth-order log-concave sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18021v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18021v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunbum Kook
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the zeroth-order query complexity of log-concave sampling,
specifically uniform sampling from convex bodies using membership oracles. We
propose a simple variant of the proximal sampler that achieves the query
complexity with matched R\'enyi orders between the initial warmness and output
guarantee. Specifically, for any $\varepsilon>0$ and $q\geq2$, the sampler,
initialized at $\pi_{0}$, outputs a sample whose law is $\varepsilon$-close in
$q$-R\'enyi divergence to $\pi$, the uniform distribution over a convex body in
$\mathbb{R}^{d}$, using
$\widetilde{O}(qM_{q}^{q/(q-1)}d^{2}\,\lVert\operatorname{cov}\pi\rVert\log\frac{1}{\varepsilon})$
membership queries, where
$M_{q}=\lVert\text{d}\pi_{0}/\text{d}\pi\rVert_{L^{q}(\pi)}$.
  We further introduce a simple annealing scheme that produces a warm start in
$q$-R\'enyi divergence (i.e., $M_{q}=O(1)$) using
$\widetilde{O}(qd^{2}R^{3/2}\,\lVert\operatorname{cov}\pi\rVert^{1/4})$
queries, where $R^{2}=\mathbb{E}_{\pi}[|\cdot|^{2}]$. This interpolates between
known complexities for warm-start generation in total variation and
R\'enyi-infinity divergence. To relay a R\'enyi warmness across the annealing
scheme, we establish hypercontractivity under simultaneous heat flow and
translate it into an improved mixing guarantee for the proximal sampler under a
logarithmic Sobolev inequality. These results extend naturally to general
log-concave distributions accessible via evaluation oracles, incurring
additional quadratic queries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predictive Scaling Laws for Efficient GRPO Training of Large Reasoning
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.18014v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.18014v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Datta Nimmaturi, Vaishnavi Bhargava, Rajat Ghosh, Johnu George, Debojyoti Dutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large language models (LLMs) for reasoning tasks using
reinforcement learning methods like Group Relative Policy Optimization (GRPO)
is computationally expensive. To address this, we propose a predictive
framework that models training dynamics and helps optimize resource usage.
Through experiments on Llama and Qwen models (3B 8B), we derive an empirical
scaling law based on model size, initial performance, and training progress.
This law predicts reward trajectories and identifies three consistent training
phases: slow start, rapid improvement, and plateau. We find that training
beyond certain number of an epoch offers little gain, suggesting earlier
stopping can significantly reduce compute without sacrificing performance. Our
approach generalizes across model types, providing a practical guide for
efficient GRPO-based fine-tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pseudo-Labeling for Kernel Ridge Regression under Covariate Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10160v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10160v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaizheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop and analyze a principled approach to kernel ridge regression under
covariate shift. The goal is to learn a regression function with small mean
squared error over a target distribution, based on unlabeled data from there
and labeled data that may have a different feature distribution. We propose to
split the labeled data into two subsets, and conduct kernel ridge regression on
them separately to obtain a collection of candidate models and an imputation
model. We use the latter to fill the missing labels and then select the best
candidate accordingly. Our non-asymptotic excess risk bounds demonstrate that
our estimator adapts effectively to both the structure of the target
distribution and the covariate shift. This adaptation is quantified through a
notion of effective sample size that reflects the value of labeled source data
for the target regression task. Our estimator achieves the minimax optimal
error rate up to a polylogarithmic factor, and we find that using pseudo-labels
for model selection does not significantly hinder performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dif<span class="highlight-title">fusion</span> Beats Autoregressive in Data-Constrained Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15857v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15857v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihir Prabhudesai, Menging Wu, Amir Zadeh, Katerina Fragkiadaki, Deepak Pathak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive (AR) models have long dominated the landscape of large
language models, driving progress across a wide range of tasks. Recently,
diffusion-based language models have emerged as a promising alternative, though
their advantages over AR models remain underexplored. In this paper, we
systematically study masked diffusion models in data-constrained settings-where
training involves repeated passes over limited data-and find that they
significantly outperform AR models when compute is abundant but data is scarce.
Diffusion models make better use of repeated data, achieving lower validation
loss and superior downstream performance. We interpret this advantage as
implicit data augmentation: masked diffusion exposes the model to a diverse
distribution of token orderings and prediction tasks, unlike AR's fixed
left-to-right factorization. We find new scaling laws for diffusion models and
derive a closed-form expression for the critical compute threshold at which
diffusion begins to outperform AR. These results suggest that when data, not
compute, is the bottleneck, diffusion models offer a compelling alternative to
the standard AR paradigm. Our code is available at:
https://diffusion-scaling.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Webpage: https://diffusion-scaling.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BEARCUBS: A <span class="highlight-title">benchmark</span> for computer-using web agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.07919v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.07919v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixiao Song, Katherine Thai, Chau Minh Pham, Yapei Chang, Mazin Nadaf, Mohit Iyyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern web agents possess computer use abilities that allow them to interact
with webpages by sending commands to a virtual keyboard and mouse. While such
agents have considerable potential to assist human users with complex tasks,
evaluating their capabilities in real-world settings poses a major challenge.
To this end, we introduce BEARCUBS, a "smallbut mighty" benchmark of 111
information-seeking questions designed to evaluate a web agent's ability to
search, browse, and identify factual information from the web. Unlike prior web
agent benchmarks, solving BEARCUBS requires (1) accessing live web content
rather than synthetic or simulated pages, which captures the unpredictability
of real-world web interactions; and (2) performing a broad range of multimodal
interactions (e.g., video understanding, 3D navigation) that cannot be bypassed
via text-based workarounds. Each question in BEARCUBS has a corresponding
short, unambiguous answer and a human-validated browsing trajectory, allowing
for transparent evaluation of agent performance and strategies. A human study
confirms that BEARCUBS questions are solvable but non-trivial (84.7% human
accuracy), revealing domain knowledge gaps and overlooked details as common
failure points. We find that ChatGPT Agent significantly outperforms other
computer-using agents with an overall accuracy of 65.8% (compared to e.g.,
Operator's 23.4%), showcasing substantial progress in tasks involving real
computer use, such as playing web games and navigating 3D environments.
Nevertheless, closing the gap to human performance requires improvements in
areas like fine control, complex data filtering, and execution speed. To
facilitate future research, BEARCUBS will be updated periodically to replace
invalid or contaminated questions, keeping the benchmark fresh for future
generations of web agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Euclid: An Illustrated Guide to Modern Machine Learning with
  <span class="highlight-title">Geometric</span>, Topological, and Algebraic Structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09468v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09468v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathilde Papillon, Sophia Sanborn, Johan Mathe, Louisa Cornelis, Abby Bertics, Domas Buracas, Hansen J Lillemark, Christian Shewmake, Fatih Dinc, Xavier Pennec, Nina Miolane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The enduring legacy of Euclidean geometry underpins classical machine
learning, which, for decades, has been primarily developed for data lying in
Euclidean space. Yet, modern machine learning increasingly encounters richly
structured data that is inherently nonEuclidean. This data can exhibit
intricate geometric, topological and algebraic structure: from the geometry of
the curvature of space-time, to topologically complex interactions between
neurons in the brain, to the algebraic transformations describing symmetries of
physical systems. Extracting knowledge from such non-Euclidean data
necessitates a broader mathematical perspective. Echoing the 19th-century
revolutions that gave rise to non-Euclidean geometry, an emerging line of
research is redefining modern machine learning with non-Euclidean structures.
Its goal: generalizing classical methods to unconventional data types with
geometry, topology, and algebra. In this review, we provide an accessible
gateway to this fast-growing field and propose a graphical taxonomy that
integrates recent advances into an intuitive unified framework. We subsequently
extract insights into current challenges and highlight exciting opportunities
for future development in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16870v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16870v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Anshumann, Mohd Abbas Zaidi, Akhil Kedia, Jinwoo Ahn, Taehwak Kwon, Kangwook Lee, Haejun Lee, Joohyung Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation can be a cost-effective technique to distill knowledge
in Large Language Models, if the teacher output logits can be pre-computed and
cached. However, successfully applying this to pre-training remains largely
unexplored. In this work, we prove that naive approaches for sparse knowledge
distillation such as caching Top-K probabilities, while intuitive, provide
biased estimates of teacher probability distribution to the student, resulting
in suboptimal performance and calibration. We propose an
importance-sampling-based method `Random Sampling Knowledge Distillation',
which provides unbiased estimates, preserves the gradient in expectation, and
requires storing significantly sparser logits. Our method enables faster
training of student models with marginal overhead (<10%) compared to
cross-entropy based training, while maintaining competitive performance
compared to full distillation, across a range of model sizes from 300M to 3B.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as Oral paper at ACL 2025. Source code is available at
  https://github.com/akhilkedia/RandomSamplingKD . Anshumann, Mohd Abbas Zaidi
  and Akhil Kedia have Equal Contribution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Private Counterfactual Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13812v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13812v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Nomeir, Pasan Dissanayake, Shreya Meel, Sanghamitra Dutta, Sennur Ulukus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transparency and explainability are two extremely important aspects to be
considered when employing black-box machine learning models in high-stake
applications. Providing counterfactual explanations is one way of fulfilling
this requirement. However, this also poses a threat to the privacy of both the
institution that is providing the explanation as well as the user who is
requesting it. In this work, we propose multiple schemes inspired by private
information retrieval (PIR) techniques which ensure the \emph{user's privacy}
when retrieving counterfactual explanations. We present a scheme which
retrieves the \emph{exact} nearest neighbor counterfactual explanation from a
database of accepted points while achieving perfect (information-theoretic)
privacy for the user. While the scheme achieves perfect privacy for the user,
some leakage on the database is inevitable which we quantify using a mutual
information based metric. Furthermore, we propose strategies to reduce this
leakage to achieve an advanced degree of database privacy. We extend these
schemes to incorporate user's preference on transforming their attributes, so
that a more actionable explanation can be received. Since our schemes rely on
finite field arithmetic, we empirically validate our schemes on real datasets
to understand the trade-off between the accuracy and the finite field sizes.
Finally, we present numerical results to support our theoretical findings, and
compare the database leakage of the proposed schemes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Convergence of Gradient Descent on Learning Transformers with
  Residual Connections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05249v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05249v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Qin, Jinxin Zhou, Zhihui Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer models have emerged as fundamental tools across various
scientific and engineering disciplines, owing to their outstanding performance
in diverse applications. Despite this empirical success, the theoretical
foundations of Transformers remain relatively underdeveloped, particularly in
understanding their training dynamics. Existing research predominantly examines
isolated components--such as self-attention mechanisms and feedforward
networks--without thoroughly investigating the interdependencies between these
components, especially when residual connections are present. In this paper, we
aim to bridge this gap by analyzing the convergence behavior of a structurally
complete yet single-layer Transformer, comprising self-attention, a feedforward
network, and residual connections. We demonstrate that, under appropriate
initialization, gradient descent exhibits a linear convergence rate, where the
convergence speed is determined by the minimum and maximum singular values of
the output matrix from the attention layer. Moreover, our analysis reveals that
residual connections serve to ameliorate the ill-conditioning of this output
matrix, an issue stemming from the low-rank structure imposed by the softmax
operation, thereby promoting enhanced optimization stability. We also extend
our theoretical findings to a multi-layer Transformer architecture, confirming
the linear convergence rate of gradient descent under suitable initialization.
Empirical results corroborate our theoretical insights, illustrating the
beneficial role of residual connections in promoting convergence stability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Agentar-Fin-R1: Enhancing Financial Intelligence through Domain
  Expertise, Training Efficiency, and Advanced Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16802v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16802v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanjun Zheng, Xiyang Du, Longfei Liao, Xiaoke Zhao, Zhaowen Zhou, Jingze Song, Bo Zhang, Jiawei Liu, Xiang Qi, Zhe Li, Zhiqiang Zhang, Wei Wang, Peng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) exhibit considerable promise in financial
applications; however, prevailing models frequently demonstrate limitations
when confronted with scenarios that necessitate sophisticated reasoning
capabilities, stringent trustworthiness criteria, and efficient adaptation to
domain-specific requirements. We introduce the Agentar-Fin-R1 series of
financial large language models (8B and 32B parameters), specifically
engineered based on the Qwen3 foundation model to enhance reasoning
capabilities, reliability, and domain specialization for financial
applications. Our optimization approach integrates a high-quality, systematic
financial task label system with a comprehensive multi-layered trustworthiness
assurance framework. This framework encompasses high-quality trustworthy
knowledge engineering, multi-agent trustworthy data synthesis, and rigorous
data validation governance. Through label-guided automated difficulty-aware
optimization, tow-stage training pipeline, and dynamic attribution systems, we
achieve substantial improvements in training efficiency. Our models undergo
comprehensive evaluation on mainstream financial benchmarks including Fineva,
FinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500
and GPQA-diamond. To thoroughly assess real-world deployment capabilities, we
innovatively propose the Finova evaluation benchmark, which focuses on
agent-level financial reasoning and compliance verification. Experimental
results demonstrate that Agentar-Fin-R1 not only achieves state-of-the-art
performance on financial tasks but also exhibits exceptional general reasoning
capabilities, validating its effectiveness as a trustworthy solution for
high-stakes financial applications. The Finova bench is available at
https://github.com/antgroup/Finova.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Omni-Thinker: Scaling Cross-Domain Generalization in LLMs via Multi-Task
  RL with Hybrid Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Derek Li, Jiaming Zhou, Amirreza Kazemi, Qianyi Sun, Abbas Ghaddar, Mohammad Ali Alomrani, Liheng Ma, Yu Luo, Dong Li, Feng Wen, Jianye Hao, Mark Coates, Yingxue Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of general-purpose artificial intelligence relies on large
language models (LLMs) that excel across a wide range of tasks, from structured
reasoning to creative generation. However, post-training methods like
Supervised Fine-Tuning (SFT) often struggle with generalization, favoring
memorization over transferable learning. In this work, we introduce
Omni-Thinker, a unified reinforcement learning (RL) framework that enhances LLM
performance across diverse tasks by combining rule-based verifiable rewards
with generative preference signals via LLM-as-a-Judge evaluations. Our approach
enables consistent optimization across task types and scales RL-based training
to subjective domains. We further investigate training strategies,
demonstrating that a curriculum-based progression that orders tasks from
structured to open-ended improves performance and reduces forgetting.
Experimental results across four domains reveal that curriculum learning
improves performance by 5.2% over joint training and 9.1% over model merging.
These results highlight the importance of task-aware sampling and hybrid
supervision in scaling RL-based post-training for general-purpose LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are
  Important 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.04704v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.04704v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manlai Liang, JiaMing Zhang, Xiong Li, Jinlong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing size of the Key-Value (KV) cache during the Large Language
Models long-context inference is the main obstacle for its balance between the
deployment cost and task accuracy. To reduce the KV cache size in such
scenarios, most previous efforts leveraged on the attention weight to evict
non-critical cache tokens. But there is a trade-off in those methods, they
usually require major modification of the inference infrastructure and
significant computation overhead. Based on the fact that the Large Language
models are autoregressive models, we propose LagKV, a KV compression strategy
only relying on straight forward comparison among KV themselves. It is a
totally attention free method which offers easy integration to the main stream
inference platform and comparable performance comparing to other complicated KV
compression methods. Results on RULER benchmark show that, our approach
outperforms SnapKV and StreamingLLM in different compression ratios. Especially
in the 64-digit passkey retrieval task, our method outperforms the attention
weight based method $H_2O$ over $50\%$ with same compression ratios. Our code
is available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zeroth-Order Fine-Tuning of LLMs in Random Subspaces <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08989v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08989v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziming Yu, Pan Zhou, Sike Wang, Jia Li, Mi Tian, Hua Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning Large Language Models (LLMs) has proven effective for a variety
of downstream tasks. However, as LLMs grow in size, the memory demands for
backpropagation become increasingly prohibitive. Zeroth-order (ZO) optimization
methods offer a memory-efficient alternative by using forward passes to
estimate gradients, but the variance of gradient estimates typically scales
linearly with the model's parameter dimension$\unicode{x2013}$a significant
issue for LLMs. In this paper, we propose the random Subspace Zeroth-order
(SubZero) optimization to address the challenges posed by LLMs' high
dimensionality. We introduce a low-rank perturbation tailored for LLMs that
significantly reduces memory consumption while improving training performance.
Additionally, we prove that our gradient estimation closely approximates the
backpropagation gradient, exhibits lower variance than traditional ZO methods,
and ensures convergence when combined with SGD. Experimental results show that
SubZero enhances fine-tuning performance and achieves faster convergence
compared to standard ZO approaches like MeZO across various language modeling
tasks. Code is available at https://github.com/zimingyy/SubZero.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025 camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Gentle <span class="highlight-title">Grasp</span>ing Using Vision, Sound, and Touch <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.07926v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.07926v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ken Nakahara, Roberto Calandra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In our daily life, we often encounter objects that are fragile and can be
damaged by excessive grasping force, such as fruits. For these objects, it is
paramount to grasp gently -- not using the maximum amount of force possible,
but rather the minimum amount of force necessary. This paper proposes using
visual, tactile, and auditory signals to learn to grasp and regrasp objects
stably and gently. Specifically, we use audio signals as an indicator of
gentleness during the grasping, and then train an end-to-end action-conditional
model from raw visuo-tactile inputs that predicts both the stability and the
gentleness of future grasping candidates, thus allowing the selection and
execution of the most promising action. Experimental results on a
multi-fingered hand over 1,500 grasping trials demonstrated that our model is
useful for gentle grasping by validating the predictive performance (3.27%
higher accuracy than the vision-only variant) and providing interpretations of
their behavior. Finally, real-world experiments confirmed that the grasping
performance with the trained multi-modal model outperformed other baselines
(17% higher rate for stable and gentle grasps than vision-only). Our approach
requires neither tactile sensor calibration nor analytical force modeling,
drastically reducing the engineering effort to grasp fragile objects. Dataset
and videos are available at https://lasr.org/research/gentle-grasping.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages. Accepted by 2025 IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ External Knowledge Injection for CLIP-Based Class-Incremental Learning <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.08510v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.08510v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Da-Wei Zhou, Kai-Wen Li, Jingyi Ning, Han-Jia Ye, Lijun Zhang, De-Chuan Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-Incremental Learning (CIL) enables learning systems to continuously
adapt to evolving data streams. With the advancement of pre-training,
leveraging pre-trained vision-language models (e.g., CLIP) offers a promising
starting point for CIL. However, CLIP makes decisions by matching visual
embeddings to class names, overlooking the rich contextual information conveyed
through language. For instance, the concept of ``cat'' can be decomposed into
features like tail, fur, and face for recognition. Besides, since the model is
continually updated, these detailed features are overwritten in CIL, requiring
external knowledge for compensation. In this paper, we introduce ExterNal
knowledGe INjEction (ENGINE) for CLIP-based CIL. To enhance knowledge transfer
from outside the dataset, we propose a dual-branch injection tuning framework
that encodes informative knowledge from both visual and textual modalities. The
visual branch is enhanced with data augmentation to enrich the visual features,
while the textual branch leverages GPT-4 to rewrite discriminative descriptors.
In addition to this on-the-fly knowledge injection, we also implement
post-tuning knowledge by re-ranking the prediction results during inference.
With the injected knowledge, the model can better capture informative features
for downstream tasks as data evolves. Extensive experiments demonstrate the
state-of-the-art performance of ENGINE. Code is available at:
https://github.com/LAMDA-CL/ICCV25-ENGINE
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV 2025. Code is available at:
  https://github.com/LAMDA-CL/ICCV25-ENGINE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Diffuse and Disperse: Image Generation with Representation
  Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09027v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09027v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runqian Wang, <span class="highlight-author">Kaiming He</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of diffusion-based generative models over the past decade has
largely proceeded independently of progress in representation learning. These
diffusion models typically rely on regression-based objectives and generally
lack explicit regularization. In this work, we propose \textit{Dispersive
Loss}, a simple plug-and-play regularizer that effectively improves
diffusion-based generative models. Our loss function encourages internal
representations to disperse in the hidden space, analogous to contrastive
self-supervised learning, with the key distinction that it requires no positive
sample pairs and therefore does not interfere with the sampling process used
for regression. Compared to the recent method of representation alignment
(REPA), our approach is self-contained and minimalist, requiring no
pre-training, no additional parameters, and no external data. We evaluate
Dispersive Loss on the ImageNet dataset across a range of models and report
consistent improvements over widely used and strong baselines. We hope our work
will help bridge the gap between generative modeling and representation
learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are AI-Generated Fixes Secure? Analyzing LLM and Agent Patches on
  SWE-bench 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02976v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02976v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirali Sajadi, Kostadin Damevski, Preetha Chatterjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) and their agentic frameworks are increasingly
adopted to automate software development tasks such as issue resolution and
program repair. While prior work has identified security risks in LLM-generated
code, most evaluations have focused on synthetic or isolated settings, leaving
open questions about the security of these systems in real-world development
contexts. In this study, we present the first large-scale security analysis of
LLM-generated patches using 20,000+ issues from the SWE-bench dataset. We
evaluate patches produced by a standalone LLM (Llama 3.3) and compare them to
developer-written patches. We also assess the security of patches generated by
three top-performing agentic frameworks (OpenHands, AutoCodeRover, HoneyComb)
on a subset of our data. Finally, we analyze a wide range of code, issue, and
project-level factors to understand the conditions under which LLMs and agents
are most likely to generate insecure code. Our findings reveal that the
standalone LLM introduces nearly 9x more new vulnerabilities than developers,
with many of these exhibiting unique patterns not found in developers' code.
Agentic workflows also generate a significant number of vulnerabilities,
particularly when granting LLMs more autonomy, potentially increasing the
likelihood of misinterpreting project context or task requirements. We find
that vulnerabilities are more likely to occur in LLM patches associated with a
higher number of files, more lines of generated code, and GitHub issues that
lack specific code snippets or information about the expected code behavior and
steps to reproduce. These results suggest that contextual factors play a
critical role in the security of the generated code and point toward the need
for proactive risk assessment methods that account for both code and
issue-level information to complement existing vulnerability detection tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Transport Regularized Divergences: Application to Adversarial
  <span class="highlight-title">Robust</span>ness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.03791v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.03791v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremiah Birrell, Reza Ebrahimi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new class of optimal-transport-regularized divergences, $D^c$,
constructed via an infimal convolution between an information divergence, $D$,
and an optimal-transport (OT) cost, $C$, and study their use in
distributionally robust optimization (DRO). In particular, we propose the
$ARMOR_D$ methods as novel approaches to enhancing the adversarial robustness
of deep learning models. These DRO-based methods are defined by minimizing the
maximum expected loss over a $D^c$-neighborhood of the empirical distribution
of the training data. Viewed as a tool for constructing adversarial samples,
our method allows samples to be both transported, according to the OT cost, and
re-weighted, according to the information divergence; the addition of a
principled and dynamical adversarial re-weighting on top of adversarial sample
transport is a key innovation of $ARMOR_D$. $ARMOR_D$ can be viewed as a
generalization of the best-performing loss functions and OT costs in the
adversarial training literature; we demonstrate this flexibility by using
$ARMOR_D$ to augment the UDR, TRADES, and MART methods and obtain improved
performance on CIFAR-10 and CIFAR-100 image recognition. Specifically,
augmenting with $ARMOR_D$ leads to 1.9\% and 2.1\% improvement against
AutoAttack, a powerful ensemble of adversarial attacks, on CIFAR-10 and
CIFAR-100 respectively. To foster reproducibility, we made the code accessible
at https://github.com/star-ailab/ARMOR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GCC-Spam: Spam <span class="highlight-title">Detection</span> via GAN, Contrastive Learning, and Character
  Similarity Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14679v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14679v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijie Wang, Zixin Xu, Zhiyuan Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exponential growth of spam text on the Internet necessitates robust
detection mechanisms to mitigate risks such as information leakage and social
instability. This work addresses two principal challenges: adversarial
strategies employed by spammers and the scarcity of labeled data. We propose a
novel spam-text detection framework GCC-Spam, which integrates three core
innovations. First, a character similarity network captures orthographic and
phonetic features to counter character-obfuscation attacks and furthermore
produces sentence embeddings for downstream classification. Second, contrastive
learning enhances discriminability by optimizing the latent-space distance
between spam and normal texts. Third, a Generative Adversarial Network (GAN)
generates realistic pseudo-spam samples to alleviate data scarcity while
improving model robustness and classification accuracy. Extensive experiments
on real-world datasets demonstrate that our model outperforms baseline
approaches, achieving higher detection rates with significantly fewer labeled
examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Robust</span> sensitivity <span class="highlight-title">control</span> in digital pathology via tile score
  distribution matching <span class="chip">MICCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20144v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20144v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arthur Pignet, John Klein, Genevieve Robin, Antoine Olivier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deploying digital pathology models across medical centers is challenging due
to distribution shifts. Recent advances in domain generalization improve model
transferability in terms of aggregated performance measured by the Area Under
Curve (AUC). However, clinical regulations often require to control the
transferability of other metrics, such as prescribed sensitivity levels. We
introduce a novel approach to control the sensitivity of whole slide image
(WSI) classification models, based on optimal transport and Multiple Instance
Learning (MIL). Validated across multiple cohorts and tasks, our method enables
robust sensitivity control with only a handful of calibration samples,
providing a practical solution for reliable deployment of computational
pathology systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera ready version. Accepted at MICCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Adaptive Prompting for Compositional Zero-Shot Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20292v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20292v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyle Stein, Arash Mahyari, Guillermo Francia, Eman El-Sheikh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) have demonstrated impressive multimodal
capabilities in learning joint representations of visual and textual data,
making them powerful tools for tasks such as Compositional Zero-Shot Learning
(CZSL). CZSL requires models to generalize to novel combinations of visual
primitives--such as attributes and objects--that were not explicitly
encountered during training. Recent works in prompting for CZSL have focused on
modifying inputs for the text encoder, often using static prompts that do not
change across varying visual contexts. However, these approaches struggle to
fully capture varying visual contexts, as they focus on text adaptation rather
than leveraging visual features for compositional reasoning. To address this,
we propose a Visual Adaptive Prompting System (VAPS) that leverages a learnable
visual prompt repository and similarity-based retrieval mechanism within the
framework of VLMs to bridge the gap between semantic and visual features. Our
method introduces a dynamic visual prompt repository mechanism that selects the
most relevant attribute and object prompts based on the visual features of the
image. Our proposed system includes a visual prompt adapter that encourages the
model to learn a more generalizable embedding space. Experiments on three CZSL
benchmarks, across both closed and open-world scenarios, demonstrate
state-of-the-art results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Transfer Learning-Based Method for Water Body <span class="highlight-title">Segmentation</span> in Remote
  Sensing Imagery: A Case Study of the Zhada Tulin Area 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.10084v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.10084v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Chen, Xin Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Tibetan Plateau, known as the Asian Water Tower, faces significant water
security challenges due to its high sensitivity to climate change. Advancing
Earth observation for sustainable water monitoring is thus essential for
building climate resilience in this region. This study proposes a two-stage
transfer learning strategy using the SegFormer model to overcome domain shift
and data scarcit--key barriers in developing robust AI for climate-sensitive
applications. After pre-training on a diverse source domain, our model was
fine-tuned for the arid Zhada Tulin area. Experimental results show a
substantial performance boost: the Intersection over Union (IoU) for water body
segmentation surged from 25.50% (direct transfer) to 64.84%. This AI-driven
accuracy is crucial for disaster risk reduction, particularly in monitoring
flash flood-prone systems. More importantly, the high-precision map reveals a
highly concentrated spatial distribution of water, with over 80% of the water
area confined to less than 20% of the river channel length. This quantitative
finding provides crucial evidence for understanding hydrological processes and
designing targeted water management and climate adaptation strategies. Our work
thus demonstrates an effective technical solution for monitoring arid plateau
regions and contributes to advancing AI-powered Earth observation for disaster
preparedness in critical transboundary river headwaters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sublinear Regret for a Class of Continuous-Time Linear-Quadratic
  <span class="highlight-title">Reinforcement</span> Learning Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17226v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17226v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilie Huang, Yanwei Jia, Xun Yu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study reinforcement learning (RL) for a class of continuous-time
linear-quadratic (LQ) control problems for diffusions, where states are
scalar-valued and running control rewards are absent but volatilities of the
state processes depend on both state and control variables. We apply a
model-free approach that relies neither on knowledge of model parameters nor on
their estimations, and devise an RL algorithm to learn the optimal policy
parameter directly. Our main contributions include the introduction of an
exploration schedule and a regret analysis of the proposed algorithm. We
provide the convergence rate of the policy parameter to the optimal one, and
prove that the algorithm achieves a regret bound of $O(N^{\frac{3}{4}})$ up to
a logarithmic factor, where $N$ is the number of learning episodes. We conduct
a simulation study to validate the theoretical results and demonstrate the
effectiveness and reliability of the proposed algorithm. We also perform
numerical comparisons between our method and those of the recent model-based
stochastic LQ RL studies adapted to the state- and control-dependent volatility
setting, demonstrating a better performance of the former in terms of regret
bounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 4 figures. Accepted for publication in SIAM Journal on
  Control and Optimization (2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Masked Autoencoders that Feel the Heart: Unveiling Simplicity Bias for
  ECG Analyses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.22495v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.22495v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He-Yang Xu, Hongxiang Gao, Yuwen Li, Xiu-Shen Wei, Chengyu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The diagnostic value of electrocardiogram (ECG) lies in its dynamic
characteristics, ranging from rhythm fluctuations to subtle waveform
deformations that evolve across time and frequency domains. However, supervised
ECG models tend to overfit dominant and repetitive patterns, overlooking
fine-grained but clinically critical cues, a phenomenon known as Simplicity
Bias (SB), where models favor easily learnable signals over subtle but
informative ones. In this work, we first empirically demonstrate the presence
of SB in ECG analyses and its negative impact on diagnostic performance, while
simultaneously discovering that self-supervised learning (SSL) can alleviate
it, providing a promising direction for tackling the bias. Following the SSL
paradigm, we propose a novel method comprising two key components: 1)
Temporal-Frequency aware Filters to capture temporal-frequency features
reflecting the dynamic characteristics of ECG signals, and 2) building on this,
Multi-Grained Prototype Reconstruction for coarse and fine representation
learning across dual domains, further mitigating SB. To advance SSL in ECG
analyses, we curate a large-scale multi-site ECG dataset with 1.53 million
recordings from over 300 clinical centers. Experiments on three downstream
tasks across six ECG datasets demonstrate that our method effectively reduces
SB and achieves state-of-the-art performance. Code and dataset will be released
publicly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>there are factual errors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Preference Lambda-weighted Listwise DPO for Small-Scale Model
  Alignment <span class="chip">AAAI 2026</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19780v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19780v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhui Sun, Xiyao Wang, Zixi Li, Zhenlong Yuan, Jinman Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) demonstrate strong generalization across a wide
range of language tasks, but often generate outputs that misalign with human
preferences. Reinforcement Learning from Human Feedback (RLHF) addresses this
by optimizing models toward human preferences using a learned reward function
and reinforcement learning, yielding improved alignment but suffering from high
computational cost and instability. Direct Preference Optimization (DPO)
simplifies the process by treating alignment as a classification task over
binary preference pairs, reducing training overhead while achieving competitive
performance. However, it assumes fixed, single-dimensional preferences and only
supports pairwise supervision.
  To address these limitations, we propose Multi-Preference Lambda-weighted
Listwise DPO, which allows the model to learn from more detailed human feedback
and flexibly balance multiple goals such as helpfulness, honesty, and fluency.
Our method models full-ranked preference distributions rather than binary
comparisons, enabling more informative learning signals. The lambda vector
controls the relative importance of different alignment goals, allowing the
model to generalize across diverse human objectives. During inference, lambda
can be adjusted without retraining, providing controllable alignment behavior
for downstream use. We also introduce a learned scheduler that dynamically
samples performant lambda configurations to improve robustness.
  Notably, our method requires only 20GB of GPU memory for training, making it
suitable for compute-constrained settings such as academic labs, educational
tools, or on-device assistants. Experiments on 1B-2B scale models show that our
method consistently outperforms standard DPO on alignment benchmarks while
enabling efficient, controllable, and fine-grained adaptation suitable for
real-world deployment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 12 figures, appendix included. To appear in Proceedings of
  AAAI 2026. Code:
  https://github.com/yuhui15/Multi-Preference-Lambda-weighted-DPO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DualXDA: Towards Sparse, Efficient and Explainable Data Attribution in
  Large AI Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12118v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12118v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Galip Ümit Yolcu, Moritz Weckbecker, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models achieve remarkable performance, yet their
decision-making processes often remain opaque. In response, the field of
eXplainable Artificial Intelligence (XAI) has grown significantly over the last
decade, primarily focusing on feature attribution methods. Complementing this
perspective, Data Attribution (DA) has emerged as a promising paradigm that
shifts the focus from features to data provenance. However, existing DA
approaches suffer from prohibitively high computational costs and memory
demands. Additionally, current attribution methods exhibit low sparsity,
hindering the discovery of decisive patterns in the data. We introduce DualXDA,
a framework for sparse, efficient and explainable DA, comprised of two
interlinked approaches for Dual Data Attribution (DualDA) and eXplainable Data
Attribution (XDA): With DualDA, we propose efficient and effective DA,
leveraging Support Vector Machine theory to provide fast and naturally sparse
data attributions for AI predictions. We demonstrate that DualDA achieves high
attribution quality, excels at solving a series of evaluated downstream tasks,
while at the same time improving explanation time by a factor of up to
4,100,000$\times$ compared to the original Influence Functions method, and up
to 11,000$\times$ compared to the method's most efficient approximation from
literature. We further introduce XDA, a method for enhancing Data Attribution
with capabilities from feature attribution methods to explain why training
samples are relevant for the prediction of a test sample in terms of impactful
features. Taken together, our contributions in DualXDA ultimately point towards
a future of eXplainable AI applied at unprecedented scale, enabling
transparent, efficient and novel analysis of even the largest neural
architectures fostering a new generation of accountable AI systems. Code at
https://github.com/gumityolcu/DualXDA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PLOT-TAL: Prompt Learning with Optimal Transport for Few-Shot Temporal
  Action <span class="highlight-title">Localization</span> <span class="chip">ICCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18915v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18915v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward Fish, Andrew Gilbert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot temporal action localization (TAL) methods that adapt large models
via single-prompt tuning often fail to produce precise temporal boundaries.
This stems from the model learning a non-discriminative mean representation of
an action from sparse data, which compromises generalization. We address this
by proposing a new paradigm based on multi-prompt ensembles, where a set of
diverse, learnable prompts for each action is encouraged to specialize on
compositional sub-events. To enforce this specialization, we introduce
PLOT-TAL, a framework that leverages Optimal Transport (OT) to find a globally
optimal alignment between the prompt ensemble and the video's temporal
features. Our method establishes a new state-of-the-art on the challenging
few-shot benchmarks of THUMOS'14 and EPIC-Kitchens, without requiring complex
meta-learning. The significant performance gains, particularly at high IoU
thresholds, validate our hypothesis and demonstrate the superiority of learning
distributed, compositional representations for precise temporal localization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCVWS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EarthLink: A Self-Evolving AI Agent for Climate Science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17311v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17311v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijie Guo, Jiong Wang, Xiaoyu Yue, Wangxu Wei, Zhe Jiang, Wanghan Xu, Ben Fei, Wenlong Zhang, Xinyu Gu, Lijing Cheng, Jing-Jia Luo, Chao Li, Yaqiang Wang, Tao Chen, Wanli Ouyang, Fenghua Ling, Lei Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern Earth science is at an inflection point. The vast, fragmented, and
complex nature of Earth system data, coupled with increasingly sophisticated
analytical demands, creates a significant bottleneck for rapid scientific
discovery. Here we introduce EarthLink, the first AI agent designed as an
interactive copilot for Earth scientists. It automates the end-to-end research
workflow, from planning and code generation to multi-scenario analysis. Unlike
static diagnostic tools, EarthLink can learn from user interaction,
continuously refining its capabilities through a dynamic feedback loop. We
validated its performance on a number of core scientific tasks of climate
change, ranging from model-observation comparisons to the diagnosis of complex
phenomena. In a multi-expert evaluation, EarthLink produced scientifically
sound analyses and demonstrated an analytical competency that was rated as
comparable to specific aspects of a human junior researcher's workflow.
Additionally, its transparent, auditable workflows and natural language
interface empower scientists to shift from laborious manual execution to
strategic oversight and hypothesis generation. EarthLink marks a pivotal step
towards an efficient, trustworthy, and collaborative paradigm for Earth system
research in an era of accelerating global change. The system is accessible at
our website https://earthlink.intern-ai.org.cn.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Unsupervised</span> Concept Drift <span class="highlight-title">Detection</span> from Deep Learning Representations
  in Real-time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17813v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17813v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salvatore Greco, Bartolomeo Vacchetti, Daniele Apiletti, Tania Cerquitelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept drift is the phenomenon in which the underlying data distributions
and statistical properties of a target domain change over time, leading to a
degradation in model performance. Consequently, production models require
continuous drift detection monitoring. Most drift detection methods to date are
supervised, relying on ground-truth labels. However, they are inapplicable in
many real-world scenarios, as true labels are often unavailable. Although
recent efforts have proposed unsupervised drift detectors, many lack the
accuracy required for reliable detection or are too computationally intensive
for real-time use in high-dimensional, large-scale production environments.
Moreover, they often fail to characterize or explain drift effectively.
  To address these limitations, we propose \textsc{DriftLens}, an unsupervised
framework for real-time concept drift detection and characterization. Designed
for deep learning classifiers handling unstructured data, \textsc{DriftLens}
leverages distribution distances in deep learning representations to enable
efficient and accurate detection. Additionally, it characterizes drift by
analyzing and explaining its impact on each label. Our evaluation across
classifiers and data-types demonstrates that \textsc{DriftLens} (i) outperforms
previous methods in detecting drift in 15/17 use cases; (ii) runs at least 5
times faster; (iii) produces drift curves that align closely with actual drift
(correlation $\geq\!0.85$); (iv) effectively identifies representative drift
samples as explanations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE Transactions on Knowledge and Data Engineering
  (TKDE)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Faithful, Interpretable Chest X-ray Diagnosis with Anti-Aliased B-cos
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16761v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16761v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcel Kleinmann, Shashank Agnihotri, Margret Keuper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Faithfulness and interpretability are essential for deploying deep neural
networks (DNNs) in safety-critical domains such as medical imaging. B-cos
networks offer a promising solution by replacing standard linear layers with a
weight-input alignment mechanism, producing inherently interpretable,
class-specific explanations without post-hoc methods. While maintaining
diagnostic performance competitive with state-of-the-art DNNs, standard B-cos
models suffer from severe aliasing artifacts in their explanation maps, making
them unsuitable for clinical use where clarity is essential. In this work, we
address these limitations by introducing anti-aliasing strategies using
FLCPooling (FLC) and BlurPool (BP) to significantly improve explanation
quality. Our experiments on chest X-ray datasets demonstrate that the modified
$\text{B-cos}_\text{FLC}$ and $\text{B-cos}_\text{BP}$ preserve strong
predictive performance while providing faithful and artifact-free explanations
suitable for clinical application in multi-class and multi-label settings. Code
available at: GitHub repository (url:
https://github.com/mkleinma/B-cos-medical-paper).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Outcome-Based Online <span class="highlight-title">Reinforcement</span> Learning: Algorithms and Fundamental
  Limits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20268v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20268v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Chen, Zeyu Jia, Alexander Rakhlin, Tengyang Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning with outcome-based feedback faces a fundamental
challenge: when rewards are only observed at trajectory endpoints, how do we
assign credit to the right actions? This paper provides the first comprehensive
analysis of this problem in online RL with general function approximation. We
develop a provably sample-efficient algorithm achieving $\widetilde{O}({C_{\rm
cov} H^3}/{\epsilon^2})$ sample complexity, where $C_{\rm cov}$ is the
coverability coefficient of the underlying MDP. By leveraging general function
approximation, our approach works effectively in large or infinite state spaces
where tabular methods fail, requiring only that value functions and reward
functions can be represented by appropriate function classes. Our results also
characterize when outcome-based feedback is statistically separated from
per-step rewards, revealing an unavoidable exponential separation for certain
MDPs. For deterministic MDPs, we show how to eliminate the completeness
assumption, dramatically simplifying the algorithm. We further extend our
approach to preference-based feedback settings, proving that equivalent
statistical efficiency can be achieved even under more limited information.
Together, these results constitute a theoretical foundation for understanding
the statistical properties of outcome-based reinforcement learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IPCGRL: Language-Instructed <span class="highlight-title">Reinforcement</span> Learning for Procedural Level
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12358v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12358v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        In-Chang Baek, Sung-Hyun Kim, Seo-Young Lee, Dong-Hyeon Kim, Kyung-Joong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has highlighted the significance of natural language in
enhancing the controllability of generative models. While various efforts have
been made to leverage natural language for content generation, research on deep
reinforcement learning (DRL) agents utilizing text-based instructions for
procedural content generation remains limited. In this paper, we propose
IPCGRL, an instruction-based procedural content generation method via
reinforcement learning, which incorporates a sentence embedding model. IPCGRL
fine-tunes task-specific embedding representations to effectively compress
game-level conditions. We evaluate IPCGRL in a two-dimensional level generation
task and compare its performance with a general-purpose embedding method. The
results indicate that IPCGRL achieves up to a 21.4% improvement in
controllability and a 17.2% improvement in generalizability for unseen
instructions. Furthermore, the proposed method extends the modality of
conditional input, enabling a more flexible and expressive interaction
framework for procedural content generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 9 figures, 3 tables, accepted to Conference on Games 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How do language models learn facts? <span class="highlight-title">Dynamic</span>s, curricula and
  hallucinations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21676v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21676v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Zucchet, Jörg Bornschein, Stephanie Chan, Andrew Lampinen, Razvan Pascanu, Soham De
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models accumulate vast knowledge during pre-training, yet the
dynamics governing this acquisition remain poorly understood. This work
investigates the learning dynamics of language models on a synthetic factual
recall task, uncovering three key findings: First, language models learn in
three phases, exhibiting a performance plateau before acquiring precise factual
knowledge. Mechanistically, this plateau coincides with the formation of
attention-based circuits that support recall. Second, the training data
distribution significantly impacts learning dynamics, as imbalanced
distributions lead to shorter plateaus. Finally, hallucinations emerge
simultaneously with knowledge, and integrating new knowledge into the model
through fine-tuning is challenging, as it quickly corrupts its existing
parametric memories. Our results emphasize the importance of data distribution
in knowledge acquisition and suggest novel data scheduling strategies to
accelerate neural network training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 2nd Conference on Language Modeling (2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Residual Prior-driven Frequency-aware Network for Image <span class="highlight-title">Fusion</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06735v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06735v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guan Zheng, Xue Wang, Wenhua Qian, Peng Liu, Runzhuo Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image fusion aims to integrate complementary information across modalities to
generate high-quality fused images, thereby enhancing the performance of
high-level vision tasks. While global spatial modeling mechanisms show
promising results, constructing long-range feature dependencies in the spatial
domain incurs substantial computational costs. Additionally, the absence of
ground-truth exacerbates the difficulty of capturing complementary features
effectively. To tackle these challenges, we propose a Residual Prior-driven
Frequency-aware Network, termed as RPFNet. Specifically, RPFNet employs a
dual-branch feature extraction framework: the Residual Prior Module (RPM)
extracts modality-specific difference information from residual maps, thereby
providing complementary priors for fusion; the Frequency Domain Fusion Module
(FDFM) achieves efficient global feature modeling and integration through
frequency-domain convolution. Additionally, the Cross Promotion Module (CPM)
enhances the synergistic perception of local details and global structures
through bidirectional feature interaction. During training, we incorporate an
auxiliary decoder and saliency structure loss to strengthen the model's
sensitivity to modality-specific differences. Furthermore, a combination of
adaptive weight-based frequency contrastive loss and SSIM loss effectively
constrains the solution space, facilitating the joint capture of local details
and global features while ensuring the retention of complementary information.
Extensive experiments validate the fusion performance of RPFNet, which
effectively integrates discriminative features, enhances texture details and
salient objects, and can effectively facilitate the deployment of the
high-level vision task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging the Structure of Medical Data for Improved Representation
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02987v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02987v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Agostini, Sonia Laguna, Alain Ryser, Samuel Ruiperez-Campillo, Moritz Vandenhirtz, Nicolas Deperrois, Farhad Nooralahzadeh, Michael Krauthammer, Thomas M. Sutter, Julia E. Vogt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building generalizable medical AI systems requires pretraining strategies
that are data-efficient and domain-aware. Unlike internet-scale corpora,
clinical datasets such as MIMIC-CXR offer limited image counts and scarce
annotations, but exhibit rich internal structure through multi-view imaging. We
propose a self-supervised framework that leverages the inherent structure of
medical datasets. Specifically, we treat paired chest X-rays (i.e., frontal and
lateral views) as natural positive pairs, learning to reconstruct each view
from sparse patches while aligning their latent embeddings. Our method requires
no textual supervision and produces informative representations. Evaluated on
MIMIC-CXR, we show strong performance compared to supervised objectives and
baselines being trained without leveraging structure. This work provides a
lightweight, modality-agnostic blueprint for domain-specific pretraining where
data is structured but scarce
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Space Alignment for AI-Native MIMO Semantic Communications <span class="chip">IJCNN 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16680v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16680v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mario Edoardo Pandolfo, Simone Fiorellino, Emilio Calvanese Strinati, Paolo Di Lorenzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic communications focus on prioritizing the understanding of the
meaning behind transmitted data and ensuring the successful completion of tasks
that motivate the exchange of information. However, when devices rely on
different languages, logic, or internal representations, semantic mismatches
may occur, potentially hindering mutual understanding. This paper introduces a
novel approach to addressing latent space misalignment in semantic
communications, exploiting multiple-input multiple-output (MIMO)
communications. Specifically, our method learns a MIMO precoder/decoder pair
that jointly performs latent space compression and semantic channel
equalization, mitigating both semantic mismatches and physical channel
impairments. We explore two solutions: (i) a linear model, optimized by solving
a biconvex optimization problem via the alternating direction method of
multipliers (ADMM); (ii) a neural network-based model, which learns semantic
MIMO precoder/decoder under transmission power budget and complexity
constraints. Numerical results demonstrate the effectiveness of the proposed
approach in a goal-oriented semantic communication scenario, illustrating the
main trade-offs between accuracy, communication burden, and complexity of the
solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proc. of IEEE IJCNN 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Position: An Empirically Grounded Identifiability Theory Will Accelerate
  <span class="highlight-title">Self-Supervised</span> Learning Research <span class="chip">ICML2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.13101v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.13101v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrik Reizinger, Randall Balestriero, David Klindt, Wieland Brendel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-Supervised Learning (SSL) powers many current AI systems. As research
interest and investment grow, the SSL design space continues to expand. The
Platonic view of SSL, following the Platonic Representation Hypothesis (PRH),
suggests that despite different methods and engineering approaches, all
representations converge to the same Platonic ideal. However, this phenomenon
lacks precise theoretical explanation. By synthesizing evidence from
Identifiability Theory (IT), we show that the PRH can emerge in SSL. However,
current IT cannot explain SSL's empirical success. To bridge the gap between
theory and practice, we propose expanding IT into what we term Singular
Identifiability Theory (SITh), a broader theoretical framework encompassing the
entire SSL pipeline. SITh would allow deeper insights into the implicit data
assumptions in SSL and advance the field towards learning more interpretable
and generalizable representations. We highlight three critical directions for
future research: 1) training dynamics and convergence properties of SSL; 2) the
impact of finite samples, batch size, and data diversity; and 3) the role of
inductive biases in architecture, augmentations, initialization schemes, and
optimizers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML2025 camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I-CEE: Tailoring Explanations of Image Classification Models to User
  Expertise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12102v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12102v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Rong, Peizhu Qian, Vaibhav Unhelkar, Enkelejda Kasneci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effectively explaining decisions of black-box machine learning models is
critical to responsible deployment of AI systems that rely on them. Recognizing
their importance, the field of explainable AI (XAI) provides several techniques
to generate these explanations. Yet, there is relatively little emphasis on the
user (the explainee) in this growing body of work and most XAI techniques
generate "one-size-fits-all" explanations. To bridge this gap and achieve a
step closer towards human-centered XAI, we present I-CEE, a framework that
provides Image Classification Explanations tailored to User Expertise. Informed
by existing work, I-CEE explains the decisions of image classification models
by providing the user with an informative subset of training data (i.e.,
example images), corresponding local explanations, and model decisions.
However, unlike prior work, I-CEE models the informativeness of the example
images to depend on user expertise, resulting in different examples for
different users. We posit that by tailoring the example set to user expertise,
I-CEE can better facilitate users' understanding and simulatability of the
model. To evaluate our approach, we conduct detailed experiments in both
simulation and with human participants (N = 100) on multiple datasets.
Experiments with simulated users show that I-CEE improves users' ability to
accurately predict the model's decisions (simulatability) compared to
baselines, providing promising preliminary results. Experiments with human
participants demonstrate that our method significantly improves user
simulatability accuracy, highlighting the importance of human-centered XAI
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GNN-ACLP: <span class="highlight-title">Graph</span> Neural Networks Based Analog Circuit Link <span class="highlight-title">Prediction</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.10240v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.10240v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanyuan Pan, Tiansheng Zhou, Bingtao Ma, Yaqi Wang, Jianxiang Zhao, Zhi Li, Yugui Lin, Pietro Lio, Shuai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Circuit link prediction identifying missing component connections from
incomplete netlists is crucial in analog circuit design automation. However,
existing methods face three main challenges: 1) Insufficient use of topological
patterns in circuit graphs reduces prediction accuracy; 2) Data scarcity due to
the complexity of annotations hinders model generalization; 3) Limited
adaptability to various netlist formats. We propose GNN-ACLP, a graph neural
networks (GNNs) based method featuring three innovations to tackle these
challenges. First, we introduce the SEAL (learning from Subgraphs, Embeddings,
and Attributes for Link prediction) framework and achieve port-level accuracy
in circuit link prediction. Second, we propose Netlist Babel Fish, a netlist
format conversion tool leveraging retrieval-augmented generation (RAG) with a
large language model (LLM) to improve the compatibility of netlist formats.
Finally, we construct SpiceNetlist, a comprehensive dataset that contains 775
annotated circuits across 10 different component classes. Experiments
demonstrate accuracy improvements of 16.08% on SpiceNetlist, 11.38% on
Image2Net, and 16.01% on Masala-CHAI compared to the baseline in intra-dataset
evaluation, while maintaining accuracy from 92.05% to 99.07% in cross-dataset
evaluation, exhibiting robust feature transfer capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and data will be made available on request to the corresponding
  author. V4 Update: Add Future Work; Improve Typesetting</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variational inference for pile-up removal at hadron colliders with
  dif<span class="highlight-title">fusion</span> models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22074v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22074v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malte Algren, Tobias Golling, Christopher Pollard, John Andrew Raine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a novel method for pile-up removal of $pp$
interactions using variational inference with diffusion models, called vipr.
Instead of using classification methods to identify which particles are from
the primary collision, a generative model is trained to predict the
constituents of the hard-scatter particle jets with pile-up removed. This
results in an estimate of the full posterior over hard-scatter jet
constituents, which has not yet been explored in the context of pile-up
removal, yielding a clear advantage over existing methods especially in the
presence of imperfect detector efficiency. We evaluate the performance of vipr
in a sample of jets from simulated $t\bar{t}$ events overlain with pile-up
contamination. vipr outperforms softdrop and has comparable performance to
puppiml in predicting the substructure of the hard-scatter jets over a wide
range of pile-up scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17596v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17596v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej K. Wozniak, Lianhang Liu, Yixi Cai, Patric Jensfelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While end-to-end autonomous driving models show promising results, their
practical deployment is often hindered by large model sizes, a reliance on
expensive LiDAR sensors and computationally intensive BEV feature
representations. This limits their scalability, especially for mass-market
vehicles equipped only with cameras. To address these challenges, we propose
PRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving
architecture operates using only camera data, without explicit BEV
representation and forgoing the need for LiDAR. PRIX leverages a visual feature
extractor coupled with a generative planning head to predict safe trajectories
from raw pixel inputs directly. A core component of our architecture is the
Context-aware Recalibration Transformer (CaRT), a novel module designed to
effectively enhance multi-level visual features for more robust planning. We
demonstrate through comprehensive experiments that PRIX achieves
state-of-the-art performance on the NavSim and nuScenes benchmarks, matching
the capabilities of larger, multimodal diffusion planners while being
significantly more efficient in terms of inference speed and model size, making
it a practical solution for real-world deployment. Our work is open-source and
the code will be at https://maxiuw.github.io/prix.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BEAVER: Building Environments with Assessable Variation for Evaluating
  Multi-Objective <span class="highlight-title">Reinforcement</span> Learning <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07769v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07769v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruohong Liu, Jack Umenberger, Yize Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have seen significant advancements in designing reinforcement
learning (RL)-based agents for building energy management. While individual
success is observed in simulated or controlled environments, the scalability of
RL approaches in terms of efficiency and generalization across building
dynamics and operational scenarios remains an open question. In this work, we
formally characterize the generalization space for the cross-environment,
multi-objective building energy management task, and formulate the
multi-objective contextual RL problem. Such a formulation helps understand the
challenges of transferring learned policies across varied operational contexts
such as climate and heat convection dynamics under multiple control objectives
such as comfort level and energy consumption. We provide a principled way to
parameterize such contextual information in realistic building RL environments,
and construct a novel benchmark to facilitate the evaluation of generalizable
RL algorithms in practical building control tasks. Our results show that
existing multi-objective RL methods are capable of achieving reasonable
trade-offs between conflicting objectives. However, their performance degrades
under certain environment variations, underscoring the importance of
incorporating dynamics-dependent contextual information into the policy
learning process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Workshop on Computational Optimization of Buildings
  (ICML CO-BUILD), 42nd International Conference on Machine Learning (ICML
  2025), Vancouver, Canada</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Alternative Loss Function in <span class="highlight-title">Evaluation</span> of Transformer Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16548v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16548v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Michańków, Paweł Sakowski, Robert Ślepaczuk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proper design and architecture of testing machine learning models,
especially in their application to quantitative finance problems, is crucial.
The most important aspect of this process is selecting an adequate loss
function for training, validation, estimation purposes, and hyperparameter
tuning. Therefore, in this research, through empirical experiments on equity
and cryptocurrency assets, we apply the Mean Absolute Directional Loss (MADL)
function, which is more adequate for optimizing forecast-generating models used
in algorithmic investment strategies. The MADL function results are compared
between Transformer and LSTM models, and we show that in almost every case,
Transformer results are significantly better than those obtained with LSTM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, fixed grammar, typos and minor error in tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SyncMapV2: <span class="highlight-title">Robust</span> and Adaptive <span class="highlight-title">Unsupervised</span> <span class="highlight-title">Segmentation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16297v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16297v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng Zhang, Zikang Wan, Danilo Vasconcellos Vargas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human vision excels at segmenting visual cues without the need for explicit
training, and it remains remarkably robust even as noise severity increases. In
contrast, existing AI algorithms struggle to maintain accuracy under similar
conditions. Here, we present SyncMapV2, the first to solve unsupervised
segmentation with state-of-the-art robustness. SyncMapV2 exhibits a minimal
drop in mIoU, only 0.01%, under digital corruption, compared to a 23.8% drop
observed in SOTA methods. This superior performance extends across various
types of corruption: noise (7.3% vs. 37.7%), weather (7.5% vs. 33.8%), and blur
(7.0% vs. 29.5%). Notably, SyncMapV2 accomplishes this without any robust
training, supervision, or loss functions. It is based on a learning paradigm
that uses self-organizing dynamical equations combined with concepts from
random networks. Moreover, unlike conventional methods that require
re-initialization for each new input, SyncMapV2 adapts online, mimicking the
continuous adaptability of human vision. Thus, we go beyond the accurate and
robust results, and present the first algorithm that can do all the above
online, adapting to input rather than re-initializing. In adaptability tests,
SyncMapV2 demonstrates near-zero performance degradation, which motivates and
fosters a new generation of robust and adaptive intelligence in the near
future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Robust</span> <span class="highlight-title">Multi-View</span> Learning via Representation <span class="highlight-title">Fusion</span> of Sample-Level
  Attention and Alignment of Simulated Perturbation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04151v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04151v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Xu, Na Zhao, Gang Niu, Masashi Sugiyama, Xiaofeng Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, multi-view learning (MVL) has garnered significant attention due to
its ability to fuse discriminative information from multiple views. However,
real-world multi-view datasets are often heterogeneous and imperfect, which
usually causes MVL methods designed for specific combinations of views to lack
application potential and limits their effectiveness. To address this issue, we
propose a novel robust MVL method (namely RML) with simultaneous representation
fusion and alignment. Specifically, we introduce a simple yet effective
multi-view transformer fusion network where we transform heterogeneous
multi-view data into homogeneous word embeddings, and then integrate multiple
views by the sample-level attention mechanism to obtain a fused representation.
Furthermore, we propose a simulated perturbation based multi-view contrastive
learning framework that dynamically generates the noise and unusable
perturbations for simulating imperfect data conditions. The simulated noisy and
unusable data obtain two distinct fused representations, and we utilize
contrastive learning to align them for learning discriminative and robust
representations. Our RML is self-supervised and can also be applied for
downstream tasks as a regularization. In experiments, we employ it in
multi-view unsupervised clustering, noise-label classification, and as a
plug-and-play module for cross-modal hashing retrieval. Extensive comparison
experiments and ablation studies validate RML's effectiveness. Code is
available at https://github.com/SubmissionsIn/RML.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compositional Coordination for Multi-<span class="highlight-title">Robot</span> Teams with Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16068v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16068v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhehui Huang, Guangyao Shi, Yuwei Wu, Vijay Kumar, Gaurav S. Sukhatme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-robot coordination has traditionally relied on a mission-specific and
expert-driven pipeline, where natural language mission descriptions are
manually translated by domain experts into mathematical formulation, algorithm
design, and executable code. This conventional process is labor-intensive,
inaccessible to non-experts, and inflexible to changes in mission requirements.
Here, we propose LAN2CB (Language to Collective Behavior), a novel framework
that leverages large language models (LLMs) to streamline and generalize the
multi-robot coordination pipeline. LAN2CB transforms natural language (NL)
mission descriptions into executable Python code for multi-robot systems
through two core modules: (1) Mission Analysis, which parses mission
descriptions into behavior trees, and (2) Code Generation, which leverages the
behavior tree and a structured knowledge base to generate robot control code.
We further introduce a dataset of natural language mission descriptions to
support development and benchmarking. Experiments in both simulation and
real-world environments demonstrate that LAN2CB enables robust and flexible
multi-robot coordination from natural language, significantly reducing manual
engineering effort and supporting broad generalization across diverse mission
types. Website: https://sites.google.com/view/lan-cb
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Why Do Class-Dependent <span class="highlight-title">Evaluation</span> Effects Occur with Time Series Feature
  Attributions? A Synthetic Data Investigation <span class="chip">ECML-PKDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11790v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11790v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gregor Baer, Isel Grau, Chao Zhang, Pieter Van Gorp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating feature attribution methods represents a critical challenge in
explainable AI (XAI), as researchers typically rely on perturbation-based
metrics when ground truth is unavailable. However, recent work reveals that
these evaluation metrics can show different performance across predicted
classes within the same dataset. These "class-dependent evaluation effects"
raise questions about whether perturbation analysis reliably measures
attribution quality, with direct implications for XAI method development and
evaluation trustworthiness. We investigate under which conditions these
class-dependent effects arise by conducting controlled experiments with
synthetic time series data where ground truth feature locations are known. We
systematically vary feature types and class contrasts across binary
classification tasks, then compare perturbation-based degradation scores with
ground truth-based precision-recall metrics using multiple attribution methods.
Our experiments demonstrate that class-dependent effects emerge with both
evaluation approaches, even in simple scenarios with temporally localized
features, triggered by basic variations in feature amplitude or temporal extent
between classes. Most critically, we find that perturbation-based and ground
truth metrics frequently yield contradictory assessments of attribution quality
across classes, with weak correlations between evaluation approaches. These
findings suggest that researchers should interpret perturbation-based metrics
with care, as they may not always align with whether attributions correctly
identify discriminating features. By showing this disconnect, our work points
toward reconsidering what attribution evaluation actually measures and
developing more rigorous evaluation methods that capture multiple dimensions of
attribution quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at TempXAI Workshop @ ECML-PKDD 2025 (Explainable AI for
  Time Series and Data Streams)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Role of the Time-Dependent Hessian in High-Dimensional <span class="highlight-title">Optimization</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02418v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02418v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tony Bonnaire, Giulio Biroli, Chiara Cammarota
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gradient descent is commonly used to find minima in rough landscapes,
particularly in recent machine learning applications. However, a theoretical
understanding of why good solutions are found remains elusive, especially in
strongly non-convex and high-dimensional settings. Here, we focus on the phase
retrieval problem as a typical example, which has received a lot of attention
recently in theoretical machine learning. We analyze the Hessian during
gradient descent, identify a dynamical transition in its spectral properties,
and relate it to the ability of escaping rough regions in the loss landscape.
When the signal-to-noise ratio (SNR) is large enough, an informative negative
direction exists in the Hessian at the beginning of the descent, i.e in the
initial condition. While descending, a BBP transition in the spectrum takes
place in finite time: the direction is lost, and the dynamics is trapped in a
rugged region filled with marginally stable bad minima. Surprisingly, for
finite system sizes, this window of negative curvature allows the system to
recover the signal well before the theoretical SNR found for infinite sizes,
emphasizing the central role of initialization and early-time dynamics for
efficiently navigating rough landscapes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Low-rank Decomposition: A Shortcut Approach for Efficient
  On-Device Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05086v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05086v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le-Trung Nguyen, Ael Quelennec, Van-Tam Nguyen, Enzo Tartaglione
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  On-device learning has emerged as a promising direction for AI development,
particularly because of its potential to reduce latency issues and mitigate
privacy risks associated with device-server communication, while improving
energy efficiency. Despite these advantages, significant memory and
computational constraints still represent major challenges for its deployment.
Drawing on previous studies on low-rank decomposition methods that address
activation memory bottlenecks in backpropagation, we propose a novel shortcut
approach as an alternative. Our analysis and experiments demonstrate that our
method can reduce activation memory usage, even up to $120.09\times$ compared
to vanilla training, while also reducing overall training FLOPs up to
$1.86\times$ when evaluated on traditional benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A general language model for peptide identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15610v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15610v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jixiu Zhai, Tianchi Lu, Haitian Zhong, Ziyang Xu, Yuhuan Liu, Shengrui Xu, Jingwan Wang, Dan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate identification of bioactive peptides (BPs) and protein
post-translational modifications (PTMs) is essential for understanding protein
function and advancing therapeutic discovery. However, most computational
methods remain limited in their generalizability across diverse peptide
functions. Here, we present PDeepPP, a unified deep learning framework that
integrates pretrained protein language models with a hybrid
transformer-convolutional architecture, enabling robust identification across
diverse peptide classes and PTM sites. We curated comprehensive benchmark
datasets and implemented strategies to address data imbalance, allowing PDeepPP
to systematically extract both global and local sequence features. Through
extensive analyses-including dimensionality reduction and comparison
studies-PDeepPP demonstrates strong, interpretable peptide representations and
achieves state-of-the-art performance in 25 of the 33 biological identification
tasks. Notably, PDeepPP attains high accuracy in antimicrobial (0.9726) and
phosphorylation site (0.9984) identification, with 99.5% specificity in
glycosylation site prediction and substantial reduction in false negatives in
antimalarial tasks. By enabling large-scale, accurate peptide analysis, PDeepPP
supports biomedical research and the discovery of novel therapeutic targets for
disease treatment. All code, datasets, and pretrained models are publicly
available via GitHub:https://github.com/fondress/PDeepPP and Hugging
Face:https://huggingface.co/fondress/PDeppPP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 9 figures, 4 tables, submitted to arXiv</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Statistical Runtime Verification for LLMs via <span class="highlight-title">Robust</span>ness <span class="highlight-title">Estimation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.17723v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.17723v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Natan Levy, Adiel Ashrov, Guy Katz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial robustness verification is essential for ensuring the safe
deployment of Large Language Models (LLMs) in runtime-critical applications.
However, formal verification techniques remain computationally infeasible for
modern LLMs due to their exponential runtime and white-box access requirements.
This paper presents a case study adapting and extending the RoMA statistical
verification framework to assess its feasibility as an online runtime
robustness monitor for LLMs in black-box deployment settings. Our adaptation of
RoMA analyzes confidence score distributions under semantic perturbations to
provide quantitative robustness assessments with statistically validated
bounds. Our empirical validation against formal verification baselines
demonstrates that RoMA achieves comparable accuracy (within 1\% deviation), and
reduces verification times from hours to minutes. We evaluate this framework
across semantic, categorial, and orthographic perturbation domains. Our results
demonstrate RoMA's effectiveness for robustness monitoring in operational LLM
deployments. These findings point to RoMA as a potentially scalable alternative
when formal methods are infeasible, with promising implications for runtime
verification in LLM-based systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SDSC:A Structure-Aware Metric for Semantic Signal Representation
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14516v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14516v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeyoung Lee, Hochul Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose the Signal Dice Similarity Coefficient (SDSC), a structure-aware
metric function for time series self-supervised representation learning. Most
Self-Supervised Learning (SSL) methods for signals commonly adopt
distance-based objectives such as mean squared error (MSE), which are sensitive
to amplitude, invariant to waveform polarity, and unbounded in scale. These
properties hinder semantic alignment and reduce interpretability. SDSC
addresses this by quantifying structural agreement between temporal signals
based on the intersection of signed amplitudes, derived from the Dice
Similarity Coefficient (DSC).Although SDSC is defined as a structure-aware
metric, it can be used as a loss by subtracting from 1 and applying a
differentiable approximation of the Heaviside function for gradient-based
optimization. A hybrid loss formulation is also proposed to combine SDSC with
MSE, improving stability and preserving amplitude where necessary. Experiments
on forecasting and classification benchmarks demonstrate that SDSC-based
pre-training achieves comparable or improved performance over MSE, particularly
in in-domain and low-resource scenarios. The results suggest that structural
fidelity in signal representations enhances the semantic representation
quality, supporting the consideration of structure-aware metrics as viable
alternatives to conventional distance-based methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Robust</span> Non-adaptive Group Testing under Errors in Group Membership
  Specifications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05345v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05345v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuvayan Banerjee, Radhendushka Srivastava, James Saunderson, Ajit Rajwade
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given $p$ samples, each of which may or may not be defective, group testing
(GT) aims to determine their defect status by performing tests on $n < p$
`groups', where a group is formed by mixing a subset of the $p$ samples.
Assuming that the number of defective samples is very small compared to $p$, GT
algorithms have provided excellent recovery of the status of all $p$ samples
with even a small number of groups. Most existing methods, however, assume that
the group memberships are accurately specified. This assumption may not always
be true in all applications, due to various resource constraints. Such errors
could occur, eg, when a technician, preparing the groups in a laboratory,
unknowingly mixes together an incorrect subset of samples as compared to what
was specified. We develop a new GT method, the Debiased Robust Lasso Test
Method (DRLT), that handles such group membership specification errors. The
proposed DRLT method is based on an approach to debias, or reduce the inherent
bias in, estimates produced by Lasso, a popular and effective sparse regression
technique. We also provide theoretical upper bounds on the reconstruction error
produced by our estimator. Our approach is then combined with two carefully
designed hypothesis tests respectively for (i) the identification of defective
samples in the presence of errors in group membership specifications, and (ii)
the identification of groups with erroneous membership specifications. The DRLT
approach extends the literature on bias mitigation of statistical estimators
such as the LASSO, to handle the important case when some of the measurements
contain outliers, due to factors such as group membership specification errors.
We present numerical results which show that our approach outperforms several
baselines and robust regression techniques for identification of defective
samples as well as erroneously specified groups.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DAA*: Deep Angular A Star for Image-based Path <span class="highlight-title">Planning</span> <span class="chip">ICCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.09305v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.09305v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Path smoothness is often overlooked in path imitation learning from expert
demonstrations. In this paper, we introduce a novel learning method, termed
deep angular A* (DAA*), by incorporating the proposed path angular freedom
(PAF) into A* to improve path similarity through adaptive path smoothness. The
PAF aims to explore the effect of move angles on path node expansion by finding
the trade-off between their minimum and maximum values, allowing for high
adaptiveness for imitation learning. DAA* improves path optimality by closely
aligning with the reference path through joint optimization of path shortening
and smoothing, which correspond to heuristic distance and PAF, respectively.
Throughout comprehensive evaluations on 7 datasets, including 4 maze datasets,
2 video-game datasets, and a real-world drone-view dataset containing 2
scenarios, we demonstrate remarkable improvements of our DAA* over neural A* in
path similarity between the predicted and reference paths with a shorter path
length when the shortest path is plausible, improving by 9.0% SPR, 6.9% ASIM,
and 3.9% PSIM. Furthermore, when jointly learning pathfinding with both path
loss and path probability map loss, DAA* significantly outperforms the
state-of-the-art TransPath by 6.3% SPR, 6.0% PSIM, and 3.7% ASIM. We also
discuss the minor trade-off between path optimality and search efficiency where
applicable. Our code and model weights are available at
https://github.com/zwxu064/DAAStar.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Computer Vision (ICCV), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TOC-UCO: a comprehensive repository of tabular ordinal classification
  <span class="highlight-title">dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17348v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17348v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Ayllón-Gavilán, David Guijo-Rubio, Antonio Manuel Gómez-Orellana, Francisco Bérchez-Moreno, Víctor Manuel Vargas-Yun, Pedro A. Gutiérrez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An ordinal classification (OC) problem corresponds to a special type of
classification characterised by the presence of a natural order relationship
among the classes. This type of problem can be found in a number of real-world
applications, motivating the design and development of many ordinal
methodologies over the last years. However, it is important to highlight that
the development of the OC field suffers from one main disadvantage: the lack of
a comprehensive set of datasets on which novel approaches to the literature
have to be benchmarked. In order to approach this objective, this manuscript
from the University of C\'ordoba (UCO), which have previous experience on the
OC field, provides the literature with a publicly available repository of
tabular data for a robust validation of novel OC approaches, namely TOC-UCO
(Tabular Ordinal Classification repository of the UCO). Specifically, this
repository includes a set of $46$ tabular ordinal datasets, preprocessed under
a common framework and ensured to have a reasonable number of patterns and an
appropriate class distribution. We also provide the sources and preprocessing
steps of each dataset, along with details on how to benchmark a novel approach
using the TOC-UCO repository. For this, indices for $30$ different randomised
train-test partitions are provided to facilitate the reproducibility of the
experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 single column pages, 5 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of Deep Learning for Geometry Problem Solving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.11936v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.11936v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianzhe Ma, Wenxuan Wang, Qin Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geometry problem solving is a key area of mathematical reasoning, which is
widely involved in many important fields such as education, mathematical
ability assessment of artificial intelligence, and multimodal ability
assessment. In recent years, the rapid development of deep learning technology,
especially the rise of multimodal large language models, has triggered a
widespread research boom. This paper provides a survey of the applications of
deep learning in geometry problem solving, including (i) a comprehensive
summary of the relevant tasks in geometry problem solving; (ii) a thorough
review of related deep learning methods; (iii) a detailed analysis of
evaluation metrics and methods; and (iv) a critical discussion of the current
challenges and future directions that can be explored. Our goal is to provide a
comprehensive and practical reference of deep learning for geometry problem
solving to promote further developments in this field. We create a continuously
updated list of papers on GitHub: https://github.com/majianz/dl4gps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VCDiag: Classifying Erroneous Waveforms for Failure Triage Acceleration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03590v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03590v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh Luu, Surya Jasper, Khoi Le, Evan Pan, Michael Quinn, Aakash Tyagi, Jiang Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Failure triage in design functional verification is critical but
time-intensive, relying on manual specification reviews, log inspections, and
waveform analyses. While machine learning (ML) has improved areas like stimulus
generation and coverage closure, its application to RTL-level simulation
failure triage, particularly for large designs, remains limited. VCDiag offers
an efficient, adaptable approach using VCD data to classify failing waveforms
and pinpoint likely failure locations. In the largest experiment, VCDiag
achieves over 94% accuracy in identifying the top three most likely modules.
The framework introduces a novel signal selection and statistical compression
approach, achieving over 120x reduction in raw data size while preserving
features essential for classification. It can also be integrated into diverse
Verilog/SystemVerilog designs and testbenches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalizing Adam to Manifolds for Efficiently Training Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16901v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16901v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedikt Brantner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the primary reasons behind the success of neural networks has been the
emergence of an array of new, highly-successful optimizers, perhaps most
importantly the Adam optimizer. It is widely used for training neural networks,
yet notoriously hard to interpret. Lacking a clear physical intuition, Adam is
difficult to generalize to manifolds. Some attempts have been made to directly
apply parts of the Adam algorithm to manifolds or to find an underlying
structure, but a full generalization has remained elusive.
  In this work a new approach is presented that leverages the special structure
of the manifolds which are relevant for optimization of neural networks, such
as the Stiefel manifold, the symplectic Stiefel manifold and the Grassmann
manifold: all of these are homogeneous spaces and as such admit a global
tangent space representation - a common vector space (Lie subspace) in which
all tangent spaces can easily be represented.
  This global tangent space representation is used to perform all of the steps
in the Adam optimizer and we are able to fully generalize the optimizer to
manifolds without a projection step. The resulting algorithm is then applied to
train a transformer for which orthogonality constraints are enforced up to
machine precision and we observe significant speed-ups in the training process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 6 figures (some of which contain subfigures), presented at
  Enumath2023 and Enumath2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Impact of Pseudo-Science in Financial Loans Risk <span class="highlight-title">Prediction</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16182v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16182v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Scarone, Ricardo Baeza-Yates
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the societal impact of pseudo-scientific assumptions for predicting
the behavior of people in a straightforward application of machine learning to
risk prediction in financial lending. This use case also exemplifies the impact
of survival bias in loan return prediction. We analyze the models in terms of
their accuracy and social cost, showing that the socially optimal model may not
imply a significant accuracy loss for this downstream task. Our results are
verified for commonly used learning methods and datasets. Our findings also
show that there is a natural dynamic when training models that suffer survival
bias where accuracy slightly deteriorates, and whose recall and precision
improves with time. These results act as an illusion, leading the observer to
believe that the system is getting better, when in fact the model is suffering
from increasingly more unfairness and survival bias.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Approximation of Stationary Processes using the ARMA Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10610v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10610v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anand Ganesh, Babhrubahan Bose, Anand Rajagopalan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We revisit an old problem related to Autoregressive Moving Average (ARMA)
models, on quantifying and bounding the approximation error between a true
stationary process $X_t$ and an ARMA model $Y_t$. We take the transfer function
representation of an ARMA model and show that the associated $L^{\infty}$ norm
provides a valid alternate norm that controls the $L^2$ norm and has structural
properties comparable to the cepstral norm. We show that a certain subspace of
stationary processes, which includes ARMA models, forms a Banach algebra under
the $L^{\infty}$ norm that respects the group structure of $H^{\infty}$
transfer functions. The natural definition of invertibility in this algebra is
consistent with the original definition of ARMA invertibility, and generalizes
better to non-ARMA processes than Wiener's $\ell^1$ condition. Finally, we
calculate some explicit approximation bounds in the simpler context of
continuous transfer functions, and critique some heuristic ideas on Pad\'e
approximations and parsimonious models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Long-Short Distance <span class="highlight-title">Graph</span> Neural Networks and Improved Curriculum
  Learning for Emotion Recognition in Conversation <span class="chip">ECAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15205v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15205v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinran Li, Xiujuan Xu, Jiaqi Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion Recognition in Conversation (ERC) is a practical and challenging
task. This paper proposes a novel multimodal approach, the Long-Short Distance
Graph Neural Network (LSDGNN). Based on the Directed Acyclic Graph (DAG), it
constructs a long-distance graph neural network and a short-distance graph
neural network to obtain multimodal features of distant and nearby utterances,
respectively. To ensure that long- and short-distance features are as distinct
as possible in representation while enabling mutual influence between the two
modules, we employ a Differential Regularizer and incorporate a BiAffine Module
to facilitate feature interaction. In addition, we propose an Improved
Curriculum Learning (ICL) to address the challenge of data imbalance. By
computing the similarity between different emotions to emphasize the shifts in
similar emotions, we design a "weighted emotional shift" metric and develop a
difficulty measurer, enabling a training process that prioritizes learning easy
samples before harder ones. Experimental results on the IEMOCAP and MELD
datasets demonstrate that our model outperforms existing benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 28th European Conference on Artificial Intelligence
  (ECAI 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM Web <span class="highlight-title">Dynamic</span>s: Tracing Model Collapse in a Network of LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15690v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15690v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Wang, Akira Horiguchi, Lingyou Pang, Carey E. Priebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing use of synthetic data from the public Internet has enhanced
data usage efficiency in large language model (LLM) training. However, the
potential threat of model collapse remains insufficiently explored. Existing
studies primarily examine model collapse in a single model setting or rely
solely on statistical surrogates. In this work, we introduce LLM Web Dynamics
(LWD), an efficient framework for investigating model collapse at the network
level. By simulating the Internet with a retrieval-augmented generation (RAG)
database, we analyze the convergence pattern of model outputs. Furthermore, we
provide theoretical guarantees for this convergence by drawing an analogy to
interacting Gaussian Mixture Models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Principled Approach for Data Bias Mitigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.12312v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.12312v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Scarone, Alfredo Viola, Renée J. Miller, Ricardo Baeza-Yates
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread use of machine learning and data-driven algorithms for
decision making has been steadily increasing over many years. \emph{Bias} in
the data can adversely affect this decision-making. We present a new mitigation
strategy to address data bias. Our methods are explainable and come with
mathematical guarantees of correctness. They can take advantage of new work on
table discovery to find new tuples that can be added to a dataset to create
real datasets that are unbiased or less biased. Our framework covers data with
non-binary labels and with multiple sensitive attributes. Hence, we are able to
measure and mitigate bias that does not appear over a single attribute (or
feature), but only intersectionally, when considering a combination of
attributes. We evaluate our techniques on publicly available datasets and
provide a theoretical analysis of our results, highlighting novel insights into
data bias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AIES 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compliant Residual DAgger: Improving Real-World Contact-Rich
  Manipulation with Human Corrections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.16685v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.16685v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaomeng Xu, Yifan Hou, Zeyi Liu, Shuran Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address key challenges in Dataset Aggregation (DAgger) for real-world
contact-rich manipulation: how to collect informative human correction data and
how to effectively update policies with this new data. We introduce Compliant
Residual DAgger (CR-DAgger), which contains two novel components: 1) a
Compliant Intervention Interface that leverages compliance control, allowing
humans to provide gentle, accurate delta action corrections without
interrupting the ongoing robot policy execution; and 2) a Compliant Residual
Policy formulation that learns from human corrections while incorporating force
feedback and force control. Our system significantly enhances performance on
precise contact-rich manipulation tasks using minimal correction data,
improving base policy success rates by over 50\% on two challenging tasks (book
flipping and belt assembly) while outperforming both retraining-from-scratch
and finetuning approaches. Through extensive real-world experiments, we provide
practical guidance for implementing effective DAgger in real-world robot
learning tasks. Result videos are available at:
https://compliant-residual-dagger.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-Tuned Language Models Generate Stable Inorganic Materials as Text <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04379v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04379v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nate Gruver, Anuroop Sriram, Andrea Madotto, Andrew Gordon Wilson, C. Lawrence Zitnick, Zachary Ulissi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose fine-tuning large language models for generation of stable
materials. While unorthodox, fine-tuning large language models on text-encoded
atomistic data is simple to implement yet reliable, with around 90% of sampled
structures obeying physical constraints on atom positions and charges. Using
energy above hull calculations from both learned ML potentials and
gold-standard DFT calculations, we show that our strongest model (fine-tuned
LLaMA-2 70B) can generate materials predicted to be metastable at about twice
the rate (49% vs 28%) of CDVAE, a competing diffusion model. Because of text
prompting's inherent flexibility, our models can simultaneously be used for
unconditional generation of stable material, infilling of partial structures
and text-conditional generation. Finally, we show that language models' ability
to capture key symmetries of crystal structures improves with model scale,
suggesting that the biases of pretrained LLMs are surprisingly well-suited for
atomistic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024. Code available at:
  https://github.com/facebookresearch/crystal-llm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compressed and distributed least-squares regression: convergence rates
  with applications to Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.01358v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.01358v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Constantin Philippenko, Aymeric Dieuleveut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the impact of compression on stochastic
gradient algorithms for machine learning, a technique widely used in
distributed and federated learning. We underline differences in terms of
convergence rates between several unbiased compression operators, that all
satisfy the same condition on their variance, thus going beyond the classical
worst-case analysis. To do so, we focus on the case of least-squares regression
(LSR) and analyze a general stochastic approximation algorithm for minimizing
quadratic functions relying on a random field. We consider weak assumptions on
the random field, tailored to the analysis (specifically, expected H\"older
regularity), and on the noise covariance, enabling the analysis of various
randomizing mechanisms, including compression. We then extend our results to
the case of federated learning.
  More formally, we highlight the impact on the convergence of the covariance
$\mathfrak{C}_{\mathrm{ania}}$ of the additive noise induced by the algorithm.
We demonstrate despite the non-regularity of the stochastic field, that the
limit variance term scales with $\mathrm{Tr}(\mathfrak{C}_{\mathrm{ania}}
H^{-1})/K$ (where $H$ is the Hessian of the optimization problem and $K$ the
number of iterations) generalizing the rate for the vanilla LSR case where it
is $\sigma^2 \mathrm{Tr}(H H^{-1}) / K = \sigma^2 d / K$ (Bach and Moulines,
2013). Then, we analyze the dependency of $\mathfrak{C}_{\mathrm{ania}}$ on the
compression strategy and ultimately its impact on convergence, first in the
centralized case, then in two heterogeneous FL frameworks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ History-Guided Video Dif<span class="highlight-title">fusion</span> <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06764v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06764v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kiwhan Song, Boyuan Chen, Max Simchowitz, Yilun Du, Russ Tedrake, Vincent Sitzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classifier-free guidance (CFG) is a key technique for improving conditional
generation in diffusion models, enabling more accurate control while enhancing
sample quality. It is natural to extend this technique to video diffusion,
which generates video conditioned on a variable number of context frames,
collectively referred to as history. However, we find two key challenges to
guiding with variable-length history: architectures that only support
fixed-size conditioning, and the empirical observation that CFG-style history
dropout performs poorly. To address this, we propose the Diffusion Forcing
Transformer (DFoT), a video diffusion architecture and theoretically grounded
training objective that jointly enable conditioning on a flexible number of
history frames. We then introduce History Guidance, a family of guidance
methods uniquely enabled by DFoT. We show that its simplest form, vanilla
history guidance, already significantly improves video generation quality and
temporal consistency. A more advanced method, history guidance across time and
frequency further enhances motion dynamics, enables compositional
generalization to out-of-distribution history, and can stably roll out
extremely long videos. Project website: https://boyuan.space/history-guidance
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025. Project website: https://boyuan.space/history-guidance</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BlockDialect: Block-wise Fine-grained Mixed Format Quantization for
  Energy-Efficient LLM Inference <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01144v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01144v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonsuk Jang, Thierry Tambe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapidly increasing size of large language models (LLMs) presents
significant challenges in memory usage and computational costs. Quantizing both
weights and activations can address these issues, with hardware-supported
fine-grained scaling emerging as a promising solution to mitigate outliers.
However, existing methods struggle to capture nuanced block data distributions.
We propose BlockDialect, a block-wise fine-grained mixed format technique that
assigns a per-block optimal number format from a formatbook for better data
representation. Additionally, we introduce DialectFP4, a formatbook of FP4
variants (akin to dialects) that adapt to diverse data distributions. To
leverage this efficiently, we propose a two-stage approach for online
DialectFP4 activation quantization. Importantly, DialectFP4 ensures energy
efficiency by selecting representable values as scaled integers compatible with
low-precision integer arithmetic. BlockDialect achieves 10.78% (7.48%) accuracy
gain on the LLaMA3-8B (LLaMA2-7B) model compared to MXFP4 format with lower bit
usage per data, while being only 5.45% (2.69%) below full precision even when
quantizing full-path matrix multiplication. Focusing on how to represent over
how to scale, our work presents a promising path for energy-efficient LLM
inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fixing the Pitfalls of Probabilistic Time-Series Forecasting <span class="highlight-title">Evaluation</span>
  by Kernel Quadrature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.06079v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.06079v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masaki Adachi, Masahiro Fujisawa, Michael A Osborne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the significance of probabilistic time-series forecasting models,
their evaluation metrics often involve intractable integrations. The most
widely used metric, the continuous ranked probability score (CRPS), is a
strictly proper scoring function; however, its computation requires
approximation. We found that popular CRPS estimators--specifically, the
quantile-based estimator implemented in the widely used GluonTS library and the
probability-weighted moment approximation--both exhibit inherent estimation
biases. These biases lead to crude approximations, resulting in improper
rankings of forecasting model performance when CRPS values are close. To
address this issue, we introduced a kernel quadrature approach that leverages
an unbiased CRPS estimator and employs cubature construction for scalable
computation. Empirically, our approach consistently outperforms the two widely
used CRPS estimators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causally Testing Gender Bias in LLMs: A Case Study on Occupational Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10678v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10678v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuen Chen, Vethavikashini Chithrra Raghuram, Justus Mattern, Rada Mihalcea, Zhijing Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generated texts from large language models (LLMs) have been shown to exhibit
a variety of harmful, human-like biases against various demographics. These
findings motivate research efforts aiming to understand and measure such
effects. This paper introduces a causal formulation for bias measurement in
generative language models. Based on this theoretical foundation, we outline a
list of desiderata for designing robust bias benchmarks. We then propose a
benchmark called OccuGender, with a bias-measuring procedure to investigate
occupational gender bias. We test several state-of-the-art open-source LLMs on
OccuGender, including Llama, Mistral, and their instruction-tuned versions. The
results show that these models exhibit substantial occupational gender bias.
Lastly, we discuss prompting strategies for bias mitigation and an extension of
our causal formulation to illustrate the generalizability of our framework. Our
code and data https://github.com/chenyuen0103/gender-bias.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multi-Faceted <span class="highlight-title">Evaluation</span> Framework for Assessing Synthetic Data
  Generated by Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14445v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14445v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yefeng Yuan, Yuhong Liu, Liang Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancements in generative AI and large language models (LLMs) have
opened up new avenues for producing synthetic data, particularly in the realm
of structured tabular formats, such as product reviews. Despite the potential
benefits, concerns regarding privacy leakage have surfaced, especially when
personal information is utilized in the training datasets. In addition, there
is an absence of a comprehensive evaluation framework capable of quantitatively
measuring the quality of the generated synthetic data and their utility for
downstream tasks. In response to this gap, we introduce SynEval, an open-source
evaluation framework designed to assess the fidelity, utility, and privacy
preservation of synthetically generated tabular data via a suite of diverse
evaluation metrics. We validate the efficacy of our proposed framework -
SynEval - by applying it to synthetic product review data generated by three
state-of-the-art LLMs: ChatGPT, Claude, and Llama. Our experimental findings
illuminate the trade-offs between various evaluation metrics in the context of
synthetic data generation. Furthermore, SynEval stands as a critical instrument
for researchers and practitioners engaged with synthetic tabular data,,
empowering them to judiciously determine the suitability of the generated data
for their specific applications, with an emphasis on upholding user privacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 1 figure, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unisoma: A Unified Transformer-based Solver for Multi-Solid Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06021v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06021v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shilong Tao, Zhe Feng, Haonan Sun, Zhanxing Zhu, Yunhuai Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-solid systems are foundational to a wide range of real-world
applications, yet modeling their complex interactions remains challenging.
Existing deep learning methods predominantly rely on implicit modeling, where
the factors influencing solid deformation are not explicitly represented but
are instead indirectly learned. However, as the number of solids increases,
these methods struggle to accurately capture intricate physical interactions.
In this paper, we introduce a novel explicit modeling paradigm that
incorporates factors influencing solid deformation through structured modules.
Specifically, we present Unisoma, a unified and flexible Transformer-based
model capable of handling variable numbers of solids. Unisoma directly captures
physical interactions using contact modules and adaptive interaction allocation
mechanism, and learns the deformation through a triplet relationship. Compared
to implicit modeling techniques, explicit modeling is more well-suited for
multi-solid systems with diverse coupling patterns, as it enables detailed
treatment of each solid while preventing information blending and confusion.
Experimentally, Unisoma achieves consistent state-of-the-art performance across
seven well-established datasets and two complex multi-solid tasks. Code is
avaiable at https://github.com/therontau0054/Unisoma.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 42nd International Conference on Machine Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI Workflow, External Validation, and Development in Eye Disease
  Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15087v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15087v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyu Chen, Tiarnan D L Keenan, Elvira Agron, Alexis Allot, Emily Guan, Bryant Duong, Amr Elsawy, Benjamin Hou, Cancan Xue, Sanjeeb Bhandari, Geoffrey Broadhead, Chantal Cousineau-Krieger, Ellen Davis, William G Gensheimer, David Grasic, Seema Gupta, Luis Haddock, Eleni Konstantinou, Tania Lamba, Michele Maiberger, Dimosthenis Mantopoulos, Mitul C Mehta, Ayman G Nahri, Mutaz AL-Nawaflh, Arnold Oshinsky, Brittany E Powell, Boonkit Purt, Soo Shin, Hillary Stiefel, Alisa T Thavikulwat, Keith James Wroblewski, Tham Yih Chung, Chui Ming Gemmy Cheung, Ching-Yu Cheng, Emily Y Chew, Michelle R. Hribar, Michael F. Chiang, Zhiyong Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Timely disease diagnosis is challenging due to increasing disease burdens and
limited clinician availability. AI shows promise in diagnosis accuracy but
faces real-world application issues due to insufficient validation in clinical
workflows and diverse populations. This study addresses gaps in medical AI
downstream accountability through a case study on age-related macular
degeneration (AMD) diagnosis and severity classification. We designed and
implemented an AI-assisted diagnostic workflow for AMD, comparing diagnostic
performance with and without AI assistance among 24 clinicians from 12
institutions with real patient data sampled from the Age-Related Eye Disease
Study (AREDS). Additionally, we demonstrated continual enhancement of an
existing AI model by incorporating approximately 40,000 additional medical
images (named AREDS2 dataset). The improved model was then systematically
evaluated using both AREDS and AREDS2 test sets, as well as an external test
set from Singapore. AI assistance markedly enhanced diagnostic accuracy and
classification for 23 out of 24 clinicians, with the average F1-score
increasing by 20% from 37.71 (Manual) to 45.52 (Manual + AI) (P-value <
0.0001), achieving an improvement of over 50% in some cases. In terms of
efficiency, AI assistance reduced diagnostic times for 17 out of the 19
clinicians tracked, with time savings of up to 40%. Furthermore, a model
equipped with continual learning showed robust performance across three
independent datasets, recording a 29% increase in accuracy, and elevating the
F1-score from 42 to 54 in the Singapore population.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in JAMA Network Open,
  doi:10.1001/jamanetworkopen.2025.17204</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active Learning For Repairable Hardware Systems With Partial Coverage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16315v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16315v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Potter, Beyza Kalkanlı, Deniz Erdoğmuş, Michael Everett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying the optimal diagnostic test and hardware system instance to infer
reliability characteristics using field data is challenging, especially when
constrained by fixed budgets and minimal maintenance cycles. Active Learning
(AL) has shown promise for parameter inference with limited data and budget
constraints in machine learning/deep learning tasks. However, AL for
reliability model parameter inference remains underexplored for repairable
hardware systems. It requires specialized AL Acquisition Functions (AFs) that
consider hardware aging and the fact that a hardware system consists of
multiple sub-systems, which may undergo only partial testing during a given
diagnostic test. To address these challenges, we propose a relaxed Mixed
Integer Semidefinite Program (MISDP) AL AF that incorporates Diagnostic
Coverage (DC), Fisher Information Matrices (FIMs), and diagnostic testing
budgets. Furthermore, we design empirical-based simulation experiments focusing
on two diagnostic testing scenarios: (1) partial tests of a hardware system
with overlapping subsystem coverage, and (2) partial tests where one diagnostic
test fully subsumes the subsystem coverage of another. We evaluate our proposed
approach against the most widely used AL AF in the literature (entropy), as
well as several intuitive AL AFs tailored for reliability model parameter
inference. Our proposed AF ranked best on average among the alternative AFs
across 6,000 experimental configurations, with respect to Area Under the Curve
(AUC) of the Absolute Total Expected Event Error (ATEER) and Mean Squared Error
(MSE) curves, with statistical significance calculated at a 0.05 alpha level
using a Friedman hypothesis test.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Access - Reliability Society</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analyzing I<span class="highlight-title">slam</span>ophobic Discourse Using Semi-Coded Terms and LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18273v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18273v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raza Ul Mustafa, Roi Dupart, Gabrielle Smith, Noman Ashraf, Nathalie Japkowicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Islamophobia has gained significant traction across Western
societies, fueled by the rise of digital communication networks. This paper
performs a large-scale analysis of specialized, semi-coded Islamophobic terms
such as (muzrat, pislam, mudslime, mohammedan, muzzies) floated on extremist
social platforms, i.e., 4Chan, Gab, Telegram, etc. Many of these terms appear
lexically neutral or ambiguous outside of specific contexts, making them
difficult for both human moderators and automated systems to reliably identify
as hate speech. First, we use Large Language Models (LLMs) to show their
ability to understand these terms. Second, Google Perspective API suggests that
Islamophobic posts tend to receive higher toxicity scores than other categories
of hate speech like Antisemitism. Finally, we use BERT topic modeling approach
to extract different topics and Islamophobic discourse on these social
platforms. Our findings indicate that LLMs understand these Out-Of-Vocabulary
(OOV) slurs; however, further improvements in moderation strategies and
algorithmic detection are necessary to address such discourse effectively. Our
topic modeling also indicates that Islamophobic text is found across various
political, conspiratorial, and far-right movements and is particularly directed
against Muslim immigrants. Taken altogether, we performed one of the first
studies on Islamophobic semi-coded terms and shed a global light on
Islamophobia.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-Grained Uncertainty Quantification via Collisions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12127v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12127v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesse Friedbaum, Sudarshan Adiga, Ravi Tandon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new and intuitive metric for aleatoric uncertainty
quantification (UQ), the prevalence of class collisions defined as the same
input being observed in different classes. We use the rate of class collisions
to define the collision matrix, a novel and uniquely fine-grained measure of
uncertainty. For a classification problem involving $K$ classes, the $K\times
K$ collision matrix $S$ measures the inherent difficulty in distinguishing
between each pair of classes. We discuss several applications of the collision
matrix, establish its fundamental mathematical properties, as well as show its
relationship with existing UQ methods, including the Bayes error rate (BER). We
also address the new problem of estimating the collision matrix using one-hot
labeled data by proposing a series of innovative techniques to estimate $S$.
First, we learn a pair-wise contrastive model which accepts two inputs and
determines if they belong to the same class. We then show that this contrastive
model (which is PAC learnable) can be used to estimate the Gramian matrix of
$S$, defined as $G=S^TS$. Finally, we show that under reasonable assumptions,
$G$ can be used to uniquely recover $S$, a new result on non-negative matrices
which could be of independent interest. With a method to estimate $S$
established, we demonstrate how this estimate of $S$, in conjunction with the
contrastive model, can be used to estimate the posterior class portability
distribution of any point. Experimental results are also presented to validate
our methods of estimating the collision matrix and class posterior
distributions on several datasets.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-07-23T00:00:00Z">2025-07-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">52</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rapid Modeling Architecture for Lightweight <span class="highlight-title">Simulator</span> to Accelerate and
  Improve <span class="highlight-title">Decision</span> Making for Industrial Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takumi Kato, Zhi Li Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing industrial systems, such as building, improving, and automating
distribution centers and manufacturing plants, involves critical
decision-making with limited information in the early phases. The lack of
information leads to less accurate designs of the systems, which are often
difficult to resolve later. It is effective to use simulators to model the
designed system and find out the issues early. However, the modeling time
required by conventional simulators is too long to allow for rapid model
creation to meet decision-making demands. In this paper, we propose a Rapid
Modeling Architecture (RMA) for a lightweight industrial simulator that
mitigates the modeling burden while maintaining the essential details in order
to accelerate and improve decision-making. We have prototyped a simulator based
on the RMA and applied it to the actual factory layout design problem. We also
compared the modeling time of our simulator to that of an existing simulator,
and as a result, our simulator achieved a 78.3% reduction in modeling time
compared to conventional simulators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 13 figures. Manuscript accepted at the 2025 IEEE 21st
  International Conference on Automation Science and Engineering (CASE 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Brake Onset <span class="highlight-title">Detection</span> in Naturalistic Driving Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu-Yuan Liu, Johan Engström, Gustav Markkula
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Response timing measures play a crucial role in the assessment of automated
driving systems (ADS) in collision avoidance scenarios, including but not
limited to establishing human benchmarks and comparing ADS to human driver
response performance. For example, measuring the response time (of a human
driver or ADS) to a conflict requires the determination of a stimulus onset and
a response onset. In existing studies, response onset relies on manual
annotation or vehicle control signals such as accelerator and brake pedal
movements. These methods are not applicable when analyzing large scale data
where vehicle control signals are not available. This holds in particular for
the rapidly expanding sets of ADS log data where the behavior of surrounding
road users is observed via onboard sensors. To advance evaluation techniques
for ADS and enable measuring response timing when vehicle control signals are
not available, we developed a simple and efficient algorithm, based on a
piecewise linear acceleration model, to automatically estimate brake onset that
can be applied to any type of driving data that includes vehicle longitudinal
time series data. We also proposed a manual annotation method to identify brake
onset and used it as ground truth for validation. R2 was used as a confidence
metric to measure the accuracy of the algorithm, and its classification
performance was analyzed using naturalistic collision avoidance data of both
ADS and humans, where our method was validated against human manual annotation.
Although our algorithm is subject to certain limitations, it is efficient,
generalizable, applicable to any road user and scenario types, and is highly
configurable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FishDet-M: A Unified Large-Scale <span class="highlight-title">Benchmark</span> for <span class="highlight-title">Robust</span> Fish <span class="highlight-title">Detection</span> and
  CLIP-Guided Model Selection in Diverse Aquatic Visual Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muayad Abujabal, Lyes Saad Saoud, Irfan Hussain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate fish detection in underwater imagery is essential for ecological
monitoring, aquaculture automation, and robotic perception. However, practical
deployment remains limited by fragmented datasets, heterogeneous imaging
conditions, and inconsistent evaluation protocols. To address these gaps, we
present \textit{FishDet-M}, the largest unified benchmark for fish detection,
comprising 13 publicly available datasets spanning diverse aquatic environments
including marine, brackish, occluded, and aquarium scenes. All data are
harmonized using COCO-style annotations with both bounding boxes and
segmentation masks, enabling consistent and scalable cross-domain evaluation.
We systematically benchmark 28 contemporary object detection models, covering
the YOLOv8 to YOLOv12 series, R-CNN based detectors, and DETR based models.
Evaluations are conducted using standard metrics including mAP, mAP@50, and
mAP@75, along with scale-specific analyses (AP$_S$, AP$_M$, AP$_L$) and
inference profiling in terms of latency and parameter count. The results
highlight the varying detection performance across models trained on FishDet-M,
as well as the trade-off between accuracy and efficiency across models of
different architectures. To support adaptive deployment, we introduce a
CLIP-based model selection framework that leverages vision-language alignment
to dynamically identify the most semantically appropriate detector for each
input image. This zero-shot selection strategy achieves high performance
without requiring ensemble computation, offering a scalable solution for
real-time applications. FishDet-M establishes a standardized and reproducible
platform for evaluating object detection in complex aquatic scenes. All
datasets, pretrained models, and evaluation tools are publicly available to
facilitate future research in underwater computer vision and intelligent marine
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Step-by-step Guide on Nonlinear Model Predictive <span class="highlight-title">Control</span> for Safe
  Mobile <span class="highlight-title">Robot</span> <span class="highlight-title">Navigation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Benders, Laura Ferranti, Johannes Köhler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing a Model Predictive Control (MPC) scheme that enables a mobile robot
to safely navigate through an obstacle-filled environment is a complicated yet
essential task in robotics. In this technical report, safety refers to ensuring
that the robot respects state and input constraints while avoiding collisions
with obstacles despite the presence of disturbances and measurement noise. This
report offers a step-by-step approach to implementing Nonlinear Model
Predictive Control (NMPC) schemes addressing these safety requirements.
Numerous books and survey papers provide comprehensive overviews of linear MPC
(LMPC) \cite{bemporad2007robust,kouvaritakis2016model}, NMPC
\cite{rawlings2017model,allgower2004nonlinear,mayne2014model,grune2017nonlinear,saltik2018outlook},
and their applications in various domains, including robotics
\cite{nascimento2018nonholonomic,nguyen2021model,shi2021advanced,wei2022mpc}.
This report does not aim to replicate those exhaustive reviews. Instead, it
focuses specifically on NMPC as a foundation for safe mobile robot navigation.
The goal is to provide a practical and accessible path from theoretical
concepts to mathematical proofs and implementation, emphasizing safety and
performance guarantees. It is intended for researchers, robotics engineers, and
practitioners seeking to bridge the gap between theoretical NMPC formulations
and real-world robotic applications.
  This report is not necessarily meant to remain fixed over time. If someone
finds an error in the presented theory, please reach out via the given email
addresses. We are happy to update the document if necessary.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>51 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PinchBot: Long-Horizon <span class="highlight-title">Deformable</span> Manipulation with Guided Dif<span class="highlight-title">fusion</span>
  Policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alison Bartsch, Arvind Car, Amir Barati Farimani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pottery creation is a complicated art form that requires dexterous, precise
and delicate actions to slowly morph a block of clay to a meaningful, and often
useful 3D goal shape. In this work, we aim to create a robotic system that can
create simple pottery goals with only pinch-based actions. This pinch pottery
task allows us to explore the challenges of a highly multi-modal and
long-horizon deformable manipulation task. To this end, we present PinchBot, a
goal-conditioned diffusion policy model that when combined with pre-trained 3D
point cloud embeddings, task progress prediction and collision-constrained
action projection, is able to successfully create a variety of simple pottery
goals. For experimental videos and access to the demonstration dataset, please
visit our project website:
https://sites.google.com/andrew.cmu.edu/pinchbot/home.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safety Assurance for Quadrotor Kino<span class="highlight-title">dynamic</span> Motion <span class="highlight-title">Planning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theodoros Tavoulareas, Marzia Cescon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous drones have gained considerable attention for applications in
real-world scenarios, such as search and rescue, inspection, and delivery. As
their use becomes ever more pervasive in civilian applications, failure to
ensure safe operation can lead to physical damage to the system, environmental
pollution, and even loss of human life. Recent work has demonstrated that
motion planning techniques effectively generate a collision-free trajectory
during navigation. However, these methods, while creating the motion plans, do
not inherently consider the safe operational region of the system, leading to
potential safety constraints violation during deployment. In this paper, we
propose a method that leverages run time safety assurance in a kinodynamic
motion planning scheme to satisfy the system's operational constraints. First,
we use a sampling-based geometric planner to determine a high-level
collision-free path within a user-defined space. Second, we design a low-level
safety assurance filter to provide safety guarantees to the control input of a
Linear Quadratic Regulator (LQR) designed with the purpose of trajectory
tracking. We demonstrate our proposed approach in a restricted 3D simulation
environment using a model of the Crazyflie 2.0 drone.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at 2025 Modeling, Estimation and Control
  Conference (MECC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Perspective-Invariant 3D Object <span class="highlight-title">Detection</span> <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17665v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17665v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ao Liang, Lingdong Kong, Dongyue Lu, Youquan Liu, Jian Fang, Huaici Zhao, Wei Tsang Ooi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rise of robotics, LiDAR-based 3D object detection has garnered
significant attention in both academia and industry. However, existing datasets
and methods predominantly focus on vehicle-mounted platforms, leaving other
autonomous platforms underexplored. To bridge this gap, we introduce Pi3DET,
the first benchmark featuring LiDAR data and 3D bounding box annotations
collected from multiple platforms: vehicle, quadruped, and drone, thereby
facilitating research in 3D object detection for non-vehicle platforms as well
as cross-platform 3D detection. Based on Pi3DET, we propose a novel
cross-platform adaptation framework that transfers knowledge from the
well-studied vehicle platform to other platforms. This framework achieves
perspective-invariant 3D detection through robust alignment at both geometric
and feature levels. Additionally, we establish a benchmark to evaluate the
resilience and robustness of current 3D detectors in cross-platform scenarios,
providing valuable insights for developing adaptive 3D perception systems.
Extensive experiments validate the effectiveness of our approach on challenging
cross-platform tasks, demonstrating substantial gains over existing adaptation
methods. We hope this work paves the way for generalizable and unified 3D
perception systems across diverse and complex environments. Our Pi3DET dataset,
cross-platform benchmark suite, and annotation toolkit have been made publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025; 46 pages, 18 figures, 22 tables; Project Page at
  https://pi3det.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Talk2Event: Grounded Understanding of <span class="highlight-title">Dynamic</span> Scenes from Event Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingdong Kong, Dongyue Lu, Ao Liang, Rong Li, Yuhao Dong, Tianshuai Hu, Lai Xing Ng, Wei Tsang Ooi, Benoit R. Cottereau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras offer microsecond-level latency and robustness to motion blur,
making them ideal for understanding dynamic environments. Yet, connecting these
asynchronous streams to human language remains an open challenge. We introduce
Talk2Event, the first large-scale benchmark for language-driven object
grounding in event-based perception. Built from real-world driving data, we
provide over 30,000 validated referring expressions, each enriched with four
grounding attributes -- appearance, status, relation to viewer, and relation to
other objects -- bridging spatial, temporal, and relational reasoning. To fully
exploit these cues, we propose EventRefer, an attribute-aware grounding
framework that dynamically fuses multi-attribute representations through a
Mixture of Event-Attribute Experts (MoEE). Our method adapts to different
modalities and scene dynamics, achieving consistent gains over state-of-the-art
baselines in event-only, frame-only, and event-frame fusion settings. We hope
our dataset and approach will establish a foundation for advancing multimodal,
temporally-aware, and language-driven perception in real-world robotics and
autonomy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint; 42 pages, 17 figures, 16 tables; Project Page at
  https://talk2event.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Monocular Semantic Scene Completion via Masked Recurrent Networks <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17661v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17661v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuzhi Wang, Xinran Wu, Song Wang, Lingdong Kong, Ziping Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular Semantic Scene Completion (MSSC) aims to predict the voxel-wise
occupancy and semantic category from a single-view RGB image. Existing methods
adopt a single-stage framework that aims to simultaneously achieve visible
region segmentation and occluded region hallucination, while also being
affected by inaccurate depth estimation. Such methods often achieve suboptimal
performance, especially in complex scenes. We propose a novel two-stage
framework that decomposes MSSC into coarse MSSC followed by the Masked
Recurrent Network. Specifically, we propose the Masked Sparse Gated Recurrent
Unit (MS-GRU) which concentrates on the occupied regions by the proposed mask
updating mechanism, and a sparse GRU design is proposed to reduce the
computation cost. Additionally, we propose the distance attention projection to
reduce projection errors by assigning different attention scores according to
the distance to the observed surface. Experimental results demonstrate that our
proposed unified framework, MonoMRN, effectively supports both indoor and
outdoor scenes and achieves state-of-the-art performance on the NYUv2 and
SemanticKITTI datasets. Furthermore, we conduct robustness analysis under
various disturbances, highlighting the role of the Masked Recurrent Network in
enhancing the model's resilience to such challenges. The source code is
publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025; 15 pages, 10 figures, 6 tables; Code at
  https://github.com/alanWXZ/MonoMRN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Event <span class="highlight-title">Detection</span> for Active Lower Limb Prosthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        J. D. Clark, P. Ellison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate event detection is key to the successful design of semi-passive and
powered prosthetics. Kinematically, the natural knee is complex, with
translation and rotation components that have a substantial impact on gait
characteristics. When simplified to a pin joint, some of this behaviour is
lost. This study investigates the role of cruciate ligament stretch in event
detection. A bicondylar knee design was used, constrained by analogues of the
anterior and posterior cruciate ligaments. This offers the ability to
characterize knee kinematics by the stretch of the ligaments. The ligament
stretch was recorded using LVDTs parallel to the ligaments of the Russell knee
on a bent knee crutch. Which was used to capture data on a treadmill at 3
speeds. This study finds speed dependence within the stretch of the cruciate
ligaments, prominently around 5\% and 80\% of the gait cycle for the posterior
and anterior. The cycle profile remains consistent with speed; therefore, other
static events such as the turning point feature at around 90\% and 95\% of the
cycle, for the posterior and anterior, respectively, could be used as a
predictive precursor for initial contact. Likewise at 90\% and 95\%, another
pair of turning points that in this case could be used to predict foot flat.
This concludes that the use of a bicondylar knee design could improve the
detection of events during the gait cycle, and therefore could increase the
accuracy of subsequent controllers for powered prosthetics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Scan to Action: Leveraging Realistic Scans for <span class="highlight-title">Embodied</span> Scene
  Understanding <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna-Maria Halacheva, Jan-Nico Zaech, Sombit Dey, Luc Van Gool, Danda Pani Paudel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world 3D scene-level scans offer realism and can enable better
real-world generalizability for downstream applications. However, challenges
such as data volume, diverse annotation formats, and tool compatibility limit
their use. This paper demonstrates a methodology to effectively leverage these
scans and their annotations. We propose a unified annotation integration using
USD, with application-specific USD flavors. We identify challenges in utilizing
holistic real-world scan datasets and present mitigation strategies. The
efficacy of our approach is demonstrated through two downstream applications:
LLM-based scene editing, enabling effective LLM understanding and adaptation of
the data (80% success), and robotic simulation, achieving an 87% success rate
in policy learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the OpenSUN3D Workshop, CVPR 2025. This workshop paper is
  not included in the official CVPR proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KernelSOS for <span class="highlight-title">Global</span> Sampling-Based Optimal <span class="highlight-title">Control</span> and <span class="highlight-title">Estimation</span> via
  Semidefinite Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Groudiev, Fabian Schramm, Éloïse Berthier, Justin Carpentier, Frederike Dümbgen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Global optimization has gained attraction over the past decades, thanks to
the development of both theoretical foundations and efficient numerical
routines to cope with optimization problems of various complexities. Among
recent methods, Kernel Sum of Squares (KernelSOS) appears as a powerful
framework, leveraging the potential of sum of squares methods from the
polynomial optimization community with the expressivity of kernel methods
widely used in machine learning. This paper applies the kernel sum of squares
framework for solving control and estimation problems, which exhibit poor local
minima. We demonstrate that KernelSOS performs well on a selection of problems
from both domains. In particular, we show that KernelSOS is competitive with
other sum of squares approaches on estimation problems, while being applicable
to non-polynomial and non-parametric formulations. The sample-based nature of
KernelSOS allows us to apply it to trajectory optimization problems with an
integrated simulator treated as a black box, both as a standalone method and as
a powerful initialization method for local solvers, facilitating the discovery
of better solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Robot</span>-mediated physical Human-Human Interaction in Neurorehabilitation:
  a position paper 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Vianello, Matthew Short, Julia Manczurowsky, Emek Barış Küçüktabak, Francesco Di Tommaso, Alessia Noccaro, Laura Bandini, Shoshana Clark, Alaina Fiorenza, Francesca Lunardini, Alberto Canton, Marta Gandolla, Alessandra L. G. Pedrocchi, Emilia Ambrosini, Manuel Murie-Fernandez, Carmen B. Roman, Jesus Tornero, Natacha Leon, Andrew Sawers, Jim Patton, Domenico Formica, Nevio Luigi Tagliamonte, Georg Rauter, Kilian Baur, Fabian Just, Christopher J. Hasson, Vesna D. Novak, Jose L. Pons
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neurorehabilitation conventionally relies on the interaction between a
patient and a physical therapist. Robotic systems can improve and enrich the
physical feedback provided to patients after neurological injury, but they
under-utilize the adaptability and clinical expertise of trained therapists. In
this position paper, we advocate for a novel approach that integrates the
therapist's clinical expertise and nuanced decision-making with the strength,
accuracy, and repeatability of robotics: Robot-mediated physical Human-Human
Interaction. This framework, which enables two individuals to physically
interact through robotic devices, has been studied across diverse research
groups and has recently emerged as a promising link between conventional manual
therapy and rehabilitation robotics, harmonizing the strengths of both
approaches. This paper presents the rationale of a multidisciplinary
team-including engineers, doctors, and physical therapists-for conducting
research that utilizes: a unified taxonomy to describe robot-mediated
rehabilitation, a framework of interaction based on social psychology, and a
technological approach that makes robotic systems seamless facilitators of
natural human-human interaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When and Where <span class="highlight-title">Localization</span> Fails: An Analysis of the Iterative Closest
  Point in Evolving Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17531v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17531v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdel-Raouf Dannaoui, Johann Laconte, Christophe Debain, Francois Pomerleau, Paul Checchin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust relocalization in dynamic outdoor environments remains a key challenge
for autonomous systems relying on 3D lidar. While long-term localization has
been widely studied, short-term environmental changes, occurring over days or
weeks, remain underexplored despite their practical significance. To address
this gap, we present a highresolution, short-term multi-temporal dataset
collected weekly from February to April 2025 across natural and semi-urban
settings. Each session includes high-density point cloud maps, 360 deg
panoramic images, and trajectory data. Projected lidar scans, derived from the
point cloud maps and modeled with sensor-accurate occlusions, are used to
evaluate alignment accuracy against the ground truth using two Iterative
Closest Point (ICP) variants: Point-to-Point and Point-to-Plane. Results show
that Point-to-Plane offers significantly more stable and accurate registration,
particularly in areas with sparse features or dense vegetation. This study
provides a structured dataset for evaluating short-term localization
robustness, a reproducible framework for analyzing scan-to-map alignment under
noise, and a comparative evaluation of ICP performance in evolving outdoor
environments. Our analysis underscores how local geometry and environmental
variability affect localization success, offering insights for designing more
resilient robotic systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures, proceedings in European Conference on Mobile
  Robots (ECMR) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Generalized Advantage <span class="highlight-title">Estimation</span> for Distributional Policy Gradients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahil Shaik, Jonathon M. Smereka, <span class="highlight-author">Yue Wang</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalized Advantage Estimation (GAE) has been used to mitigate the
computational complexity of reinforcement learning (RL) by employing an
exponentially weighted estimation of the advantage function to reduce the
variance in policy gradient estimates. Despite its effectiveness, GAE is not
designed to handle value distributions integral to distributional RL, which can
capture the inherent stochasticity in systems and is hence more robust to
system noises. To address this gap, we propose a novel approach that utilizes
the optimal transport theory to introduce a Wasserstein-like directional
metric, which measures both the distance and the directional discrepancies
between probability distributions. Using the exponentially weighted estimation,
we leverage this Wasserstein-like directional metric to derive distributional
GAE (DGAE). Similar to traditional GAE, our proposed DGAE provides a
low-variance advantage estimate with controlled bias, making it well-suited for
policy gradient algorithms that rely on advantage estimation for policy
updates. We integrated DGAE into three different policy gradient methods.
Algorithms were evaluated across various OpenAI Gym environments and compared
with the baselines with traditional GAE to assess the performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, published at ACC 2025 Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InstructVLA: Vision-Language-Action Instruction Tuning from
  Understanding to Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Yang, Hao Li, Yilun Chen, Bin Wang, Yang Tian, Tai Wang, Hanqing Wang, Feng Zhao, Yiyi Liao, Jiangmiao Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To operate effectively in the real world, robots must integrate multimodal
reasoning with precise action generation. However, existing
vision-language-action (VLA) models often sacrifice one for the other, narrow
their abilities to task-specific manipulation data, and suffer catastrophic
forgetting of pre-trained vision-language capabilities. To bridge this gap, we
introduce InstructVLA, an end-to-end VLA model that preserves the flexible
reasoning of large vision-language models (VLMs) while delivering leading
manipulation performance. InstructVLA introduces a novel training paradigm,
Vision-Language-Action Instruction Tuning (VLA-IT), which employs multimodal
training with mixture-of-experts adaptation to jointly optimize textual
reasoning and action generation on both standard VLM corpora and a curated
650K-sample VLA-IT dataset. On in-domain SimplerEnv tasks, InstructVLA achieves
30.5% improvement over SpatialVLA. To evaluate generalization, we introduce
SimplerEnv-Instruct, an 80-task benchmark requiring closed-loop control and
high-level instruction understanding, where it outperforms a fine-tuned OpenVLA
by 92% and an action expert aided by GPT-4o by 29%. Additionally, InstructVLA
surpasses baseline VLMs on multimodal tasks and exhibits inference-time scaling
by leveraging textual reasoning to boost manipulation performance in both
simulated and real-world settings. These results demonstrate InstructVLA's
potential for bridging intuitive and steerable human-robot interaction with
efficient policy learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VLM-Guided Visual Place Recognition for Planet-Scale Geo-<span class="highlight-title">Localization</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sania Waheed, Na Min An, Michael Milford, Sarvapali D. Ramchurn, Shoaib Ehsan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geo-localization from a single image at planet scale (essentially an advanced
or extreme version of the kidnapped robot problem) is a fundamental and
challenging task in applications such as navigation, autonomous driving and
disaster response due to the vast diversity of locations, environmental
conditions, and scene variations. Traditional retrieval-based methods for
geo-localization struggle with scalability and perceptual aliasing, while
classification-based approaches lack generalization and require extensive
training data. Recent advances in vision-language models (VLMs) offer a
promising alternative by leveraging contextual understanding and reasoning.
However, while VLMs achieve high accuracy, they are often prone to
hallucinations and lack interpretability, making them unreliable as standalone
solutions. In this work, we propose a novel hybrid geo-localization framework
that combines the strengths of VLMs with retrieval-based visual place
recognition (VPR) methods. Our approach first leverages a VLM to generate a
prior, effectively guiding and constraining the retrieval search space. We then
employ a retrieval step, followed by a re-ranking mechanism that selects the
most geographically plausible matches based on feature similarity and proximity
to the initially estimated coordinates. We evaluate our approach on multiple
geo-localization benchmarks and show that it consistently outperforms prior
state-of-the-art methods, particularly at street (up to 4.51%) and city level
(up to 13.52%). Our results demonstrate that VLM-generated geographic priors in
combination with VPR lead to scalable, robust, and accurate geo-localization
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IndoorBEV: Joint <span class="highlight-title">Detection</span> and Footprint Completion of Objects via
  Mask-based <span class="highlight-title">Prediction</span> in Indoor Scenarios for Bird's-Eye View Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haichuan Li, Changda Tian, Panos Trahanias, Tomi Westerlund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting diverse objects within complex indoor 3D point clouds presents
significant challenges for robotic perception, particularly with varied object
shapes, clutter, and the co-existence of static and dynamic elements where
traditional bounding box methods falter. To address these limitations, we
propose IndoorBEV, a novel mask-based Bird's-Eye View (BEV) method for indoor
mobile robots.
  In a BEV method, a 3D scene is projected into a 2D BEV grid which handles
naturally occlusions and provides a consistent top-down view aiding to
distinguish static obstacles from dynamic agents. The obtained 2D BEV results
is directly usable to downstream robotic tasks like navigation, motion
prediction, and planning. Our architecture utilizes an axis compact encoder and
a window-based backbone to extract rich spatial features from this BEV map. A
query-based decoder head then employs learned object queries to concurrently
predict object classes and instance masks in the BEV space. This mask-centric
formulation effectively captures the footprint of both static and dynamic
objects regardless of their shape, offering a robust alternative to bounding
box regression. We demonstrate the effectiveness of IndoorBEV on a custom
indoor dataset featuring diverse object classes including static objects
  and dynamic elements like robots and miscellaneous items, showcasing its
potential for robust indoor scene understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Wilhelm Tell <span class="highlight-title">Dataset</span> of Affordance Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachel Ringe, Mihai Pomarlan, Nikolaos Tsiogkas, Stefano De Giorgis, Maria Hedblom, Rainer Malaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Affordances - i.e. possibilities for action that an environment or objects in
it provide - are important for robots operating in human environments to
perceive. Existing approaches train such capabilities on annotated static
images or shapes. This work presents a novel dataset for affordance learning of
common household tasks. Unlike previous approaches, our dataset consists of
video sequences demonstrating the tasks from first- and third-person
perspectives, along with metadata about the affordances that are manifested in
the task, and is aimed towards training perception systems to recognize
affordance manifestations. The demonstrations were collected from several
participants and in total record about seven hours of human activity. The
variety of task performances also allows studying preparatory maneuvers that
people may perform for a task, such as how they arrange their task space, which
is also relevant for collaborative service robots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>\c{opyright} 2025 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Confidence <span class="highlight-title">Calibration</span> in Vision-Language-Action Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas P Zollo, Richard Zemel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trustworthy robot behavior requires not only high levels of task success but
also that the robot can reliably quantify how likely it is to succeed. To this
end, we present the first systematic study of confidence calibration in
vision-language-action (VLA) foundation models, which map visual observations
and natural-language instructions to low-level robot motor commands. We begin
with extensive benchmarking to understand the critical relationship between
task success and calibration error across multiple datasets and VLA variants,
finding that task performance and calibration are not in tension. Next, we
introduce prompt ensembles for VLAs, a lightweight, Bayesian-inspired algorithm
that averages confidence across paraphrased instructions and consistently
improves calibration. We further analyze calibration over the task time
horizon, showing that confidence is often most reliable after making some
progress, suggesting natural points for risk-aware intervention. Finally, we
reveal differential miscalibration across action dimensions and propose
action-wise Platt scaling, a method to recalibrate each action dimension
independently to produce better confidence estimates. Our aim in this study is
to begin to develop the tools and conceptual understanding necessary to render
VLAs both highly performant and highly trustworthy via reliable uncertainty
quantification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language-Conditioned Open-Vocabulary Mobile Manipulation with Pretrained
  Models <span class="chip">IJCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shen Tan, Dong Zhou, Xiangyu Shao, Junqiao Wang, Guanghui Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-vocabulary mobile manipulation (OVMM) that involves the handling of
novel and unseen objects across different workspaces remains a significant
challenge for real-world robotic applications. In this paper, we propose a
novel Language-conditioned Open-Vocabulary Mobile Manipulation framework, named
LOVMM, incorporating the large language model (LLM) and vision-language model
(VLM) to tackle various mobile manipulation tasks in household environments.
Our approach is capable of solving various OVMM tasks with free-form natural
language instructions (e.g. "toss the food boxes on the office room desk to the
trash bin in the corner", and "pack the bottles from the bed to the box in the
guestroom"). Extensive experiments simulated in complex household environments
show strong zero-shot generalization and multi-task learning abilities of
LOVMM. Moreover, our approach can also generalize to multiple tabletop
manipulation tasks and achieve better success rates compared to other
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IJCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Exploratory Study on Human-<span class="highlight-title">Robot</span> Interaction using Semantics-based
  Situational Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianshu Ruan, Aniketh Ramesh, Rustam Stolkin, Manolis Chiou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the impact of high-level semantics (evaluation
of the environment) on Human-Robot Teams (HRT) and Human-Robot Interaction
(HRI) in the context of mobile robot deployments. Although semantics has been
widely researched in AI, how high-level semantics can benefit the HRT paradigm
is underexplored, often fuzzy, and intractable. We applied a semantics-based
framework that could reveal different indicators of the environment (i.e. how
much semantic information exists) in a mock-up disaster response mission. In
such missions, semantics are crucial as the HRT should handle complex
situations and respond quickly with correct decisions, where humans might have
a high workload and stress. Especially when human operators need to shift their
attention between robots and other tasks, they will struggle to build
Situational Awareness (SA) quickly. The experiment suggests that the presented
semantics: 1) alleviate the perceived workload of human operators; 2) increase
the operator's trust in the SA; and 3) help to reduce the reaction time in
switching the level of autonomy when needed. Additionally, we find that
participants with higher trust in the system are encouraged by high-level
semantics to use teleoperation mode more.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mobile Manipulation with Active Inference for Long-Horizon Rearrangement
  Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Corrado Pezzato, Ozan Çatal, Toon Van de Maele, Riddhi J. Pitliya, Tim Verbelen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite growing interest in active inference for robotic control, its
application to complex, long-horizon tasks remains untested. We address this
gap by introducing a fully hierarchical active inference architecture for
goal-directed behavior in realistic robotic settings. Our model combines a
high-level active inference model that selects among discrete skills realized
via a whole-body active inference controller. This unified approach enables
flexible skill composition, online adaptability, and recovery from task
failures without requiring offline training. Evaluated on the Habitat Benchmark
for mobile manipulation, our method outperforms state-of-the-art baselines
across the three long-horizon tasks, demonstrating for the first time that
active inference can scale to the complexity of modern robotics benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HuNavSim 2.0 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miguel Escudero-Jiménez, Noé Pérez-Higueras, Andrés Martínez-Silva, Fernando Caballero, Luis Merino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents a new iteration of the Human Navigation Simulator
(HuNavSim), a novel open-source tool for the simulation of different
human-agent navigation behaviors in scenarios with mobile robots. The tool,
programmed under the ROS 2 framework, can be used together with different
well-known robotics simulators such as Gazebo or NVidia Isaac Sim. The main
goal is to facilitate the development and evaluation of human-aware robot
navigation systems in simulation. In this new version, several features have
been improved and new ones added, such as the extended set of actions and
conditions that can be combined in Behavior Trees to compound complex and
realistic human behaviors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint submitted to the 8th Iberian Robotics Conference (ROBOT
  2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level
  Tactile Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianxin Bi, Kevin Yuchen Ma, Ce Hao, Mike Zheng Shou, Harold Soh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tactile feedback is generally recognized to be crucial for effective
interaction with the physical world. However, state-of-the-art
Vision-Language-Action (VLA) models lack the ability to interpret and use
tactile signals, limiting their effectiveness in contact-rich tasks.
Incorporating tactile feedback into these systems is challenging due to the
absence of large multi-modal datasets. We present VLA-Touch, an approach that
enhances generalist robot policies with tactile sensing \emph{without
fine-tuning} the base VLA. Our method introduces two key innovations: (1) a
pipeline that leverages a pretrained tactile-language model that provides
semantic tactile feedback for high-level task planning, and (2) a
diffusion-based controller that refines VLA-generated actions with tactile
signals for contact-rich manipulation. Through real-world experiments, we
demonstrate that our dual-level integration of tactile feedback improves task
planning efficiency while enhancing execution precision. Code is open-sourced
at \href{https://github.com/jxbi1010/VLA-Touch}{this URL}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prolonging Tool <span class="highlight-title">Life</span>: Learning Skillful Use of General-purpose Tools
  through <span class="highlight-title">Life</span>span-guided <span class="highlight-title">Reinforcement</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Po-Yen Wu, Cheng-Yu Kuo, Yuki Kadokawa, Takamitsu Matsubara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In inaccessible environments with uncertain task demands, robots often rely
on general-purpose tools that lack predefined usage strategies. These tools are
not tailored for particular operations, making their longevity highly sensitive
to how they are used. This creates a fundamental challenge: how can a robot
learn a tool-use policy that both completes the task and prolongs the tool's
lifespan? In this work, we address this challenge by introducing a
reinforcement learning (RL) framework that incorporates tool lifespan as a
factor during policy optimization. Our framework leverages Finite Element
Analysis (FEA) and Miner's Rule to estimate Remaining Useful Life (RUL) based
on accumulated stress, and integrates the RUL into the RL reward to guide
policy learning toward lifespan-guided behavior. To handle the fact that RUL
can only be estimated after task execution, we introduce an Adaptive Reward
Normalization (ARN) mechanism that dynamically adjusts reward scaling based on
estimated RULs, ensuring stable learning signals. We validate our method across
simulated and real-world tool use tasks, including Object-Moving and
Door-Opening with multiple general-purpose tools. The learned policies
consistently prolong tool lifespan (up to 8.01x in simulation) and transfer
effectively to real-world settings, demonstrating the practical value of
learning lifespan-guided tool use strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Delivery Logistics: Enhancing Speed and Safety with Drone
  Technology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17253v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17253v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maharshi Shastri, Ujjval Shrivastav
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing demand for fast and cost effective last mile delivery
solutions has catalyzed significant advancements in drone based logistics. This
research describes the development of an AI integrated drone delivery system,
focusing on route optimization, object detection, secure package handling, and
real time tracking. The proposed system leverages YOLOv4 Tiny for object
detection, the NEO 6M GPS module for navigation, and the A7670 SIM module for
real time communication. A comparative analysis of lightweight AI models and
hardware components is conducted to determine the optimal configuration for
real time UAV based delivery. Key challenges including battery efficiency,
regulatory compliance, and security considerations are addressed through the
integration of machine learning techniques, IoT devices, and encryption
protocols. Preliminary studies demonstrate improvement in delivery time
compared to conventional ground based logistics, along with high accuracy
recipient authentication through facial recognition. The study also discusses
ethical implications and societal acceptance of drone deliveries, ensuring
compliance with FAA, EASA and DGCA regulatory standards. Note: This paper
presents the architecture, design, and preliminary simulation results of the
proposed system. Experimental results, simulation benchmarks, and deployment
statistics are currently being acquired. A comprehensive analysis will be
included in the extended version of this work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PIG-Nav: Key Insights for Pretrained Image Goal <span class="highlight-title">Navigation</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17220v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17220v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiansong Wan, Chengming Zhou, Jinkua Liu, Xiangge Huang, Xiaoyu Chen, Xiaohan Yi, Qisen Yang, Baiting Zhu, Xin-Qiang Cai, Lixing Liu, Rushuai Yang, Chuheng Zhang, Sherif Abdelfattah, Hayong Shin, Pushi Zhang, Li Zhao, Jiang Bian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have explored pretrained (foundation) models for vision-based
robotic navigation, aiming to achieve generalizable navigation and positive
transfer across diverse environments while enhancing zero-shot performance in
unseen settings. In this work, we introduce PIG-Nav (Pretrained Image-Goal
Navigation), a new approach that further investigates pretraining strategies
for vision-based navigation models and contributes in two key areas.
Model-wise, we identify two critical design choices that consistently improve
the performance of pretrained navigation models: (1) integrating an
early-fusion network structure to combine visual observations and goal images
via appropriately pretrained Vision Transformer (ViT) image encoder, and (2)
introducing suitable auxiliary tasks to enhance global navigation
representation learning, thus further improving navigation performance.
Dataset-wise, we propose a novel data preprocessing pipeline for efficiently
labeling large-scale game video datasets for navigation model training. We
demonstrate that augmenting existing open navigation datasets with diverse
gameplay videos improves model performance. Our model achieves an average
improvement of 22.6% in zero-shot settings and a 37.5% improvement in
fine-tuning settings over existing visual navigation foundation models in two
complex simulated environments and one real-world environment. These results
advance the state-of-the-art in pretrained image-goal navigation models.
Notably, our model maintains competitive performance while requiring
significantly less fine-tuning data, highlighting its potential for real-world
deployment with minimal labeled supervision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> FAST-Calib: <span class="highlight-title">LiDAR</span>-Camera Extrinsic <span class="highlight-title">Calibration</span> in One Second 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunran Zheng, <span class="highlight-author">Fu Zhang</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes FAST-Calib, a fast and user-friendly LiDAR-camera
extrinsic calibration tool based on a custom-made 3D target. FAST-Calib
supports both mechanical and solid-state LiDARs by leveraging an efficient and
reliable edge extraction algorithm that is agnostic to LiDAR scan patterns. It
also compensates for edge dilation artifacts caused by LiDAR spot spread
through ellipse fitting, and supports joint optimization across multiple
scenes. We validate FAST-Calib on three LiDAR models (Ouster, Avia, and
Mid360), each paired with a wide-angle camera. Experimental results demonstrate
superior accuracy and robustness compared to existing methods. With
point-to-point registration errors consistently below 6.5mm and total
processing time under 0.7s, FAST-Calib provides an efficient, accurate, and
target-based automatic calibration pipeline. We have open-sourced our code and
dataset on GitHub to benefit the robotics community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reconfigurable Tendon-Driven <span class="highlight-title">Robot</span>s: Eliminating Inter-segmental
  Coupling via Independently Lockable Joints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Botao Lin, Shuang Song, Jiaole Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With a slender redundant body, the tendon-driven robot (TDR) has a large
workspace and great maneuverability while working in complex environments. TDR
comprises multiple independently controlled robot segments, each with a set of
driving tendons. While increasing the number of robot segments enhances
dexterity and expands the workspace, this structural expansion also introduces
intensified inter-segmental coupling. Therefore, achieving precise TDR control
requires more complex models and additional motors. This paper presents a
reconfigurable tendon-driven robot (RTR) equipped with innovative lockable
joints. Each joint's state (locked/free) can be individually controlled through
a pair of antagonistic tendons, and its structure eliminates the need for a
continuous power supply to maintain the state. Operators can selectively
actuate the targeted robot segments, and this scheme fundamentally eliminates
the inter-segmental coupling, thereby avoiding the requirement for complex
coordinated control between segments. The workspace of RTR has been simulated
and compared with traditional TDRs' workspace, and RTR's advantages are further
revealed. The kinematics and statics models of the RTR have been derived and
validation experiments have been conducted. Demonstrations have been performed
using a seven-joint RTR prototype to show its reconfigurability and moving
ability in complex environments with an actuator pack comprising only six
motors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JAM: Keypoint-Guided Joint <span class="highlight-title">Prediction</span> after Classification-Aware
  Marginal Proposal for Multi-Agent Interaction <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangze Lin, Ying He, Fei Yu, Hong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting the future motion of road participants is a critical task in
autonomous driving. In this work, we address the challenge of low-quality
generation of low-probability modes in multi-agent joint prediction. To tackle
this issue, we propose a two-stage multi-agent interactive prediction framework
named \textit{keypoint-guided joint prediction after classification-aware
marginal proposal} (JAM). The first stage is modeled as a marginal prediction
process, which classifies queries by trajectory type to encourage the model to
learn all categories of trajectories, providing comprehensive mode information
for the joint prediction module. The second stage is modeled as a joint
prediction process, which takes the scene context and the marginal proposals
from the first stage as inputs to learn the final joint distribution. We
explicitly introduce key waypoints to guide the joint prediction module in
better capturing and leveraging the critical information from the initial
predicted trajectories. We conduct extensive experiments on the real-world
Waymo Open Motion Dataset interactive prediction benchmark. The results show
that our approach achieves competitive performance. In particular, in the
framework comparison experiments, the proposed JAM outperforms other prediction
frameworks and achieves state-of-the-art performance in interactive trajectory
prediction. The code is available at https://github.com/LinFunster/JAM to
facilitate future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IROS 2025 Accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Falconry-like palm landing by a flapping-wing drone based on the human
  gesture interaction and distance-aware flight <span class="highlight-title">planning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazuki Numazato, Keiichiro Kan, Masaki Kitagawa, Yunong Li, Johannes Kubel, Moju Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Flapping-wing drones have attracted significant attention due to their
biomimetic flight. They are considered more human-friendly due to their
characteristics such as low noise and flexible wings, making them suitable for
human-drone interactions. However, few studies have explored the practical
interaction between humans and flapping-wing drones. On establishing a physical
interaction system with flapping-wing drones, we can acquire inspirations from
falconers who guide birds of prey to land on their arms. This interaction
interprets the human body as a dynamic landing platform, which can be utilized
in various scenarios such as crowded or spatially constrained environments.
Thus, in this study, we propose a falconry-like interaction system in which a
flapping-wing drone performs a palm landing motion on a human hand. To achieve
a safe approach toward humans, we design a trajectory planning method that
considers both physical and psychological factors of the human safety such as
the drone's velocity and distance from the user. We use a commercial flapping
platform with our implemented motion planning and conduct experiments to
evaluate the palm landing performance and safety. The results demonstrate that
our approach enables safe and smooth hand landing interactions. To the best of
our knowledge, it is the first time to achieve a contact-based interaction
between flapping-wing drones and humans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Human-level Intelligence via Human-like Whole-Body Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guang Gao, Jianan Wang, Jinbo Zuo, Junnan Jiang, Jingfan Zhang, Xianwen Zeng, Yuejiang Zhu, Lianyang Ma, Ke Chen, Minhua Sheng, Ruirui Zhang, Zhaohui An
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building general-purpose intelligent robots has long been a fundamental goal
of robotics. A promising approach is to mirror the evolutionary trajectory of
humans: learning through continuous interaction with the environment, with
early progress driven by the imitation of human behaviors. Achieving this goal
presents three core challenges: (1) designing safe robotic hardware with
human-level physical capabilities; (2) developing an intuitive and scalable
whole-body teleoperation interface for data collection; and (3) creating
algorithms capable of learning whole-body visuomotor policies from human
demonstrations. To address these challenges in a unified framework, we propose
Astribot Suite, a robot learning suite for whole-body manipulation aimed at
general daily tasks across diverse environments. We demonstrate the
effectiveness of our system on a wide range of activities that require
whole-body coordination, extensive reachability, human-level dexterity, and
agility. Our results show that Astribot's cohesive integration of embodiment,
teleoperation interface, and learning pipeline marks a significant step towards
real-world, general-purpose whole-body robotic manipulation, laying the
groundwork for the next generation of intelligent robots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Objective <span class="highlight-title">Trajectory</span> <span class="highlight-title">Planning</span> for a <span class="highlight-title">Robot</span>ic Arm in Curtain Wall
  Installation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Liu, Yunxiao Cheng, Weijun Wang, Tianlun Huang, Zhiyong Wang, Wei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of labor shortages and rising costs, construction robots are
regarded as the key to revolutionizing traditional construction methods and
improving efficiency and quality in the construction industry. In order to
ensure that construction robots can perform tasks efficiently and accurately in
complex construction environments, traditional single-objective trajectory
optimization methods are difficult to meet the complex requirements of the
changing construction environment. Therefore, we propose a multi-objective
trajectory optimization for the robotic arm used in the curtain wall
installation. First, we design a robotic arm for curtain wall installation,
integrating serial, parallel, and folding arm elements, while considering its
physical properties and motion characteristics. In addition, this paper
proposes an NSGA-III-FO algorithm (NSGA-III with Focused Operator, NSGA-III-FO)
that incorporates a focus operator screening mechanism to accelerate the
convergence of the algorithm towards the Pareto front, thereby effectively
balancing the multi-objective constraints of construction robots. The proposed
algorithm is tested against NSGA-III, MOEA/D, and MSOPS-II in ten consecutive
trials on the DTLZ3 and WFG3 test functions, showing significantly better
convergence efficiency than the other algorithms. Finally, we conduct two sets
of experiments on the designed robotic arm platform, which confirm the
efficiency and practicality of the NSGA-III-FO algorithm in solving
multi-objective trajectory planning problems for curtain wall installation
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Dynamic</span> Parameter Identification of a Curtain Wall Installation <span class="highlight-title">Robot</span>ic
  Arm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Liu, Yunxiao Cheng, Weijun Wang, Tianlun Huang, Wei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the construction industry, traditional methods fail to meet the modern
demands for efficiency and quality. The curtain wall installation is a critical
component of construction projects. We design a hydraulically driven robotic
arm for curtain wall installation and a dynamic parameter identification
method. We establish a Denavit-Hartenberg (D-H) model based on measured robotic
arm structural parameters and integrate hydraulic cylinder dynamics to
construct a composite parametric system driven by a Stribeck friction model. By
designing high-signal-to-noise ratio displacement excitation signals for
hydraulic cylinders and combining Fourier series to construct optimal
excitation trajectories that satisfy joint constraints, this method effectively
excites the characteristics of each parameter in the minimal parameter set of
the dynamic model of the robotic arm. On this basis, a hierarchical progressive
parameter identification strategy is proposed: least squares estimation is
employed to separately identify and jointly calibrate the dynamic parameters of
both the hydraulic cylinder and the robotic arm, yielding Stribeck model curves
for each joint. Experimental validation on a robotic arm platform demonstrates
residual standard deviations below 0.4 Nm between theoretical and measured
joint torques, confirming high-precision dynamic parameter identification for
the hydraulic-driven curtain wall installation robotic arm. This significantly
contributes to enhancing the intelligence level of curtain wall installation
operations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Dynamic</span> Modeling and Dimensional <span class="highlight-title">Optimization</span> of Legged Mechanisms for
  Construction <span class="highlight-title">Robot</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Liu, Xianlong Yang, Weijun Wang, Wei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of the construction industry, issues such as harsh
working environments, high-intensity and high-risk tasks, and labor shortages
have become increasingly prominent. This drives higher demands for construction
robots in terms of low energy consumption, high mobility, and high load
capacity. This paper focuses on the design and optimization of leg structures
for construction robots, aiming to improve their dynamic performance, reduce
energy consumption, and enhance load-bearing capabilities. Firstly, based on
the leg configuration of ants in nature, we design a structure for the robot's
leg. Secondly, we propose a novel structural optimization method. Using the
Lagrangian approach, a dynamic model of the leg was established. Combining the
dynamic model with the leg's motion trajectory, we formulated multiple dynamic
evaluation metrics and conducted a comprehensive optimization study on the
geometric parameters of each leg segment. The results show that the optimized
leg structure reduces peak joint torques and energy consumption by over 20%.
Finally, dynamic simulation experiments were conducted using ADAMS. The results
demonstrate a significant reduction in the driving power of each joint after
optimization, validating the effectiveness and rationality of the proposed
strategy. This study provides a theoretical foundation and technical support
for the design of heavy-load, high-performance construction robots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MARSCalib: Multi-<span class="highlight-title">robot</span>, Automatic, <span class="highlight-title">Robust</span>, Spherical Target-based
  Extrinsic <span class="highlight-title">Calibration</span> in <span class="highlight-title">Field</span> and Extraterrestrial Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seokhwan Jeong, Hogyun Kim, Younggun Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel spherical target-based LiDAR-camera extrinsic
calibration method designed for outdoor environments with multi-robot systems,
considering both target and sensor corruption. The method extracts the 2D
ellipse center from the image and the 3D sphere center from the pointcloud,
which are then paired to compute the transformation matrix. Specifically, the
image is first decomposed using the Segment Anything Model (SAM). Then, a novel
algorithm extracts an ellipse from a potentially corrupted sphere, and the
extracted center of ellipse is corrected for errors caused by the perspective
projection model. For the LiDAR pointcloud, points on the sphere tend to be
highly noisy due to the absence of flat regions. To accurately extract the
sphere from these noisy measurements, we apply a hierarchical weighted sum to
the accumulated pointcloud. Through experiments, we demonstrated that the
sphere can be robustly detected even under both types of corruption,
outperforming other targets. We evaluated our method using three different
types of LiDARs (spinning, solid-state, and non-repetitive) with cameras
positioned in three different locations. Furthermore, we validated the
robustness of our method to target corruption by experimenting with spheres
subjected to various types of degradation. These experiments were conducted in
both a planetary test and a field environment. Our code is available at
https://github.com/sparolab/MARSCalib.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> IONext: Unlocking the Next Era of Inertial <span class="highlight-title">Odometry</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17089v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17089v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanshan Zhang, Si<span class="highlight-author">yue Wang</span>, Tianshui Wen, Qi Zhang, Ziheng Zhou, Lingxiang Zheng, Yu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Researchers have increasingly adopted Transformer-based models for inertial
odometry. While Transformers excel at modeling long-range dependencies, their
limited sensitivity to local, fine-grained motion variations and lack of
inherent inductive biases often hinder localization accuracy and
generalization. Recent studies have shown that incorporating large-kernel
convolutions and Transformer-inspired architectural designs into CNN can
effectively expand the receptive field, thereby improving global motion
perception. Motivated by these insights, we propose a novel CNN-based module
called the Dual-wing Adaptive Dynamic Mixer (DADM), which adaptively captures
both global motion patterns and local, fine-grained motion features from
dynamic inputs. This module dynamically generates selective weights based on
the input, enabling efficient multi-scale feature aggregation. To further
improve temporal modeling, we introduce the Spatio-Temporal Gating Unit (STGU),
which selectively extracts representative and task-relevant motion features in
the temporal domain. This unit addresses the limitations of temporal modeling
observed in existing CNN approaches. Built upon DADM and STGU, we present a new
CNN-based inertial odometry backbone, named Next Era of Inertial Odometry
(IONext). Extensive experiments on six public datasets demonstrate that IONext
consistently outperforms state-of-the-art (SOTA) Transformer- and CNN-based
methods. For instance, on the RNIN dataset, IONext reduces the average ATE by
10% and the average RTE by 12% compared to the representative model iMOT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Robot</span> Operation of Home Appliances by Reading User Manuals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20424v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20424v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Zhang, Hanbo Zhang, Anxing Xiao, David Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Operating home appliances, among the most common tools in every household, is
a critical capability for assistive home robots. This paper presents ApBot, a
robot system that operates novel household appliances by "reading" their user
manuals. ApBot faces multiple challenges: (i) infer goal-conditioned partial
policies from their unstructured, textual descriptions in a user manual
document, (ii) ground the policies to the appliance in the physical world, and
(iii) execute the policies reliably over potentially many steps, despite
compounding errors. To tackle these challenges, ApBot constructs a structured,
symbolic model of an appliance from its manual, with the help of a large
vision-language model (VLM). It grounds the symbolic actions visually to
control panel elements. Finally, ApBot closes the loop by updating the model
based on visual feedback. Our experiments show that across a wide range of
simulated and real-world appliances, ApBot achieves consistent and
statistically significant improvements in task success rate, compared with
state-of-the-art large VLMs used directly as control policies. These results
suggest that a structured internal representations plays an important role in
robust robot operation of home appliances, especially, complex ones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Generalist <span class="highlight-title">Robot</span> Learning from Internet Video: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.19664v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.19664v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert McCarthy, Daniel C. H. Tan, Dominik Schmidt, Fernando Acero, Nathan Herr, Yilun Du, Thomas G. Thuruthel, Zhibin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling deep learning to massive and diverse internet data has driven
remarkable breakthroughs in domains such as video generation and natural
language processing. Robot learning, however, has thus far failed to replicate
this success and remains constrained by a scarcity of available data. Learning
from videos (LfV) methods aim to address this data bottleneck by augmenting
traditional robot data with large-scale internet video. This video data
provides foundational information regarding physical dynamics, behaviours, and
tasks, and can be highly informative for general-purpose robots.
  This survey systematically examines the emerging field of LfV. We first
outline essential concepts, including detailing fundamental LfV challenges such
as distribution shift and missing action labels in video data. Next, we
comprehensively review current methods for extracting knowledge from
large-scale internet video, overcoming LfV challenges, and improving robot
learning through video-informed training. The survey concludes with a critical
discussion of future opportunities. Here, we emphasize the need for scalable
foundation model approaches that can leverage the full range of available
internet video and enhance the learning of robot policies and dynamics models.
Overall, the survey aims to inform and catalyse future LfV research, driving
progress towards general-purpose robots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flow-Based Single-Step Completion for Efficient and Expressive Policy
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prajwal Koirala, Cody Fleming
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models such as diffusion and flow-matching offer expressive
policies for offline reinforcement learning (RL) by capturing rich, multimodal
action distributions, but their iterative sampling introduces high inference
costs and training instability due to gradient propagation across sampling
steps. We propose the \textit{Single-Step Completion Policy} (SSCP), a
generative policy trained with an augmented flow-matching objective to predict
direct completion vectors from intermediate flow samples, enabling accurate,
one-shot action generation. In an off-policy actor-critic framework, SSCP
combines the expressiveness of generative models with the training and
inference efficiency of unimodal policies, without requiring long
backpropagation chains. Our method scales effectively to offline,
offline-to-online, and online RL settings, offering substantial gains in speed
and adaptability over diffusion-based baselines. We further extend SSCP to
goal-conditioned RL, enabling flat policies to exploit subgoal structures
without explicit hierarchical inference. SSCP achieves strong results across
standard offline RL and behavior cloning benchmarks, positioning it as a
versatile, expressive, and efficient framework for deep RL and sequential
decision-making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How to Adapt <span class="highlight-title">Control</span> Barrier Functions? A Learning-Based Approach with
  Applications to a VTOL Quadplane 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.03038v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.03038v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taekyung Kim, Randal W. Beard, Dimitra Panagou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a novel theoretical framework for online adaptation
of Control Barrier Function (CBF) parameters, i.e., of the class K functions
included in the CBF condition, under input constraints. We introduce the
concept of locally validated CBF parameters, which are adapted online to
guarantee finite-horizon safety, based on conditions derived from Nagumo's
theorem and tangent cone analysis. To identify these parameters online, we
integrate a learning-based approach with an uncertainty-aware verification
process that account for both epistemic and aleatoric uncertainties inherent in
neural network predictions. Our method is demonstrated on a VTOL quadplane
model during challenging transition and landing maneuvers, showcasing enhanced
performance while maintaining safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2025 IEEE Conference on Decision and Control (CDC). Project page:
  https://www.taekyung.me/how-to-adapt-cbf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ First, Learn What You Don't Know: Active Information Gathering for
  Driving at the Limits of Handling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00107v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00107v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Davydov, Franck Djeumou, Marcus Greiff, Makoto Suminaka, Michael Thompson, John Subosits, Thomas Lew
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combining data-driven models that adapt online and model predictive control
(MPC) has enabled effective control of nonlinear systems. However, when
deployed on unstable systems, online adaptation may not be fast enough to
ensure reliable simultaneous learning and control. For example, a controller on
a vehicle executing highly dynamic maneuvers--such as drifting to avoid an
obstacle--may push the vehicle's tires to their friction limits, destabilizing
the vehicle and allowing modeling errors to quickly compound and cause a loss
of control. To address this challenge, we present an active information
gathering framework for identifying vehicle dynamics as quickly as possible. We
propose an expressive vehicle dynamics model that leverages Bayesian last-layer
meta-learning to enable rapid online adaptation. The model's uncertainty
estimates are used to guide informative data collection and quickly improve the
model prior to deployment. Dynamic drifting experiments on a Toyota Supra show
that (i) the framework enables reliable control of a vehicle at the edge of
stability, (ii) online adaptation alone may not suffice for zero-shot control
and can lead to undesirable transient errors or spin-outs, and (iii) active
data collection helps achieve reliable performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Virtual Holonomic Constraints in Motion <span class="highlight-title">Planning</span>: Revisiting Feasibility
  and Limitations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07983v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07983v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksim Surov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the feasibility of virtual holonomic constraints (VHCs)
in the context of motion planning for underactuated mechanical systems with a
single degree of underactuation. While existing literature has established a
widely accepted definition of VHC, we argue that this definition is overly
restrictive and excludes a broad class of admissible trajectories from
consideration. To illustrate this point, we analyze a periodic motion of the
Planar Vertical Take-Off and Landing (PVTOL) aircraft that satisfies all
standard motion planning requirements, including orbital stabilizability.
However, for this solution -- as well as for a broad class of similar ones --
there exists no VHC that satisfies the conventional definition. We further
provide a formal proof demonstrating that the conditions imposed by this
definition necessarily fail for a broad class of trajectories of mechanical
systems. These findings call for a reconsideration of the current definition of
VHCs, with the potential to significantly broaden their applicability in motion
planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 3 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Design and <span class="highlight-title">Control</span> Methods for Using Collaborative <span class="highlight-title">Robot</span>s in
  Upper-Limb Rehabilitation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18661v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18661v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dario Onfiani, Marco Caramaschi, Luigi Biagiotti, Fabio Pini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the development of a robotic rehabilitation system
for the upper limbs based on collaborative end-effector solutions. The use of
commercial collaborative robots offers significant advantages for this task, as
they are optimized from an engineering perspective and ensure safe physical
interaction with humans. However, they also come with noticeable drawbacks,
such as the limited range of sizes available on the market and the standard
control modes, which are primarily oriented towards industrial or service
applications. To address these limitations, we propose an optimization-based
design method to fully exploit the capability of the cobot in performing
rehabilitation tasks. Additionally, we introduce a novel control architecture
based on an admittance-type Virtual Fixture method, which constrains the motion
of the robot along a prescribed path. This approach allows for an intuitive
definition of the task to be performed via Programming by Demonstration and
enables the system to operate both passively and actively. In passive mode, the
system supports the patient during task execution with additional force, while
in active mode, it opposes the motion with a braking force. Experimental
results demonstrate the effectiveness of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Precision-Scalable Hardware for Microscaling (MX) Processing
  in <span class="highlight-title">Robot</span>ics Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.22404v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.22404v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stef Cuyckens, Xiaoling Yi, Nitish Satya Murthy, Chao Fang, Marian Verhelst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous robots require efficient on-device learning to adapt to new
environments without cloud dependency. For this edge training, Microscaling
(MX) data types offer a promising solution by combining integer and
floating-point representations with shared exponents, reducing energy
consumption while maintaining accuracy. However, the state-of-the-art
continuous learning processor, namely Dacapo, faces limitations with its
MXINT-only support and inefficient vector-based grouping during
backpropagation. In this paper, we present, to the best of our knowledge, the
first work that addresses these limitations with two key innovations: (1) a
precision-scalable arithmetic unit that supports all six MX data types by
exploiting sub-word parallelism and unified integer and floating-point
processing; and (2) support for square shared exponent groups to enable
efficient weight handling during backpropagation, removing storage redundancy
and quantization overhead. We evaluate our design against Dacapo under
iso-peak-throughput on four robotics workloads in TSMC 16nm FinFET technology
at 400MHz, reaching a 51% lower memory footprint, and 4x higher effective
training throughput, while achieving comparable energy efficiency, enabling
efficient robotics continual learning at the edge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in 2025 IEEE/ACM International Symposium on Low Power
  Electronics and Design (ISLPED 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Learning-Enhanced MPC for Safe Crowd <span class="highlight-title">Navigation</span> with
  Heterogeneous Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09859v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09859v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huajian Liu, Yixuan Feng, Wei Dong, Kunpeng Fan, Chao Wang, Yongzhuo Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel hierarchical framework for robot navigation
in dynamic environments with heterogeneous constraints. Our approach leverages
a graph neural network trained via reinforcement learning (RL) to efficiently
estimate the robot's cost-to-go, formulated as local goal recommendations. A
spatio-temporal path-searching module, which accounts for kinematic
constraints, is then employed to generate a reference trajectory to facilitate
solving the non-convex optimization problem used for explicit constraint
enforcement. More importantly, we introduce an incremental action-masking
mechanism and a privileged learning strategy, enabling end-to-end training of
the proposed planner. Both simulation and real-world experiments demonstrate
that the proposed method effectively addresses local planning in complex
dynamic environments, achieving state-of-the-art (SOTA) performance. Compared
with existing learning-optimization hybrid methods, our approach eliminates the
dependency on high-fidelity simulation environments, offering significant
advantages in computational efficiency and training scalability. The code will
be released as open-source upon acceptance of the paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GEMINUS: Dual-aware <span class="highlight-title">Global</span> and Scene-Adaptive Mixture-of-Experts for
  End-to-End Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14456v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14456v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Wan, Yixin Cui, Jiatong Du, Shuo Yang, Yulong Bai, Yanjun Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end autonomous driving requires adaptive and robust handling of
complex and diverse traffic environments. However, prevalent single-mode
planning methods attempt to learn an overall policy while struggling to acquire
diversified driving skills to handle diverse scenarios. Therefore, this paper
proposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework
featuring a Global Expert, a Scene-Adaptive Experts Group, and equipped with a
Dual-aware Router. Specifically, the Global Expert is trained on the overall
dataset, possessing robust performance. The Scene-Adaptive Experts are trained
on corresponding scene subsets, achieving adaptive performance. The Dual-aware
Router simultaneously considers scenario-level features and routing uncertainty
to dynamically activate expert modules. Through the effective coupling of the
Global Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,
GEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS
outperforms existing methods in the Bench2Drive closed-loop benchmark and
achieves state-of-the-art performance in Driving Score and Success Rate, even
with only monocular vision input. Furthermore, ablation studies demonstrate
significant improvements over the original single-expert baseline: 7.67% in
Driving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The
code will be available at https://github.com/newbrains1/GEMINUS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoBridge: A Hierarchical Architecture Bridging Cognition and Execution
  for General <span class="highlight-title">Robot</span>ic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.01709v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.01709v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaidong Zhang, Rongtao Xu, Pengzhen Ren, Junfan Lin, Hefeng Wu, Liang Lin, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Operating robots in open-ended scenarios with diverse tasks is a crucial
research and application direction in robotics. While recent progress in
natural language processing and large multimodal models has enhanced robots'
ability to understand complex instructions, robot manipulation still faces the
procedural skill dilemma and the declarative skill dilemma in open
environments. Existing methods often compromise cognitive and executive
capabilities. To address these challenges, in this paper, we propose RoBridge,
a hierarchical intelligent architecture for general robotic manipulation. It
consists of a high-level cognitive planner (HCP) based on a large-scale
pre-trained vision-language model (VLM), an invariant operable representation
(IOR) serving as a symbolic bridge, and a generalist embodied agent (GEA).
RoBridge maintains the declarative skill of VLM and unleashes the procedural
skill of reinforcement learning, effectively bridging the gap between cognition
and execution. RoBridge demonstrates significant performance improvements over
existing baselines, achieving a 75% success rate on new tasks and an 83%
average success rate in sim-to-real generalization using only five real-world
data samples per task. This work represents a significant step towards
integrating cognitive reasoning with physical execution in robotic systems,
offering a new paradigm for general robotic manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://abliao.github.io/RoBridge/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Range-View <span class="highlight-title">LiDAR</span> <span class="highlight-title">Segmentation</span> in Adverse Weather 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08979v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08979v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longyu Yang, Lu Zhang, Jun Liu, Yap-Peng Tan, Heng Tao Shen, Xiaofeng Zhu, Ping Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR segmentation has emerged as an important task to enrich scene
perception and understanding. Range-view-based methods have gained popularity
due to their high computational efficiency and compatibility with real-time
deployment. However, their generalized performance under adverse weather
conditions remains underexplored, limiting their reliability in real-world
environments. In this work, we identify and analyze the unique challenges that
affect the generalization of range-view LiDAR segmentation in severe weather.
To address these challenges, we propose a modular and lightweight framework
that enhances robustness without altering the core architecture of existing
models. Our method reformulates the initial stem block of standard range-view
networks into two branches to process geometric attributes and reflectance
intensity separately. Specifically, a Geometric Abnormality Suppression (GAS)
module reduces the influence of weather-induced spatial noise, and a
Reflectance Distortion Calibration (RDC) module corrects reflectance
distortions through memory-guided adaptive instance normalization. The
processed features are then fused and passed to the original segmentation
pipeline. Extensive experiments on different benchmarks and baseline models
demonstrate that our approach significantly improves generalization to adverse
weather with minimal inference overhead, offering a practical and effective
solution for real-world LiDAR segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Onto-LLM-TAMP: Knowledge-oriented Task and Motion <span class="highlight-title">Planning</span> using Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07493v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07493v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhayy Ud Din, Jan Rosell, Waseem Akram, Isiah Zaplana, Maximo A Roa, Irfan Hussain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performing complex manipulation tasks in dynamic environments requires
efficient Task and Motion Planning (TAMP) approaches that combine high-level
symbolic plans with low-level motion control. Advances in Large Language Models
(LLMs), such as GPT-4, are transforming task planning by offering natural
language as an intuitive and flexible way to describe tasks, generate symbolic
plans, and reason. However, the effectiveness of LLM-based TAMP approaches is
limited due to static and template-based prompting, which limits adaptability
to dynamic environments and complex task contexts. To address these
limitations, this work proposes a novel Onto-LLM-TAMP framework that employs
knowledge-based reasoning to refine and expand user prompts with
task-contextual reasoning and knowledge-based environment state descriptions.
Integrating domain-specific knowledge into the prompt ensures semantically
accurate and context-aware task plans. The proposed framework demonstrates its
effectiveness by resolving semantic errors in symbolic plan generation, such as
maintaining logical temporal goal ordering in scenarios involving hierarchical
object placement. The proposed framework is validated through both simulation
and real-world scenarios, demonstrating significant improvements over the
baseline approach in terms of adaptability to dynamic environments and the
generation of semantically correct task plans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to knowledge based systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ICCO: Learning an Instruction-conditioned Coordinator for
  Language-guided Task-aligned Multi-<span class="highlight-title">robot</span> <span class="highlight-title">Control</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12122v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12122v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoshiki Yano, Kazuki Shibata, Maarten Kokshoorn, Takamitsu Matsubara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have permitted the
development of language-guided multi-robot systems, which allow robots to
execute tasks based on natural language instructions. However, achieving
effective coordination in distributed multi-agent environments remains
challenging due to (1) misalignment between instructions and task requirements
and (2) inconsistency in robot behaviors when they independently interpret
ambiguous instructions. To address these challenges, we propose
Instruction-Conditioned Coordinator (ICCO), a Multi-Agent Reinforcement
Learning (MARL) framework designed to enhance coordination in language-guided
multi-robot systems. ICCO consists of a Coordinator agent and multiple Local
Agents, where the Coordinator generates Task-Aligned and Consistent
Instructions (TACI) by integrating language instructions with environmental
states, ensuring task alignment and behavioral consistency. The Coordinator and
Local Agents are jointly trained to optimize a reward function that balances
task efficiency and instruction following. A Consistency Enhancement Term is
added to the learning objective to maximize mutual information between
instructions and robot behaviors, further improving coordination. Simulation
and real-world experiments validate the effectiveness of ICCO in achieving
language-guided task-aligned multi-robot control. The demonstration can be
found at https://yanoyoshiki.github.io/ICCO/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures, to be published in the 2025 IEEE/RSJ
  International Conference on Intelligent Robots and Systems</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">121</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bearded Dragon Activity Recognition Pipeline: An AI-Based Approach to
  Behavioural Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17987v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17987v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arsen Yermukan, Pedro Machado, Feliciano Domingos, Isibor Kennedy Ihianle, Jordan J. Bird, Stefano S. K. Kaburu, Samantha J. Ward
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional monitoring of bearded dragon (Pogona Viticeps) behaviour is
time-consuming and prone to errors. This project introduces an automated system
for real-time video analysis, using You Only Look Once (YOLO) object detection
models to identify two key behaviours: basking and hunting. We trained five
YOLO variants (v5, v7, v8, v11, v12) on a custom, publicly available dataset of
1200 images, encompassing bearded dragons (600), heating lamps (500), and
crickets (100). YOLOv8s was selected as the optimal model due to its superior
balance of accuracy (mAP@0.5:0.95 = 0.855) and speed. The system processes
video footage by extracting per-frame object coordinates, applying temporal
interpolation for continuity, and using rule-based logic to classify specific
behaviours. Basking detection proved reliable. However, hunting detection was
less accurate, primarily due to weak cricket detection (mAP@0.5 = 0.392).
Future improvements will focus on enhancing cricket detection through expanded
datasets or specialised small-object detectors. This automated system offers a
scalable solution for monitoring reptile behaviour in controlled environments,
significantly improving research efficiency and data quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Benchmark</span>ing of Deep Learning Methods for Generic MRI
  Multi-OrganAbdominal <span class="highlight-title">Segmentation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17971v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17971v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deepa Krishnaswamy, Cosmin Ciausu, Steve Pieper, Ron Kikinis, Benjamin Billot, Andrey Fedorov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in deep learning have led to robust automated tools for
segmentation of abdominal computed tomography (CT). Meanwhile, segmentation of
magnetic resonance imaging (MRI) is substantially more challenging due to the
inherent signal variability and the increased effort required for annotating
training datasets. Hence, existing approaches are trained on limited sets of
MRI sequences, which might limit their generalizability. To characterize the
landscape of MRI abdominal segmentation tools, we present here a comprehensive
benchmarking of the three state-of-the-art and open-source models:
MRSegmentator, MRISegmentator-Abdomen, and TotalSegmentator MRI. Since these
models are trained using labor-intensive manual annotation cycles, we also
introduce and evaluate ABDSynth, a SynthSeg-based model purely trained on
widely available CT segmentations (no real images). More generally, we assess
accuracy and generalizability by leveraging three public datasets (not seen by
any of the evaluated methods during their training), which span all major
manufacturers, five MRI sequences, as well as a variety of subject conditions,
voxel resolutions, and fields-of-view. Our results reveal that MRSegmentator
achieves the best performance and is most generalizable. In contrast, ABDSynth
yields slightly less accurate results, but its relaxed requirements in training
data make it an alternative when the annotation budget is limited. The
evaluation code and datasets are given for future benchmarking at
https://github.com/deepakri201/AbdoBench, along with inference code and weights
for ABDSynth.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot <span class="highlight-title">Dynamic</span> Concept Personalization with Grid-Based LoRA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rameen Abdal, Or Patashnik, Ekaterina Deyneka, Hao Chen, Aliaksandr Siarohin, Sergey Tulyakov, Daniel Cohen-Or, Kfir Aberman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-to-video generation have enabled high-quality
synthesis from text and image prompts. While the personalization of dynamic
concepts, which capture subject-specific appearance and motion from a single
video, is now feasible, most existing methods require per-instance fine-tuning,
limiting scalability. We introduce a fully zero-shot framework for dynamic
concept personalization in text-to-video models. Our method leverages
structured 2x2 video grids that spatially organize input and output pairs,
enabling the training of lightweight Grid-LoRA adapters for editing and
composition within these grids. At inference, a dedicated Grid Fill module
completes partially observed layouts, producing temporally coherent and
identity preserving outputs. Once trained, the entire system operates in a
single forward pass, generalizing to previously unseen dynamic concepts without
any test-time optimization. Extensive experiments demonstrate high-quality and
consistent results across a wide range of subjects beyond trained concepts and
editing scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page and Video :
  https://snap-research.github.io/zero-shot-dynamic-concepts/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OPEN: A <span class="highlight-title">Benchmark</span> <span class="highlight-title">Dataset</span> and Baseline for Older Adult Patient
  Engagement Recognition in Virtual Rehabilitation Learning Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Abedi, Sadaf Safa, Tracey J. F. Colella, Shehroz S. Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Engagement in virtual learning is essential for participant satisfaction,
performance, and adherence, particularly in online education and virtual
rehabilitation, where interactive communication plays a key role. Yet,
accurately measuring engagement in virtual group settings remains a challenge.
There is increasing interest in using artificial intelligence (AI) for
large-scale, real-world, automated engagement recognition. While engagement has
been widely studied in younger academic populations, research and datasets
focused on older adults in virtual and telehealth learning settings remain
limited. Existing methods often neglect contextual relevance and the
longitudinal nature of engagement across sessions. This paper introduces OPEN
(Older adult Patient ENgagement), a novel dataset supporting AI-driven
engagement recognition. It was collected from eleven older adults participating
in weekly virtual group learning sessions over six weeks as part of cardiac
rehabilitation, producing over 35 hours of data, making it the largest dataset
of its kind. To protect privacy, raw video is withheld; instead, the released
data include facial, hand, and body joint landmarks, along with affective and
behavioral features extracted from video. Annotations include binary engagement
states, affective and behavioral labels, and context-type indicators, such as
whether the instructor addressed the group or an individual. The dataset offers
versions with 5-, 10-, 30-second, and variable-length samples. To demonstrate
utility, multiple machine learning and deep learning models were trained,
achieving engagement recognition accuracy of up to 81 percent. OPEN provides a
scalable foundation for personalized engagement modeling in aging populations
and contributes to broader engagement recognition research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 3 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VIBE: Video-Input Brain Encoder for fMRI Response Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17958v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17958v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Carlstrom Schad, Shrey Dixit, Janis Keck, Viktor Studenyak, Aleksandr Shpilevoi, Andrej Bicanski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present VIBE, a two-stage Transformer that fuses multi-modal video, audio,
and text features to predict fMRI activity. Representations from open-source
models (Qwen2.5, BEATs, Whisper, SlowFast, V-JEPA) are merged by a
modality-fusion transformer and temporally decoded by a prediction transformer
with rotary embeddings. Trained on 65 hours of movie data from the CNeuroMod
dataset and ensembled across 20 seeds, VIBE attains mean parcel-wise Pearson
correlations of 32.25 on in-distribution Friends S07 and 21.25 on six
out-of-distribution films. An earlier iteration of the same architecture
obtained 0.3198 and 0.2096, respectively, winning Phase-1 and placing second
overall in the Algonauts 2025 Challenge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AFRDA: Attentive Feature Refinement for Domain Adaptive Semantic
  <span class="highlight-title">Segmentation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md. Al-Masrur Khan, Durgakant Pushp, Lantao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Unsupervised Domain Adaptive Semantic Segmentation (UDA-SS), a model is
trained on labeled source domain data (e.g., synthetic images) and adapted to
an unlabeled target domain (e.g., real-world images) without access to target
annotations. Existing UDA-SS methods often struggle to balance fine-grained
local details with global contextual information, leading to segmentation
errors in complex regions. To address this, we introduce the Adaptive Feature
Refinement (AFR) module, which enhances segmentation accuracy by refining
highresolution features using semantic priors from low-resolution logits. AFR
also integrates high-frequency components, which capture fine-grained
structures and provide crucial boundary information, improving object
delineation. Additionally, AFR adaptively balances local and global information
through uncertaintydriven attention, reducing misclassifications. Its
lightweight design allows seamless integration into HRDA-based UDA methods,
leading to state-of-the-art segmentation performance. Our approach improves
existing UDA-SS methods by 1.05% mIoU on GTA V --> Cityscapes and 1.04% mIoU on
Synthia-->Cityscapes. The implementation of our framework is available at:
https://github.com/Masrur02/AFRDA
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Dif<span class="highlight-title">fusion</span> Framework for Pseudo-Healthy Brain MRI Inpainting
  with Enhanced 3D Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17911v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17911v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dou Hoon Kwark, Shirui Luo, Xiyue Zhu, Yudu Li, Zhi-Pei Liang, Volodymyr Kindratenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pseudo-healthy image inpainting is an essential preprocessing step for
analyzing pathological brain MRI scans. Most current inpainting methods favor
slice-wise 2D models for their high in-plane fidelity, but their independence
across slices produces discontinuities in the volume. Fully 3D models alleviate
this issue, but their high model capacity demands extensive training data for
reliable, high-fidelity synthesis -- often impractical in medical settings. We
address these limitations with a hierarchical diffusion framework by replacing
direct 3D modeling with two perpendicular coarse-to-fine 2D stages. An axial
diffusion model first yields a coarse, globally consistent inpainting; a
coronal diffusion model then refines anatomical details. By combining
perpendicular spatial views with adaptive resampling, our method balances data
efficiency and volumetric consistency. Our experiments show our approach
outperforms state-of-the-art baselines in both realism and volumetric
consistency, making it a promising solution for pseudo-healthy image
inpainting. Code is available at
https://github.com/dou0000/3dMRI-Consistent-Inpaint.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Recurrent Ensembles for Predicting Brain Responses to
  Naturalistic Movies (Algonauts 2025) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Semih Eren, Deniz Kucukahmetler, Nico Scherf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately predicting distributed cortical responses to naturalistic stimuli
requires models that integrate visual, auditory and semantic information over
time. We present a hierarchical multimodal recurrent ensemble that maps
pretrained video, audio, and language embeddings to fMRI time series recorded
while four subjects watched almost 80 hours of movies provided by the Algonauts
2025 challenge. Modality-specific bidirectional RNNs encode temporal dynamics;
their hidden states are fused and passed to a second recurrent layer, and
lightweight subject-specific heads output responses for 1000 cortical parcels.
Training relies on a composite MSE-correlation loss and a curriculum that
gradually shifts emphasis from early sensory to late association regions.
Averaging 100 model variants further boosts robustness. The resulting system
ranked third on the competition leaderboard, achieving an overall Pearson r =
0.2094 and the highest single-parcel peak score (mean r = 0.63) among all
participants, with particularly strong gains for the most challenging subject
(Subject 5). The approach establishes a simple, extensible baseline for future
multimodal brain-encoding benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures, 1 table. Invited report, CCN 2025 Algonauts
  Project session (3rd-place team). Code:
  https://github.com/erensemih/Algonauts2025_ModalityRNN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiNAT-IR: Exploring Dilated Neighborhood Attention for High-Quality
  Image Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanzhou Liu, Binghan Li, Chengkai Liu, Mi Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers, with their self-attention mechanisms for modeling long-range
dependencies, have become a dominant paradigm in image restoration tasks.
However, the high computational cost of self-attention limits scalability to
high-resolution images, making efficiency-quality trade-offs a key research
focus. To address this, Restormer employs channel-wise self-attention, which
computes attention across channels instead of spatial dimensions. While
effective, this approach may overlook localized artifacts that are crucial for
high-quality image restoration. To bridge this gap, we explore Dilated
Neighborhood Attention (DiNA) as a promising alternative, inspired by its
success in high-level vision tasks. DiNA balances global context and local
precision by integrating sliding-window attention with mixed dilation factors,
effectively expanding the receptive field without excessive overhead. However,
our preliminary experiments indicate that directly applying this global-local
design to the classic deblurring task hinders accurate visual restoration,
primarily due to the constrained global context understanding within local
attention. To address this, we introduce a channel-aware module that
complements local attention, effectively integrating global context without
sacrificing pixel-level precision. The proposed DiNAT-IR, a Transformer-based
architecture specifically designed for image restoration, achieves competitive
results across multiple benchmarks, offering a high-quality solution for
diverse low-level computer vision problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Feature Selection and Machine Learning for Nitrogen
  Assessment in Grapevine Leaves using In-<span class="highlight-title">Field</span> Hyperspectral Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atif Bilal Asad, Achyut Paudel, Safal Kshetri, Chenchen Kang, Salik Ram Khanal, Nataliya Shcherbatyuk, Pierre Davadant, R. Paul Schreiner, Santosh Kalauni, Manoj Karkee, Markus Keller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nitrogen (N) is one of the most crucial nutrients in vineyards, affecting
plant growth and subsequent products such as wine and juice. Because soil N has
high spatial and temporal variability, it is desirable to accurately estimate
the N concentration of grapevine leaves and manage fertilization at the
individual plant level to optimally meet plant needs. In this study, we used
in-field hyperspectral images with wavelengths ranging from $400 to 1000nm of
four different grapevine cultivars collected from distinct vineyards and over
two growth stages during two growing seasons to develop models for predicting N
concentration at the leaf-level and canopy-level. After image processing, two
feature selection methods were employed to identify the optimal set of spectral
bands that were responsive to leaf N concentrations. The selected spectral
bands were used to train and test two different Machine Learning (ML) models,
Gradient Boosting and XGBoost, for predicting nitrogen concentrations. The
comparison of selected bands for both leaf-level and canopy-level datasets
showed that most of the spectral regions identified by the feature selection
methods were across both methods and the dataset types (leaf- and canopy-level
datasets), particularly in the key regions, 500-525nm, 650-690nm, 750-800nm,
and 900-950nm. These findings indicated the robustness of these spectral
regions for predicting nitrogen content. The results for N prediction
demonstrated that the ML model achieved an R square of 0.49 for canopy-level
data and an R square of 0.57 for leaf-level data, despite using different sets
of selected spectral bands for each analysis level. The study demonstrated the
potential of using in-field hyperspectral imaging and the use of spectral data
in integrated feature selection and ML techniques to monitor N status in
vineyards.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Facilitated Fairness Assessment of AI-based Skin Lesion
  Classifiers Through GenAI-based Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ko Watanabe. Stanislav Frolov. Adriano Lucieri. Andreas Dengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Deep Learning and its application on the edge hold
great potential for the revolution of routine screenings for skin cancers like
Melanoma. Along with the anticipated benefits of this technology, potential
dangers arise from unforseen and inherent biases. Thus, assessing and improving
the fairness of such systems is of utmost importance. A key challenge in
fairness assessment is to ensure that the evaluation dataset is sufficiently
representative of different Personal Identifiable Information (PII) (sex, age,
and race) and other minority groups. Against the backdrop of this challenge,
this study leverages the state-of-the-art Generative AI (GenAI) LightningDiT
model to assess the fairness of publicly available melanoma classifiers. The
results suggest that fairness assessment using highly realistic synthetic data
is a promising direction. Yet, our findings indicate that verifying fairness
becomes difficult when the melanoma-detection model used for evaluation is
trained on data that differ from the dataset underpinning the synthetic images.
Nonetheless, we propose that our approach offers a valuable new avenue for
employing synthetic data to gauge and enhance fairness in medical-imaging GenAI
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FishDet-M: A Unified Large-Scale <span class="highlight-title">Benchmark</span> for <span class="highlight-title">Robust</span> Fish <span class="highlight-title">Detection</span> and
  CLIP-Guided Model Selection in Diverse Aquatic Visual Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muayad Abujabal, Lyes Saad Saoud, Irfan Hussain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate fish detection in underwater imagery is essential for ecological
monitoring, aquaculture automation, and robotic perception. However, practical
deployment remains limited by fragmented datasets, heterogeneous imaging
conditions, and inconsistent evaluation protocols. To address these gaps, we
present \textit{FishDet-M}, the largest unified benchmark for fish detection,
comprising 13 publicly available datasets spanning diverse aquatic environments
including marine, brackish, occluded, and aquarium scenes. All data are
harmonized using COCO-style annotations with both bounding boxes and
segmentation masks, enabling consistent and scalable cross-domain evaluation.
We systematically benchmark 28 contemporary object detection models, covering
the YOLOv8 to YOLOv12 series, R-CNN based detectors, and DETR based models.
Evaluations are conducted using standard metrics including mAP, mAP@50, and
mAP@75, along with scale-specific analyses (AP$_S$, AP$_M$, AP$_L$) and
inference profiling in terms of latency and parameter count. The results
highlight the varying detection performance across models trained on FishDet-M,
as well as the trade-off between accuracy and efficiency across models of
different architectures. To support adaptive deployment, we introduce a
CLIP-based model selection framework that leverages vision-language alignment
to dynamically identify the most semantically appropriate detector for each
input image. This zero-shot selection strategy achieves high performance
without requiring ensemble computation, offering a scalable solution for
real-time applications. FishDet-M establishes a standardized and reproducible
platform for evaluating object detection in complex aquatic scenes. All
datasets, pretrained models, and evaluation tools are publicly available to
facilitate future research in underwater computer vision and intelligent marine
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detail++: Training-Free Detail Enhancer for Text-to-Image Dif<span class="highlight-title">fusion</span>
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lifeng Chen, Jiner Wang, Zihao Pan, Beier Zhu, Xiaofeng Yang, Chi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-to-image (T2I) generation have led to impressive
visual results. However, these models still face significant challenges when
handling complex prompt, particularly those involving multiple subjects with
distinct attributes. Inspired by the human drawing process, which first
outlines the composition and then incrementally adds details, we propose
Detail++, a training-free framework that introduces a novel Progressive Detail
Injection (PDI) strategy to address this limitation. Specifically, we decompose
a complex prompt into a sequence of simplified sub-prompts, guiding the
generation process in stages. This staged generation leverages the inherent
layout-controlling capacity of self-attention to first ensure global
composition, followed by precise refinement. To achieve accurate binding
between attributes and corresponding subjects, we exploit cross-attention
mechanisms and further introduce a Centroid Alignment Loss at test time to
reduce binding noise and enhance attribute consistency. Extensive experiments
on T2I-CompBench and a newly constructed style composition benchmark
demonstrate that Detail++ significantly outperforms existing methods,
particularly in scenarios involving multiple objects and complex stylistic
conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SV3.3B: A Sports Video Understanding Model for Action Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Varun Kodathala, Yashwanth Reddy Vutukoori, Rakesh Vunnam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenge of automated sports video analysis, which
has traditionally been limited by computationally intensive models requiring
server-side processing and lacking fine-grained understanding of athletic
movements. Current approaches struggle to capture the nuanced biomechanical
transitions essential for meaningful sports analysis, often missing critical
phases like preparation, execution, and follow-through that occur within
seconds. To address these limitations, we introduce SV3.3B, a lightweight 3.3B
parameter video understanding model that combines novel temporal motion
difference sampling with self-supervised learning for efficient on-device
deployment. Our approach employs a DWT-VGG16-LDA based keyframe extraction
mechanism that intelligently identifies the 16 most representative frames from
sports sequences, followed by a V-DWT-JEPA2 encoder pretrained through
mask-denoising objectives and an LLM decoder fine-tuned for sports action
description generation. Evaluated on a subset of the NSVA basketball dataset,
SV3.3B achieves superior performance across both traditional text generation
metrics and sports-specific evaluation criteria, outperforming larger
closed-source models including GPT-4o variants while maintaining significantly
lower computational requirements. Our model demonstrates exceptional capability
in generating technically detailed and analytically rich sports descriptions,
achieving 29.2% improvement over GPT-4o in ground truth validation metrics,
with substantial improvements in information density, action complexity, and
measurement precision metrics essential for comprehensive athletic analysis.
Model Available at https://huggingface.co/sportsvision/SV3.3B.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 4 tables. Submitted to AIxSET 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Learning Rates Simultaneously Achieve <span class="highlight-title">Robust</span>ness to Spurious
  Correlations and Compressibility <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Melih Barsbey, Lucas Prieto, Stefanos Zafeiriou, Tolga Birdal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robustness and resource-efficiency are two highly desirable properties for
modern machine learning models. However, achieving them jointly remains a
challenge. In this paper, we position high learning rates as a facilitator for
simultaneously achieving robustness to spurious correlations and network
compressibility. We demonstrate that large learning rates also produce
desirable representation properties such as invariant feature utilization,
class separation, and activation sparsity. Importantly, our findings indicate
that large learning rates compare favorably to other hyperparameters and
regularization methods, in consistently satisfying these properties in tandem.
In addition to demonstrating the positive effect of large learning rates across
diverse spurious correlation datasets, models, and optimizers, we also present
strong evidence that the previously documented success of large learning rates
in standard classification tasks is likely due to its effect on addressing
hidden/rare spurious correlations in the training dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCV 2025, 23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ultra3D: Efficient and High-Fidelity 3D Generation with Part Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Chen, Zhihao Li, Yikai Wang, Hu Zhang, Qin Li, Chi Zhang, Guosheng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in sparse voxel representations have significantly improved
the quality of 3D content generation, enabling high-resolution modeling with
fine-grained geometry. However, existing frameworks suffer from severe
computational inefficiencies due to the quadratic complexity of attention
mechanisms in their two-stage diffusion pipelines. In this work, we propose
Ultra3D, an efficient 3D generation framework that significantly accelerates
sparse voxel modeling without compromising quality. Our method leverages the
compact VecSet representation to efficiently generate a coarse object layout in
the first stage, reducing token count and accelerating voxel coordinate
prediction. To refine per-voxel latent features in the second stage, we
introduce Part Attention, a geometry-aware localized attention mechanism that
restricts attention computation within semantically consistent part regions.
This design preserves structural continuity while avoiding unnecessary global
attention, achieving up to 6.7x speed-up in latent generation. To support this
mechanism, we construct a scalable part annotation pipeline that converts raw
meshes into part-labeled sparse voxels. Extensive experiments demonstrate that
Ultra3D supports high-resolution 3D generation at 1024 resolution and achieves
state-of-the-art performance in both visual fidelity and user preference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://buaacyw.github.io/ultra3d/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Yume: An Interactive World Generation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Mao, Shaoheng Lin, Zhen Li, Chuanhao Li, Wenshuo Peng, Tong He, Jiangmiao Pang, Mingmin Chi, Yu Qiao, Kaipeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Yume aims to use images, text, or videos to create an interactive, realistic,
and dynamic world, which allows exploration and control using peripheral
devices or neural signals. In this report, we present a preview version of
\method, which creates a dynamic world from an input image and allows
exploration of the world using keyboard actions. To achieve this high-fidelity
and interactive video world generation, we introduce a well-designed framework,
which consists of four main components, including camera motion quantization,
video generation architecture, advanced sampler, and model acceleration. First,
we quantize camera motions for stable training and user-friendly interaction
using keyboard inputs. Then, we introduce the Masked Video Diffusion
Transformer~(MVDT) with a memory module for infinite video generation in an
autoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)
and Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)
are introduced to the sampler for better visual quality and more precise
control. Moreover, we investigate model acceleration by synergistic
optimization of adversarial distillation and caching mechanisms. We use the
high-quality world exploration dataset \sekai to train \method, and it achieves
remarkable results in diverse scenes and applications. All data, codebase, and
model weights are available on https://github.com/stdstu12/YUME. Yume will
update monthly to achieve its original goal. Project page:
https://stdstu12.github.io/YUME-Project/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive <span class="highlight-title">Evaluation</span> Framework for the Study of the Effects of
  Facial Filters on Face Recognition Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kagan Ozturk, Louisa Conwill, Jacob Gutierrez, Kevin Bowyer, Walter J. Scheirer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial filters are now commonplace for social media users around the world.
Previous work has demonstrated that facial filters can negatively impact
automated face recognition performance. However, these studies focus on small
numbers of hand-picked filters in particular styles. In order to more
effectively incorporate the wide ranges of filters present on various social
media applications, we introduce a framework that allows for larger-scale study
of the impact of facial filters on automated recognition. This framework
includes a controlled dataset of face images, a principled filter selection
process that selects a representative range of filters for experimentation, and
a set of experiments to evaluate the filters' impact on recognition. We
demonstrate our framework with a case study of filters from the American
applications Instagram and Snapchat and the Chinese applications Meitu and Pitu
to uncover cross-cultural differences. Finally, we show how the filtering
effect in a face embedding space can easily be detected and restored to improve
face recognition performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lumina-mGPT 2.0: Stand-Alone AutoRegressive Image Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Xin, Juncheng Yan, Qi Qin, Zhen Li, Dongyang Liu, Shicheng Li, Victor Shea-Jay Huang, Yupeng Zhou, Renrui Zhang, Le Zhuo, Tiancheng Han, Xiaoqing Sun, Siqi Luo, Mengmeng Wang, Bin Fu, Yuewen Cao, Hongsheng Li, Guangtao Zhai, Xiaohong Liu, Yu Qiao, Peng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Lumina-mGPT 2.0, a stand-alone, decoder-only autoregressive model
that revisits and revitalizes the autoregressive paradigm for high-quality
image generation and beyond. Unlike existing approaches that rely on pretrained
components or hybrid architectures, Lumina-mGPT 2.0 is trained entirely from
scratch, enabling unrestricted architectural design and licensing freedom. It
achieves generation quality on par with state-of-the-art diffusion models such
as DALL-E 3 and SANA, while preserving the inherent flexibility and
compositionality of autoregressive modeling. Our unified tokenization scheme
allows the model to seamlessly handle a wide spectrum of tasks-including
subject-driven generation, image editing, controllable synthesis, and dense
prediction-within a single generative framework. To further boost usability, we
incorporate efficient decoding strategies like inference-time scaling and
speculative Jacobi sampling to improve quality and speed, respectively.
Extensive evaluations on standard text-to-image benchmarks (e.g., GenEval, DPG)
demonstrate that Lumina-mGPT 2.0 not only matches but in some cases surpasses
diffusion-based models. Moreover, we confirm its multi-task capabilities on the
Graph200K benchmark, with the native Lumina-mGPT 2.0 performing exceptionally
well. These results position Lumina-mGPT 2.0 as a strong, flexible foundation
model for unified multimodal generation. We have released our training details,
code, and models at https://github.com/Alpha-VLLM/Lumina-mGPT-2.0.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech Report, 23 pages, 11 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Interaction of Compressibility and Adversarial <span class="highlight-title">Robust</span>ness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Melih Barsbey, Antônio H. Ribeiro, Umut Şimşekli, Tolga Birdal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern neural networks are expected to simultaneously satisfy a host of
desirable properties: accurate fitting to training data, generalization to
unseen inputs, parameter and computational efficiency, and robustness to
adversarial perturbations. While compressibility and robustness have each been
studied extensively, a unified understanding of their interaction still remains
elusive. In this work, we develop a principled framework to analyze how
different forms of compressibility - such as neuron-level sparsity and spectral
compressibility - affect adversarial robustness. We show that these forms of
compression can induce a small number of highly sensitive directions in the
representation space, which adversaries can exploit to construct effective
perturbations. Our analysis yields a simple yet instructive robustness bound,
revealing how neuron and spectral compressibility impact $L_\infty$ and $L_2$
robustness via their effects on the learned representations. Crucially, the
vulnerabilities we identify arise irrespective of how compression is achieved -
whether via regularization, architectural bias, or implicit learning dynamics.
Through empirical evaluations across synthetic and realistic tasks, we confirm
our theoretical predictions, and further demonstrate that these vulnerabilities
persist under adversarial training and transfer learning, and contribute to the
emergence of universal adversarial perturbations. Our findings show a
fundamental tension between structured compressibility and robustness, and
suggest new pathways for designing models that are both efficient and secure.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BetterCheck: Towards Safeguarding VLMs for Automotive Perception Systems <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malsha Ashani Mahawatta Dona, Beatriz Cabrero-Daniel, Yinan Yu, Christian Berger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are growingly extended to process multimodal
data such as text and video simultaneously. Their remarkable performance in
understanding what is shown in images is surpassing specialized neural networks
(NNs) such as Yolo that is supporting only a well-formed but very limited
vocabulary, ie., objects that they are able to detect. When being
non-restricted, LLMs and in particular state-of-the-art vision language models
(VLMs) show impressive performance to describe even complex traffic situations.
This is making them potentially suitable components for automotive perception
systems to support the understanding of complex traffic situations or edge case
situation. However, LLMs and VLMs are prone to hallucination, which mean to
either potentially not seeing traffic agents such as vulnerable road users who
are present in a situation, or to seeing traffic agents who are not there in
reality. While the latter is unwanted making an ADAS or autonomous driving
systems (ADS) to unnecessarily slow down, the former could lead to disastrous
decisions from an ADS. In our work, we are systematically assessing the
performance of 3 state-of-the-art VLMs on a diverse subset of traffic
situations sampled from the Waymo Open Dataset to support safety guardrails for
capturing such hallucinations in VLM-supported perception systems. We observe
that both, proprietary and open VLMs exhibit remarkable image understanding
capabilities even paying thorough attention to fine details sometimes difficult
to spot for us humans. However, they are also still prone to making up elements
in their descriptions to date requiring hallucination detection strategies such
as BetterCheck that we propose in our work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in The IEEE International Conference on Intelligent
  Transportation Systems (ITSC)2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Joint Asymmetric Loss for Learning with Noisy Labels <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialiang Wang, Xian<span class="highlight-author">ming Liu</span>, Xiong Zhou, Gangfeng Hu, Deming Zhai, Junjun Jiang, Xiangyang Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning with noisy labels is a crucial task for training accurate deep
neural networks. To mitigate label noise, prior studies have proposed various
robust loss functions, particularly symmetric losses. Nevertheless, symmetric
losses usually suffer from the underfitting issue due to the overly strict
constraint. To address this problem, the Active Passive Loss (APL) jointly
optimizes an active and a passive loss to mutually enhance the overall fitting
ability. Within APL, symmetric losses have been successfully extended, yielding
advanced robust loss functions. Despite these advancements, emerging
theoretical analyses indicate that asymmetric losses, a new class of robust
loss functions, possess superior properties compared to symmetric losses.
However, existing asymmetric losses are not compatible with advanced
optimization frameworks such as APL, limiting their potential and
applicability. Motivated by this theoretical gap and the prospect of asymmetric
losses, we extend the asymmetric loss to the more complex passive loss scenario
and propose the Asymetric Mean Square Error (AMSE), a novel asymmetric loss. We
rigorously establish the necessary and sufficient condition under which AMSE
satisfies the asymmetric condition. By substituting the traditional symmetric
passive loss in APL with our proposed AMSE, we introduce a novel robust loss
framework termed Joint Asymmetric Loss (JAL). Extensive experiments demonstrate
the effectiveness of our method in mitigating label noise. Code available at:
https://github.com/cswjl/joint-asymmetric-loss
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audio-Vision Contrastive Learning for Phonological Class Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daiqi Liu, Tomás Arias-Vergara, Jana Hutter, Andreas Maier, Paula Andrea Pérez-Toro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate classification of articulatory-phonological features plays a vital
role in understanding human speech production and developing robust speech
technologies, particularly in clinical contexts where targeted phonemic
analysis and therapy can improve disease diagnosis accuracy and personalized
rehabilitation. In this work, we propose a multimodal deep learning framework
that combines real-time magnetic resonance imaging (rtMRI) and speech signals
to classify three key articulatory dimensions: manner of articulation, place of
articulation, and voicing. We perform classification on 15 phonological classes
derived from the aforementioned articulatory dimensions and evaluate the system
with four audio/vision configurations: unimodal rtMRI, unimodal audio signals,
multimodal middle fusion, and contrastive learning-based audio-vision fusion.
Experimental results on the USC-TIMIT dataset show that our contrastive
learning-based approach achieves state-of-the-art performance, with an average
F1-score of 0.81, representing an absolute increase of 0.23 over the unimodal
baseline. The results confirm the effectiveness of contrastive representation
learning for multimodal articulatory analysis. Our code and processed dataset
will be made publicly available at
https://github.com/DaE-plz/AC_Contrastive_Phonology to support future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>conference to TSD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MCM: Mamba-based Cardiac Motion Tracking using Sequential Images in MRI <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahui Yin, Xinxing Cheng, Jinming Duan, Yan Pang, Declan O'Regan, Hadrien Reynaud, Qingjie Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Myocardial motion tracking is important for assessing cardiac function and
diagnosing cardiovascular diseases, for which cine cardiac magnetic resonance
(CMR) has been established as the gold standard imaging modality. Many existing
methods learn motion from single image pairs consisting of a reference frame
and a randomly selected target frame from the cardiac cycle. However, these
methods overlook the continuous nature of cardiac motion and often yield
inconsistent and non-smooth motion estimations. In this work, we propose a
novel Mamba-based cardiac motion tracking network (MCM) that explicitly
incorporates target image sequence from the cardiac cycle to achieve smooth and
temporally consistent motion tracking. By developing a bi-directional Mamba
block equipped with a bi-directional scanning mechanism, our method facilitates
the estimation of plausible deformation fields. With our proposed motion
decoder that integrates motion information from frames adjacent to the target
frame, our method further enhances temporal coherence. Moreover, by taking
advantage of Mamba's structured state-space formulation, the proposed method
learns the continuous dynamics of the myocardium from sequential images without
increasing computational complexity. We evaluate the proposed method on two
public datasets. The experimental results demonstrate that the proposed method
quantitatively and qualitatively outperforms both conventional and
state-of-the-art learning-based cardiac motion tracking methods. The code is
available at https://github.com/yjh-0104/MCM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Medical Image Computing and Computer-Assisted Intervention (MICCAI),
  Reconstruction and Imaging Motion Estimation Workshop (RIME), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Multislice Electron Ptycho<span class="highlight-title">graph</span>y with a Generative Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian K. Belardi, Chia-Hao Lee, Yingheng Wang, Justin Lovelace, Kilian Q. Weinberger, David A. Muller, Carla P. Gomes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multislice electron ptychography (MEP) is an inverse imaging technique that
computationally reconstructs the highest-resolution images of atomic crystal
structures from diffraction patterns. Available algorithms often solve this
inverse problem iteratively but are both time consuming and produce suboptimal
solutions due to their ill-posed nature. We develop MEP-Diffusion, a diffusion
model trained on a large database of crystal structures specifically for MEP to
augment existing iterative solvers. MEP-Diffusion is easily integrated as a
generative prior into existing reconstruction methods via Diffusion Posterior
Sampling (DPS). We find that this hybrid approach greatly enhances the quality
of the reconstructed 3D volumes, achieving a 90.50% improvement in SSIM over
existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 10 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Perspective-Invariant 3D Object <span class="highlight-title">Detection</span> <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17665v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17665v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ao Liang, Lingdong Kong, Dongyue Lu, Youquan Liu, Jian Fang, Huaici Zhao, Wei Tsang Ooi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rise of robotics, LiDAR-based 3D object detection has garnered
significant attention in both academia and industry. However, existing datasets
and methods predominantly focus on vehicle-mounted platforms, leaving other
autonomous platforms underexplored. To bridge this gap, we introduce Pi3DET,
the first benchmark featuring LiDAR data and 3D bounding box annotations
collected from multiple platforms: vehicle, quadruped, and drone, thereby
facilitating research in 3D object detection for non-vehicle platforms as well
as cross-platform 3D detection. Based on Pi3DET, we propose a novel
cross-platform adaptation framework that transfers knowledge from the
well-studied vehicle platform to other platforms. This framework achieves
perspective-invariant 3D detection through robust alignment at both geometric
and feature levels. Additionally, we establish a benchmark to evaluate the
resilience and robustness of current 3D detectors in cross-platform scenarios,
providing valuable insights for developing adaptive 3D perception systems.
Extensive experiments validate the effectiveness of our approach on challenging
cross-platform tasks, demonstrating substantial gains over existing adaptation
methods. We hope this work paves the way for generalizable and unified 3D
perception systems across diverse and complex environments. Our Pi3DET dataset,
cross-platform benchmark suite, and annotation toolkit have been made publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025; 46 pages, 18 figures, 22 tables; Project Page at
  https://pi3det.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Talk2Event: Grounded Understanding of <span class="highlight-title">Dynamic</span> Scenes from Event Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingdong Kong, Dongyue Lu, Ao Liang, Rong Li, Yuhao Dong, Tianshuai Hu, Lai Xing Ng, Wei Tsang Ooi, Benoit R. Cottereau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras offer microsecond-level latency and robustness to motion blur,
making them ideal for understanding dynamic environments. Yet, connecting these
asynchronous streams to human language remains an open challenge. We introduce
Talk2Event, the first large-scale benchmark for language-driven object
grounding in event-based perception. Built from real-world driving data, we
provide over 30,000 validated referring expressions, each enriched with four
grounding attributes -- appearance, status, relation to viewer, and relation to
other objects -- bridging spatial, temporal, and relational reasoning. To fully
exploit these cues, we propose EventRefer, an attribute-aware grounding
framework that dynamically fuses multi-attribute representations through a
Mixture of Event-Attribute Experts (MoEE). Our method adapts to different
modalities and scene dynamics, achieving consistent gains over state-of-the-art
baselines in event-only, frame-only, and event-frame fusion settings. We hope
our dataset and approach will establish a foundation for advancing multimodal,
temporally-aware, and language-driven perception in real-world robotics and
autonomy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint; 42 pages, 17 figures, 16 tables; Project Page at
  https://talk2event.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mammo-Mamba: A Hybrid State-Space and Transformer Architecture with
  Sequential Mixture of Experts for <span class="highlight-title">Multi-View</span> Mammo<span class="highlight-title">graph</span>y 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farnoush Bayatmakou, Reza Taleei, Nicole Simone, Arash Mohammadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Breast cancer (BC) remains one of the leading causes of cancer-related
mortality among women, despite recent advances in Computer-Aided Diagnosis
(CAD) systems. Accurate and efficient interpretation of multi-view mammograms
is essential for early detection, driving a surge of interest in Artificial
Intelligence (AI)-powered CAD models. While state-of-the-art multi-view
mammogram classification models are largely based on Transformer architectures,
their computational complexity scales quadratically with the number of image
patches, highlighting the need for more efficient alternatives. To address this
challenge, we propose Mammo-Mamba, a novel framework that integrates Selective
State-Space Models (SSMs), transformer-based attention, and expert-driven
feature refinement into a unified architecture. Mammo-Mamba extends the
MambaVision backbone by introducing the Sequential Mixture of Experts (SeqMoE)
mechanism through its customized SecMamba block. The SecMamba is a modified
MambaVision block that enhances representation learning in high-resolution
mammographic images by enabling content-adaptive feature refinement. These
blocks are integrated into the deeper stages of MambaVision, allowing the model
to progressively adjust feature emphasis through dynamic expert gating,
effectively mitigating the limitations of traditional Transformer models.
Evaluated on the CBIS-DDSM benchmark dataset, Mammo-Mamba achieves superior
classification performance across all key metrics while maintaining
computational efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Monocular Semantic Scene Completion via Masked Recurrent Networks <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17661v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17661v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuzhi Wang, Xinran Wu, Song Wang, Lingdong Kong, Ziping Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular Semantic Scene Completion (MSSC) aims to predict the voxel-wise
occupancy and semantic category from a single-view RGB image. Existing methods
adopt a single-stage framework that aims to simultaneously achieve visible
region segmentation and occluded region hallucination, while also being
affected by inaccurate depth estimation. Such methods often achieve suboptimal
performance, especially in complex scenes. We propose a novel two-stage
framework that decomposes MSSC into coarse MSSC followed by the Masked
Recurrent Network. Specifically, we propose the Masked Sparse Gated Recurrent
Unit (MS-GRU) which concentrates on the occupied regions by the proposed mask
updating mechanism, and a sparse GRU design is proposed to reduce the
computation cost. Additionally, we propose the distance attention projection to
reduce projection errors by assigning different attention scores according to
the distance to the observed surface. Experimental results demonstrate that our
proposed unified framework, MonoMRN, effectively supports both indoor and
outdoor scenes and achieves state-of-the-art performance on the NYUv2 and
SemanticKITTI datasets. Furthermore, we conduct robustness analysis under
various disturbances, highlighting the role of the Masked Recurrent Network in
enhancing the model's resilience to such challenges. The source code is
publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025; 15 pages, 10 figures, 6 tables; Code at
  https://github.com/alanWXZ/MonoMRN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ See the Forest and the Trees: A Synergistic Reasoning Framework for
  Knowledge-Based Visual Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Wang, Yunhan Tang, Yijie Wang, Zhihao Yuan, Huan Wang, Yangfan He, Bin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have pushed the frontiers of
Knowledge-Based Visual Question Answering (KBVQA), yet their reasoning is
fundamentally bottlenecked by a reliance on uni-dimensional evidence. This
"seeing only the trees, but not the forest" approach prevents robust,
multi-faceted understanding. Inspired by the principle of seeing both the
forest and trees, we propose Synergos-VQA, a novel synergistic reasoning
framework. At its core, Synergos-VQA concurrently generates and fuses three
complementary evidence streams at inference time: (1) Holistic Evidence to
perceive the entire scene (the "forest"), (2) Structural Evidence from a
prototype-driven module to identify key objects (the "trees"), and (3) Causal
Evidence from a counterfactual probe to ensure the reasoning is robustly
grounded. By synergistically fusing this multi-faceted evidence, our framework
achieves a more comprehensive and reliable reasoning process. Extensive
experiments show that Synergos-VQA decisively establishes a new
state-of-the-art on three challenging benchmarks, including OK-VQA and A-OKVQA.
Furthermore, our approach demonstrates strong plug-and-play capabilities,
significantly boosting various open-source MLLMs and proving that superior
methodological design can outperform sheer model scale.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention (as Discrete-Time Markov) Chains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yotam Erel, Olaf Dünkel, Rishabh Dabral, Vladislav Golyanik, Christian Theobalt, Amit H. Bermano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new interpretation of the attention matrix as a discrete-time
Markov chain. Our interpretation sheds light on common operations involving
attention scores such as selection, summation, and averaging in a unified
framework. It further extends them by considering indirect attention,
propagated through the Markov chain, as opposed to previous studies that only
model immediate effects. Our main observation is that tokens corresponding to
semantically similar regions form a set of metastable states, where the
attention clusters, while noisy attention scores tend to disperse. Metastable
states and their prevalence can be easily computed through simple matrix
multiplication and eigenanalysis, respectively. Using these lightweight tools,
we demonstrate state-of-the-art zero-shot segmentation. Lastly, we define
TokenRank -- the steady state vector of the Markov chain, which measures global
token importance. We demonstrate that using it brings improvements in
unconditional image generation. We believe our framework offers a fresh view of
how tokens are being attended in modern visual transformers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://yoterel.github.io/attention_chains/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CNS-Bench: <span class="highlight-title">Benchmark</span>ing Image Classifier <span class="highlight-title">Robust</span>ness Under Continuous
  Nuisance Shifts <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olaf Dünkel, Artur Jesslen, Jiahao Xie, Christian Theobalt, Christian Rupprecht, Adam Kortylewski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An important challenge when using computer vision models in the real world is
to evaluate their performance in potential out-of-distribution (OOD) scenarios.
While simple synthetic corruptions are commonly applied to test OOD robustness,
they often fail to capture nuisance shifts that occur in the real world.
Recently, diffusion models have been applied to generate realistic images for
benchmarking, but they are restricted to binary nuisance shifts. In this work,
we introduce CNS-Bench, a Continuous Nuisance Shift Benchmark to quantify OOD
robustness of image classifiers for continuous and realistic generative
nuisance shifts. CNS-Bench allows generating a wide range of individual
nuisance shifts in continuous severities by applying LoRA adapters to diffusion
models. To address failure cases, we propose a filtering mechanism that
outperforms previous methods, thereby enabling reliable benchmarking with
generative models. With the proposed benchmark, we perform a large-scale study
to evaluate the robustness of more than 40 classifiers under various nuisance
shifts. Through carefully designed comparisons and analyses, we find that model
rankings can change for varying shifts and shift scales, which cannot be
captured when applying common binary shifts. Additionally, we show that
evaluating the model performance on a continuous scale allows the
identification of model failure points, providing a more nuanced understanding
of model robustness. Project page including code and data:
https://genintel.github.io/CNS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025. Project page: https://genintel.github.io/CNS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Early Bird Identifies the Worm: You Can't Beat a Head Start in
  <span class="highlight-title">Long-Term</span> Body Re-ID (ECHO-BID) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas M. Metz, Matthew Q. Hill, Alice J. O'Toole
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Person identification in unconstrained viewing environments presents
significant challenges due to variations in distance, viewpoint, imaging
conditions, and clothing. We introduce $\textbf{E}$va $\textbf{C}$lothes-Change
from $\textbf{H}$idden $\textbf{O}$bjects - $\textbf{B}$ody
$\textbf{ID}$entification (ECHO-BID), a class of long-term re-id models built
on object-pretrained EVA-02 Large backbones. We compare ECHO-BID to 9 other
models that vary systematically in backbone architecture, model size, scale of
object classification pretraining, and transfer learning protocol. Models were
evaluated on benchmark datasets across constrained, unconstrained, and occluded
settings. ECHO-BID, with transfer learning on the most challenging
clothes-change data, achieved state-of-the-art results on long-term re-id --
substantially outperforming other methods. ECHO-BID also surpassed other
methods by a wide margin in occluded viewing scenarios. A combination of
increased model size and Masked Image Modeling during pretraining underlie
ECHO-BID's strong performance on long-term re-id. Notably, a smaller, but more
challenging transfer learning dataset, generalized better across datasets than
a larger, less challenging one. However, the larger dataset with an additional
fine-tuning step proved best on the most difficult data. Selecting the correct
pretrained backbone architecture and transfer learning protocols can drive
substantial gains in long-term re-id performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Reusing Attention for One-stage Lane Topology Understanding <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Li, Zongzheng Zhang, Xuchong Qiu, Xinrun Li, Zi<span class="highlight-author">ming Liu</span>, Leichen Wang, Ruikai Li, Zhenxin Zhu, Huan-ang Gao, Xiaojian Lin, Zhiyong Cui, <span class="highlight-author">Hang Zhao</span>, Hao Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding lane toplogy relationships accurately is critical for safe
autonomous driving. However, existing two-stage methods suffer from
inefficiencies due to error propagations and increased computational overheads.
To address these challenges, we propose a one-stage architecture that
simultaneously predicts traffic elements, lane centerlines and topology
relationship, improving both the accuracy and inference speed of lane topology
understanding for autonomous driving. Our key innovation lies in reusing
intermediate attention resources within distinct transformer decoders. This
approach effectively leverages the inherent relational knowledge within the
element detection module to enable the modeling of topology relationships among
traffic elements and lanes without requiring additional computationally
expensive graph networks. Furthermore, we are the first to demonstrate that
knowledge can be distilled from models that utilize standard definition (SD)
maps to those operates without using SD maps, enabling superior performance
even in the absence of SD maps. Extensive experiments on the OpenLane-V2
dataset show that our approach outperforms baseline methods in both accuracy
and efficiency, achieving superior results in lane detection, traffic element
identification, and topology reasoning. Our code is available at
https://github.com/Yang-Li-2000/one-stage.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IROS 2025, Project Page:
  https://github.com/Yang-Li-2000/one-stage.git</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision Transformer attention alignment with human visual perception in
  aesthetic object <span class="highlight-title">evaluation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miguel Carrasco, César González-Martín, José Aranda, Luis Oliveros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual attention mechanisms play a crucial role in human perception and
aesthetic evaluation. Recent advances in Vision Transformers (ViTs) have
demonstrated remarkable capabilities in computer vision tasks, yet their
alignment with human visual attention patterns remains underexplored,
particularly in aesthetic contexts. This study investigates the correlation
between human visual attention and ViT attention mechanisms when evaluating
handcrafted objects. We conducted an eye-tracking experiment with 30
participants (9 female, 21 male, mean age 24.6 years) who viewed 20 artisanal
objects comprising basketry bags and ginger jars. Using a Pupil Labs
eye-tracker, we recorded gaze patterns and generated heat maps representing
human visual attention. Simultaneously, we analyzed the same objects using a
pre-trained ViT model with DINO (Self-DIstillation with NO Labels), extracting
attention maps from each of the 12 attention heads. We compared human and ViT
attention distributions using Kullback-Leibler divergence across varying
Gaussian parameters (sigma=0.1 to 3.0). Statistical analysis revealed optimal
correlation at sigma=2.4 +-0.03, with attention head #12 showing the strongest
alignment with human visual patterns. Significant differences were found
between attention heads, with heads #7 and #9 demonstrating the greatest
divergence from human attention (p< 0.05, Tukey HSD test). Results indicate
that while ViTs exhibit more global attention patterns compared to human focal
attention, certain attention heads can approximate human visual behavior,
particularly for specific object features like buckles in basketry items. These
findings suggest potential applications of ViT attention mechanisms in product
design and aesthetic evaluation, while highlighting fundamental differences in
attention strategies between human perception and current AI models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InvRGB+L: Inverse Rendering of Complex Scenes with Unified Color and
  <span class="highlight-title">LiDAR</span> Reflectance Modeling <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxue Chen, Bhargav Chandaka, Chih-Hao Lin, Ya-Qin Zhang, David Forsyth, Hao Zhao, Shenlong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present InvRGB+L, a novel inverse rendering model that reconstructs large,
relightable, and dynamic scenes from a single RGB+LiDAR sequence. Conventional
inverse graphics methods rely primarily on RGB observations and use LiDAR
mainly for geometric information, often resulting in suboptimal material
estimates due to visible light interference. We find that LiDAR's intensity
values-captured with active illumination in a different spectral range-offer
complementary cues for robust material estimation under variable lighting.
Inspired by this, InvRGB+L leverages LiDAR intensity cues to overcome
challenges inherent in RGB-centric inverse graphics through two key
innovations: (1) a novel physics-based LiDAR shading model and (2) RGB-LiDAR
material consistency losses. The model produces novel-view RGB and LiDAR
renderings of urban and indoor scenes and supports relighting, night
simulations, and dynamic object insertions, achieving results that surpass
current state-of-the-art methods in both scene-level urban inverse rendering
and LiDAR simulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable AI for Collaborative Assessment of 2D/3D <span class="highlight-title">Registration</span>
  Quality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sue Min Cho, Alexander Do, Russell H. Taylor, Mathias Unberath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As surgery embraces digital transformation--integrating sophisticated
imaging, advanced algorithms, and robotics to support and automate complex
sub-tasks--human judgment of system correctness remains a vital safeguard for
patient safety. This shift introduces new "operator-type" roles tasked with
verifying complex algorithmic outputs, particularly at critical junctures of
the procedure, such as the intermediary check before drilling or implant
placement. A prime example is 2D/3D registration, a key enabler of image-based
surgical navigation that aligns intraoperative 2D images with preoperative 3D
data. Although registration algorithms have advanced significantly, they
occasionally yield inaccurate results. Because even small misalignments can
lead to revision surgery or irreversible surgical errors, there is a critical
need for robust quality assurance. Current visualization-based strategies alone
have been found insufficient to enable humans to reliably detect 2D/3D
registration misalignments. In response, we propose the first artificial
intelligence (AI) framework trained specifically for 2D/3D registration quality
verification, augmented by explainability features that clarify the model's
decision-making. Our explainable AI (XAI) approach aims to enhance informed
decision-making for human operators by providing a second opinion together with
a rationale behind it. Through algorithm-centric and human-centered
evaluations, we systematically compare four conditions: AI-only, human-only,
human-AI, and human-XAI. Our findings reveal that while explainability features
modestly improve user trust and willingness to override AI errors, they do not
exceed the standalone AI in aggregate performance. Nevertheless, future work
extending both the algorithmic design and the human-XAI collaboration elements
holds promise for more robust quality assurance of 2D/3D registration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Remix<span class="highlight-title">Fusion</span>: Residual-based Mixed Representation for Large-scale Online
  RGB-D Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqing Lan, Chenyang Zhu, Shuaifeng Zhi, Jiazhao Zhang, Zhoufeng Wang, Renjiao Yi, Yijie Wang, Kai Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The introduction of the neural implicit representation has notably propelled
the advancement of online dense reconstruction techniques. Compared to
traditional explicit representations, such as TSDF, it improves the mapping
completeness and memory efficiency. However, the lack of reconstruction details
and the time-consuming learning of neural representations hinder the widespread
application of neural-based methods to large-scale online reconstruction. We
introduce RemixFusion, a novel residual-based mixed representation for scene
reconstruction and camera pose estimation dedicated to high-quality and
large-scale online RGB-D reconstruction. In particular, we propose a
residual-based map representation comprised of an explicit coarse TSDF grid and
an implicit neural module that produces residuals representing fine-grained
details to be added to the coarse grid. Such mixed representation allows for
detail-rich reconstruction with bounded time and memory budget, contrasting
with the overly-smoothed results by the purely implicit representations, thus
paving the way for high-quality camera tracking. Furthermore, we extend the
residual-based representation to handle multi-frame joint pose optimization via
bundle adjustment (BA). In contrast to the existing methods, which optimize
poses directly, we opt to optimize pose changes. Combined with a novel
technique for adaptive gradient amplification, our method attains better
optimization convergence and global optimality. Furthermore, we adopt a local
moving volume to factorize the mixed scene representation with a
divide-and-conquer design to facilitate efficient online learning in our
residual-based framework. Extensive experiments demonstrate that our method
surpasses all state-of-the-art ones, including those based either on explicit
or implicit representations, in terms of the accuracy of both mapping and
tracking on large-scale scenes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual-branch Prompting for Multimodal Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Wang, Zhendong Yang, Liansong Zong, Xiaobo Zhang, Dexian Wang, Ji Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Machine Translation (MMT) typically enhances text-only translation
by incorporating aligned visual features. Despite the remarkable progress,
state-of-the-art MMT approaches often rely on paired image-text inputs at
inference and are sensitive to irrelevant visual noise, which limits their
robustness and practical applicability. To address these issues, we propose
D2P-MMT, a diffusion-based dual-branch prompting framework for robust
vision-guided translation. Specifically, D2P-MMT requires only the source text
and a reconstructed image generated by a pre-trained diffusion model, which
naturally filters out distracting visual details while preserving semantic
cues. During training, the model jointly learns from both authentic and
reconstructed images using a dual-branch prompting strategy, encouraging rich
cross-modal interactions. To bridge the modality gap and mitigate
training-inference discrepancies, we introduce a distributional alignment loss
that enforces consistency between the output distributions of the two branches.
Extensive experiments on the Multi30K dataset demonstrate that D2P-MMT achieves
superior translation performance compared to existing state-of-the-art
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Scan to Action: Leveraging Realistic Scans for <span class="highlight-title">Embodied</span> Scene
  Understanding <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna-Maria Halacheva, Jan-Nico Zaech, Sombit Dey, Luc Van Gool, Danda Pani Paudel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world 3D scene-level scans offer realism and can enable better
real-world generalizability for downstream applications. However, challenges
such as data volume, diverse annotation formats, and tool compatibility limit
their use. This paper demonstrates a methodology to effectively leverage these
scans and their annotations. We propose a unified annotation integration using
USD, with application-specific USD flavors. We identify challenges in utilizing
holistic real-world scan datasets and present mitigation strategies. The
efficacy of our approach is demonstrated through two downstream applications:
LLM-based scene editing, enabling effective LLM understanding and adaptation of
the data (80% success), and robotic simulation, achieving an 87% success rate
in policy learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the OpenSUN3D Workshop, CVPR 2025. This workshop paper is
  not included in the official CVPR proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Ray Search Procedure of Hard-label Attacks with Transfer-based
  Priors <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Ma, Xinjie Xu, Shuyu Cheng, Qi Xuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the most practical and challenging types of black-box adversarial
attacks is the hard-label attack, where only the top-1 predicted label is
available. One effective approach is to search for the optimal ray direction
from the benign image that minimizes the $\ell_p$-norm distance to the
adversarial region. The unique advantage of this approach is that it transforms
the hard-label attack into a continuous optimization problem. The objective
function value is the ray's radius, which can be obtained via binary search at
a high query cost. Existing methods use a "sign trick" in gradient estimation
to reduce the number of queries. In this paper, we theoretically analyze the
quality of this gradient estimation and propose a novel prior-guided approach
to improve ray search efficiency both theoretically and empirically.
Specifically, we utilize the transfer-based priors from surrogate models, and
our gradient estimators appropriately integrate them by approximating the
projection of the true gradient onto the subspace spanned by these priors and
random directions, in a query-efficient manner. We theoretically derive the
expected cosine similarities between the obtained gradient estimators and the
true gradient, and demonstrate the improvement achieved by incorporating
priors. Extensive experiments on the ImageNet and CIFAR-10 datasets show that
our approach significantly outperforms 11 state-of-the-art methods in terms of
query efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2025 (Spotlight paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An h-space Based Adversarial Attack for Protection Against Few-shot
  Personalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xide Xu, Sandesh Kamath, Muhammad Atif Butt, Bogdan Raducanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The versatility of diffusion models in generating customized images from few
samples raises significant privacy concerns, particularly regarding
unauthorized modifications of private content. This concerning issue has
renewed the efforts in developing protection mechanisms based on adversarial
attacks, which generate effective perturbations to poison diffusion models. Our
work is motivated by the observation that these models exhibit a high degree of
abstraction within their semantic latent space (`h-space'), which encodes
critical high-level features for generating coherent and meaningful content. In
this paper, we propose a novel anti-customization approach, called HAAD
(h-space based Adversarial Attack for Diffusion models), that leverages
adversarial attacks to craft perturbations based on the h-space that can
efficiently degrade the image generation process. Building upon HAAD, we
further introduce a more efficient variant, HAAD-KV, that constructs
perturbations solely based on the KV parameters of the h-space. This strategy
offers a stronger protection, that is computationally less expensive. Despite
their simplicity, our methods outperform state-of-the-art adversarial attacks,
highlighting their effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 15 figures. Accepted by ACM Multimedia 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration
  Through Clinical Cognitive Chain Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyao Liu, Diping Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) demonstrate significant potential in
the field of medical diagnosis. However, they face critical challenges in
specialized domains such as ophthalmology, particularly the fragmentation of
annotation granularity and inconsistencies in clinical reasoning logic, which
hinder precise cross-modal understanding. This paper introduces FundusExpert,
an ophthalmology-specific MLLM with integrated positioning-diagnosis reasoning
capabilities, along with FundusGen, a dataset constructed through the
intelligent Fundus-Engine system. Fundus-Engine automates localization and
leverages MLLM-based semantic expansion to integrate global disease
classification, local object detection, and fine-grained feature analysis
within a single fundus image. Additionally, by constructing a clinically
aligned cognitive chain, it guides the model to generate interpretable
reasoning paths. FundusExpert, fine-tuned with instruction data from FundusGen,
achieves the best performance in ophthalmic question-answering tasks,
surpassing the average accuracy of the 40B MedRegA by 26.6%. It also excels in
zero-shot report generation tasks, achieving a clinical consistency of 77.0%,
significantly outperforming GPT-4o's 47.6%. Furthermore, we reveal a scaling
law between data quality and model capability ($L \propto N^{0.068}$),
demonstrating that the cognitive alignment annotations in FundusGen enhance
data utilization efficiency. By integrating region-level localization with
diagnostic reasoning chains, our work develops a scalable, clinically-aligned
MLLM and explores a pathway toward bridging the visual-language gap in specific
MLLMs. Our project can be found at https://github.com/MeteorElf/FundusExpert.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-modal Multi-task Pre-training for Improved <span class="highlight-title">Point Cloud</span>
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liwen Liu, Weidong Yang, Lipeng Ma, Ben Fei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in multi-modal pre-training methods have shown promising
effectiveness in learning 3D representations by aligning multi-modal features
between 3D shapes and their corresponding 2D counterparts. However, existing
multi-modal pre-training frameworks primarily rely on a single pre-training
task to gather multi-modal data in 3D applications. This limitation prevents
the models from obtaining the abundant information provided by other relevant
tasks, which can hinder their performance in downstream tasks, particularly in
complex and diverse domains. In order to tackle this issue, we propose MMPT, a
Multi-modal Multi-task Pre-training framework designed to enhance point cloud
understanding. Specifically, three pre-training tasks are devised: (i)
Token-level reconstruction (TLR) aims to recover masked point tokens, endowing
the model with representative learning abilities. (ii) Point-level
reconstruction (PLR) is integrated to predict the masked point positions
directly, and the reconstructed point cloud can be considered as a transformed
point cloud used in the subsequent task. (iii) Multi-modal contrastive learning
(MCL) combines feature correspondences within and across modalities, thus
assembling a rich learning signal from both 3D point cloud and 2D image
modalities in a self-supervised manner. Moreover, this framework operates
without requiring any 3D annotations, making it scalable for use with large
datasets. The trained encoder can be effectively transferred to various
downstream tasks. To demonstrate its effectiveness, we evaluated its
performance compared to state-of-the-art methods in various discriminant and
generative applications under widely-used benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STQE: Spatial-Temporal Quality Enhancement for G-PCC Compressed <span class="highlight-title">Dynamic</span>
  <span class="highlight-title">Point Cloud</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tian Guo, Hui Yuan, Xiaolong Mao, Shiqi Jiang, Raouf Hamzaoui, Sam Kwong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Very few studies have addressed quality enhancement for compressed dynamic
point clouds. In particular, the effective exploitation of spatial-temporal
correlations between point cloud frames remains largely unexplored. Addressing
this gap, we propose a spatial-temporal attribute quality enhancement (STQE)
network that exploits both spatial and temporal correlations to improve the
visual quality of G-PCC compressed dynamic point clouds. Our contributions
include a recoloring-based motion compensation module that remaps reference
attribute information to the current frame geometry to achieve precise
inter-frame geometric alignment, a channel-aware temporal attention module that
dynamically highlights relevant regions across bidirectional reference frames,
a Gaussian-guided neighborhood feature aggregation module that efficiently
captures spatial dependencies between geometry and color attributes, and a
joint loss function based on the Pearson correlation coefficient, designed to
alleviate over-smoothing effects typical of point-wise mean squared error
optimization. When applied to the latest G-PCC test model, STQE achieved
improvements of 0.855 dB, 0.682 dB, and 0.828 dB in delta PSNR, with
Bj{\o}ntegaard Delta rate (BD-rate) reductions of -25.2%, -31.6%, and -32.5%
for the Luma, Cb, and Cr components, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InstructVLA: Vision-Language-Action Instruction Tuning from
  Understanding to Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Yang, Hao Li, Yilun Chen, Bin Wang, Yang Tian, Tai Wang, Hanqing Wang, Feng Zhao, Yiyi Liao, Jiangmiao Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To operate effectively in the real world, robots must integrate multimodal
reasoning with precise action generation. However, existing
vision-language-action (VLA) models often sacrifice one for the other, narrow
their abilities to task-specific manipulation data, and suffer catastrophic
forgetting of pre-trained vision-language capabilities. To bridge this gap, we
introduce InstructVLA, an end-to-end VLA model that preserves the flexible
reasoning of large vision-language models (VLMs) while delivering leading
manipulation performance. InstructVLA introduces a novel training paradigm,
Vision-Language-Action Instruction Tuning (VLA-IT), which employs multimodal
training with mixture-of-experts adaptation to jointly optimize textual
reasoning and action generation on both standard VLM corpora and a curated
650K-sample VLA-IT dataset. On in-domain SimplerEnv tasks, InstructVLA achieves
30.5% improvement over SpatialVLA. To evaluate generalization, we introduce
SimplerEnv-Instruct, an 80-task benchmark requiring closed-loop control and
high-level instruction understanding, where it outperforms a fine-tuned OpenVLA
by 92% and an action expert aided by GPT-4o by 29%. Additionally, InstructVLA
surpasses baseline VLMs on multimodal tasks and exhibits inference-time scaling
by leveraging textual reasoning to boost manipulation performance in both
simulated and real-world settings. These results demonstrate InstructVLA's
potential for bridging intuitive and steerable human-robot interaction with
efficient policy learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ URPO: A Unified Reward & Policy <span class="highlight-title">Optimization</span> Framework for Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songshuo Lu, Hua Wang, Zhi Chen, Yaohua Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale alignment pipelines typically pair a policy model with a
separately trained reward model whose parameters remain frozen during
reinforcement learning (RL). This separation creates a complex,
resource-intensive pipeline and suffers from a performance ceiling due to a
static reward signal. We propose a novel framework, Unified Reward & Policy
Optimization (URPO), that unifies instruction-following ("player") and reward
modeling ("referee") within a single model and a single training phase. Our
method recasts all alignment data-including preference pairs, verifiable
reasoning, and open-ended instructions-into a unified generative format
optimized by a single Group-Relative Policy Optimization (GRPO) loop. This
enables the model to learn from ground-truth preferences and verifiable logic
while simultaneously generating its own rewards for open-ended tasks.
Experiments on the Qwen2.5-7B model demonstrate URPO's superiority. Our unified
model significantly outperforms a strong baseline using a separate generative
reward model, boosting the instruction-following score on AlpacaEval from 42.24
to 44.84 and the composite reasoning average from 32.66 to 35.66. Furthermore,
URPO cultivates a superior internal evaluator as a byproduct of training,
achieving a RewardBench score of 85.15 and surpassing the dedicated reward
model it replaces (83.55). By eliminating the need for a separate reward model
and fostering a co-evolutionary dynamic between generation and evaluation, URPO
presents a simpler, more efficient, and more effective path towards robustly
aligned language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating Parallel Dif<span class="highlight-title">fusion</span> Model Serving with Residual Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajun Luo, Yicheng Xiao, Jianru Xu, Yangxiu You, Rongwei Lu, Chen Tang, Jingyan Jiang, Zhi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models produce realistic images and videos but require substantial
computational resources, necessitating multi-accelerator parallelism for
real-time deployment. However, parallel inference introduces significant
communication overhead from exchanging large activations between devices,
limiting efficiency and scalability. We present CompactFusion, a compression
framework that significantly reduces communication while preserving generation
quality. Our key observation is that diffusion activations exhibit strong
temporal redundancy-adjacent steps produce highly similar activations,
saturating bandwidth with near-duplicate data carrying little new information.
To address this inefficiency, we seek a more compact representation that
encodes only the essential information. CompactFusion achieves this via
Residual Compression that transmits only compressed residuals (step-wise
activation differences). Based on empirical analysis and theoretical
justification, we show that it effectively removes redundant data, enabling
substantial data reduction while maintaining high fidelity. We also integrate
lightweight error feedback to prevent error accumulation. CompactFusion
establishes a new paradigm for parallel diffusion inference, delivering lower
latency and significantly higher generation quality than prior methods. On
4xL20, it achieves 3.0x speedup while greatly improving fidelity. It also
uniquely supports communication-heavy strategies like sequence parallelism on
slow networks, achieving 6.7x speedup over prior overlap-based method.
CompactFusion applies broadly across diffusion models and parallel settings,
and integrates easily without requiring pipeline rework. Portable
implementation demonstrated on xDiT is publicly available at
https://github.com/Cobalt-27/CompactFusion
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Illicit object <span class="highlight-title">detection</span> in X-ray imaging using deep learning
  techniques: A comparative <span class="highlight-title">evaluation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jorgen Cani, Christos Diou, Spyridon Evangelatos, Vasileios Argyriou, Panagiotis Radoglou-Grammatikis, Panagiotis Sarigiannidis, Iraklis Varlamis, Georgios Th. Papadopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated X-ray inspection is crucial for efficient and unobtrusive security
screening in various public settings. However, challenges such as object
occlusion, variations in the physical properties of items, diversity in X-ray
scanning devices, and limited training data hinder accurate and reliable
detection of illicit items. Despite the large body of research in the field,
reported experimental evaluations are often incomplete, with frequently
conflicting outcomes. To shed light on the research landscape and facilitate
further research, a systematic, detailed, and thorough comparative evaluation
of recent Deep Learning (DL)-based methods for X-ray object detection is
conducted. For this, a comprehensive evaluation framework is developed,
composed of: a) Six recent, large-scale, and widely used public datasets for
X-ray illicit item detection (OPIXray, CLCXray, SIXray, EDS, HiXray, and
PIDray), b) Ten different state-of-the-art object detection schemes covering
all main categories in the literature, including generic Convolutional Neural
Network (CNN), custom CNN, generic transformer, and hybrid CNN-transformer
architectures, and c) Various detection (mAP50 and mAP50:95) and
time/computational-complexity (inference time (ms), parameter size (M), and
computational load (GFLOPS)) metrics. A thorough analysis of the results leads
to critical observations and insights, emphasizing key aspects such as: a)
Overall behavior of the object detection schemes, b) Object-level detection
performance, c) Dataset-specific observations, and d) Time efficiency and
computational complexity analysis. To support reproducibility of the reported
experimental results, the evaluation code and model weights are made publicly
available at https://github.com/jgenc/xray-comparative-evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DNT: a Deeply Normalized Transformer that can be trained by Momentum SGD 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianbiao Qi, Marco Chen, Wenjie Xiao, Jiaquan Ye, Yelin He, Chun-Guang Li, Zhouchen Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have become the de facto backbone of modern deep learning, yet
their training typically demands an advanced optimizer with adaptive learning
rate like AdamW, rather than a momentum SGDW (mSGDW). Previous works show that
it is mainly due to a heavy-tailed distribution of the gradients. In this
paper, we introduce a Deeply Normalized Transformer (DNT), which is
meticulously engineered to overcome this limitation enabling seamless training
with vanilla mSGDW while yielding comparable performance to the Transformers
trained via AdamW. To be specific, in DNT, we strategically integrate
normalization techniques at proper positions in the Transformers to effectively
modulate the Jacobian matrices of each layer, balance the influence of weights,
activations, and their interactions, and thus enable the distributions of
gradients concentrated. We provide both theoretical justifications of the
normalization technique used in our DNT and extensive empirical evaluation on
two popular Transformer architectures to validate that: a) DNT outperforms its
counterparts (\ie, ViT and GPT), and b) DNT can be effectively trained with
vanilla mSGDW.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We have introduced a novel architecture, Deeply Normalized
  Transformer (DNT), which enables efficient training with vanilla momentum
  SGDW (mSGDW), achieving performance on par with AdamW-optimized Transformers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DFDNet: <span class="highlight-title">Dynamic</span> Frequency-Guided De-Flare Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minglong Xue, Aoxiang Ning, Shivakumara Palaiahnakote, Mingliang Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Strong light sources in nighttime photography frequently produce flares in
images, significantly degrading visual quality and impacting the performance of
downstream tasks. While some progress has been made, existing methods continue
to struggle with removing large-scale flare artifacts and repairing structural
damage in regions near the light source. We observe that these challenging
flare artifacts exhibit more significant discrepancies from the reference
images in the frequency domain compared to the spatial domain. Therefore, this
paper presents a novel dynamic frequency-guided deflare network (DFDNet) that
decouples content information from flare artifacts in the frequency domain,
effectively removing large-scale flare artifacts. Specifically, DFDNet consists
mainly of a global dynamic frequency-domain guidance (GDFG) module and a local
detail guidance module (LDGM). The GDFG module guides the network to perceive
the frequency characteristics of flare artifacts by dynamically optimizing
global frequency domain features, effectively separating flare information from
content information. Additionally, we design an LDGM via a contrastive learning
strategy that aligns the local features of the light source with the reference
image, reduces local detail damage from flare removal, and improves
fine-grained image restoration. The experimental results demonstrate that the
proposed method outperforms existing state-of-the-art methods in terms of
performance. The code is available at
\href{https://github.com/AXNing/DFDNet}{https://github.com/AXNing/DFDNet}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Unsupervised</span> anomaly <span class="highlight-title">detection</span> using Bayesian flow networks: application
  to brain FDG PET in the context of Alzheimer's disease 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugues Roy, Reuben Dorent, Ninon Burgos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised anomaly detection (UAD) plays a crucial role in neuroimaging for
identifying deviations from healthy subject data and thus facilitating the
diagnosis of neurological disorders. In this work, we focus on Bayesian flow
networks (BFNs), a novel class of generative models, which have not yet been
applied to medical imaging or anomaly detection. BFNs combine the strength of
diffusion frameworks and Bayesian inference. We introduce AnoBFN, an extension
of BFNs for UAD, designed to: i) perform conditional image generation under
high levels of spatially correlated noise, and ii) preserve subject specificity
by incorporating a recursive feedback from the input image throughout the
generative process. We evaluate AnoBFN on the challenging task of Alzheimer's
disease-related anomaly detection in FDG PET images. Our approach outperforms
other state-of-the-art methods based on VAEs (beta-VAE), GANs (f-AnoGAN), and
diffusion models (AnoDDPM), demonstrating its effectiveness at detecting
anomalies while reducing false positive rates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SRMambaV2: Biomimetic Attention for Sparse <span class="highlight-title">Point Cloud</span> Upsampling in
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuang Chen, Xiaolin Qin, Jing Hu, Wenyi Ge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Upsampling LiDAR point clouds in autonomous driving scenarios remains a
significant challenge due to the inherent sparsity and complex 3D structures of
the data. Recent studies have attempted to address this problem by converting
the complex 3D spatial scenes into 2D image super-resolution tasks. However,
due to the sparse and blurry feature representation of range images, accurately
reconstructing detailed and complex spatial topologies remains a major
difficulty. To tackle this, we propose a novel sparse point cloud upsampling
method named SRMambaV2, which enhances the upsampling accuracy in long-range
sparse regions while preserving the overall geometric reconstruction quality.
Specifically, inspired by human driver visual perception, we design a
biomimetic 2D selective scanning self-attention (2DSSA) mechanism to model the
feature distribution in distant sparse areas. Meanwhile, we introduce a
dual-branch network architecture to enhance the representation of sparse
features. In addition, we introduce a progressive adaptive loss (PAL) function
to further refine the reconstruction of fine-grained details during the
upsampling process. Experimental results demonstrate that SRMambaV2 achieves
superior performance in both qualitative and quantitative evaluations,
highlighting its effectiveness and practical value in automotive sparse point
cloud upsampling tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probing Vision-Language Understanding through the Visual Entailment
  Task: promises and pitfalls 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17467v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17467v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elena Pitta, Tom Kouwenhoven, Tessa Verhoef
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the extent to which the Visual Entailment (VE) task
serves as a reliable probe of vision-language understanding in multimodal
language models, using the LLaMA 3.2 11B Vision model as a test case. Beyond
reporting performance metrics, we aim to interpret what these results reveal
about the underlying possibilities and limitations of the VE task. We conduct a
series of experiments across zero-shot, few-shot, and fine-tuning settings,
exploring how factors such as prompt design, the number and order of in-context
examples and access to visual information might affect VE performance. To
further probe the reasoning processes of the model, we used explanation-based
evaluations. Results indicate that three-shot inference outperforms the
zero-shot baselines. However, additional examples introduce more noise than
they provide benefits. Additionally, the order of the labels in the prompt is a
critical factor that influences the predictions. In the absence of visual
information, the model has a strong tendency to hallucinate and imagine
content, raising questions about the model's over-reliance on linguistic
priors. Fine-tuning yields strong results, achieving an accuracy of 83.3% on
the e-SNLI-VE dataset and outperforming the state-of-the-art OFA-X model.
Additionally, the explanation evaluation demonstrates that the fine-tuned model
provides semantically meaningful explanations similar to those of humans, with
a BERTScore F1-score of 89.2%. We do, however, find comparable BERTScore
results in experiments with limited vision, questioning the visual grounding of
this task. Overall, our results highlight both the utility and limitations of
VE as a diagnostic task for vision-language understanding and point to
directions for refining multimodal evaluation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LUHME: 2nd Workshop on Language Understanding in the Human-Machine
  Era</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> ERMV: Editing 4D <span class="highlight-title">Robot</span>ic <span class="highlight-title">Multi-view</span> images to enhance <span class="highlight-title">embodied</span> agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Nie, Guangming Wang, Zhe Lie, <span class="highlight-author">Hesheng Wang</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot imitation learning relies on 4D multi-view sequential images. However,
the high cost of data collection and the scarcity of high-quality data severely
constrain the generalization and application of embodied intelligence policies
like Vision-Language-Action (VLA) models. Data augmentation is a powerful
strategy to overcome data scarcity, but methods for editing 4D multi-view
sequential images for manipulation tasks are currently lacking. Thus, we
propose ERMV (Editing Robotic Multi-View 4D data), a novel data augmentation
framework that efficiently edits an entire multi-view sequence based on
single-frame editing and robot state conditions. This task presents three core
challenges: (1) maintaining geometric and appearance consistency across dynamic
views and long time horizons; (2) expanding the working window with low
computational costs; and (3) ensuring the semantic integrity of critical
objects like the robot arm. ERMV addresses these challenges through a series of
innovations. First, to ensure spatio-temporal consistency in motion blur, we
introduce a novel Epipolar Motion-Aware Attention (EMA-Attn) mechanism that
learns pixel shift caused by movement before applying geometric constraints.
Second, to maximize the editing working window, ERMV pioneers a Sparse
Spatio-Temporal (STT) module, which decouples the temporal and spatial views
and remodels a single-frame multi-view problem through sparse sampling of the
views to reduce computational demands. Third, to alleviate error accumulation,
we incorporate a feedback intervention Mechanism, which uses a Multimodal Large
Language Model (MLLM) to check editing inconsistencies and request targeted
expert guidance only when necessary. Extensive experiments demonstrate that
ERMV-augmented data significantly boosts the robustness and generalization of
VLA models in both simulated and real-world environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Dynamic</span> Scoring with Enhanced Semantics for Training-Free Human-Object
  Interaction <span class="highlight-title">Detection</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Tonini, Lorenzo Vaquero, Alessandro Conti, Cigdem Beyan, Elisa Ricci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-Object Interaction (HOI) detection aims to identify humans and objects
within images and interpret their interactions. Existing HOI methods rely
heavily on large datasets with manual annotations to learn interactions from
visual cues. These annotations are labor-intensive to create, prone to
inconsistency, and limit scalability to new domains and rare interactions. We
argue that recent advances in Vision-Language Models (VLMs) offer untapped
potential, particularly in enhancing interaction representation. While prior
work has injected such potential and even proposed training-free methods, there
remain key gaps. Consequently, we propose a novel training-free HOI detection
framework for Dynamic Scoring with enhanced semantics (DYSCO) that effectively
utilizes textual and visual interaction representations within a multimodal
registry, enabling robust and nuanced interaction understanding. This registry
incorporates a small set of visual cues and uses innovative interaction
signatures to improve the semantic alignment of verbs, facilitating effective
generalization to rare interactions. Additionally, we propose a unique
multi-head attention mechanism that adaptively weights the contributions of the
visual and textual features. Experimental results demonstrate that our DYSCO
surpasses training-free state-of-the-art models and is competitive with
training-based approaches, particularly excelling in rare interactions. Code is
available at https://github.com/francescotonini/dysco.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM Multimedia 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VLM-Guided Visual Place Recognition for Planet-Scale Geo-<span class="highlight-title">Localization</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sania Waheed, Na Min An, Michael Milford, Sarvapali D. Ramchurn, Shoaib Ehsan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geo-localization from a single image at planet scale (essentially an advanced
or extreme version of the kidnapped robot problem) is a fundamental and
challenging task in applications such as navigation, autonomous driving and
disaster response due to the vast diversity of locations, environmental
conditions, and scene variations. Traditional retrieval-based methods for
geo-localization struggle with scalability and perceptual aliasing, while
classification-based approaches lack generalization and require extensive
training data. Recent advances in vision-language models (VLMs) offer a
promising alternative by leveraging contextual understanding and reasoning.
However, while VLMs achieve high accuracy, they are often prone to
hallucinations and lack interpretability, making them unreliable as standalone
solutions. In this work, we propose a novel hybrid geo-localization framework
that combines the strengths of VLMs with retrieval-based visual place
recognition (VPR) methods. Our approach first leverages a VLM to generate a
prior, effectively guiding and constraining the retrieval search space. We then
employ a retrieval step, followed by a re-ranking mechanism that selects the
most geographically plausible matches based on feature similarity and proximity
to the initially estimated coordinates. We evaluate our approach on multiple
geo-localization benchmarks and show that it consistently outperforms prior
state-of-the-art methods, particularly at street (up to 4.51%) and city level
(up to 13.52%). Our results demonstrate that VLM-generated geographic priors in
combination with VPR lead to scalable, robust, and accurate geo-localization
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Dynamic</span>-DINO: Fine-Grained Mixture of Experts Tuning for Real-time
  Open-Vocabulary Object <span class="highlight-title">Detection</span> <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yehao Lu, Minghe Weng, Zekang Xiao, Rui Jiang, Wei Su, Guangcong Zheng, Ping Lu, Xi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Mixture of Experts (MoE) architecture has excelled in Large
Vision-Language Models (LVLMs), yet its potential in real-time open-vocabulary
object detectors, which also leverage large-scale vision-language datasets but
smaller models, remains unexplored. This work investigates this domain,
revealing intriguing insights. In the shallow layers, experts tend to cooperate
with diverse peers to expand the search space. While in the deeper layers,
fixed collaborative structures emerge, where each expert maintains 2-3 fixed
partners and distinct expert combinations are specialized in processing
specific patterns. Concretely, we propose Dynamic-DINO, which extends Grounding
DINO 1.5 Edge from a dense model to a dynamic inference framework via an
efficient MoE-Tuning strategy. Additionally, we design a granularity
decomposition mechanism to decompose the Feed-Forward Network (FFN) of base
model into multiple smaller expert networks, expanding the subnet search space.
To prevent performance degradation at the start of fine-tuning, we further
propose a pre-trained weight allocation strategy for the experts, coupled with
a specific router initialization. During inference, only the input-relevant
experts are activated to form a compact subnet. Experiments show that,
pretrained with merely 1.56M open-source data, Dynamic-DINO outperforms
Grounding DINO 1.5 Edge, pretrained on the private Grounding20M dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAPRI-CT: Causal Analysis and Predictive Reasoning for Image Quality
  <span class="highlight-title">Optimization</span> in Computed Tomo<span class="highlight-title">graph</span>y 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sneha George Gnanakalavathy, Hairil Abdul Razak, Robert Meertens, Jonathan E. Fieldsend, Xujiong Ye, Mohammed M. Abdelsamea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In computed tomography (CT), achieving high image quality while minimizing
radiation exposure remains a key clinical challenge. This paper presents
CAPRI-CT, a novel causal-aware deep learning framework for Causal Analysis and
Predictive Reasoning for Image Quality Optimization in CT imaging. CAPRI-CT
integrates image data with acquisition metadata (such as tube voltage, tube
current, and contrast agent types) to model the underlying causal relationships
that influence image quality. An ensemble of Variational Autoencoders (VAEs) is
employed to extract meaningful features and generate causal representations
from observational data, including CT images and associated imaging parameters.
These input features are fused to predict the Signal-to-Noise Ratio (SNR) and
support counterfactual inference, enabling what-if simulations, such as changes
in contrast agents (types and concentrations) or scan parameters. CAPRI-CT is
trained and validated using an ensemble learning approach, achieving strong
predictive performance. By facilitating both prediction and interpretability,
CAPRI-CT provides actionable insights that could help radiologists and
technicians design more efficient CT protocols without repeated physical scans.
The source code and dataset are publicly available at
https://github.com/SnehaGeorge22/capri-ct.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Content-based 3D Image Retrieval and a ColBERT-inspired Re-ranking for
  Tumor Flagging and Staging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farnaz Khun Jush, Steffen Vogler, Matthias Lenga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing volume of medical images poses challenges for radiologists in
retrieving relevant cases. Content-based image retrieval (CBIR) systems offer
potential for efficient access to similar cases, yet lack standardized
evaluation and comprehensive studies. Building on prior studies for tumor
characterization via CBIR, this study advances CBIR research for volumetric
medical images through three key contributions: (1) a framework eliminating
reliance on pre-segmented data and organ-specific datasets, aligning with large
and unstructured image archiving systems, i.e. PACS in clinical practice; (2)
introduction of C-MIR, a novel volumetric re-ranking method adapting ColBERT's
contextualized late interaction mechanism for 3D medical imaging; (3)
comprehensive evaluation across four tumor sites using three feature extractors
and three database configurations. Our evaluations highlight the significant
advantages of C-MIR. We demonstrate the successful adaptation of the late
interaction principle to volumetric medical images, enabling effective
context-aware re-ranking. A key finding is C-MIR's ability to effectively
localize the region of interest, eliminating the need for pre-segmentation of
datasets and offering a computationally efficient alternative to systems
relying on expensive data enrichment steps. C-MIR demonstrates promising
improvements in tumor flagging, achieving improved performance, particularly
for colon and lung tumors (p<0.05). C-MIR also shows potential for improving
tumor staging, warranting further exploration of its capabilities. Ultimately,
our work seeks to bridge the gap between advanced retrieval techniques and
their practical applications in healthcare, paving the way for improved
diagnostic processes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-based Human Pose <span class="highlight-title">Estimation</span> from a Single Moving RGB Camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayce Idil Aytekin, Chuqiao Li, Diogo Luvizon, Rishabh Dabral, Martin Oswald, Marc Habermann, Christian Theobalt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most monocular and physics-based human pose tracking methods, while achieving
state-of-the-art results, suffer from artifacts when the scene does not have a
strictly flat ground plane or when the camera is moving. Moreover, these
methods are often evaluated on in-the-wild real world videos without
ground-truth data or on synthetic datasets, which fail to model the real world
light transport, camera motion, and pose-induced appearance and geometry
changes. To tackle these two problems, we introduce MoviCam, the first
non-synthetic dataset containing ground-truth camera trajectories of a
dynamically moving monocular RGB camera, scene geometry, and 3D human motion
with human-scene contact labels. Additionally, we propose PhysDynPose, a
physics-based method that incorporates scene geometry and physical constraints
for more accurate human motion tracking in case of camera motion and non-flat
scenes. More precisely, we use a state-of-the-art kinematics estimator to
obtain the human pose and a robust SLAM method to capture the dynamic camera
trajectory, enabling the recovery of the human pose in the world frame. We then
refine the kinematic pose estimate using our scene-aware physics optimizer.
From our new benchmark, we found that even state-of-the-art methods struggle
with this inherently challenging setting, i.e. a moving camera and non-planar
environments, while our method robustly estimates both human and camera poses
in world coordinates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HLFormer: Enhancing Partially Relevant Video Retrieval with Hyperbolic
  Learning <span class="chip">ICCV'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Jun, Wang Jinpeng, Tan Chaolei, Lian Niu, Chen Long, Zhang Min, Wang Yaowei, Xia Shu-Tao, Chen Bin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partially Relevant Video Retrieval (PRVR) addresses the critical challenge of
matching untrimmed videos with text queries describing only partial content.
Existing methods suffer from geometric distortion in Euclidean space that
sometimes misrepresents the intrinsic hierarchical structure of videos and
overlooks certain hierarchical semantics, ultimately leading to suboptimal
temporal modeling. To address this issue, we propose the first hyperbolic
modeling framework for PRVR, namely HLFormer, which leverages hyperbolic space
learning to compensate for the suboptimal hierarchical modeling capabilities of
Euclidean space. Specifically, HLFormer integrates the Lorentz Attention Block
and Euclidean Attention Block to encode video embeddings in hybrid spaces,
using the Mean-Guided Adaptive Interaction Module to dynamically fuse features.
Additionally, we introduce a Partial Order Preservation Loss to enforce "text <
video" hierarchy through Lorentzian cone constraints. This approach further
enhances cross-modal matching by reinforcing partial relevance between video
content and text queries. Extensive experiments show that HLFormer outperforms
state-of-the-art methods. Code is released at
https://github.com/lijun2005/ICCV25-HLFormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV'25. 13 pages, 6 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HiProbe-VAD: Video Anomaly <span class="highlight-title">Detection</span> via Hidden States Probing in
  Tuning-Free Multimodal LLMs <span class="chip">ACM MM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17394v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17394v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaolin Cai, Fan Li, Ziwei Zheng, Yanjun Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Anomaly Detection (VAD) aims to identify and locate deviations from
normal patterns in video sequences. Traditional methods often struggle with
substantial computational demands and a reliance on extensive labeled datasets,
thereby restricting their practical applicability. To address these
constraints, we propose HiProbe-VAD, a novel framework that leverages
pre-trained Multimodal Large Language Models (MLLMs) for VAD without requiring
fine-tuning. In this paper, we discover that the intermediate hidden states of
MLLMs contain information-rich representations, exhibiting higher sensitivity
and linear separability for anomalies compared to the output layer. To
capitalize on this, we propose a Dynamic Layer Saliency Probing (DLSP)
mechanism that intelligently identifies and extracts the most informative
hidden states from the optimal intermediate layer during the MLLMs reasoning.
Then a lightweight anomaly scorer and temporal localization module efficiently
detects anomalies using these extracted hidden states and finally generate
explanations. Experiments on the UCF-Crime and XD-Violence datasets demonstrate
that HiProbe-VAD outperforms existing training-free and most traditional
approaches. Furthermore, our framework exhibits remarkable cross-model
generalization capabilities in different MLLMs without any tuning, unlocking
the potential of pre-trained MLLMs for video anomaly detection and paving the
way for more practical and scalable solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> EndoGen: Conditional Autoregressive Endoscopic Video Generation <span class="chip">MICCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Liu, Hengyu Liu, Cheng Wang, Tian<span class="highlight-author">ming Liu</span>, Yixuan Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Endoscopic video generation is crucial for advancing medical imaging and
enhancing diagnostic capabilities. However, prior efforts in this field have
either focused on static images, lacking the dynamic context required for
practical applications, or have relied on unconditional generation that fails
to provide meaningful references for clinicians. Therefore, in this paper, we
propose the first conditional endoscopic video generation framework, namely
EndoGen. Specifically, we build an autoregressive model with a tailored
Spatiotemporal Grid-Frame Patterning (SGP) strategy. It reformulates the
learning of generating multiple frames as a grid-based image generation
pattern, which effectively capitalizes the inherent global dependency modeling
capabilities of autoregressive architectures. Furthermore, we propose a
Semantic-Aware Token Masking (SAT) mechanism, which enhances the model's
ability to produce rich and diverse content by selectively focusing on
semantically meaningful regions during the generation process. Through
extensive experiments, we demonstrate the effectiveness of our framework in
generating high-quality, conditionally guided endoscopic content, and improves
the performance of downstream task of polyp segmentation. Code released at
https://www.github.com/CUHK-AIM-Group/EndoGen.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Conditional Probability Framework for Compositional Zero-shot Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Wu, Qiuxia Lai, Hao Fang, Guo-Sen Xie, Yilong Yin, Xiankai Lu, Wenguan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compositional Zero-Shot Learning (CZSL) aims to recognize unseen combinations
of known objects and attributes by leveraging knowledge from previously seen
compositions. Traditional approaches primarily focus on disentangling
attributes and objects, treating them as independent entities during learning.
However, this assumption overlooks the semantic constraints and contextual
dependencies inside a composition. For example, certain attributes naturally
pair with specific objects (e.g., "striped" applies to "zebra" or "shirts" but
not "sky" or "water"), while the same attribute can manifest differently
depending on context (e.g., "young" in "young tree" vs. "young dog"). Thus,
capturing attribute-object interdependence remains a fundamental yet
long-ignored challenge in CZSL. In this paper, we adopt a Conditional
Probability Framework (CPF) to explicitly model attribute-object dependencies.
We decompose the probability of a composition into two components: the
likelihood of an object and the conditional likelihood of its attribute. To
enhance object feature learning, we incorporate textual descriptors to
highlight semantically relevant image regions. These enhanced object features
then guide attribute learning through a cross-attention mechanism, ensuring
better contextual alignment. By jointly optimizing object likelihood and
conditional attribute likelihood, our method effectively captures compositional
dependencies and generalizes well to unseen compositions. Extensive experiments
on multiple CZSL benchmarks demonstrate the superiority of our approach. Code
is available at here.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SFUOD: Source-Free Unknown Object <span class="highlight-title">Detection</span> <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keon-Hee Park, Seun-An Choe, Gyeong-Moon Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Source-free object detection adapts a detector pre-trained on a source domain
to an unlabeled target domain without requiring access to labeled source data.
While this setting is practical as it eliminates the need for the source
dataset during domain adaptation, it operates under the restrictive assumption
that only pre-defined objects from the source domain exist in the target
domain. This closed-set setting prevents the detector from detecting undefined
objects. To ease this assumption, we propose Source-Free Unknown Object
Detection (SFUOD), a novel scenario which enables the detector to not only
recognize known objects but also detect undefined objects as unknown objects.
To this end, we propose CollaPAUL (Collaborative tuning and Principal
Axis-based Unknown Labeling), a novel framework for SFUOD. Collaborative tuning
enhances knowledge adaptation by integrating target-dependent knowledge from
the auxiliary encoder with source-dependent knowledge from the pre-trained
detector through a cross-domain attention mechanism. Additionally, principal
axes-based unknown labeling assigns pseudo-labels to unknown objects by
estimating objectness via principal axes projection and confidence scores from
model predictions. The proposed CollaPAUL achieves state-of-the-art
performances on SFUOD benchmarks, and extensive experiments validate its
effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Spatial Diversity for Region-based Active Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17367v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17367v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lile Cai, Xun Xu, Lining Zhang, Chuan-Sheng Foo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art methods for semantic segmentation are based on deep neural
networks trained on large-scale labeled datasets. Acquiring such datasets would
incur large annotation costs, especially for dense pixel-level prediction tasks
like semantic segmentation. We consider region-based active learning as a
strategy to reduce annotation costs while maintaining high performance. In this
setting, batches of informative image regions instead of entire images are
selected for labeling. Importantly, we propose that enforcing local spatial
diversity is beneficial for active learning in this case, and to incorporate
spatial diversity along with the traditional active selection criterion, e.g.,
data sample uncertainty, in a unified optimization framework for region-based
active learning. We apply this framework to the Cityscapes and PASCAL VOC
datasets and demonstrate that the inclusion of spatial diversity effectively
improves the performance of uncertainty-based and feature diversity-based
active learning methods. Our framework achieves $95\%$ performance of fully
supervised methods with only $5-9\%$ of the labeled pixels, outperforming all
state-of-the-art region-based active learning methods for semantic
segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>published in IEEE Transactions on Image Processing, 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Active Learning for Semiconductor Defect <span class="highlight-title">Segmentation</span> <span class="chip">ICIP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17359v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17359v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lile Cai, Ramanpreet Singh Pahwa, Xun Xu, Jie Wang, Richard Chang, Lining Zhang, Chuan-Sheng Foo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of X-Ray microscopy (XRM) technology has enabled
non-destructive inspection of semiconductor structures for defect
identification. Deep learning is widely used as the state-of-the-art approach
to perform visual analysis tasks. However, deep learning based models require
large amount of annotated data to train. This can be time-consuming and
expensive to obtain especially for dense prediction tasks like semantic
segmentation. In this work, we explore active learning (AL) as a potential
solution to alleviate the annotation burden. We identify two unique challenges
when applying AL on semiconductor XRM scans: large domain shift and severe
class-imbalance. To address these challenges, we propose to perform contrastive
pretraining on the unlabelled data to obtain the initialization weights for
each AL cycle, and a rareness-aware acquisition function that favors the
selection of samples containing rare classes. We evaluate our method on a
semiconductor dataset that is compiled from XRM scans of high bandwidth memory
structures composed of logic and memory dies, and demonstrate that our method
achieves state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to ICIP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Active Learning for Label-Efficient Training of Semantic
  Neural Radiance <span class="highlight-title">Field</span> <span class="chip">ICME 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhe Zhu, Lile Cai, Kangkang Lu, Fayao Liu, Xulei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Field (NeRF) models are implicit neural scene representation
methods that offer unprecedented capabilities in novel view synthesis.
Semantically-aware NeRFs not only capture the shape and radiance of a scene,
but also encode semantic information of the scene. The training of
semantically-aware NeRFs typically requires pixel-level class labels, which can
be prohibitively expensive to collect. In this work, we explore active learning
as a potential solution to alleviate the annotation burden. We investigate
various design choices for active learning of semantically-aware NeRF,
including selection granularity and selection strategies. We further propose a
novel active learning strategy that takes into account 3D geometric constraints
in sample selection. Our experiments demonstrate that active learning can
effectively reduce the annotation cost of training semantically-aware NeRF,
achieving more than 2X reduction in annotation cost compared to random
sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICME 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Principled Multimodal Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohao Liu, Xiaobo Xia, See-Kiong Ng, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal representation learning seeks to create a unified representation
space by integrating diverse data modalities to improve multimodal
understanding. Traditional methods often depend on pairwise contrastive
learning, which relies on a predefined anchor modality, restricting alignment
across all modalities. Recent advances have investigated the simultaneous
alignment of multiple modalities, yet several challenges remain, such as
limitations imposed by fixed anchor points and instability arising from
optimizing the product of singular values. To address the challenges, in this
paper, we propose Principled Multimodal Representation Learning (PMRL), a novel
framework that achieves simultaneous alignment of multiple modalities without
anchor dependency in a more stable manner. Specifically, grounded in the
theoretical insight that full alignment corresponds to a rank-1 Gram matrix,
PMRL optimizes the dominant singular value of the representation matrix to
align modalities along a shared leading direction. We propose a softmax-based
loss function that treats singular values as logits to prioritize the largest
singular value. Besides, instance-wise contrastive regularization on the
leading eigenvectors maintains inter-instance separability and prevents
representation collapse. Extensive experiments across diverse tasks demonstrate
PMRL's superiority compared to baseline methods. The source code will be
publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 9 figures, 10 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeMo++: Motion Decoupling for Autonomous Driving <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17342v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17342v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bozhou Zhang, Nan Song, Xiatian Zhu, Li Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion forecasting and planning are tasked with estimating the trajectories
of traffic agents and the ego vehicle, respectively, to ensure the safety and
efficiency of autonomous driving systems in dynamically changing environments.
State-of-the-art methods typically adopt a one-query-one-trajectory paradigm,
where each query corresponds to a unique trajectory for predicting multi-mode
trajectories. While this paradigm can produce diverse motion intentions, it
often falls short in modeling the intricate spatiotemporal evolution of
trajectories, which can lead to collisions or suboptimal outcomes. To overcome
this limitation, we propose DeMo++, a framework that decouples motion
estimation into two distinct components: holistic motion intentions to capture
the diverse potential directions of movement, and fine spatiotemporal states to
track the agent's dynamic progress within the scene and enable a
self-refinement capability. Further, we introduce a cross-scene trajectory
interaction mechanism to explore the relationships between motions in adjacent
scenes. This allows DeMo++ to comprehensively model both the diversity of
motion intentions and the spatiotemporal evolution of each trajectory. To
effectively implement this framework, we developed a hybrid model combining
Attention and Mamba. This architecture leverages the strengths of both
mechanisms for efficient scene information aggregation and precise trajectory
state sequence modeling. Extensive experiments demonstrate that DeMo++ achieves
state-of-the-art performance across various benchmarks, including motion
forecasting (Argoverse 2 and nuScenes), motion planning (nuPlan), and
end-to-end planning (NAVSIM).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Journal extension of NeurIPS 2024. arXiv admin note: substantial text
  overlap with arXiv:2410.05982</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TransLPRNet: Lite Vision-Language Network for Single/Dual-line Chinese
  License Plate Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangzhu Xu, Zhi Ke, Pengcheng Zuo, Bangjun Lei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  License plate recognition in open environments is widely applicable across
various domains; however, the diversity of license plate types and imaging
conditions presents significant challenges. To address the limitations
encountered by CNN and CRNN-based approaches in license plate recognition, this
paper proposes a unified solution that integrates a lightweight visual encoder
with a text decoder, within a pre-training framework tailored for single and
double-line Chinese license plates. To mitigate the scarcity of double-line
license plate datasets, we constructed a single/double-line license plate
dataset by synthesizing images, applying texture mapping onto real scenes, and
blending them with authentic license plate images. Furthermore, to enhance the
system's recognition accuracy, we introduce a perspective correction network
(PTN) that employs license plate corner coordinate regression as an implicit
variable, supervised by license plate view classification information. This
network offers improved stability, interpretability, and low annotation costs.
The proposed algorithm achieves an average recognition accuracy of 99.34% on
the corrected CCPD test set under coarse localization disturbance. When
evaluated under fine localization disturbance, the accuracy further improves to
99.58%. On the double-line license plate test set, it achieves an average
recognition accuracy of 98.70%, with processing speeds reaching up to 167
frames per second, indicating strong practical applicability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal Point-Supervised Signal Reconstruction: A Human-Annotation-Free
  Framework for Weak Moving Target <span class="highlight-title">Detection</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihua Gao, Chunxu Ren, Wenlong Niu, Xiaodong Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In low-altitude surveillance and early warning systems, detecting weak moving
targets remains a significant challenge due to low signal energy, small spatial
extent, and complex background clutter. Existing methods struggle with
extracting robust features and suffer from the lack of reliable annotations. To
address these limitations, we propose a novel Temporal Point-Supervised (TPS)
framework that enables high-performance detection of weak targets without any
manual annotations.Instead of conventional frame-based detection, our framework
reformulates the task as a pixel-wise temporal signal modeling problem, where
weak targets manifest as short-duration pulse-like responses. A Temporal Signal
Reconstruction Network (TSRNet) is developed under the TPS paradigm to
reconstruct these transient signals.TSRNet adopts an encoder-decoder
architecture and integrates a Dynamic Multi-Scale Attention (DMSAttention)
module to enhance its sensitivity to diverse temporal patterns. Additionally, a
graph-based trajectory mining strategy is employed to suppress false alarms and
ensure temporal consistency.Extensive experiments on a purpose-built low-SNR
dataset demonstrate that our framework outperforms state-of-the-art methods
while requiring no human annotations. It achieves strong detection performance
and operates at over 1000 FPS, underscoring its potential for real-time
deployment in practical scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CartoonAlive: Towards Expressive Live2D Modeling from Single Portraits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao He, Jianqiang Ren, Jianjing Xiang, Xiejie Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of large foundation models, AIGC, cloud rendering,
and real-time motion capture technologies, digital humans are now capable of
achieving synchronized facial expressions and body movements, engaging in
intelligent dialogues driven by natural language, and enabling the fast
creation of personalized avatars. While current mainstream approaches to
digital humans primarily focus on 3D models and 2D video-based representations,
interactive 2D cartoon-style digital humans have received relatively less
attention. Compared to 3D digital humans that require complex modeling and high
rendering costs, and 2D video-based solutions that lack flexibility and
real-time interactivity, 2D cartoon-style Live2D models offer a more efficient
and expressive alternative. By simulating 3D-like motion through layered
segmentation without the need for traditional 3D modeling, Live2D enables
dynamic and real-time manipulation. In this technical report, we present
CartoonAlive, an innovative method for generating high-quality Live2D digital
humans from a single input portrait image. CartoonAlive leverages the shape
basis concept commonly used in 3D face modeling to construct facial blendshapes
suitable for Live2D. It then infers the corresponding blendshape weights based
on facial keypoints detected from the input image. This approach allows for the
rapid generation of a highly expressive and visually accurate Live2D model that
closely resembles the input portrait, within less than half a minute. Our work
provides a practical and scalable solution for creating interactive 2D cartoon
characters, opening new possibilities in digital content creation and virtual
character animation. The project homepage is
https://human3daigc.github.io/CartoonAlive_webpage/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CasP: Improving Semi-Dense Feature Matching Pipeline Leveraging Cascaded
  Correspondence Priors for Guidance <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiqi Chen, Lei Yu, Yi Wan, Yingying Pei, Xinyi Liu, Yongxiang Yao, Yingying Zhang, Lixiang Ru, Liheng Zhong, Jingdong Chen, Ming Yang, Yongjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-dense feature matching methods have shown strong performance in
challenging scenarios. However, the existing pipeline relies on a global search
across the entire feature map to establish coarse matches, limiting further
improvements in accuracy and efficiency. Motivated by this limitation, we
propose a novel pipeline, CasP, which leverages cascaded correspondence priors
for guidance. Specifically, the matching stage is decomposed into two
progressive phases, bridged by a region-based selective cross-attention
mechanism designed to enhance feature discriminability. In the second phase,
one-to-one matches are determined by restricting the search range to the
one-to-many prior areas identified in the first phase. Additionally, this
pipeline benefits from incorporating high-level features, which helps reduce
the computational costs of low-level feature extraction. The acceleration gains
of CasP increase with higher resolution, and our lite model achieves a speedup
of $\sim2.2\times$ at a resolution of 1152 compared to the most efficient
method, ELoFTR. Furthermore, extensive experiments demonstrate its superiority
in geometric estimation, particularly with impressive cross-domain
generalization. These advantages highlight its potential for latency-sensitive
and high-robustness applications, such as SLAM and UAV systems. Code is
available at https://github.com/pq-chen/CasP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning-based Stage Verification System in Manual Assembly Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingjian Zhang, Yutong Duan, Zaishu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of Industry 4.0, effective monitoring of multiple targets and
states during assembly processes is crucial, particularly when constrained to
using only visual sensors. Traditional methods often rely on either multiple
sensor types or complex hardware setups to achieve high accuracy in monitoring,
which can be cost-prohibitive and difficult to implement in dynamic industrial
environments. This study presents a novel approach that leverages multiple
machine learning models to achieve precise monitoring under the limitation of
using a minimal number of visual sensors. By integrating state information from
identical timestamps, our method detects and confirms the current stage of the
assembly process with an average accuracy exceeding 92%. Furthermore, our
approach surpasses conventional methods by offering enhanced error detection
and visuali-zation capabilities, providing real-time, actionable guidance to
operators. This not only improves the accuracy and efficiency of assembly
monitoring but also re-duces dependency on expensive hardware solutions, making
it a more practical choice for modern industrial applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Versatile Pathology Co-pilot via Reasoning Enhanced Multimodal Large
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Xu, Ziyi Liu, Junlin Hou, Jiabo Ma, Cheng Jin, Yihui Wang, Zhixuan Chen, Zhengyu Zhang, Zhengrui Guo, Fengtao Zhou, Yingxue Xu, Xi Wang, Ronald Cheong Kin Chan, Li Liang, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have emerged as powerful tools for
computational pathology, offering unprecedented opportunities to integrate
pathological images with language context for comprehensive diagnostic
analysis. These models hold particular promise for automating complex tasks
that traditionally require expert interpretation of pathologists. However,
current MLLM approaches in pathology demonstrate significantly constrained
reasoning capabilities, primarily due to their reliance on expensive
chain-of-thought annotations. Additionally, existing methods remain limited to
simplex application of visual question answering (VQA) at region-of-interest
(ROI) level, failing to address the full spectrum of diagnostic needs such as
ROI classification, detection, segmentation, whole-slide-image (WSI)
classification and VQA in clinical practice. In this study, we present
SmartPath-R1, a versatile MLLM capable of simultaneously addressing both
ROI-level and WSI-level tasks while demonstrating robust pathological reasoning
capability. Our framework combines scale-dependent supervised fine-tuning and
task-aware reinforcement fine-tuning, which circumvents the requirement for
chain-of-thought supervision by leveraging the intrinsic knowledge within MLLM.
Furthermore, SmartPath-R1 integrates multiscale and multitask analysis through
a mixture-of-experts mechanism, enabling dynamic processing for diverse tasks.
We curate a large-scale dataset comprising 2.3M ROI samples and 188K WSI
samples for training and evaluation. Extensive experiments across 72 tasks
validate the effectiveness and superiority of the proposed approach. This work
represents a significant step toward developing versatile, reasoning-enhanced
AI systems for precision pathology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PointLAMA: Latent Attention meets Mamba for Efficient <span class="highlight-title">Point Cloud</span>
  Pretraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanyu Lin, Xiaona Zeng, Xianwei Zheng, Xutao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mamba has recently gained widespread attention as a backbone model for point
cloud modeling, leveraging a state-space architecture that enables efficient
global sequence modeling with linear complexity. However, its lack of local
inductive bias limits its capacity to capture fine-grained geometric structures
in 3D data. To address this limitation, we propose \textbf{PointLAMA}, a point
cloud pretraining framework that combines task-aware point cloud serialization,
a hybrid encoder with integrated Latent Attention and Mamba blocks, and a
conditional diffusion mechanism built upon the Mamba backbone. Specifically,
the task-aware point cloud serialization employs Hilbert/Trans-Hilbert
space-filling curves and axis-wise sorting to structurally align point tokens
for classification and segmentation tasks, respectively. Our lightweight Latent
Attention block features a Point-wise Multi-head Latent Attention (PMLA)
module, which is specifically designed to align with the Mamba architecture by
leveraging the shared latent space characteristics of PMLA and Mamba. This
enables enhanced local context modeling while preserving overall efficiency. To
further enhance representation learning, we incorporate a conditional diffusion
mechanism during pretraining, which denoises perturbed feature sequences
without relying on explicit point-wise reconstruction. Experimental results
demonstrate that PointLAMA achieves competitive performance on multiple
benchmark datasets with minimal parameter count and FLOPs, validating its
effectiveness for efficient point cloud pretraining.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fully Automated SAM for Single-source Domain Generalization in Medical
  Image <span class="highlight-title">Segmentation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanli Zhuo, Leilei Ma, Haifeng Zhao, Shiwei Zhou, Dengdi Sun, Yanping Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although SAM-based single-source domain generalization models for medical
image segmentation can mitigate the impact of domain shift on the model in
cross-domain scenarios, these models still face two major challenges. First,
the segmentation of SAM is highly dependent on domain-specific expert-annotated
prompts, which prevents SAM from achieving fully automated medical image
segmentation and therefore limits its application in clinical settings. Second,
providing poor prompts (such as bounding boxes that are too small or too large)
to the SAM prompt encoder can mislead SAM into generating incorrect mask
results. Therefore, we propose the FA-SAM, a single-source domain
generalization framework for medical image segmentation that achieves fully
automated SAM. FA-SAM introduces two key innovations: an Auto-prompted
Generation Model (AGM) branch equipped with a Shallow Feature Uncertainty
Modeling (SUFM) module, and an Image-Prompt Embedding Fusion (IPEF) module
integrated into the SAM mask decoder. Specifically, AGM models the uncertainty
distribution of shallow features through the SUFM module to generate bounding
box prompts for the target domain, enabling fully automated segmentation with
SAM. The IPEF module integrates multiscale information from SAM image
embeddings and prompt embeddings to capture global and local details of the
target object, enabling SAM to mitigate the impact of poor prompts. Extensive
experiments on publicly available prostate and fundus vessel datasets validate
the effectiveness of FA-SAM and highlight its potential to address the above
challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript has been accepted for presentation at the IEEE
  International Conference on Systems, Man, and Cybernetics (IEEE SMC 2025) and
  is copyrighted by IEEE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Robust</span> Real-Time Lane <span class="highlight-title">Detection</span> Method with Fog-Enhanced Feature
  <span class="highlight-title">Fusion</span> for Foggy Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.06121v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.06121v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ronghui Zhang, Yuhang Ma, Tengfei Li, Ziyu Lin, Yueying Wu, Junzhou Chen, Lin Zhang, Jia Hu, Tony Z. Qiu, Konghui Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lane detection is a critical component of Advanced Driver Assistance Systems
(ADAS). Existing lane detection algorithms generally perform well under
favorable weather conditions. However, their performance degrades significantly
in adverse conditions, such as fog, which increases the risk of traffic
accidents. This challenge is compounded by the lack of specialized datasets and
methods designed for foggy environments. To address this, we introduce the
FoggyLane dataset, captured in real-world foggy scenarios, and synthesize two
additional datasets, FoggyCULane and FoggyTusimple, from existing popular lane
detection datasets. Furthermore, we propose a robust Fog-Enhanced Network for
lane detection, incorporating a Global Feature Fusion Module (GFFM) to capture
global relationships in foggy images, a Kernel Feature Fusion Module (KFFM) to
model the structural and positional relationships of lane instances, and a
Low-level Edge Enhanced Module (LEEM) to address missing edge details in foggy
conditions. Comprehensive experiments demonstrate that our method achieves
state-of-the-art performance, with F1-scores of 95.04 on FoggyLane, 79.85 on
FoggyCULane, and 96.95 on FoggyTusimple. Additionally, with TensorRT
acceleration, the method reaches a processing speed of 38.4 FPS on the NVIDIA
Jetson AGX Orin, confirming its real-time capabilities and robustness in foggy
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing Multimodal LLMs by Large-Scale 3D Visual Instruction <span class="highlight-title">Dataset</span>
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.08513v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.08513v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liu He, Xiao Zeng, Yizhi Song, Albert Y. C. Chen, Lu Xia, Shashwat Verma, Sankalp Dayal, Min Sun, Cheng-Hao Kuo, Daniel Aliaga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) struggle with accurately capturing
camera-object relations, especially for object orientation, camera viewpoint,
and camera shots. This stems from the fact that existing MLLMs are trained on
images with limited diverse camera-object relations and corresponding textual
descriptions. To address this, we propose a synthetic generation pipeline to
create large-scale 3D visual instruction datasets. Our framework takes 3D
assets as input and uses rendering and diffusion-based image generation models
to create photorealistic images preserving precise camera-object relations.
Additionally, large language models (LLMs) are used to generate text prompts
for guiding visual instruction tuning and controlling image generation. We
create Ultimate3D, a dataset of 240K VQAs with precise camera-object
annotations, and corresponding benchmark. MLLMs fine-tuned on our proposed
dataset outperform commercial models by a large margin, achieving an average
accuracy improvement of 33.4% on camera-object relation recognition tasks. Our
code, dataset, and benchmark will contribute to broad MLLM applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ODES: Domain Adaptation with Expert Guidance for Online Medical Image
  <span class="highlight-title">Segmentation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05407v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05407v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Shazid Islam, Sayak Nag, Arindam Dutta, Miraj Ahmed, Fahim Faisal Niloy, Amit K. Roy-Chowdhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised domain adaptive segmentation typically relies on self-training
using pseudo labels predicted by a pre-trained network on an unlabeled target
dataset. However, the noisy nature of such pseudo-labels presents a major
bottleneck in adapting a network to the distribution shift between source and
target datasets. This challenge is exaggerated when the network encounters an
incoming data stream in online fashion, where the network is constrained to
adapt to incoming streams of target domain data in exactly one round of forward
and backward passes. In this scenario, relying solely on inaccurate
pseudo-labels can lead to low-quality segmentation, which is detrimental to
medical image analysis where accuracy and precision are of utmost priority. We
hypothesize that a small amount of pixel-level annotation obtained from an
expert can address this problem, thereby enhancing the performance of domain
adaptation of online streaming data, even in the absence of dedicated training
data. We call our method ODES: Domain Adaptation with Expert Guidance for
Online Medical Image Segmentation that adapts to each incoming data batch in an
online setup, incorporating feedback from an expert through active learning.
Through active learning, the most informative pixels in each image can be
selected for expert annotation. However, the acquisition of pixel-level
annotations across all images in a batch often leads to redundant information
while increasing temporal overhead in online learning. To reduce the annotation
acquisition time and make the adaptation process more online-friendly, we
further propose a novel image-pruning strategy that selects the most useful
subset of images from the current batch for active learning. Our proposed
approach outperforms existing online adaptation approaches and produces
competitive results compared to offline domain adaptive active learning
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PerceptionLM: Open-Access Data and Models for Detailed Visual
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.13180v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.13180v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jang Hyun Cho, Andrea Madotto, Effrosyni Mavroudi, Triantafyllos Afouras, Tushar Nagarajan, Muhammad Maaz, Yale Song, Tengyu Ma, Shuming Hu, Suyog Jain, Miguel Martin, Huiyu Wang, Hanoona Rasheed, Peize Sun, Po-Yao Huang, Daniel Bolya, Nikhila Ravi, Shashank Jain, Tammy Stark, Shane Moon, Babak Damavandi, Vivian Lee, Andrew Westbury, Salman Khan, Philipp Krähenbühl, Piotr Dollár, Lorenzo Torresani, Kristen Grauman, Christoph Feichtenhofer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models are integral to computer vision research, yet many
high-performing models remain closed-source, obscuring their data, design and
training recipe. The research community has responded by using distillation
from black-box models to label training data, achieving strong benchmark
results, at the cost of measurable scientific progress. However, without
knowing the details of the teacher model and its data sources, scientific
progress remains difficult to measure. In this paper, we study building a
Perception Language Model (PLM) in a fully open and reproducible framework for
transparent research in image and video understanding. We analyze standard
training pipelines without distillation from proprietary models and explore
large-scale synthetic data to identify critical data gaps, particularly in
detailed video understanding. To bridge these gaps, we release 2.8M
human-labeled instances of fine-grained video question-answer pairs and
spatio-temporally grounded video captions. Additionally, we introduce
PLM-VideoBench, a suite for evaluating challenging video understanding tasks
focusing on the ability to reason about "what", "where", "when", and "how" of a
video. We make our work fully reproducible by providing data, training recipes,
code & models. https://github.com/facebookresearch/perception_models
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PALADIN : <span class="highlight-title">Robust</span> Neural Fingerprinting for Text-to-Image Dif<span class="highlight-title">fusion</span>
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03170v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03170v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Murthy L, Subarna Tripathi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The risk of misusing text-to-image generative models for malicious uses,
especially due to the open-source development of such models, has become a
serious concern. As a risk mitigation strategy, attributing generative models
with neural fingerprinting is emerging as a popular technique. There has been a
plethora of recent work that aim for addressing neural fingerprinting. A
trade-off between the attribution accuracy and generation quality of such
models has been studied extensively. None of the existing methods yet achieved
100% attribution accuracy. However, any model with less than cent percent
accuracy is practically non-deployable. In this work, we propose an accurate
method to incorporate neural fingerprinting for text-to-image diffusion models
leveraging the concepts of cyclic error correcting codes from the literature of
coding theory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Unsupervised</span> Feature Disentanglement and Augmentation Network for
  One-class Face Anti-spoofing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.22929v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.22929v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pei-Kai Huang, Jun-Xiong Chong, Ming-Tsung Hsu, Fang-Yu Hsu, Yi-Ting Lin, Kai-Heng Chien, Hao-Chiang Shao, Chiou-Ting Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face anti-spoofing (FAS) techniques aim to enhance the security of facial
identity authentication by distinguishing authentic live faces from deceptive
attempts. While two-class FAS methods risk overfitting to training attacks to
achieve better performance, one-class FAS approaches handle unseen attacks well
but are less robust to domain information entangled within the liveness
features. To address this, we propose an Unsupervised Feature Disentanglement
and Augmentation Network (\textbf{UFDANet}), a one-class FAS technique that
enhances generalizability by augmenting face images via disentangled features.
The \textbf{UFDANet} employs a novel unsupervised feature disentangling method
to separate the liveness and domain features, facilitating discriminative
feature learning. It integrates an out-of-distribution liveness feature
augmentation scheme to synthesize new liveness features of unseen spoof
classes, which deviate from the live class, thus enhancing the representability
and discriminability of liveness features. Additionally, \textbf{UFDANet}
incorporates a domain feature augmentation routine to synthesize unseen domain
features, thereby achieving better generalizability. Extensive experiments
demonstrate that the proposed \textbf{UFDANet} outperforms previous one-class
FAS methods and achieves comparable performance to state-of-the-art two-class
FAS methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAD-AD: Masked Dif<span class="highlight-title">fusion</span> for <span class="highlight-title">Unsupervised</span> Brain Anomaly <span class="highlight-title">Detection</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16943v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16943v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farzad Beizaee, Gregory Lodygensky, Christian Desrosiers, Jose Dolz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised anomaly detection in brain images is crucial for identifying
injuries and pathologies without access to labels. However, the accurate
localization of anomalies in medical images remains challenging due to the
inherent complexity and variability of brain structures and the scarcity of
annotated abnormal data. To address this challenge, we propose a novel approach
that incorporates masking within diffusion models, leveraging their generative
capabilities to learn robust representations of normal brain anatomy. During
training, our model processes only normal brain MRI scans and performs a
forward diffusion process in the latent space that adds noise to the features
of randomly-selected patches. Following a dual objective, the model learns to
identify which patches are noisy and recover their original features. This
strategy ensures that the model captures intricate patterns of normal brain
structures while isolating potential anomalies as noise in the latent space. At
inference, the model identifies noisy patches corresponding to anomalies and
generates a normal counterpart for these patches by applying a reverse
diffusion process. Our method surpasses existing unsupervised anomaly detection
techniques, demonstrating superior performance in generating accurate normal
counterparts and localizing anomalies. The code is available at
hhttps://github.com/farzad-bz/MAD-AD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Choosing Public <span class="highlight-title">Dataset</span>s for Private Machine Learning via Gradient
  Subspace Distance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01256v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01256v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Gu, Gautam Kamath, Zhiwei Steven Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentially private stochastic gradient descent privatizes model training
by injecting noise into each iteration, where the noise magnitude increases
with the number of model parameters. Recent works suggest that we can reduce
the noise by leveraging public data for private machine learning, by projecting
gradients onto a subspace prescribed by the public data. However, given a
choice of public datasets, it is not a priori clear which one may be most
appropriate for the private task. We give an algorithm for selecting a public
dataset by measuring a low-dimensional subspace distance between gradients of
the public and private examples. We provide theoretical analysis demonstrating
that the excess risk scales with this subspace distance. This distance is easy
to compute and robust to modifications in the setting. Empirical evaluation
shows that trained model accuracy is monotone in this distance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to SaTML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Video Discovery: Agentic Search with Tool Use for Long-form Video
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.18079v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.18079v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyi Zhang, Zhaoyang Jia, Zongyu Guo, Jiahao Li, Bin Li, Houqiang Li, Yan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-form video understanding presents significant challenges due to
extensive temporal-spatial complexity and the difficulty of question answering
under such extended contexts. While Large Language Models (LLMs) have
demonstrated considerable advancements in video analysis capabilities and long
context handling, they continue to exhibit limitations when processing
information-dense hour-long videos. To overcome such limitations, we propose
the Deep Video Discovery agent to leverage an agentic search strategy over
segmented video clips. Different from previous video agents manually designing
a rigid workflow, our approach emphasizes the autonomous nature of agents. By
providing a set of search-centric tools on multi-granular video database, our
DVD agent leverages the advanced reasoning capability of LLM to plan on its
current observation state, strategically selects tools, formulates appropriate
parameters for actions, and iteratively refines its internal reasoning in light
of the gathered information. We perform comprehensive evaluation on multiple
long video understanding benchmarks that demonstrates the advantage of the
entire system design. Our DVD agent achieves SOTA performance, significantly
surpassing prior works by a large margin on the challenging LVBench dataset.
Comprehensive ablation studies and in-depth tool analyses are also provided,
yielding insights to further advance intelligent agents tailored for long-form
video understanding tasks. The code has been released in
https://github.com/microsoft/DeepVideoDiscovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>V3 draft. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Can We Generate Images with CoT? Let's Verify and Reinforce Image
  Generation Step by Step 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13926v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13926v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Rui Huang, Haoquan Zhang, Manyuan Zhang, Jia<span class="highlight-author">ming Liu</span>, Shanghang Zhang, Peng Gao, Hongsheng Li, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-Thought (CoT) reasoning has been extensively explored in large
models to tackle complex understanding tasks. However, it still remains an open
question whether such strategies can be applied to verifying and reinforcing
image generation scenarios. In this paper, we provide the first comprehensive
investigation of the potential of CoT reasoning to enhance autoregressive image
generation. We focus on three techniques: scaling test-time computation for
verification, aligning model preferences with Direct Preference Optimization
(DPO), and integrating these techniques for complementary effects. Our results
demonstrate that these approaches can be effectively adapted and combined to
significantly improve image generation performance. Furthermore, given the
pivotal role of reward models in our findings, we propose the Potential
Assessment Reward Model (PARM) and PARM++, specialized for autoregressive image
generation. PARM adaptively assesses each generation step through a potential
assessment approach, merging the strengths of existing reward models, and
PARM++ further introduces a reflection mechanism to self-correct the generated
unsatisfactory image, which is the first to incorporate reflection in
autoregressive image generation. Using our investigated reasoning strategies,
we enhance a baseline model, Show-o, to achieve superior results, with a
significant +24% improvement on the GenEval benchmark, surpassing Stable
Diffusion 3 by +15%. We hope our study provides unique insights and paves a new
path for integrating CoT reasoning with autoregressive image generation. Code
and models are released at https://github.com/ZiyuGuo99/Image-Generation-CoT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Journal Version. Code and models are released at
  https://github.com/ZiyuGuo99/Image-Generation-CoT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Box<span class="highlight-title">Fusion</span>: Reconstruction-Free Open-Vocabulary 3D Object <span class="highlight-title">Detection</span> via
  Real-Time <span class="highlight-title">Multi-View</span> Box <span class="highlight-title">Fusion</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15610v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15610v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqing Lan, Chenyang Zhu, Zhirui Gao, Jiazhao Zhang, Yihan Cao, Renjiao Yi, Yijie Wang, Kai Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-vocabulary 3D object detection has gained significant interest due to
its critical applications in autonomous driving and embodied AI. Existing
detection methods, whether offline or online, typically rely on dense point
cloud reconstruction, which imposes substantial computational overhead and
memory constraints, hindering real-time deployment in downstream tasks. To
address this, we propose a novel reconstruction-free online framework tailored
for memory-efficient and real-time 3D detection. Specifically, given streaming
posed RGB-D video input, we leverage Cubify Anything as a pre-trained visual
foundation model (VFM) for single-view 3D object detection by bounding boxes,
coupled with CLIP to capture open-vocabulary semantics of detected objects. To
fuse all detected bounding boxes across different views into a unified one, we
employ an association module for correspondences of multi-views and an
optimization module to fuse the 3D bounding boxes of the same instance
predicted in multi-views. The association module utilizes 3D Non-Maximum
Suppression (NMS) and a box correspondence matching module, while the
optimization module uses an IoU-guided efficient random optimization technique
based on particle filtering to enforce multi-view consistency of the 3D
bounding boxes while minimizing computational complexity. Extensive experiments
on ScanNetV2 and CA-1M datasets demonstrate that our method achieves
state-of-the-art performance among online methods. Benefiting from this novel
reconstruction-free paradigm for 3D object detection, our method exhibits great
generalization abilities in various scenarios, enabling real-time perception
even in environments exceeding 1000 square meters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://lanlan96.github.io/BoxFusion/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Application of YOLOv8 in monocular downward multiple Car Target
  <span class="highlight-title">detection</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10016v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10016v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving technology is progressively transforming traditional car
driving methods, marking a significant milestone in modern transportation.
Object detection serves as a cornerstone of autonomous systems, playing a vital
role in enhancing driving safety, enabling autonomous functionality, improving
traffic efficiency, and facilitating effective emergency responses. However,
current technologies such as radar for environmental perception, cameras for
road perception, and vehicle sensor networks face notable challenges, including
high costs, vulnerability to weather and lighting conditions, and limited
resolution.To address these limitations, this paper presents an improved
autonomous target detection network based on YOLOv8. By integrating structural
reparameterization technology, a bidirectional pyramid structure network model,
and a novel detection pipeline into the YOLOv8 framework, the proposed approach
achieves highly efficient and precise detection of multi-scale, small, and
remote objects. Experimental results demonstrate that the enhanced model can
effectively detect both large and small objects with a detection accuracy of
65%, showcasing significant advancements over traditional methods.This improved
model holds substantial potential for real-world applications and is
well-suited for autonomous driving competitions, such as the Formula Student
Autonomous China (FSAC), particularly excelling in scenarios involving
single-target and small-object detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This submission included authors who did not consent to the
  submission. The paper is being withdrawn until authorship issues are resolved</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ORL-LDM: Offline <span class="highlight-title">Reinforcement</span> Learning Guided Latent Dif<span class="highlight-title">fusion</span> Model
  Super-Resolution Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10027v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10027v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of remote sensing technology, super-resolution
image reconstruction is of great research and practical significance. Existing
deep learning methods have made progress but still face limitations in handling
complex scenes and preserving image details. This paper proposes a
reinforcement learning-based latent diffusion model (LDM) fine-tuning method
for remote sensing image super-resolution. The method constructs a
reinforcement learning environment with states, actions, and rewards,
optimizing decision objectives through proximal policy optimization (PPO)
during the reverse denoising process of the LDM model. Experiments on the
RESISC45 dataset show significant improvements over the baseline model in PSNR,
SSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11,
and LPIPS reducing by 0.06-0.10, particularly in structured and complex natural
scenes. The results demonstrate the method's effectiveness in enhancing
super-resolution quality and adaptability across scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This submission included authors who did not consent to the
  submission. The paper is being withdrawn until authorship issues are resolved</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Context Dif<span class="highlight-title">fusion</span>: In-Context Aware Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03584v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03584v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivona Najdenkoska, Animesh Sinha, Abhimanyu Dubey, Dhruv Mahajan, Vignesh Ramanathan, Filip Radenovic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Context Diffusion, a diffusion-based framework that enables image
generation models to learn from visual examples presented in context. Recent
work tackles such in-context learning for image generation, where a query image
is provided alongside context examples and text prompts. However, the quality
and context fidelity of the generated images deteriorate when the prompt is not
present, demonstrating that these models cannot truly learn from the visual
context. To address this, we propose a novel framework that separates the
encoding of the visual context and the preservation of the desired image
layout. This results in the ability to learn from the visual context and
prompts, but also from either of them. Furthermore, we enable our model to
handle few-shot settings, to effectively address diverse in-context learning
scenarios. Our experiments and human evaluation demonstrate that Context
Diffusion excels in both in-domain and out-of-domain tasks, resulting in an
overall enhancement in image quality and context fidelity compared to
counterpart models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text2Stereo: Repurposing Stable Dif<span class="highlight-title">fusion</span> for Stereo Generation with
  Consistency Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05367v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05367v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aakash Garg, Libing Zeng, Andrii Tsarov, Nima Khademi Kalantari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel diffusion-based approach to generate stereo
images given a text prompt. Since stereo image datasets with large baselines
are scarce, training a diffusion model from scratch is not feasible. Therefore,
we propose leveraging the strong priors learned by Stable Diffusion and
fine-tuning it on stereo image datasets to adapt it to the task of stereo
generation. To improve stereo consistency and text-to-image alignment, we
further tune the model using prompt alignment and our proposed stereo
consistency reward functions. Comprehensive experiments demonstrate the
superiority of our approach in generating high-quality stereo images across
diverse scenarios, outperforming existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RGBX-Dif<span class="highlight-title">fusion</span>Det: A Framework for Multi-Modal RGB-X Object <span class="highlight-title">Detection</span>
  Using Dif<span class="highlight-title">fusion</span>Det 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.02586v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.02586v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eliraz Orfaig, Inna Stainvas, Igal Bilik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces RGBX-DiffusionDet, an object detection framework
extending the DiffusionDet model to fuse the heterogeneous 2D data (X) with RGB
imagery via an adaptive multimodal encoder. To enable cross-modal interaction,
we design the dynamic channel reduction within a convolutional block attention
module (DCR-CBAM), which facilitates cross-talk between subnetworks by
dynamically highlighting salient channel features. Furthermore, the dynamic
multi-level aggregation block (DMLAB) is proposed to refine spatial feature
representations through adaptive multiscale fusion. Finally, novel
regularization losses that enforce channel saliency and spatial selectivity are
introduced, leading to compact and discriminative feature embeddings. Extensive
experiments using RGB-Depth (KITTI), a novel annotated RGB-Polarimetric
dataset, and RGB-Infrared (M$^3$FD) benchmark dataset were conducted. We
demonstrate consistent superiority of the proposed approach over the baseline
RGB-only DiffusionDet. The modular architecture maintains the original decoding
complexity, ensuring efficiency. These results establish the proposed
RGBX-DiffusionDet as a flexible multimodal object detection approach, providing
new insights into integrating diverse 2D sensing modalities into
diffusion-based detection pipelines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving the Reasoning of Multi-Image Grounding in MLLMs via
  <span class="highlight-title">Reinforcement</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.00748v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.00748v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bob Zhang, Haoran Li, Tao Zhang, Cilin Yan, Jiayin Cai, Yanbin Hao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Multimodal Large Language Models (MLLMs) excel at visual grounding
in single-image scenarios with textual references. However, their performance
degrades when handling real-world applications that involve complex multi-image
compositions and multi-modal instructions, revealing limitations in cross-image
reasoning and generalization. To address these challenges, we adopt a
Reinforcement Learning (RL) based post-training strategy to improve the
reasoning of MLLMs in multi-image grounding tasks. Our approach begins with
synthesizing high-quality chain-of-thought (CoT) data for cold-start
initialization, followed by supervised fine-tuning (SFT) using low-rank
adaptation (LoRA). The cold-start training stage enables the model to identify
correct solutions. Subsequently, we perform rejection sampling using the merged
SFT model to curate high-quality RL data and leverage rule-based RL to guide
the model toward optimal reasoning paths. Extensive experimental results
demonstrate the effectiveness of our approach, yielding improvements of +9.04%
on MIG-Bench, +6.37% on MC-Bench, and +4.98% on several out-of-domain reasoning
grounding benchmarks compared to the SFT baseline. Furthermore, our method
exhibits strong generalization in multi-image perception, with gains of +3.1%
and +2.4% over the base model on BLINK and MMIU benchmarks, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Infinite Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.09068v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.09068v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dell Zhang, Xiangyu Chen, Jixiang Luo, Mengxi Jia, Changzhi Sun, Ruilong Ren, Jingren Liu, Hao Sun, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancements in Large Language Models (LLMs) and their multimodal
extensions (MLLMs) have ushered in remarkable progress in video understanding.
However, a fundamental challenge persists: effectively processing and
comprehending video content that extends beyond minutes or hours. While recent
efforts like Video-XL-2 have demonstrated novel architectural solutions for
extreme efficiency, and advancements in positional encoding such as HoPE and
VideoRoPE++ aim to improve spatio-temporal understanding over extensive
contexts, current state-of-the-art models still encounter significant
computational and memory constraints when faced with the sheer volume of visual
tokens from lengthy sequences. Furthermore, maintaining temporal coherence,
tracking complex events, and preserving fine-grained details over extended
periods remain formidable hurdles, despite progress in agentic reasoning
systems like Deep Video Discovery. This position paper posits that a logical,
albeit ambitious, next frontier for multimedia research is Infinite Video
Understanding -- the capability for models to continuously process, understand,
and reason about video data of arbitrary, potentially never-ending duration. We
argue that framing Infinite Video Understanding as a blue-sky research
objective provides a vital north star for the multimedia, and the wider AI,
research communities, driving innovation in areas such as streaming
architectures, persistent memory mechanisms, hierarchical and adaptive
representations, event-centric reasoning, and novel evaluation paradigms.
Drawing inspiration from recent work on long/ultra-long video understanding and
several closely related fields, we outline the core challenges and key research
directions towards achieving this transformative capability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RALAD: Bridging the Real-to-Sim Domain Gap in Autonomous Driving with
  Retrieval-Augmented Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12296v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12296v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Zuo, Haibo Hu, Zikang Zhou, Yufei Cui, Ziquan Liu, Jianping Wang, Nan Guan, Jin Wang, Chun Jason Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the pursuit of robust autonomous driving systems, models trained on
real-world datasets often struggle to adapt to new environments, particularly
when confronted with corner cases such as extreme weather conditions.
Collecting these corner cases in the real world is non-trivial, which
necessitates the use of simulators for validation. However,the high
computational cost and the domain gap in data distribution have hindered the
seamless transition between real and simulated driving scenarios. To tackle
this challenge, we propose Retrieval-Augmented Learning for Autonomous Driving
(RALAD), a novel framework designed to bridge the real-to-sim gap at a low
cost. RALAD features three primary designs, including (1) domain adaptation via
an enhanced Optimal Transport (OT) method that accounts for both individual and
grouped image distances, (2) a simple and unified framework that can be applied
to various models, and (3) efficient fine-tuning techniques that freeze the
computationally expensive layers while maintaining robustness. Experimental
results demonstrate that RALAD compensates for the performance degradation in
simulated environments while maintaining accuracy in real-world scenarios
across three different models. Taking Cross View as an example, the mIOU and
mAP metrics in real-world scenarios remain stable before and after RALAD
fine-tuning, while in simulated environments,the mIOU and mAP metrics are
improved by 10.30% and 12.29%, respectively. Moreover, the re-training cost of
our approach is reduced by approximately 88.1%. Our code is available at
https://github.com/JiachengZuo/RALAD.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PoemTale Dif<span class="highlight-title">fusion</span>: Minimising Information Loss in Poem to Image
  Generation with Multi-Stage Prompt Refinement <span class="chip">ECAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13708v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13708v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sofia Jamil, Bollampalli Areen Reddy, Raghvendra Kumar, Sriparna Saha, Koustava Goswami, K. J. Joseph
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in text-to-image diffusion models have achieved
remarkable success in generating realistic and diverse visual content. A
critical factor in this process is the model's ability to accurately interpret
textual prompts. However, these models often struggle with creative
expressions, particularly those involving complex, abstract, or highly
descriptive language. In this work, we introduce a novel training-free approach
tailored to improve image generation for a unique form of creative language:
poetic verse, which frequently features layered, abstract, and dual meanings.
Our proposed PoemTale Diffusion approach aims to minimise the information that
is lost during poetic text-to-image conversion by integrating a multi stage
prompt refinement loop into Language Models to enhance the interpretability of
poetic texts. To support this, we adapt existing state-of-the-art diffusion
models by modifying their self-attention mechanisms with a consistent
self-attention technique to generate multiple consistent images, which are then
collectively used to convey the poem's meaning. Moreover, to encourage research
in the field of poetry, we introduce the P4I (PoemForImage) dataset, consisting
of 1111 poems sourced from multiple online and offline resources. We engaged a
panel of poetry experts for qualitative assessments. The results from both
human and quantitative evaluations validate the efficacy of our method and
contribute a novel perspective to poem-to-image generation with enhanced
information capture in the generated images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MolX: Enhancing Large Language Models for Molecular Understanding With A
  Multi-Modal Extension <span class="chip">KDD '25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06777v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06777v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khiem Le, Zhichun Guo, Kaiwen Dong, Xiaobao Huang, Bozhao Nan, Roshni Iyer, Xiangliang Zhang, Olaf Wiest, Wei Wang, Ting Hua, Nitesh V. Chawla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) with their strong task-handling capabilities
have shown remarkable advancements across a spectrum of fields, moving beyond
natural language understanding. However, their proficiency within the chemistry
domain remains restricted, especially in solving molecule-related tasks. This
challenge is attributed to their inherent limitations in comprehending
molecules using only common textual representations, i.e. SMILES strings. In
this study, we seek to enhance the ability of LLMs to comprehend molecules by
equipping them with a multi-modal external module, termed MolX. Instead of
directly using SMILES strings to represent a molecule, we utilize specific
encoders to extract fine-grained features from both SMILES string and 2D
molecular graph representations for feeding into an LLM. A hand-crafted
molecular fingerprint is incorporated to leverage its embedded domain
knowledge. To establish an alignment between MolX and the LLM's textual input
space, the model in which the LLM is frozen, is pre-trained with a strategy
including a diverse set of tasks. Experimental evaluations show that our
proposed method outperforms baselines across 4 downstream molecule-related
tasks ranging from molecule-to-text translation to retrosynthesis, with and
without fine-tuning the LLM, while only introducing a small number of trainable
parameters--0.53\% and 0.82\%, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MLoG-GenAI@KDD '25 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Mapping</span> of Weed Management Methods in Orchards using Sentinel-2 and
  PlanetScope Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.19991v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.19991v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ioannis Kontogiorgakis, Iason Tsardanidis, Dimitrios Bormpoudakis, Ilias Tsoumas, Dimitra A. Loka, Christos Noulas, Alexandros Tsitouras, Charalampos Kontoes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective weed management is crucial for improving agricultural productivity,
as weeds compete with crops for vital resources like nutrients and water.
Accurate maps of weed management methods are essential for policymakers to
assess farmer practices, evaluate impacts on vegetation health, biodiversity,
and climate, as well as ensure compliance with policies and subsidies. However,
monitoring weed management methods is challenging as they commonly rely on
ground-based field surveys, which are often costly, time-consuming and subject
to delays. In order to tackle this problem, we leverage earth observation data
and Machine Learning (ML). Specifically, we developed separate ML models using
Sentinel-2 and PlanetScope satellite time series data, respectively, to
classify four distinct weed management methods (Mowing, Tillage,
Chemical-spraying, and No practice) in orchards. The findings demonstrate the
potential of ML-driven remote sensing to enhance the efficiency and accuracy of
weed management mapping in orchards.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for 2025 IEEE International Geoscience and Remote Sensing
  Symposium (IGARSS 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Deep Learning Approach for Augmenting Perceptional Understanding of
  Histopathology Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.06894v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.06894v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoqian Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Recent Years, Digital Technologies Have Made Significant Strides In
Augmenting-Human-Health, Cognition, And Perception, Particularly Within The
Field Of Computational-Pathology. This Paper Presents A Novel Approach To
Enhancing The Analysis Of Histopathology Images By Leveraging A
Mult-modal-Model That Combines Vision Transformers (Vit) With Gpt-2 For Image
Captioning. The Model Is Fine-Tuned On The Specialized Arch-Dataset, Which
Includes Dense Image Captions Derived From Clinical And Academic Resources, To
Capture The Complexities Of Pathology Images Such As Tissue Morphologies,
Staining Variations, And Pathological Conditions. By Generating Accurate,
Contextually Captions, The Model Augments The Cognitive Capabilities Of
Healthcare Professionals, Enabling More Efficient Disease Classification,
Segmentation, And Detection. The Model Enhances The Perception Of Subtle
Pathological Features In Images That Might Otherwise Go Unnoticed, Thereby
Improving Diagnostic Accuracy. Our Approach Demonstrates The Potential For
Digital Technologies To Augment Human Cognitive Abilities In Medical Image
Analysis, Providing Steps Toward More Personalized And Accurate Healthcare
Outcomes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by International Conference on Semantic & Natural Language
  Processing (SNLP 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JEDI: The Force of Jensen-Shannon Divergence in Disentangling Dif<span class="highlight-title">fusion</span>
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19166v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19166v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Tillmann Bill, Enis Simsar, Thomas Hofmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce JEDI, a test-time adaptation method that enhances subject
separation and compositional alignment in diffusion models without requiring
retraining or external supervision. JEDI operates by minimizing semantic
entanglement in attention maps using a novel Jensen-Shannon divergence based
objective. To improve efficiency, we leverage adversarial optimization,
reducing the number of updating steps required. JEDI is model-agnostic and
applicable to architectures such as Stable Diffusion 1.5 and 3.5, consistently
improving prompt alignment and disentanglement in complex scenes. Additionally,
JEDI provides a lightweight, CLIP-free disentanglement score derived from
internal attention distributions, offering a principled benchmark for
compositional alignment under test-time conditions. Code and results are
available at https://ericbill21.github.io/JEDI/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TaoAvatar: Real-Time <span class="highlight-title">Life</span>like Full-Body Talking Avatars for Augmented
  Reality via 3D Gaussian Splatting <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17032v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17032v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianchuan Chen, Jingchuan Hu, Gaige Wang, Zhonghua Jiang, Tiansong Zhou, Zhiwen Chen, Chengfei Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Realistic 3D full-body talking avatars hold great potential in AR, with
applications ranging from e-commerce live streaming to holographic
communication. Despite advances in 3D Gaussian Splatting (3DGS) for lifelike
avatar creation, existing methods struggle with fine-grained control of facial
expressions and body movements in full-body talking tasks. Additionally, they
often lack sufficient details and cannot run in real-time on mobile devices. We
present TaoAvatar, a high-fidelity, lightweight, 3DGS-based full-body talking
avatar driven by various signals. Our approach starts by creating a
personalized clothed human parametric template that binds Gaussians to
represent appearances. We then pre-train a StyleUnet-based network to handle
complex pose-dependent non-rigid deformation, which can capture high-frequency
appearance details but is too resource-intensive for mobile devices. To
overcome this, we "bake" the non-rigid deformations into a lightweight
MLP-based network using a distillation technique and develop blend shapes to
compensate for details. Extensive experiments show that TaoAvatar achieves
state-of-the-art rendering quality while running in real-time across various
devices, maintaining 90 FPS on high-definition stereo devices such as the Apple
Vision Pro.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025 (Highlight), project page:
  https://PixelAI-Team.github.io/TaoAvatar</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AirCache: Activating Inter-modal Relevancy KV Cache Compression for
  Efficient Large Vision-Language Model Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.23956v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.23956v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Huang, Hao Zou, Bochen Wang, Ye Xi, Zhen Xie, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Visual Language Models (LVLMs) have gained
significant attention due to their remarkable reasoning capabilities and
proficiency in generalization. However, processing a large number of visual
tokens and generating long-context outputs impose substantial computational
overhead, leading to excessive demands for key-value (KV) cache. To address
this critical bottleneck, we propose AirCache, a novel KV cache compression
method aimed at accelerating LVLMs inference. This work systematically
investigates the correlations between visual and textual tokens within the
attention mechanisms of LVLMs. Our empirical analysis reveals considerable
redundancy in cached visual tokens, wherein strategically eliminating these
tokens preserves model performance while significantly accelerating context
generation. Inspired by these findings, we introduce an elite observation
window for assessing the importance of visual components in the KV cache,
focusing on stable inter-modal relevancy modeling with enhanced
multi-perspective consistency. Additionally, we develop an adaptive layer-wise
budget allocation strategy that capitalizes on the strength and skewness of
token importance distribution, showcasing superior efficiency compared to
uniform allocation. Comprehensive evaluations across multiple LVLMs and
benchmarks demonstrate that our method achieves comparable performance to the
full cache while retaining only 10% of visual KV cache, thereby reducing
decoding latency by 29% to 66% across various batch size and prompt length of
inputs. Notably, as cache retention rates decrease, our method exhibits
increasing performance advantages over existing approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation
  Models on Standard Computer Vision Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01955v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01955v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahul Ramachandran, Ali Garjani, Roman Bachmann, Andrei Atanov, Oğuzhan Fatih Kar, Amir Zamir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal foundation models, such as GPT-4o, have recently made remarkable
progress, but it is not clear where exactly these models stand in terms of
understanding vision. In this paper, we benchmark the performance of popular
multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0
Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision
tasks (semantic segmentation, object detection, image classification, depth and
surface normal prediction) using established datasets (e.g., COCO, ImageNet and
its variants, etc).
  The main challenges to performing this are: 1) most models are trained to
output text and cannot natively express versatile domains, such as segments or
3D geometry, and 2) many leading models are proprietary and accessible only at
an API level, i.e., there is no weight access to adapt them. We address these
challenges by translating standard vision tasks into equivalent text-promptable
and API-compatible tasks via prompt chaining to create a standardized
benchmarking framework.
  We observe that 1) the models are not close to the state-of-the-art
specialist models at any task. However, 2) they are respectable generalists;
this is remarkable as they are presumably trained on primarily image-text-based
tasks. 3) They perform semantic tasks notably better than geometric ones. 4)
While the prompt-chaining techniques affect performance, better models exhibit
less sensitivity to prompt variations. 5) GPT-4o performs the best among
non-reasoning models, securing the top position in 4 out of 6 tasks, 6)
reasoning models, e.g. o3, show improvements in geometric tasks, and 7) a
preliminary analysis of models with native image generation, like the latest
GPT-4o, shows they exhibit quirks like hallucinations and spatial
misalignments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page at https://fm-vision-evals.epfl.ch/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InceptionMamba: An Efficient Hybrid Network with Large Band Convolution
  and Bottleneck Mamba 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08735v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08735v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Wang, Jun Li, Zhijian Wu, Jifeng Shen, Jianhua Xu, Wankou Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Within the family of convolutional neural networks, InceptionNeXt has shown
excellent competitiveness in image classification and a number of downstream
tasks. Built on parallel one-dimensional strip convolutions, however, it
suffers from limited ability of capturing spatial dependencies along different
dimensions and fails to fully explore spatial modeling in local neighborhood.
Besides, inherent locality constraints of convolution operations are
detrimental to effective global context modeling. To overcome these
limitations, we propose a novel backbone architecture termed InceptionMamba in
this study. More specifically, the traditional one-dimensional strip
convolutions are replaced by orthogonal band convolutions in our InceptionMamba
to achieve cohesive spatial modeling. Furthermore, global contextual modeling
can be achieved via a bottleneck Mamba module, facilitating enhanced
cross-channel information fusion and enlarged receptive field. Extensive
evaluations on classification and various downstream tasks demonstrate that the
proposed InceptionMamba achieves state-of-the-art performance with superior
parameter and computational efficiency. The source code will be available at
https://github.com/Wake1021/InceptionMamba.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vascular <span class="highlight-title">Segmentation</span> of Functional Ultrasound Images using Deep
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22365v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22365v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hana Sebia, Thomas Guyet, Mickaël Pereira, Marco Valdebenito, Hugues Berry, Benjamin Vidal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmentation of medical images is a fundamental task with numerous
applications. While MRI, CT, and PET modalities have significantly benefited
from deep learning segmentation techniques, more recent modalities, like
functional ultrasound (fUS), have seen limited progress. fUS is a non invasive
imaging method that measures changes in cerebral blood volume (CBV) with high
spatio-temporal resolution. However, distinguishing arterioles from venules in
fUS is challenging due to opposing blood flow directions within the same pixel.
Ultrasound localization microscopy (ULM) can enhance resolution by tracking
microbubble contrast agents but is invasive, and lacks dynamic CBV
quantification. In this paper, we introduce the first deep learning-based
segmentation tool for fUS images, capable of differentiating signals from
different vascular compartments, based on ULM automatic annotation and enabling
dynamic CBV quantification. We evaluate various UNet architectures on fUS
images of rat brains, achieving competitive segmentation performance, with 90%
accuracy, a 71% F1 score, and an IoU of 0.59, using only 100 temporal frames
from a fUS stack. These results are comparable to those from tubular structure
segmentation in other imaging modalities. Additionally, models trained on
resting-state data generalize well to images captured during visual
stimulation, highlighting robustness. This work offers a non-invasive,
cost-effective alternative to ULM, enhancing fUS data interpretation and
improving understanding of vessel function. Our pipeline shows high linear
correlation coefficients between signals from predicted and actual compartments
in both cortical and deeper regions, showcasing its ability to accurately
capture blood flow dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Monitoring digestate application on agricultural crops using Sentinel-2
  Satellite imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.19996v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.19996v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Kalogeras, Dimitrios Bormpoudakis, Iason Tsardanidis, Dimitra A. Loka, Charalampos Kontoes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread use of Exogenous Organic Matter in agriculture necessitates
monitoring to assess its effects on soil and crop health. This study evaluates
optical Sentinel-2 satellite imagery for detecting digestate application, a
practice that enhances soil fertility but poses environmental risks like
microplastic contamination and nitrogen losses. In the first instance,
Sentinel-2 satellite image time series (SITS) analysis of specific indices
(EOMI, NDVI, EVI) was used to characterize EOM's spectral behavior after
application on the soils of four different crop types in Thessaly, Greece.
Furthermore, Machine Learning (ML) models (namely Random Forest, k-NN, Gradient
Boosting and a Feed-Forward Neural Network), were used to investigate digestate
presence detection, achieving F1-scores up to 0.85. The findings highlight the
potential of combining remote sensing and ML for scalable and cost-effective
monitoring of EOM applications, supporting precision agriculture and
sustainability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for 2025 IEEE International Geoscience and Remote Sensing
  Symposium (IGARSS 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prompt Guidance and Human Proximal Perception for HOT <span class="highlight-title">Prediction</span> with
  Regional Joint Loss <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01630v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01630v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiao Wang, Yu Lei, Zhenao Wei, Weiying Xue, Xinyu Jiang, Nan Zhuang, Qi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of Human-Object conTact (HOT) detection involves identifying the
specific areas of the human body that are touching objects. Nevertheless,
current models are restricted to just one type of image, often leading to too
much segmentation in areas with little interaction, and struggling to maintain
category consistency within specific regions. To tackle this issue, a HOT
framework, termed \textbf{P3HOT}, is proposed, which blends \textbf{P}rompt
guidance and human \textbf{P}roximal \textbf{P}erception. To begin with, we
utilize a semantic-driven prompt mechanism to direct the network's attention
towards the relevant regions based on the correlation between image and text.
Then a human proximal perception mechanism is employed to dynamically perceive
key depth range around the human, using learnable parameters to effectively
eliminate regions where interactions are not expected. Calculating depth
resolves the uncertainty of the overlap between humans and objects in a 2D
perspective, providing a quasi-3D viewpoint. Moreover, a Regional Joint Loss
(RJLoss) has been created as a new loss to inhibit abnormal categories in the
same area. A new evaluation metric called ``AD-Acc.'' is introduced to address
the shortcomings of existing methods in addressing negative samples.
Comprehensive experimental results demonstrate that our approach achieves
state-of-the-art performance in four metrics across two benchmark datasets.
Specifically, our model achieves an improvement of \textbf{0.7}$\uparrow$,
\textbf{2.0}$\uparrow$, \textbf{1.6}$\uparrow$, and \textbf{11.0}$\uparrow$ in
SC-Acc., mIoU, wIoU, and AD-Acc. metrics, respectively, on the HOT-Annotated
dataset. The sources code are available at
https://github.com/YuxiaoWang-AI/P3HOT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpiLiFormer: Enhancing Spiking Transformers with Lateral Inhibition <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.15986v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.15986v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeqi Zheng, Yanchen Huang, Yingchao Yu, Zizheng Zhu, Junfeng Tang, Zhaofei Yu, Yaochu Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Neural Networks (SNNs) based on Transformers have garnered
significant attention due to their superior performance and high energy
efficiency. However, the spiking attention modules of most existing
Transformer-based SNNs are adapted from those of analog Transformers, failing
to fully address the issue of over-allocating attention to irrelevant contexts.
To fix this fundamental yet overlooked issue, we propose a Lateral
Inhibition-inspired Spiking Transformer (SpiLiFormer). It emulates the brain's
lateral inhibition mechanism, guiding the model to enhance attention to
relevant tokens while suppressing attention to irrelevant ones. Our model
achieves state-of-the-art (SOTA) performance across multiple datasets,
including CIFAR-10 (+0.45%), CIFAR-100 (+0.48%), CIFAR10-DVS (+2.70%),
N-Caltech101 (+1.94%), and ImageNet-1K (+1.6%). Notably, on the ImageNet-1K
dataset, SpiLiFormer (69.9M parameters, 4 time steps, 384 resolution)
outperforms E-SpikeFormer (173.0M parameters, 8 time steps, 384 resolution), a
SOTA spiking Transformer, by 0.46% using only 39% of the parameters and half
the time steps. The code and model checkpoints are publicly available at
https://github.com/KirinZheng/SpiLiFormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2025. The first two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GEMINUS: Dual-aware <span class="highlight-title">Global</span> and Scene-Adaptive Mixture-of-Experts for
  End-to-End Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14456v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14456v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Wan, Yixin Cui, Jiatong Du, Shuo Yang, Yulong Bai, Yanjun Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end autonomous driving requires adaptive and robust handling of
complex and diverse traffic environments. However, prevalent single-mode
planning methods attempt to learn an overall policy while struggling to acquire
diversified driving skills to handle diverse scenarios. Therefore, this paper
proposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework
featuring a Global Expert, a Scene-Adaptive Experts Group, and equipped with a
Dual-aware Router. Specifically, the Global Expert is trained on the overall
dataset, possessing robust performance. The Scene-Adaptive Experts are trained
on corresponding scene subsets, achieving adaptive performance. The Dual-aware
Router simultaneously considers scenario-level features and routing uncertainty
to dynamically activate expert modules. Through the effective coupling of the
Global Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,
GEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS
outperforms existing methods in the Bench2Drive closed-loop benchmark and
achieves state-of-the-art performance in Driving Score and Success Rate, even
with only monocular vision input. Furthermore, ablation studies demonstrate
significant improvements over the original single-expert baseline: 7.67% in
Driving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The
code will be available at https://github.com/newbrains1/GEMINUS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual-Language Model Knowledge Distillation Method for Image Quality
  Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15680v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15680v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongkang Hou, Jiarun Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image Quality Assessment (IQA) is a core task in computer vision. Multimodal
methods based on vision-language models, such as CLIP, have demonstrated
exceptional generalization capabilities in IQA tasks. To address the issues of
excessive parameter burden and insufficient ability to identify local distorted
features in CLIP for IQA, this study proposes a visual-language model knowledge
distillation method aimed at guiding the training of models with architectural
advantages using CLIP's IQA knowledge. First, quality-graded prompt templates
were designed to guide CLIP to output quality scores. Then, CLIP is fine-tuned
to enhance its capabilities in IQA tasks. Finally, a modality-adaptive
knowledge distillation strategy is proposed to achieve guidance from the CLIP
teacher model to the student model. Our experiments were conducted on multiple
IQA datasets, and the results show that the proposed method significantly
reduces model complexity while outperforming existing IQA methods,
demonstrating strong potential for practical deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Qf<span class="highlight-title">fusion</span>: <span class="highlight-title">Control</span>lable Portrait Video Editing via Quadrant-Grid
  Attention Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06438v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06438v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maomao Li, Lijian Lin, Yunfei Liu, Ye Zhu, Yu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Qffusion, a dual-frame-guided framework for portrait
video editing. Specifically, we consider a design principle of ``animation for
editing'', and train Qffusion as a general animation framework from two still
reference images while we can use it for portrait video editing easily by
applying modified start and end frames as references during inference.
Leveraging the powerful generative power of Stable Diffusion, we propose a
Quadrant-grid Arrangement (QGA) scheme for latent re-arrangement, which
arranges the latent codes of two reference images and that of four facial
conditions into a four-grid fashion, separately. Then, we fuse features of
these two modalities and use self-attention for both appearance and temporal
learning, where representations at different times are jointly modeled under
QGA. Our Qffusion can achieve stable video editing without additional networks
or complex training stages, where only the input format of Stable Diffusion is
modified. Further, we propose a Quadrant-grid Propagation (QGP) inference
strategy, which enjoys a unique advantage on stable arbitrary-length video
generation by processing reference and condition frames recursively. Through
extensive experiments, Qffusion consistently outperforms state-of-the-art
techniques on portrait video editing. Project page:
https://qffusion.github.io/page/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-domain Multi-step Thinking: Zero-shot Fine-grained Traffic Sign
  Recognition in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01534v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01534v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaozong Gan, Guang Li, Ren Togo, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we propose Cross-domain Multi-step Thinking (CdMT) to improve
zero-shot fine-grained traffic sign recognition (TSR) performance in the wild.
Zero-shot fine-grained TSR in the wild is challenging due to the cross-domain
problem between clean template traffic signs and real-world counterparts, and
existing approaches particularly struggle with cross-country TSR scenarios,
where traffic signs typically differ between countries. The proposed CdMT
framework tackles these challenges by leveraging the multi-step reasoning
capabilities of large multimodal models (LMMs). We introduce context,
characteristic, and differential descriptions to design multiple thinking
processes for LMMs. Context descriptions, which are enhanced by center
coordinate prompt optimization, enable the precise localization of target
traffic signs in complex road images and filter irrelevant responses via novel
prior traffic sign hypotheses. Characteristic descriptions, which are derived
from in-context learning with template traffic signs, bridge cross-domain gaps
and enhance fine-grained TSR. Differential descriptions refine the multimodal
reasoning ability of LMMs by distinguishing subtle differences among similar
signs. CdMT is independent of training data and requires only simple and
uniform instructions, enabling it to achieve cross-country TSR. We conducted
extensive experiments on three benchmark datasets and two real-world datasets
from different countries. The proposed CdMT framework achieved superior
performance compared with other state-of-the-art methods on all five datasets,
with recognition accuracies of 0.93, 0.89, 0.97, 0.89, and 0.85 on the GTSRB,
BTSD, TT-100K, Sapporo, and Yokohama datasets, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published by Knowledge-Based Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human-Activity AGV Quality Assessment: A <span class="highlight-title">Benchmark</span> <span class="highlight-title">Dataset</span> and an
  Objective <span class="highlight-title">Evaluation</span> Metric 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16619v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16619v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichao Zhang, Wei Sun, Xinyue Li, Yunhao Li, Qihang Ge, Jun Jia, Zicheng Zhang, Zhongpeng Ji, Fengyu Sun, Shangling Jui, Xiongkuo Min, Guangtao Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-driven video generation techniques have made significant progress in
recent years. However, AI-generated videos (AGVs) involving human activities
often exhibit substantial visual and semantic distortions, hindering the
practical application of video generation technologies in real-world scenarios.
To address this challenge, we conduct a pioneering study on human activity AGV
quality assessment, focusing on visual quality evaluation and the
identification of semantic distortions. First, we construct the AI-Generated
Human activity Video Quality Assessment (Human-AGVQA) dataset, consisting of
6,000 AGVs derived from 15 popular text-to-video (T2V) models using 400 text
prompts that describe diverse human activities. We conduct a subjective study
to evaluate the human appearance quality, action continuity quality, and
overall video quality of AGVs, and identify semantic issues of human body
parts. Based on Human-AGVQA, we benchmark the performance of T2V models and
analyze their strengths and weaknesses in generating different categories of
human activities. Second, we develop an objective evaluation metric, named
AI-Generated Human activity Video Quality metric (GHVQ), to automatically
analyze the quality of human activity AGVs. GHVQ systematically extracts
human-focused quality features, AI-generated content-aware quality features,
and temporal continuity features, making it a comprehensive and explainable
quality metric for human activity AGVs. The extensive experimental results show
that GHVQ outperforms existing quality metrics on the Human-AGVQA dataset by a
large margin, demonstrating its efficacy in assessing the quality of human
activity AGVs. The Human-AGVQA dataset and GHVQ metric will be released at
https://github.com/zczhang-sjtu/GHVQ.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACMMM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniCUE: Unified Recognition and Generation Framework for Chinese Cued
  Speech Video-to-Speech Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04134v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04134v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinting Wang, Shan Yang, Li Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cued Speech (CS) enhances lipreading through hand coding, providing precise
speech perception support for the hearing-impaired. CS Video-to-Speech
generation (CSV2S) task aims to convert the CS visual expressions (CS videos)
of hearing-impaired individuals into comprehensible speech signals. Direct
generation of speech from CS video (called single CSV2S) yields poor
performance due to insufficient CS data. Current research mostly focuses on CS
Recognition (CSR), which convert video content into linguistic text. Based on
this, one straightforward way of CSV2S is to combine CSR with a Text-to-Speech
system. This combined architecture relies on text as an intermediate medium for
stepwise cross-modal alignment, which may lead to error propagation and
temporal misalignment between speech and video dynamics. To address these
challenges, we propose a novel approach that directly generates speech from CS
videos without relying on intermediate text. Building upon this, we propose
UniCUE, the first unified framework for CSV2S, whose core innovation lies in
the integration of the CSR task that provides fine-grained visual-semantic
information to facilitate speech generation from CS videos. More precisely, (1)
a novel fine-grained semantic alignment pool to ensure precise mapping between
visual features and speech contents; (2) a VisioPhonetic adapter to bridge
cross-task representations, ensuring seamless compatibility between two
distinct tasks (i.e., CSV2S and CSR); (3) a pose-aware visual processor is
introduced to enhance fine-grained spatiotemporal correlations between lip and
hand movements in CS video. Experiments on our new established Chinese CS
dataset show that our UniCUE achieves state-of-the-art performance across
various metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dif<span class="highlight-title">fusion</span>-Guided Knowledge Distillation for Weakly-Supervised Low-Light
  Semantic <span class="highlight-title">Segmentation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07578v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07578v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunyan Wang, Dong Zhang, Jinhui Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly-supervised semantic segmentation aims to assign category labels to
each pixel using weak annotations, significantly reducing manual annotation
costs. Although existing methods have achieved remarkable progress in well-lit
scenarios, their performance significantly degrades in low-light environments
due to two fundamental limitations: severe image quality degradation (e.g., low
contrast, noise, and color distortion) and the inherent constraints of weak
supervision. These factors collectively lead to unreliable class activation
maps and semantically ambiguous pseudo-labels, ultimately compromising the
model's ability to learn discriminative feature representations. To address
these problems, we propose Diffusion-Guided Knowledge Distillation for
Weakly-Supervised Low-light Semantic Segmentation (DGKD-WLSS), a novel
framework that synergistically combines Diffusion-Guided Knowledge Distillation
(DGKD) with Depth-Guided Feature Fusion (DGF2). DGKD aligns normal-light and
low-light features via diffusion-based denoising and knowledge distillation,
while DGF2 integrates depth maps as illumination-invariant geometric priors to
enhance structural feature learning. Extensive experiments demonstrate the
effectiveness of DGKD-WLSS, which achieves state-of-the-art performance in
weakly supervised semantic segmentation tasks under low-light conditions. The
source codes have been released at:https://github.com/ChunyanWang1/DGKD-WLSS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM Multimedia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Advancing Multimodal Reasoning via <span class="highlight-title">Reinforcement</span> Learning with Cold
  Start 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.22334v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.22334v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lai Wei, Yuting Li, Kaipeng Zheng, Chen Wang, <span class="highlight-author">Yue Wang</span>, Linghe Kong, Lichao Sun, Weiran Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have demonstrated
impressive chain-of-thought reasoning capabilities, with reinforcement learning
(RL) playing a crucial role in this progress. While "aha moment"
patterns--where models exhibit self-correction through reflection--are often
attributed to emergent properties from RL, we first demonstrate that these
patterns exist in multimodal LLMs (MLLMs) prior to RL training but may not
necessarily correlate with improved reasoning performance. Building on these
insights, we present a comprehensive study on enhancing multimodal reasoning
through a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start
with structured chain-of-thought reasoning patterns, followed by (2)
reinforcement learning via GRPO to further refine these capabilities. Our
extensive experiments show that this combined approach consistently outperforms
both SFT-only and RL-only methods across challenging multimodal reasoning
benchmarks. The resulting models achieve state-of-the-art performance among
open-source MLLMs at both 3B and 7B scales, with our 7B model showing
substantial improvements over base models (e.g., 66.3 %$\rightarrow$73.4 % on
MathVista, 62.9 %$\rightarrow$70.4 % on We-Math) and our 3B model achieving
performance competitive with several 7B models. Overall, this work provides
practical guidance for building advanced multimodal reasoning models. Our code
is available at https://github.com/waltonfuture/RL-with-Cold-Start.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SegQuant: A Semantics-Aware and Generalizable Quantization Framework for
  Dif<span class="highlight-title">fusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14811v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14811v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaji Zhang, Ruichao Sun, Hailiang Zhao, Jiaju Wu, Peng Chen, Hao Li, Yuying Liu, Xinkui Zhao, Kingsum Chow, Gang Xiong, Shuiguang Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated exceptional generative capabilities but
are computationally intensive, posing significant challenges for deployment in
resource-constrained or latency-sensitive environments. Quantization offers an
effective means to reduce model size and computational cost, with post-training
quantization (PTQ) being particularly appealing due to its compatibility with
pre-trained models without requiring retraining or training data. However,
existing PTQ methods for diffusion models often rely on architecture-specific
heuristics that limit their generalizability and hinder integration with
industrial deployment pipelines. To address these limitations, we propose
SegQuant, a unified quantization framework that adaptively combines
complementary techniques to enhance cross-model versatility. SegQuant consists
of a segment-aware, graph-based quantization strategy (SegLinear) that captures
structural semantics and spatial heterogeneity, along with a dual-scale
quantization scheme (DualScale) that preserves polarity-asymmetric activations,
which is crucial for maintaining visual fidelity in generated outputs. SegQuant
is broadly applicable beyond Transformer-based diffusion models, achieving
strong performance while ensuring seamless compatibility with mainstream
deployment tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video
  Understanding <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.02591v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.02591v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weili Xu, Enxin Song, Wenhao Chai, Xuexiang Wen, Tian Ye, Gaoang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The challenge of long video understanding lies in its high computational
complexity and prohibitive memory cost, since the memory and computation
required by transformer-based LLMs scale quadratically with input sequence
length. We propose AuroraLong to address this challenge by replacing the LLM
component in MLLMs with a linear RNN language model that handles input sequence
of arbitrary length with constant-size hidden states. To further increase
throughput and efficiency, we combine visual token merge with linear RNN models
by reordering the visual tokens by their sizes in ascending order. Despite
having only 2B parameters and being trained exclusively on public data,
AuroraLong achieves performance comparable to Transformer-based models of
similar size trained on private datasets across multiple video benchmarks. This
demonstrates the potential of efficient, linear RNNs to democratize long video
understanding by lowering its computational entry barrier. To our best
knowledge, we are the first to use a linear RNN based LLM backbone in a
LLaVA-like model for open-ended video understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025 Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence <span class="chip" style="font-size: 60%">136</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthesis of timeline-based <span class="highlight-title">planning</span> strategies avoiding determinization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dario Della Monica, Angelo Montanari, Pietro Sala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Qualitative timeline-based planning models domains as sets of independent,
but
  interacting, components whose behaviors over time, the timelines, are
governed
  by sets of qualitative temporal constraints (ordering relations), called
  synchronization rules.
  Its plan-existence problem has been shown to be PSPACE-complete; in
  particular, PSPACE-membership has been proved via reduction to the
  nonemptiness problem for nondeterministic finite automata.
  However, nondeterministic automata cannot be directly used to synthesize
  planning strategies as a costly determinization step is needed.
  In this paper, we identify a fragment of qualitative timeline-based planning
  whose plan-existence problem can be directly mapped into the nonemptiness
  problem of deterministic finite automata, which can then
  synthesize strategies.
  In addition, we identify a maximal subset of Allen's relations that fits into
  such a deterministic fragment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2410.22757</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoding Instructional Dialogue: Human-AI Collaborative Analysis of
  Teacher Use of AI Tool at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17985v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17985v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Liu, Lief Esbenshade, Shawon Sarkar, Victor Tian, Zachary Zhang, Kevin He, Min Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of large language models (LLMs) into educational tools has
the potential to substantially impact how teachers plan instruction, support
diverse learners, and engage in professional reflection. Yet little is known
about how educators actually use these tools in practice and how their
interactions with AI can be meaningfully studied at scale. This paper presents
a human-AI collaborative methodology for large-scale qualitative analysis of
over 140,000 educator-AI messages drawn from a generative AI platform used by
K-12 teachers. Through a four-phase coding pipeline, we combined inductive
theme discovery, codebook development, structured annotation, and model
benchmarking to examine patterns of educator engagement and evaluate the
performance of LLMs in qualitative coding tasks. We developed a hierarchical
codebook aligned with established teacher evaluation frameworks, capturing
educators' instructional goals, contextual needs, and pedagogical strategies.
Our findings demonstrate that LLMs, particularly Claude 3.5 Haiku, can reliably
support theme identification, extend human recognition in complex scenarios,
and outperform open-weight models in both accuracy and structural reliability.
The analysis also reveals substantive patterns in how educators inquire AI to
enhance instructional practices (79.7 percent of total conversations), create
or adapt content (76.1 percent), support assessment and feedback loop (46.9
percent), attend to student needs for tailored instruction (43.3 percent), and
assist other professional responsibilities (34.2 percent), highlighting
emerging AI-related competencies that have direct implications for teacher
preparation and professional development. This study offers a scalable,
transparent model for AI-augmented qualitative research and provides
foundational insights into the evolving role of generative AI in educational
practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Unlearning of Traffic State <span class="highlight-title">Estimation</span> and <span class="highlight-title">Prediction</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17984v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17984v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Wang, R. Tyrrell Rockafellar,  Xuegang,  Ban
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven traffic state estimation and prediction (TSEP) relies heavily on
data sources that contain sensitive information. While the abundance of data
has fueled significant breakthroughs, particularly in machine learning-based
methods, it also raises concerns regarding privacy, cybersecurity, and data
freshness. These issues can erode public trust in intelligent transportation
systems. Recently, regulations have introduced the "right to be forgotten",
allowing users to request the removal of their private data from models. As
machine learning models can remember old data, simply removing it from back-end
databases is insufficient in such systems. To address these challenges, this
study introduces a novel learning paradigm for TSEP-Machine Unlearning
TSEP-which enables a trained TSEP model to selectively forget
privacy-sensitive, poisoned, or outdated data. By empowering models to
"unlearn," we aim to enhance the trustworthiness and reliability of data-driven
traffic TSEP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MeAJOR Corpus: A Multi-Source <span class="highlight-title">Dataset</span> for Phishing Email <span class="highlight-title">Detection</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paulo Mendes, Eva Maia, Isabel Praça
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Phishing emails continue to pose a significant threat to cybersecurity by
exploiting human vulnerabilities through deceptive content and malicious
payloads. While Machine Learning (ML) models are effective at detecting
phishing threats, their performance largely relies on the quality and diversity
of the training data. This paper presents MeAJOR (Merged email Assets from
Joint Open-source Repositories) Corpus, a novel, multi-source phishing email
dataset designed to overcome critical limitations in existing resources. It
integrates 135894 samples representing a broad number of phishing tactics and
legitimate emails, with a wide spectrum of engineered features. We evaluated
the dataset's utility for phishing detection research through systematic
experiments with four classification models (RF, XGB, MLP, and CNN) across
multiple feature configurations. Results highlight the dataset's effectiveness,
achieving 98.34% F1 with XGB. By integrating broad features from multiple
categories, our dataset provides a reusable and consistent resource, while
addressing common challenges like class imbalance, generalisability and
reproducibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 tables, WI-IAT 2025 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving the Computational Efficiency and Explainability of
  GeoAggregator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17977v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17977v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Deng, Ziqi Li, Mingshu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate modeling and explaining geospatial tabular data (GTD) are critical
for understanding geospatial phenomena and their underlying processes. Recent
work has proposed a novel transformer-based deep learning model named
GeoAggregator (GA) for this purpose, and has demonstrated that it outperforms
other statistical and machine learning approaches. In this short paper, we
further improve GA by 1) developing an optimized pipeline that accelerates the
dataloading process and streamlines the forward pass of GA to achieve better
computational efficiency; and 2) incorporating a model ensembling strategy and
a post-hoc model explanation function based on the GeoShapley framework to
enhance model explainability. We validate the functionality and efficiency of
the proposed strategies by applying the improved GA model to synthetic
datasets. Experimental results show that our implementation improves the
prediction accuracy and inference speed of GA compared to the original
implementation. Moreover, explanation experiments indicate that GA can
effectively captures the inherent spatial effects in the designed synthetic
dataset. The complete pipeline has been made publicly available for community
use (https://github.com/ruid7181/GA-sklearn).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Natural Language Processing for Tigrinya: Current State and Future
  Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fitsum Gaim, Jong C. Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite being spoken by millions of people, Tigrinya remains severely
underrepresented in Natural Language Processing (NLP) research. This work
presents a comprehensive survey of NLP research for Tigrinya, analyzing over 40
studies spanning more than a decade of work from 2011 to 2025. We
systematically review the current state of computational resources, models, and
applications across ten distinct downstream tasks, including morphological
processing, machine translation, speech recognition, and question-answering.
Our analysis reveals a clear trajectory from foundational, rule-based systems
to modern neural architectures, with progress consistently unlocked by resource
creation milestones. We identify key challenges rooted in Tigrinya's
morphological complexity and resource scarcity, while highlighting promising
research directions, including morphology-aware modeling, cross-lingual
transfer, and community-centered resource development. This work serves as both
a comprehensive reference for researchers and a roadmap for advancing Tigrinya
NLP. A curated metadata of the surveyed studies and resources is made publicly
available.\footnote{Tigrinya NLP Anthology:
https://github.com/fgaim/tigrinya-nlp-anthology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VIBE: Video-Input Brain Encoder for fMRI Response Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17958v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17958v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Carlstrom Schad, Shrey Dixit, Janis Keck, Viktor Studenyak, Aleksandr Shpilevoi, Andrej Bicanski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present VIBE, a two-stage Transformer that fuses multi-modal video, audio,
and text features to predict fMRI activity. Representations from open-source
models (Qwen2.5, BEATs, Whisper, SlowFast, V-JEPA) are merged by a
modality-fusion transformer and temporally decoded by a prediction transformer
with rotary embeddings. Trained on 65 hours of movie data from the CNeuroMod
dataset and ensembled across 20 seeds, VIBE attains mean parcel-wise Pearson
correlations of 32.25 on in-distribution Friends S07 and 21.25 on six
out-of-distribution films. An earlier iteration of the same architecture
obtained 0.3198 and 0.2096, respectively, winning Phase-1 and placing second
overall in the Algonauts 2025 Challenge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are LLM Belief Updates Consistent with Bayes' Theorem? <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sohaib Imran, Ihor Kendiukhov, Matthew Broerman, Aditya Thomas, Riccardo Campanella, Rob Lamb, Peter M. Atkinson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Do larger and more capable language models learn to update their "beliefs"
about propositions more consistently with Bayes' theorem when presented with
evidence in-context? To test this, we formulate a Bayesian Coherence
Coefficient (BCC) metric and generate a dataset with which to measure the BCC.
We measure BCC for multiple pre-trained-only language models across five model
families, comparing against the number of model parameters, the amount of
training data, and model scores on common benchmarks. Our results provide
evidence for our hypothesis that larger and more capable pre-trained language
models assign credences that are more coherent with Bayes' theorem. These
results have important implications for our understanding and governance of
LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the ICML 2025 Workshop on Assessing World Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VERIRAG: Healthcare Claim Verification via Statistical Audit in
  Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17948v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17948v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubham Mohole, Hongjun Choi, Shusen Liu, Christine Klymko, Shashank Kushwaha, Derek Shi, Wesam Sakla, Sainyam Galhotra, Ruben Glatt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) systems are increasingly adopted in
clinical decision support, yet they remain methodologically blind-they retrieve
evidence but cannot vet its scientific quality. A paper claiming "Antioxidant
proteins decreased after alloferon treatment" and a rigorous multi-laboratory
replication study will be treated as equally credible, even if the former
lacked scientific rigor or was even retracted. To address this challenge, we
introduce VERIRAG, a framework that makes three notable contributions: (i) the
Veritable, an 11-point checklist that evaluates each source for methodological
rigor, including data integrity and statistical validity; (ii) a Hard-to-Vary
(HV) Score, a quantitative aggregator that weights evidence by its quality and
diversity; and (iii) a Dynamic Acceptance Threshold, which calibrates the
required evidence based on how extraordinary a claim is. Across four
datasets-comprising retracted, conflicting, comprehensive, and settled science
corpora-the VERIRAG approach consistently outperforms all baselines, achieving
absolute F1 scores ranging from 0.53 to 0.65, representing a 10 to 14 point
improvement over the next-best method in each respective dataset. We will
release all materials necessary for reproducing our results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the Performance of AI Text <span class="highlight-title">Detect</span>ors, Few-Shot and
  Chain-of-Thought Prompting Using DeepSeek Generated Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hulayyil Alshammari, Praveen Rao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have rapidly transformed the creation of written
materials. LLMs have led to questions about writing integrity, thereby driving
the creation of artificial intelligence (AI) detection technologies.
Adversarial attacks, such as standard and humanized paraphrasing, inhibit
detectors' ability to detect machine-generated text. Previous studies have
mainly focused on ChatGPT and other well-known LLMs and have shown varying
accuracy across detectors. However, there is a clear gap in the literature
about DeepSeek, a recently published LLM. Therefore, in this work, we
investigate whether six generally accessible AI detection tools -- AI Text
Classifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, and GPTZero -- can
consistently recognize text generated by DeepSeek. The detectors were exposed
to the aforementioned adversarial attacks. We also considered DeepSeek as a
detector by performing few-shot prompting and chain-of-thought reasoning (CoT)
for classifying AI and human-written text. We collected 49 human-authored
question-answer pairs from before the LLM era and generated matching responses
using DeepSeek-v3, producing 49 AI-generated samples. Then, we applied
adversarial techniques such as paraphrasing and humanizing to add 196 more
samples. These were used to challenge detector robustness and assess accuracy
impact. While QuillBot and Copyleaks showed near-perfect performance on
original and paraphrased DeepSeek text, others -- particularly AI Text
Classifier and GPT-2 -- showed inconsistent results. The most effective attack
was humanization, reducing accuracy to 71% for Copyleaks, 58% for QuillBot, and
52% for GPTZero. Few-shot and CoT prompting showed high accuracy, with the best
five-shot result misclassifying only one of 49 samples (AI recall 96%, human
recall 100%).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Minimax Data Sanitization with Distortion Constraint and Adversarial
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17942v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17942v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirarsalan Moatazedian, Yauhen Yakimenka, Rémi A. Chou, Jörg Kliewer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a privacy-preserving data-sharing setting where a privatizer
transforms private data into a sanitized version observed by an authorized
reconstructor and two unauthorized adversaries, each with access to side
information correlated with the private data.
  The reconstructor is evaluated under a distortion function, while each
adversary is evaluated using a separate loss function. The privatizer ensures
the reconstructor distortion remains below a fixed threshold while maximizing
the minimum loss across the two adversaries. This two-adversary setting models
cases where individual users cannot reconstruct the data accurately, but their
combined side information enables estimation within the distortion threshold.
The privatizer maximizes individual loss while permitting accurate
reconstruction only through collaboration. This echoes secret-sharing
principles, but with lossy rather than perfect recovery. We frame this as a
constrained data-driven minimax optimization problem and propose a data-driven
training procedure that alternately updates the privatizer, reconstructor, and
adversaries. We also analyze the Gaussian and binary cases as special scenarios
where optimal solutions can be obtained. These theoretical optimal results are
benchmarks for evaluating the proposed minimax training approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE ITW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bob's Confetti: Phonetic Memorization Attacks in Music and Video
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaechul Roh, Zachary Novack, Yuefeng Peng, Niloofar Mireshghallah, Taylor Berg-Kirkpatrick, Amir Houmansadr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lyrics-to-Song (LS2) generation models promise end-to-end music synthesis
from text, yet their vulnerability to training data memorization remains
underexplored. We introduce Adversarial PhoneTic Prompting (APT), a novel
attack where lyrics are semantically altered while preserving their acoustic
structure through homophonic substitutions (e.g., Eminem's famous "mom's
spaghetti" $\rightarrow$ "Bob's confetti"). Despite these distortions, we
uncover a powerful form of sub-lexical memorization: models like SUNO and YuE
regenerate outputs strikingly similar to known training content, achieving high
similarity across audio-domain metrics, including CLAP, AudioJudge, and
CoverID. This vulnerability persists across multiple languages and genres. More
surprisingly, we discover that phoneme-altered lyrics alone can trigger visual
memorization in text-to-video models. When prompted with phonetically modified
lyrics from Lose Yourself, Veo 3 reconstructs visual elements from the original
music video -- including character appearance and scene composition -- despite
no visual cues in the prompt. We term this phenomenon phonetic-to-visual
regurgitation. Together, these findings expose a critical vulnerability in
transcript-conditioned multimodal generation: phonetic prompting alone can
unlock memorized audiovisual content, raising urgent questions about copyright,
safety, and content provenance in modern generative systems. Example
generations are available on our demo page (jrohsc.github.io/music_attack/).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SMARTAPS: Tool-augmented LLMs for Operations Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17927v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17927v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timothy Tin Long Yu, Mahdi Mostajabdaveh, Jabo Serge Byusa, Rindra Ramamonjison, Giuseppe Carenini, Kun Mao, Zirui Zhou, Yong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) present intriguing opportunities to enhance user
interaction with traditional algorithms and tools in real-world applications.
An advanced planning system (APS) is a sophisticated software that leverages
optimization to help operations planners create, interpret, and modify an
operational plan. While highly beneficial, many customers are priced out of
using an APS due to the ongoing costs of consultants responsible for
customization and maintenance. To address the need for a more accessible APS
expressed by supply chain planners, we present SmartAPS, a conversational
system built on a tool-augmented LLM. Our system provides operations planners
with an intuitive natural language chat interface, allowing them to query
information, perform counterfactual reasoning, receive recommendations, and
execute scenario analysis to better manage their operation. A short video
demonstrating the system has been released: https://youtu.be/KtIrJjlDbyw
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://aaai.org/conference/aaai/aaai-25/bridge-ai-orms/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UrbanPulse: A Cross-City Deep Learning Framework for Ultra-Fine-Grained
  Population Transfer <span class="highlight-title">Prediction</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongrong Yang, Markus Schlaepfer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate population flow prediction is essential for urban planning,
transportation management, and public health. Yet existing methods face key
limitations: traditional models rely on static spatial assumptions, deep
learning models struggle with cross-city generalization, and Large Language
Models (LLMs) incur high computational costs while failing to capture spatial
structure. Moreover, many approaches sacrifice resolution by clustering Points
of Interest (POIs) or restricting coverage to subregions, limiting their
utility for city-wide analytics. We introduce UrbanPulse, a scalable deep
learning framework that delivers ultra-fine-grained, city-wide OD flow
predictions by treating each POI as an individual node. It combines a temporal
graph convolutional encoder with a transformer-based decoder to model
multi-scale spatiotemporal dependencies. To ensure robust generalization across
urban contexts, UrbanPulse employs a three-stage transfer learning strategy:
pretraining on large-scale urban graphs, cold-start adaptation, and
reinforcement learning fine-tuning.Evaluated on over 103 million cleaned GPS
records from three metropolitan areas in California, UrbanPulse achieves
state-of-the-art accuracy and scalability. Through efficient transfer learning,
UrbanPulse takes a key step toward making high-resolution, AI-powered urban
forecasting deployable in practice across diverse cities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Seed to Harvest: Augmenting Human Creativity with AI for
  Red-teaming Text-to-Image Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jessica Quaye, Charvi Rastogi, Alicia Parrish, Oana Inel, Minsuk Kahng, Lora Aroyo, Vijay Janapa Reddi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image (T2I) models have become prevalent across numerous
applications, making their robust evaluation against adversarial attacks a
critical priority. Continuous access to new and challenging adversarial prompts
across diverse domains is essential for stress-testing these models for
resilience against novel attacks from multiple vectors. Current techniques for
generating such prompts are either entirely authored by humans or synthetically
generated. On the one hand, datasets of human-crafted adversarial prompts are
often too small in size and imbalanced in their cultural and contextual
representation. On the other hand, datasets of synthetically-generated prompts
achieve scale, but typically lack the realistic nuances and creative
adversarial strategies found in human-crafted prompts. To combine the strengths
of both human and machine approaches, we propose Seed2Harvest, a hybrid
red-teaming method for guided expansion of culturally diverse, human-crafted
adversarial prompt seeds. The resulting prompts preserve the characteristics
and attack patterns of human prompts while maintaining comparable average
attack success rates (0.31 NudeNet, 0.36 SD NSFW, 0.12 Q16). Our expanded
dataset achieves substantially higher diversity with 535 unique geographic
locations and a Shannon entropy of 7.48, compared to 58 locations and 5.28
entropy in the original dataset. Our work demonstrates the importance of
human-machine collaboration in leveraging human creativity and machine
computational capacity to achieve comprehensive, scalable red-teaming for
continuous T2I model safety evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep learning-aided inverse design of porous metamaterials 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phu Thien Nguyen, Yousef Heider, Dennis M. Kochmann, Fadi Aldakheel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ultimate aim of the study is to explore the inverse design of porous
metamaterials using a deep learning-based generative framework. Specifically,
we develop a property-variational autoencoder (pVAE), a variational autoencoder
(VAE) augmented with a regressor, to generate structured metamaterials with
tailored hydraulic properties, such as porosity and permeability. While this
work uses the lattice Boltzmann method (LBM) to generate intrinsic permeability
tensor data for limited porous microstructures, a convolutional neural network
(CNN) is trained using a bottom-up approach to predict effective hydraulic
properties. This significantly reduces the computational cost compared to
direct LBM simulations. The pVAE framework is trained on two datasets: a
synthetic dataset of artificial porous microstructures and CT-scan images of
volume elements from real open-cell foams. The encoder-decoder architecture of
the VAE captures key microstructural features, mapping them into a compact and
interpretable latent space for efficient structure-property exploration. The
study provides a detailed analysis and interpretation of the latent space,
demonstrating its role in structure-property mapping, interpolation, and
inverse design. This approach facilitates the generation of new metamaterials
with desired properties. The datasets and codes used in this study will be made
open-access to support further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 29 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VeriMinder: Mitigating Analytical Vulnerabilities in NL2SQL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17896v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17896v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubham Mohole, Sainyam Galhotra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Application systems using natural language interfaces to databases (NLIDBs)
have democratized data analysis. This positive development has also brought
forth an urgent challenge to help users who might use these systems without a
background in statistical analysis to formulate bias-free analytical questions.
Although significant research has focused on text-to-SQL generation accuracy,
addressing cognitive biases in analytical questions remains underexplored. We
present VeriMinder, https://veriminder.ai, an interactive system for detecting
and mitigating such analytical vulnerabilities. Our approach introduces three
key innovations: (1) a contextual semantic mapping framework for biases
relevant to specific analysis contexts (2) an analytical framework that
operationalizes the Hard-to-Vary principle and guides users in systematic data
analysis (3) an optimized LLM-powered system that generates high-quality,
task-specific prompts using a structured process involving multiple candidates,
critic feedback, and self-reflection.
  User testing confirms the merits of our approach. In direct user experience
evaluation, 82.5% participants reported positively impacting the quality of the
analysis. In comparative evaluation, VeriMinder scored significantly higher
than alternative approaches, at least 20% better when considered for metrics of
the analysis's concreteness, comprehensiveness, and accuracy. Our system,
implemented as a web application, is set to help users avoid "wrong question"
vulnerability during data analysis. VeriMinder code base with prompts,
https://reproducibility.link/veriminder, is available as an MIT-licensed
open-source software to facilitate further research and adoption within the
community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Action-List <span class="highlight-title">Reinforcement</span> Learning Syndrome Decoding for Binary Linear
  Block Codes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milad Taghipour, Bane Vasic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the application of reinforcement learning techniques to
enhance the performance of decoding of linear block codes based on flipping
bits and finding optimal decisions. We describe the methodology for mapping the
iterative decoding process into Markov Decision Processes (MDPs) and propose
different methods to reduce the number of states in the MDP. A truncated MDP is
proposed to reduce the number of states in the MDP by learning a Hamming ball
with a specified radius around codewords. We then propose a general scheme for
reinforcement learning based decoders applicable to any class of codes to
improve the performance of decoders. We call this scheme an action-list
decoding. We design an action-list decoder based on the Deep-Q network values
that substantially enhance performance. We also get benefit of automorphism
group of code to further improve the code performance. Additionally, we propose
a feedback-based method to exploit and enhance the performance of existing
high-performing decoders by applying reinforcement learning algorithms after
the existing decoders. These approaches effectively reduces the complexity of
the reinforcement learning block. Finally, we present experimental results for
the Low-Density Parity Check (LDPC) codes over the Binary Symmetric Channel
(BSC) to demonstrate the efficiency of the proposed methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ I2I-STRADA -- Information to Insights via Structured Reasoning Agent for
  Data Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17874v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17874v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        SaiBarath Sundar, Pranav Satheesan, Udayaadithya Avadhanam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in agentic systems for data analysis have emphasized
automation of insight generation through multi-agent frameworks, and
orchestration layers. While these systems effectively manage tasks like query
translation, data transformation, and visualization, they often overlook the
structured reasoning process underlying analytical thinking. Reasoning large
language models (LLMs) used for multi-step problem solving are trained as
general-purpose problem solvers. As a result, their reasoning or thinking steps
do not adhere to fixed processes for specific tasks. Real-world data analysis
requires a consistent cognitive workflow: interpreting vague goals, grounding
them in contextual knowledge, constructing abstract plans, and adapting
execution based on intermediate outcomes. We introduce I2I-STRADA
(Information-to-Insight via Structured Reasoning Agent for Data Analysis), an
agentic architecture designed to formalize this reasoning process. I2I-STRADA
focuses on modeling how analysis unfolds via modular sub-tasks that reflect the
cognitive steps of analytical reasoning. Evaluations on the DABstep and DABench
benchmarks show that I2I-STRADA outperforms prior systems in planning coherence
and insight alignment, highlighting the importance of structured cognitive
workflows in agent design for data analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Facilitated Fairness Assessment of AI-based Skin Lesion
  Classifiers Through GenAI-based Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ko Watanabe. Stanislav Frolov. Adriano Lucieri. Andreas Dengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Deep Learning and its application on the edge hold
great potential for the revolution of routine screenings for skin cancers like
Melanoma. Along with the anticipated benefits of this technology, potential
dangers arise from unforseen and inherent biases. Thus, assessing and improving
the fairness of such systems is of utmost importance. A key challenge in
fairness assessment is to ensure that the evaluation dataset is sufficiently
representative of different Personal Identifiable Information (PII) (sex, age,
and race) and other minority groups. Against the backdrop of this challenge,
this study leverages the state-of-the-art Generative AI (GenAI) LightningDiT
model to assess the fairness of publicly available melanoma classifiers. The
results suggest that fairness assessment using highly realistic synthetic data
is a promising direction. Yet, our findings indicate that verifying fairness
becomes difficult when the melanoma-detection model used for evaluation is
trained on data that differ from the dataset underpinning the synthetic images.
Nonetheless, we propose that our approach offers a valuable new avenue for
employing synthetic data to gauge and enhance fairness in medical-imaging GenAI
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detail++: Training-Free Detail Enhancer for Text-to-Image Dif<span class="highlight-title">fusion</span>
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lifeng Chen, Jiner Wang, Zihao Pan, Beier Zhu, Xiaofeng Yang, Chi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-to-image (T2I) generation have led to impressive
visual results. However, these models still face significant challenges when
handling complex prompt, particularly those involving multiple subjects with
distinct attributes. Inspired by the human drawing process, which first
outlines the composition and then incrementally adds details, we propose
Detail++, a training-free framework that introduces a novel Progressive Detail
Injection (PDI) strategy to address this limitation. Specifically, we decompose
a complex prompt into a sequence of simplified sub-prompts, guiding the
generation process in stages. This staged generation leverages the inherent
layout-controlling capacity of self-attention to first ensure global
composition, followed by precise refinement. To achieve accurate binding
between attributes and corresponding subjects, we exploit cross-attention
mechanisms and further introduce a Centroid Alignment Loss at test time to
reduce binding noise and enhance attribute consistency. Extensive experiments
on T2I-CompBench and a newly constructed style composition benchmark
demonstrate that Detail++ significantly outperforms existing methods,
particularly in scenarios involving multiple objects and complex stylistic
conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Performance <span class="highlight-title">Evaluation</span> and Threat Mitigation in Large-scale 5G Core
  Deployment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17850v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17850v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigo Moreira, Larissa F. Rodrigues Moreira, Flávio de Oliveira Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of large-scale software-based 5G core functions presents
significant challenges due to their reliance on optimized and intelligent
resource provisioning for their services. Many studies have focused on
analyzing the impact of resource allocation for complex deployments using
mathematical models, queue theories, or even Artificial Intelligence (AI). This
paper elucidates the effects of chaotic workloads, generated by Distributed
Denial of Service (DDoS) on different Network Functions (NFs) on User Equipment
registration performance. Our findings highlight the necessity of diverse
resource profiles to ensure Service-Level Agreement (SLA) compliance in
large-scale 5G core deployments. Additionally, our analysis of packet capture
approaches demonstrates the potential of kernel-based monitoring for scalable
security threat defense. Finally, our empirical evaluation provides insights
into the effective deployment of 5G NFs in complex scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SV3.3B: A Sports Video Understanding Model for Action Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Varun Kodathala, Yashwanth Reddy Vutukoori, Rakesh Vunnam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenge of automated sports video analysis, which
has traditionally been limited by computationally intensive models requiring
server-side processing and lacking fine-grained understanding of athletic
movements. Current approaches struggle to capture the nuanced biomechanical
transitions essential for meaningful sports analysis, often missing critical
phases like preparation, execution, and follow-through that occur within
seconds. To address these limitations, we introduce SV3.3B, a lightweight 3.3B
parameter video understanding model that combines novel temporal motion
difference sampling with self-supervised learning for efficient on-device
deployment. Our approach employs a DWT-VGG16-LDA based keyframe extraction
mechanism that intelligently identifies the 16 most representative frames from
sports sequences, followed by a V-DWT-JEPA2 encoder pretrained through
mask-denoising objectives and an LLM decoder fine-tuned for sports action
description generation. Evaluated on a subset of the NSVA basketball dataset,
SV3.3B achieves superior performance across both traditional text generation
metrics and sports-specific evaluation criteria, outperforming larger
closed-source models including GPT-4o variants while maintaining significantly
lower computational requirements. Our model demonstrates exceptional capability
in generating technically detailed and analytically rich sports descriptions,
achieving 29.2% improvement over GPT-4o in ground truth validation metrics,
with substantial improvements in information density, action complexity, and
measurement precision metrics essential for comprehensive athletic analysis.
Model Available at https://huggingface.co/sportsvision/SV3.3B.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 4 tables. Submitted to AIxSET 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Learning Rates Simultaneously Achieve <span class="highlight-title">Robust</span>ness to Spurious
  Correlations and Compressibility <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Melih Barsbey, Lucas Prieto, Stefanos Zafeiriou, Tolga Birdal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robustness and resource-efficiency are two highly desirable properties for
modern machine learning models. However, achieving them jointly remains a
challenge. In this paper, we position high learning rates as a facilitator for
simultaneously achieving robustness to spurious correlations and network
compressibility. We demonstrate that large learning rates also produce
desirable representation properties such as invariant feature utilization,
class separation, and activation sparsity. Importantly, our findings indicate
that large learning rates compare favorably to other hyperparameters and
regularization methods, in consistently satisfying these properties in tandem.
In addition to demonstrating the positive effect of large learning rates across
diverse spurious correlation datasets, models, and optimizers, we also present
strong evidence that the previously documented success of large learning rates
in standard classification tasks is likely due to its effect on addressing
hidden/rare spurious correlations in the training dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCV 2025, 23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pretraining on the Test Set Is No Longer All You Need: A Debate-Driven
  Approach to QA <span class="highlight-title">Benchmark</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linbo Cao, Jinman Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As frontier language models increasingly saturate standard QA benchmarks,
concerns about data contamination, memorization, and escalating dataset
creation costs persist. We propose a debate-driven evaluation paradigm that
transforms any existing QA dataset into structured adversarial debates--where
one model is given the official answer to defend, and another constructs and
defends an alternative answer--adjudicated by a judge model blind to the
correct solution. By forcing multi-round argumentation, this approach
substantially increases difficulty while penalizing shallow memorization, yet
reuses QA items to reduce curation overhead. We make two main contributions:
(1) an evaluation pipeline to systematically convert QA tasks into debate-based
assessments, and (2) a public benchmark that demonstrates our paradigm's
effectiveness on a subset of MMLU-Pro questions, complete with standardized
protocols and reference models. Empirical results validate the robustness of
the method and its effectiveness against data contamination--a Llama 3.1 model
fine-tuned on test questions showed dramatic accuracy improvements (50% -> 82%)
but performed worse in debates. Results also show that even weaker judges can
reliably differentiate stronger debaters, highlighting how debate-based
evaluation can scale to future, more capable systems while maintaining a
fraction of the cost of creating new benchmarks. Overall, our framework
underscores that "pretraining on the test set is no longer all you need,"
offering a sustainable path for measuring the genuine reasoning ability of
advanced language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 7 figures. Accepted to COLM 2025. Code available at:
  github.com/l6cao/Debate-Driven-Evaluation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rubrics as Rewards: <span class="highlight-title">Reinforcement</span> Learning Beyond Verifiable Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anisha Gunjal, Anthony Wang, Elaine Lau, Vaskar Nath, Bing Liu, Sean Hendryx
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extending Reinforcement Learning with Verifiable Rewards (RLVR) to real-world
tasks often requires balancing objective and subjective evaluation criteria.
However, many such tasks lack a single, unambiguous ground truth-making it
difficult to define reliable reward signals for post-training language models.
While traditional preference-based methods offer a workaround, they rely on
opaque reward functions that are difficult to interpret and prone to spurious
correlations. We introduce $\textbf{Rubrics as Rewards}$ (RaR), a framework
that uses structured, checklist-style rubrics as interpretable reward signals
for on-policy training with GRPO. Our best RaR method yields up to a $28\%$
relative improvement on HealthBench-1k compared to simple Likert-based
approaches, while matching or surpassing the performance of reward signals
derived from expert-written references. By treating rubrics as structured
reward signals, we show that RaR enables smaller-scale judge models to better
align with human preferences and sustain robust performance across model
scales.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ultra3D: Efficient and High-Fidelity 3D Generation with Part Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Chen, Zhihao Li, Yikai Wang, Hu Zhang, Qin Li, Chi Zhang, Guosheng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in sparse voxel representations have significantly improved
the quality of 3D content generation, enabling high-resolution modeling with
fine-grained geometry. However, existing frameworks suffer from severe
computational inefficiencies due to the quadratic complexity of attention
mechanisms in their two-stage diffusion pipelines. In this work, we propose
Ultra3D, an efficient 3D generation framework that significantly accelerates
sparse voxel modeling without compromising quality. Our method leverages the
compact VecSet representation to efficiently generate a coarse object layout in
the first stage, reducing token count and accelerating voxel coordinate
prediction. To refine per-voxel latent features in the second stage, we
introduce Part Attention, a geometry-aware localized attention mechanism that
restricts attention computation within semantically consistent part regions.
This design preserves structural continuity while avoiding unnecessary global
attention, achieving up to 6.7x speed-up in latent generation. To support this
mechanism, we construct a scalable part annotation pipeline that converts raw
meshes into part-labeled sparse voxels. Extensive experiments demonstrate that
Ultra3D supports high-resolution 3D generation at 1024 resolution and achieves
state-of-the-art performance in both visual fidelity and user preference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://buaacyw.github.io/ultra3d/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Yume: An Interactive World Generation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Mao, Shaoheng Lin, Zhen Li, Chuanhao Li, Wenshuo Peng, Tong He, Jiangmiao Pang, Mingmin Chi, Yu Qiao, Kaipeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Yume aims to use images, text, or videos to create an interactive, realistic,
and dynamic world, which allows exploration and control using peripheral
devices or neural signals. In this report, we present a preview version of
\method, which creates a dynamic world from an input image and allows
exploration of the world using keyboard actions. To achieve this high-fidelity
and interactive video world generation, we introduce a well-designed framework,
which consists of four main components, including camera motion quantization,
video generation architecture, advanced sampler, and model acceleration. First,
we quantize camera motions for stable training and user-friendly interaction
using keyboard inputs. Then, we introduce the Masked Video Diffusion
Transformer~(MVDT) with a memory module for infinite video generation in an
autoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)
and Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)
are introduced to the sampler for better visual quality and more precise
control. Moreover, we investigate model acceleration by synergistic
optimization of adversarial distillation and caching mechanisms. We use the
high-quality world exploration dataset \sekai to train \method, and it achieves
remarkable results in diverse scenes and applications. All data, codebase, and
model weights are available on https://github.com/stdstu12/YUME. Yume will
update monthly to achieve its original goal. Project page:
https://stdstu12.github.io/YUME-Project/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flow Matching Meets Biology and <span class="highlight-title">Life</span> Science: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Li, Zhichen Zeng, Xiao Lin, Feihao Fang, Yanru Qu, Zhe Xu, Zhining Liu, Xuying Ning, Tianxin Wei, Ge Liu, Hanghang Tong, Jingrui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past decade, advances in generative modeling, such as generative
adversarial networks, masked autoencoders, and diffusion models, have
significantly transformed biological research and discovery, enabling
breakthroughs in molecule design, protein generation, drug discovery, and
beyond. At the same time, biological applications have served as valuable
testbeds for evaluating the capabilities of generative models. Recently, flow
matching has emerged as a powerful and efficient alternative to diffusion-based
generative modeling, with growing interest in its application to problems in
biology and life sciences. This paper presents the first comprehensive survey
of recent developments in flow matching and its applications in biological
domains. We begin by systematically reviewing the foundations and variants of
flow matching, and then categorize its applications into three major areas:
biological sequence modeling, molecule generation and design, and peptide and
protein generation. For each, we provide an in-depth review of recent progress.
We also summarize commonly used datasets and software tools, and conclude with
a discussion of potential future directions. The corresponding curated
resources are available at
https://github.com/Violet24K/Awesome-Flow-Matching-Meets-Biology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, 27 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Submission and <span class="highlight-title">Evaluation</span> System Design for Competition
  Operations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Chen, Daniel Harabor, Ryan Hechnenberger, Nathan R. Sturtevant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research communities have developed benchmark datasets across domains to
compare the performance of algorithms and techniques However, tracking the
progress in these research areas is not easy, as publications appear in
different venues at the same time, and many of them claim to represent the
state-of-the-art. To address this, research communities often organise periodic
competitions to evaluate the performance of various algorithms and techniques,
thereby tracking advancements in the field. However, these competitions pose a
significant operational burden. The organisers must manage and evaluate a large
volume of submissions. Furthermore, participants typically develop their
solutions in diverse environments, leading to compatibility issues during the
evaluation of their submissions. This paper presents an online competition
system that automates the submission and evaluation process for a competition.
The competition system allows organisers to manage large numbers of submissions
efficiently, utilising isolated environments to evaluate submissions. This
system has already been used successfully for several competitions, including
the Grid-Based Pathfinding Competition and the League of Robot Runners
competition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work was presented at the Workshop on the International Planning
  Competition (WIPC 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Interaction of Compressibility and Adversarial <span class="highlight-title">Robust</span>ness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Melih Barsbey, Antônio H. Ribeiro, Umut Şimşekli, Tolga Birdal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern neural networks are expected to simultaneously satisfy a host of
desirable properties: accurate fitting to training data, generalization to
unseen inputs, parameter and computational efficiency, and robustness to
adversarial perturbations. While compressibility and robustness have each been
studied extensively, a unified understanding of their interaction still remains
elusive. In this work, we develop a principled framework to analyze how
different forms of compressibility - such as neuron-level sparsity and spectral
compressibility - affect adversarial robustness. We show that these forms of
compression can induce a small number of highly sensitive directions in the
representation space, which adversaries can exploit to construct effective
perturbations. Our analysis yields a simple yet instructive robustness bound,
revealing how neuron and spectral compressibility impact $L_\infty$ and $L_2$
robustness via their effects on the learned representations. Crucially, the
vulnerabilities we identify arise irrespective of how compression is achieved -
whether via regularization, architectural bias, or implicit learning dynamics.
Through empirical evaluations across synthetic and realistic tasks, we confirm
our theoretical predictions, and further demonstrate that these vulnerabilities
persist under adversarial training and transfer learning, and contribute to the
emergence of universal adversarial perturbations. Our findings show a
fundamental tension between structured compressibility and robustness, and
suggest new pathways for designing models that are both efficient and secure.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI Telephone <span class="highlight-title">Survey</span>ing: Automating Quantitative Data Collection with an
  AI Interviewer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danny D. Leybzon, Shreyas Tirumala, Nishant Jain, Summer Gillen, Michael Jackson, Cameron McPhee, Jennifer Schmidt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rise of voice-enabled artificial intelligence (AI) systems,
quantitative survey researchers have access to a new data-collection mode: AI
telephone surveying. By using AI to conduct phone interviews, researchers can
scale quantitative studies while balancing the dual goals of human-like
interactivity and methodological rigor. Unlike earlier efforts that used
interactive voice response (IVR) technology to automate these surveys, voice AI
enables a more natural and adaptive respondent experience as it is more robust
to interruptions, corrections, and other idiosyncrasies of human speech.
  We built and tested an AI system to conduct quantitative surveys based on
large language models (LLM), automatic speech recognition (ASR), and speech
synthesis technologies. The system was specifically designed for quantitative
research, and strictly adhered to research best practices like question order
randomization, answer order randomization, and exact wording.
  To validate the system's effectiveness, we deployed it to conduct two pilot
surveys with the SSRS Opinion Panel and followed-up with a separate
human-administered survey to assess respondent experiences. We measured three
key metrics: the survey completion rates, break-off rates, and respondent
satisfaction scores. Our results suggest that shorter instruments and more
responsive AI interviewers may contribute to improvements across all three
metrics studied.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Feedback to Checklists: Grounded <span class="highlight-title">Evaluation</span> of AI-Generated
  Clinical Notes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karen Zhou, John Giorgi, Pranav Mani, Peng Xu, Davis Liang, Chenhao Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-generated clinical notes are increasingly used in healthcare, but
evaluating their quality remains a challenge due to high subjectivity and
limited scalability of expert review. Existing automated metrics often fail to
align with real-world physician preferences. To address this, we propose a
pipeline that systematically distills real user feedback into structured
checklists for note evaluation. These checklists are designed to be
interpretable, grounded in human feedback, and enforceable by LLM-based
evaluators. Using deidentified data from over 21,000 clinical encounters,
prepared in accordance with the HIPAA safe harbor standard, from a deployed AI
medical scribe system, we show that our feedback-derived checklist outperforms
baseline approaches in our offline evaluations in coverage, diversity, and
predictive power for human ratings. Extensive experiments confirm the
checklist's robustness to quality-degrading perturbations, significant
alignment with clinician preferences, and practical value as an evaluation
methodology. In offline research settings, the checklist can help identify
notes likely to fall below our chosen quality thresholds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Thinking Isn't an Illusion: Overcoming the Limitations of Reasoning
  Models via Tool Augmentations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhao Song, Song Yue, Jiahao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Reasoning Models (LRMs) have become a central focus in today's large
language model (LLM) research, where models are designed to output a
step-by-step thinking process before arriving at a final answer to handle
complex reasoning tasks. Despite their promise, recent empirical studies (e.g.,
[Shojaee et al., 2025] from Apple) suggest that this thinking process may not
actually enhance reasoning ability, where LLMs without explicit reasoning
actually outperform LRMs on tasks with low or high complexity. In this work, we
revisit these findings and investigate whether the limitations of LRMs persist
when tool augmentations are introduced. We incorporate two types of tools,
Python interpreters and scratchpads, and evaluate three representative LLMs and
their LRM counterparts on Apple's benchmark reasoning puzzles. Our results show
that, with proper tool use, LRMs consistently outperform their non-reasoning
counterparts across all levels of task complexity. These findings challenge the
recent narrative that reasoning is an illusion and highlight the potential of
tool-augmented LRMs for solving complex problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilias Chatzistefanidis, Navid Nikaein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM)-based autonomous agents are expected to play a
vital role in the evolution of 6G networks, by empowering real-time
decision-making related to management and service provisioning to end-users.
This shift facilitates the transition from a specialized intelligence approach,
where artificial intelligence (AI) algorithms handle isolated tasks, to
artificial general intelligence (AGI)-driven networks, where agents possess
broader reasoning capabilities and can manage diverse network functions. In
this paper, we introduce a novel agentic paradigm that combines LLMs with
real-time optimization algorithms towards Trustworthy AI, defined as symbiotic
agents. Optimizers at the LLM's input-level provide bounded uncertainty
steering for numerically precise tasks, whereas output-level optimizers
supervised by the LLM enable adaptive real-time control. We design and
implement two novel agent types including: (i) Radio Access Network optimizers,
and (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We
further propose an end-to-end architecture for AGI networks and evaluate it on
a 5G testbed capturing channel fluctuations from moving vehicles. Results show
that symbiotic agents reduce decision errors fivefold compared to standalone
LLM-based agents, while smaller language models (SLM) achieve similar accuracy
with a 99.9% reduction in GPU resource overhead and in near-real-time loops of
82 ms. A multi-agent demonstration for collaborative RAN on the real-world
testbed highlights significant flexibility in service-level agreement and
resource allocation, reducing RAN over-utilization by approximately 44%.
Drawing on our findings and open-source implementations, we introduce the
symbiotic paradigm as the foundation for next-generation, AGI-driven
networks-systems designed to remain adaptable, efficient, and trustworthy even
as LLMs advance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Computer Networks AI for 6G</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CASCADE: LLM-Powered JavaScript Deobfuscator at Google 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shan Jiang, Pranoy Kovuri, David Tao, Zhixun Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software obfuscation, particularly prevalent in JavaScript, hinders code
comprehension and analysis, posing significant challenges to software testing,
static analysis, and malware detection. This paper introduces CASCADE, a novel
hybrid approach that integrates the advanced coding capabilities of Gemini with
the deterministic transformation capabilities of a compiler Intermediate
Representation (IR), specifically JavaScript IR (JSIR). By employing Gemini to
identify critical prelude functions, the foundational components underlying the
most prevalent obfuscation techniques, and leveraging JSIR for subsequent code
transformations, CASCADE effectively recovers semantic elements like original
strings and API names, and reveals original program behaviors. This method
overcomes limitations of existing static and dynamic deobfuscation techniques,
eliminating hundreds to thousands of hardcoded rules while achieving
reliability and flexibility. CASCADE is already deployed in Google's production
environment, demonstrating substantial improvements in JavaScript deobfuscation
efficiency and reducing reverse engineering efforts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simulating multiple human perspectives in socio-ecological systems using
  large language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchao Zeng, Calum Brown, Ioannis Kyriakou, Ronja Hotz, Mark Rounsevell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding socio-ecological systems requires insights from diverse
stakeholder perspectives, which are often hard to access. To enable
alternative, simulation-based exploration of different stakeholder
perspectives, we develop the HoPeS (Human-Oriented Perspective Shifting)
modelling framework. HoPeS employs agents powered by large language models
(LLMs) to represent various stakeholders; users can step into the agent roles
to experience perspectival differences. A simulation protocol serves as a
"scaffold" to streamline multiple perspective-taking simulations, supporting
users in reflecting on, transitioning between, and integrating across
perspectives. A prototype system is developed to demonstrate HoPeS in the
context of institutional dynamics and land use change, enabling both
narrative-driven and numerical experiments. In an illustrative experiment, a
user successively adopts the perspectives of a system observer and a researcher
- a role that analyses data from the embedded land use model to inform
evidence-based decision-making for other LLM agents representing various
institutions. Despite the user's effort to recommend technically sound
policies, discrepancies persist between the policy recommendation and
implementation due to stakeholders' competing advocacies, mirroring real-world
misalignment between researcher and policymaker perspectives. The user's
reflection highlights the subjective feelings of frustration and disappointment
as a researcher, especially due to the challenge of maintaining political
neutrality while attempting to gain political influence. Despite this, the user
exhibits high motivation to experiment with alternative narrative framing
strategies, suggesting the system's potential in exploring different
perspectives. Further system and protocol refinement are likely to enable new
forms of interdisciplinary collaboration in socio-ecological simulations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Should We Meta-Learn <span class="highlight-title">Reinforcement</span> Learning Algorithms? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander David Goldie, Zilin Wang, Jakob Nicolaus Foerster, Shimon Whiteson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The process of meta-learning algorithms from data, instead of relying on
manual design, is growing in popularity as a paradigm for improving the
performance of machine learning systems. Meta-learning shows particular promise
for reinforcement learning (RL), where algorithms are often adapted from
supervised or unsupervised learning despite their suboptimality for RL.
However, until now there has been a severe lack of comparison between different
meta-learning algorithms, such as using evolution to optimise over black-box
functions or LLMs to propose code. In this paper, we carry out this empirical
comparison of the different approaches when applied to a range of meta-learned
algorithms which target different parts of the RL pipeline. In addition to
meta-train and meta-test performance, we also investigate factors including the
interpretability, sample cost and train time for each meta-learning algorithm.
Based on these findings, we propose several guidelines for meta-learning new RL
algorithms which will help ensure that future learned algorithms are as
performant as possible.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted paper at Reinforcement Learning Conference (RLC) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision Transformer attention alignment with human visual perception in
  aesthetic object <span class="highlight-title">evaluation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miguel Carrasco, César González-Martín, José Aranda, Luis Oliveros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual attention mechanisms play a crucial role in human perception and
aesthetic evaluation. Recent advances in Vision Transformers (ViTs) have
demonstrated remarkable capabilities in computer vision tasks, yet their
alignment with human visual attention patterns remains underexplored,
particularly in aesthetic contexts. This study investigates the correlation
between human visual attention and ViT attention mechanisms when evaluating
handcrafted objects. We conducted an eye-tracking experiment with 30
participants (9 female, 21 male, mean age 24.6 years) who viewed 20 artisanal
objects comprising basketry bags and ginger jars. Using a Pupil Labs
eye-tracker, we recorded gaze patterns and generated heat maps representing
human visual attention. Simultaneously, we analyzed the same objects using a
pre-trained ViT model with DINO (Self-DIstillation with NO Labels), extracting
attention maps from each of the 12 attention heads. We compared human and ViT
attention distributions using Kullback-Leibler divergence across varying
Gaussian parameters (sigma=0.1 to 3.0). Statistical analysis revealed optimal
correlation at sigma=2.4 +-0.03, with attention head #12 showing the strongest
alignment with human visual patterns. Significant differences were found
between attention heads, with heads #7 and #9 demonstrating the greatest
divergence from human attention (p< 0.05, Tukey HSD test). Results indicate
that while ViTs exhibit more global attention patterns compared to human focal
attention, certain attention heads can approximate human visual behavior,
particularly for specific object features like buckles in basketry items. These
findings suggest potential applications of ViT attention mechanisms in product
design and aesthetic evaluation, while highlighting fundamental differences in
attention strategies between human perception and current AI models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Quantum Federated Learning with Fisher Information-Based
  <span class="highlight-title">Optimization</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amandeep Singh Bhatia, Sabre Kais
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) has become increasingly popular across different
sectors, offering a way for clients to work together to train a global model
without sharing sensitive data. It involves multiple rounds of communication
between the global model and participating clients, which introduces several
challenges like high communication costs, heterogeneous client data, prolonged
processing times, and increased vulnerability to privacy threats. In recent
years, the convergence of federated learning and parameterized quantum circuits
has sparked significant research interest, with promising implications for
fields such as healthcare and finance. By enabling decentralized training of
quantum models, it allows clients or institutions to collaboratively enhance
model performance and outcomes while preserving data privacy. Recognizing that
Fisher information can quantify the amount of information that a quantum state
carries under parameter changes, thereby providing insight into its geometric
and statistical properties. We intend to leverage this property to address the
aforementioned challenges. In this work, we propose a Quantum Federated
Learning (QFL) algorithm that makes use of the Fisher information computed on
local client models, with data distributed across heterogeneous partitions.
This approach identifies the critical parameters that significantly influence
the quantum model's performance, ensuring they are preserved during the
aggregation process. Our research assessed the effectiveness and feasibility of
QFL by comparing its performance against other variants, and exploring the
benefits of incorporating Fisher information in QFL settings. Experimental
results on ADNI and MNIST datasets demonstrate the effectiveness of our
approach in achieving better performance and robustness against the quantum
federated averaging method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration
  Through Clinical Cognitive Chain Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyao Liu, Diping Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) demonstrate significant potential in
the field of medical diagnosis. However, they face critical challenges in
specialized domains such as ophthalmology, particularly the fragmentation of
annotation granularity and inconsistencies in clinical reasoning logic, which
hinder precise cross-modal understanding. This paper introduces FundusExpert,
an ophthalmology-specific MLLM with integrated positioning-diagnosis reasoning
capabilities, along with FundusGen, a dataset constructed through the
intelligent Fundus-Engine system. Fundus-Engine automates localization and
leverages MLLM-based semantic expansion to integrate global disease
classification, local object detection, and fine-grained feature analysis
within a single fundus image. Additionally, by constructing a clinically
aligned cognitive chain, it guides the model to generate interpretable
reasoning paths. FundusExpert, fine-tuned with instruction data from FundusGen,
achieves the best performance in ophthalmic question-answering tasks,
surpassing the average accuracy of the 40B MedRegA by 26.6%. It also excels in
zero-shot report generation tasks, achieving a clinical consistency of 77.0%,
significantly outperforming GPT-4o's 47.6%. Furthermore, we reveal a scaling
law between data quality and model capability ($L \propto N^{0.068}$),
demonstrating that the cognitive alignment annotations in FundusGen enhance
data utilization efficiency. By integrating region-level localization with
diagnostic reasoning chains, our work develops a scalable, clinically-aligned
MLLM and explores a pathway toward bridging the visual-language gap in specific
MLLMs. Our project can be found at https://github.com/MeteorElf/FundusExpert.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Majorize-Minimization: Beyond Parameter Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aymeric Dieuleveut, Gersende Fort, Mahmoud Hegazy, Hoi-To Wai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a unified approach for designing stochastic optimization
algorithms that robustly scale to the federated learning setting. Our work
studies a class of Majorize-Minimization (MM) problems, which possesses a
linearly parameterized family of majorizing surrogate functions. This framework
encompasses (proximal) gradient-based algorithms for (regularized) smooth
objectives, the Expectation Maximization algorithm, and many problems seen as
variational surrogate MM. We show that our framework motivates a unifying
algorithm called Stochastic Approximation Stochastic Surrogate MM (\SSMM),
which includes previous stochastic MM procedures as special instances. We then
extend \SSMM\ to the federated setting, while taking into consideration common
bottlenecks such as data heterogeneity, partial participation, and
communication constraints; this yields \QSMM. The originality of \QSMM\ is to
learn locally and then aggregate information characterizing the
\textit{surrogate majorizing function}, contrary to classical algorithms which
learn and aggregate the \textit{original parameter}. Finally, to showcase the
flexibility of this methodology beyond our theoretical setting, we use it to
design an algorithm for computing optimal transport maps in the federated
setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Physics-Based and Data-Driven Approaches for Probabilistic
  Building Energy Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leandro Von Krannichfeldt, Kristina Orehounig, Olga Fink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building energy modeling is a key tool for optimizing the performance of
building energy systems. Historically, a wide spectrum of methods has been
explored -- ranging from conventional physics-based models to purely
data-driven techniques. Recently, hybrid approaches that combine the strengths
of both paradigms have gained attention. These include strategies such as
learning surrogates for physics-based models, modeling residuals between
simulated and observed data, fine-tuning surrogates with real-world
measurements, using physics-based outputs as additional inputs for data-driven
models, and integrating the physics-based output into the loss function the
data-driven model. Despite this progress, two significant research gaps remain.
First, most hybrid methods focus on deterministic modeling, often neglecting
the inherent uncertainties caused by factors like weather fluctuations and
occupant behavior. Second, there has been little systematic comparison within a
probabilistic modeling framework. This study addresses these gaps by evaluating
five representative hybrid approaches for probabilistic building energy
modeling, focusing on quantile predictions of building thermodynamics in a
real-world case study. Our results highlight two main findings. First, the
performance of hybrid approaches varies across different building room types,
but residual learning with a Feedforward Neural Network performs best on
average. Notably, the residual approach is the only model that produces
physically intuitive predictions when applied to out-of-distribution test data.
Second, Quantile Conformal Prediction is an effective procedure for calibrating
quantile predictions in case of indoor temperature modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enabling Cyber Security Education through Digital Twins and Generative
  AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vita Santa Barletta, Vito Bavaro, Miriana Calvano, Antonio Curci, Antonio Piccinno, Davide Pio Posa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital Twins (DTs) are gaining prominence in cybersecurity for their ability
to replicate complex IT (Information Technology), OT (Operational Technology),
and IoT (Internet of Things) infrastructures, allowing for real time
monitoring, threat analysis, and system simulation. This study investigates how
integrating DTs with penetration testing tools and Large Language Models (LLMs)
can enhance cybersecurity education and operational readiness. By simulating
realistic cyber environments, this approach offers a practical, interactive
framework for exploring vulnerabilities and defensive strategies. At the core
of this research is the Red Team Knife (RTK), a custom penetration testing
toolkit aligned with the Cyber Kill Chain model. RTK is designed to guide
learners through key phases of cyberattacks, including reconnaissance,
exploitation, and response within a DT powered ecosystem. The incorporation of
Large Language Models (LLMs) further enriches the experience by providing
intelligent, real-time feedback, natural language threat explanations, and
adaptive learning support during training exercises. This combined DT LLM
framework is currently being piloted in academic settings to develop hands on
skills in vulnerability assessment, threat detection, and security operations.
Initial findings suggest that the integration significantly improves the
effectiveness and relevance of cybersecurity training, bridging the gap between
theoretical knowledge and real-world application. Ultimately, the research
demonstrates how DTs and LLMs together can transform cybersecurity education to
meet evolving industry demands.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TAI Scan Tool: A RAG-Based Tool With Minimalistic Input for Trustworthy
  AI Self-Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Athanasios Davvetas, Xenia Ziouvelou, Ypatia Dami, Alexis Kaponis, Konstantina Giouvanopoulou, Michael Papademas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the TAI Scan Tool, a RAG-based TAI self-assessment tool
with minimalistic input. The current version of the tool supports the legal TAI
assessment, with a particular emphasis on facilitating compliance with the AI
Act. It involves a two-step approach with a pre-screening and an assessment
phase. The assessment output of the system includes insight regarding the
risk-level of the AI system according to the AI Act, while at the same time
retrieving relevant articles to aid with compliance and notify on their
obligations. Our qualitative evaluation using use-case scenarios yields
promising results, correctly predicting risk levels while retrieving relevant
articles across three distinct semantic groups. Furthermore, interpretation of
results shows that the tool's reasoning relies on comparison with the setting
of high-risk systems, a behaviour attributed to their deployment requiring
careful consideration, and therefore frequently presented within the AI Act.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 1 figure, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HOTA: Hamiltonian framework for Optimal Transport Advection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nazar Buzun, Daniil Shlenskii, Maxim Bobrin, Dmitry V. Dylov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimal transport (OT) has become a natural framework for guiding the
probability flows. Yet, the majority of recent generative models assume trivial
geometry (e.g., Euclidean) and rely on strong density-estimation assumptions,
yielding trajectories that do not respect the true principles of optimality in
the underlying manifold. We present Hamiltonian Optimal Transport Advection
(HOTA), a Hamilton-Jacobi-Bellman based method that tackles the dual dynamical
OT problem explicitly through Kantorovich potentials, enabling efficient and
scalable trajectory optimization. Our approach effectively evades the need for
explicit density modeling, performing even when the cost functionals are
non-smooth. Empirically, HOTA outperforms all baselines in standard benchmarks,
as well as in custom datasets with non-differentiable costs, both in terms of
feasibility and optimality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can One Domain Help Others? A Data-Centric Study on Multi-Domain
  Reasoning via <span class="highlight-title">Reinforcement</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Li, Zhuoshi Pan, Honglin Lin, Mengyuan Sun, Conghui He, Lijun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for enhancing the reasoning capabilities of LLMs. Existing
research has predominantly concentrated on isolated reasoning domains such as
mathematical problem-solving, coding tasks, or logical reasoning. However, real
world reasoning scenarios inherently demand an integrated application of
multiple cognitive skills. Despite this, the interplay among these reasoning
skills under reinforcement learning remains poorly understood. To bridge this
gap, we present a systematic investigation of multi-domain reasoning within the
RLVR framework, explicitly focusing on three primary domains: mathematical
reasoning, code generation, and logical puzzle solving. We conduct a
comprehensive study comprising four key components: (1) Leveraging the GRPO
algorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the
models' in-domain improvements and cross-domain generalization capabilities
when trained on single-domain datasets. (2) Additionally, we examine the
intricate interactions including mutual enhancements and conflicts that emerge
during combined cross-domain training. (3) To further understand the influence
of SFT on RL, we also analyze and compare performance differences between base
and instruct models under identical RL configurations. (4) Furthermore, we
delve into critical RL training details, systematically exploring the impacts
of curriculum learning strategies, variations in reward design, and
language-specific factors. Through extensive experiments, our results offer
significant insights into the dynamics governing domain interactions, revealing
key factors influencing both specialized and generalizable reasoning
performance. These findings provide valuable guidance for optimizing RL
methodologies to foster comprehensive, multi-domain reasoning capabilities in
LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 24 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ To Trust or Not to Trust: On <span class="highlight-title">Calibration</span> in ML-based Resource Allocation
  for Wireless Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rashika Raina, Nidhi Simmons, David E. Simmons, Michel Daoud Yacoub, Trung Q. Duong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In next-generation communications and networks, machine learning (ML) models
are expected to deliver not only accurate predictions but also well-calibrated
confidence scores that reflect the true likelihood of correct decisions. This
paper studies the calibration performance of an ML-based outage predictor
within a single-user, multi-resource allocation framework. We first establish
key theoretical properties of this system's outage probability (OP) under
perfect calibration. Importantly, we show that as the number of resources
grows, the OP of a perfectly calibrated predictor approaches the expected
output conditioned on it being below the classification threshold. In contrast,
when only one resource is available, the system's OP equals the model's overall
expected output. We then derive the OP conditions for a perfectly calibrated
predictor. These findings guide the choice of the classification threshold to
achieve a desired OP, helping system designers meet specific reliability
requirements. We also demonstrate that post-processing calibration cannot
improve the system's minimum achievable OP, as it does not introduce new
information about future channel states. Additionally, we show that
well-calibrated models are part of a broader class of predictors that
necessarily improve OP. In particular, we establish a monotonicity condition
that the accuracy-confidence function must satisfy for such improvement to
occur. To demonstrate these theoretical properties, we conduct a rigorous
simulation-based analysis using post-processing calibration techniques: Platt
scaling and isotonic regression. As part of this framework, the predictor is
trained using an outage loss function specifically designed for this system.
Furthermore, this analysis is performed on Rayleigh fading channels with
temporal correlation captured by Clarke's 2D model, which accounts for receiver
mobility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Hybrid Grounding Using Structural and Data-Driven Heuristics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Beiser, Markus Hecher, Stefan Woltran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The grounding bottleneck poses one of the key challenges that hinders the
widespread adoption of Answer Set Programming in industry. Hybrid Grounding is
a step in alleviating the bottleneck by combining the strength of standard
bottom-up grounding with recently proposed techniques where rule bodies are
decoupled during grounding. However, it has remained unclear when hybrid
grounding shall use body-decoupled grounding and when to use standard bottom-up
grounding. In this paper, we address this issue by developing automated hybrid
grounding: we introduce a splitting algorithm based on data-structural
heuristics that detects when to use body-decoupled grounding and when standard
grounding is beneficial. We base our heuristics on the structure of rules and
an estimation procedure that incorporates the data of the instance. The
experiments conducted on our prototypical implementation demonstrate promising
results, which show an improvement on hard-to-ground scenarios, whereas on
hard-to-solve instances we approach state-of-the-art performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CQE under Epistemic Dependencies: Algorithms and Experiments (extended
  version) <span class="chip">ISWC 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Marconi, Flavia Ricci, Riccardo Rosati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate Controlled Query Evaluation (CQE) over ontologies, where
information disclosure is regulated by epistemic dependencies (EDs), a family
of logical rules recently proposed for the CQE framework. In particular, we
combine EDs with the notion of optimal GA censors, i.e. maximal sets of ground
atoms that are entailed by the ontology and can be safely revealed. We focus on
answering Boolean unions of conjunctive queries (BUCQs) with respect to the
intersection of all optimal GA censors - an approach that has been shown in
other contexts to ensure strong security guarantees with favorable
computational behavior. First, we characterize the security of this
intersection-based approach and identify a class of EDs (namely, full EDs) for
which it remains safe. Then, for a subclass of EDs and for DL-Lite_R
ontologies, we show that answering BUCQs in the above CQE semantics is in AC^0
in data complexity by presenting a suitable, detailed first-order rewriting
algorithm. Finally, we report on experiments conducted in two different
evaluation scenarios, showing the practical feasibility of our rewriting
function.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of paper accepted at the 24th International Semantic
  Web Conference (ISWC 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Unsupervised</span> anomaly <span class="highlight-title">detection</span> using Bayesian flow networks: application
  to brain FDG PET in the context of Alzheimer's disease 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugues Roy, Reuben Dorent, Ninon Burgos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised anomaly detection (UAD) plays a crucial role in neuroimaging for
identifying deviations from healthy subject data and thus facilitating the
diagnosis of neurological disorders. In this work, we focus on Bayesian flow
networks (BFNs), a novel class of generative models, which have not yet been
applied to medical imaging or anomaly detection. BFNs combine the strength of
diffusion frameworks and Bayesian inference. We introduce AnoBFN, an extension
of BFNs for UAD, designed to: i) perform conditional image generation under
high levels of spatially correlated noise, and ii) preserve subject specificity
by incorporating a recursive feedback from the input image throughout the
generative process. We evaluate AnoBFN on the challenging task of Alzheimer's
disease-related anomaly detection in FDG PET images. Our approach outperforms
other state-of-the-art methods based on VAEs (beta-VAE), GANs (f-AnoGAN), and
diffusion models (AnoDDPM), demonstrating its effectiveness at detecting
anomalies while reducing false positive rates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LTLZinc: a <span class="highlight-title">Benchmark</span>ing Framework for Continual Learning and
  Neuro-Symbolic Temporal Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Salvatore Lorello, Nikolaos Manginas, Marco Lippi, Stefano Melacci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neuro-symbolic artificial intelligence aims to combine neural architectures
with symbolic approaches that can represent knowledge in a human-interpretable
formalism. Continual learning concerns with agents that expand their knowledge
over time, improving their skills while avoiding to forget previously learned
concepts. Most of the existing approaches for neuro-symbolic artificial
intelligence are applied to static scenarios only, and the challenging setting
where reasoning along the temporal dimension is necessary has been seldom
explored. In this work we introduce LTLZinc, a benchmarking framework that can
be used to generate datasets covering a variety of different problems, against
which neuro-symbolic and continual learning methods can be evaluated along the
temporal and constraint-driven dimensions. Our framework generates expressive
temporal reasoning and continual learning tasks from a linear temporal logic
specification over MiniZinc constraints, and arbitrary image classification
datasets. Fine-grained annotations allow multiple neural and neuro-symbolic
training settings on the same generated datasets. Experiments on six
neuro-symbolic sequence classification and four class-continual learning tasks
generated by LTLZinc, demonstrate the challenging nature of temporal learning
and reasoning, and highlight limitations of current state-of-the-art methods.
We release the LTLZinc generator and ten ready-to-use tasks to the
neuro-symbolic and continual learning communities, in the hope of fostering
research towards unified temporal learning and reasoning frameworks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Uncertainty-Driven Adaptive Self-Alignment Framework for Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Sun, Zekun Zhang, Shaoning Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable progress in
instruction following and general-purpose reasoning. However, achieving
high-quality alignment with human intent and safety norms without human
annotations remains a fundamental challenge. In this work, we propose an
Uncertainty-Driven Adaptive Self-Alignment (UDASA) framework designed to
improve LLM alignment in a fully automated manner. UDASA first generates
multiple responses for each input and quantifies output uncertainty across
three dimensions: semantics, factuality, and value alignment. Based on these
uncertainty scores, the framework constructs preference pairs and categorizes
training samples into three stages, conservative, moderate, and exploratory,
according to their uncertainty difference. The model is then optimized
progressively across these stages. In addition, we conduct a series of
preliminary studies to validate the core design assumptions and provide strong
empirical motivation for the proposed framework. Experimental results show that
UDASA outperforms existing alignment methods across multiple tasks, including
harmlessness, helpfulness, truthfulness, and controlled sentiment generation,
significantly improving model performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiNRC: A Challenging and Native Multilingual Reasoning <span class="highlight-title">Evaluation</span>
  <span class="highlight-title">Benchmark</span> for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander R. Fabbri, Diego Mares, Jorge Flores, Meher Mankikar, Ernesto Hernandez, Dean Lee, Bing Liu, Chen Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although recent Large Language Models (LLMs) have shown rapid improvement on
reasoning benchmarks in English, the evaluation of such LLMs' multilingual
reasoning capability across diverse languages and cultural contexts remains
limited. Existing multilingual reasoning benchmarks are typically constructed
by translating existing English reasoning benchmarks, biasing these benchmarks
towards reasoning problems with context in English language/cultures. In this
work, we introduce the Multilingual Native Reasoning Challenge (MultiNRC), a
benchmark designed to assess LLMs on more than 1,000 native, linguistic and
culturally grounded reasoning questions written by native speakers in French,
Spanish, and Chinese. MultiNRC covers four core reasoning categories:
language-specific linguistic reasoning, wordplay & riddles, cultural/tradition
reasoning, and math reasoning with cultural relevance. For cultural/tradition
reasoning and math reasoning with cultural relevance, we also provide English
equivalent translations of the multilingual questions by manual translation
from native speakers fluent in English. This set of English equivalents can
provide a direct comparison of LLM reasoning capacity in other languages vs.
English on the same reasoning questions. We systematically evaluate current 14
leading LLMs covering most LLM families on MultiNRC and its English equivalent
set. The results show that (1) current LLMs are still not good at native
multilingual reasoning, with none scoring above 50% on MultiNRC; (2) LLMs
exhibit distinct strengths and weaknesses in handling linguistic, cultural, and
logical reasoning tasks; (3) Most models perform substantially better in math
reasoning in English compared to in original languages (+10%), indicating
persistent challenges with culturally grounded knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BGM-HAN: A Hierarchical Attention Network for Accurate and Fair <span class="highlight-title">Decision</span>
  Assessment on Semi-Structured Profiles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhua Liu, Roy Ka-Wei Lee, Kwan Hui Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human decision-making in high-stakes domains often relies on expertise and
heuristics, but is vulnerable to hard-to-detect cognitive biases that threaten
fairness and long-term outcomes. This work presents a novel approach to
enhancing complex decision-making workflows through the integration of
hierarchical learning alongside various enhancements. Focusing on university
admissions as a representative high-stakes domain, we propose BGM-HAN, an
enhanced Byte-Pair Encoded, Gated Multi-head Hierarchical Attention Network,
designed to effectively model semi-structured applicant data. BGM-HAN captures
multi-level representations that are crucial for nuanced assessment, improving
both interpretability and predictive performance. Experimental results on real
admissions data demonstrate that our proposed model significantly outperforms
both state-of-the-art baselines from traditional machine learning to large
language models, offering a promising framework for augmenting decision-making
in domains where structure, context, and fairness matter. Source code is
available at: https://github.com/junhua/bgm-han.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ASONAM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Demonstration of Efficient Predictive Surrogates for Large-scale Quantum
  Processors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei-You Liao, Yuxuan Du, Xinbiao Wang, Tian-Ci Tian, Yong Luo, Bo Du, Dacheng Tao, He-Liang Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ongoing development of quantum processors is driving breakthroughs in
scientific discovery. Despite this progress, the formidable cost of fabricating
large-scale quantum processors means they will remain rare for the foreseeable
future, limiting their widespread application. To address this bottleneck, we
introduce the concept of predictive surrogates, which are classical learning
models designed to emulate the mean-value behavior of a given quantum processor
with provably computational efficiency. In particular, we propose two
predictive surrogates that can substantially reduce the need for quantum
processor access in diverse practical scenarios. To demonstrate their potential
in advancing digital quantum simulation, we use these surrogates to emulate a
quantum processor with up to 20 programmable superconducting qubits, enabling
efficient pre-training of variational quantum eigensolvers for families of
transverse-field Ising models and identification of non-equilibrium Floquet
symmetry-protected topological phases. Experimental results reveal that the
predictive surrogates not only reduce measurement overhead by orders of
magnitude, but can also surpass the performance of conventional,
quantum-resource-intensive approaches. Collectively, these findings establish
predictive surrogates as a practical pathway to broadening the impact of
advanced quantum processors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>53 pages, 15 figures, comments are welcome</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probing Vision-Language Understanding through the Visual Entailment
  Task: promises and pitfalls 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17467v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17467v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elena Pitta, Tom Kouwenhoven, Tessa Verhoef
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the extent to which the Visual Entailment (VE) task
serves as a reliable probe of vision-language understanding in multimodal
language models, using the LLaMA 3.2 11B Vision model as a test case. Beyond
reporting performance metrics, we aim to interpret what these results reveal
about the underlying possibilities and limitations of the VE task. We conduct a
series of experiments across zero-shot, few-shot, and fine-tuning settings,
exploring how factors such as prompt design, the number and order of in-context
examples and access to visual information might affect VE performance. To
further probe the reasoning processes of the model, we used explanation-based
evaluations. Results indicate that three-shot inference outperforms the
zero-shot baselines. However, additional examples introduce more noise than
they provide benefits. Additionally, the order of the labels in the prompt is a
critical factor that influences the predictions. In the absence of visual
information, the model has a strong tendency to hallucinate and imagine
content, raising questions about the model's over-reliance on linguistic
priors. Fine-tuning yields strong results, achieving an accuracy of 83.3% on
the e-SNLI-VE dataset and outperforming the state-of-the-art OFA-X model.
Additionally, the explanation evaluation demonstrates that the fine-tuned model
provides semantically meaningful explanations similar to those of humans, with
a BERTScore F1-score of 89.2%. We do, however, find comparable BERTScore
results in experiments with limited vision, questioning the visual grounding of
this task. Overall, our results highlight both the utility and limitations of
VE as a diagnostic task for vision-language understanding and point to
directions for refining multimodal evaluation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LUHME: 2nd Workshop on Language Understanding in the Human-Machine
  Era</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reasoning-Driven Retrosynthesis <span class="highlight-title">Prediction</span> with Large Language Models
  via <span class="highlight-title">Reinforcement</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Situo Zhang, Hanqi Li, Lu Chen, Zihan Zhao, Xuanze Lin, Zichen Zhu, Bo Chen, Xin Chen, Kai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrosynthesis planning, essential in organic synthesis and drug discovery,
has greatly benefited from recent AI-driven advancements. Nevertheless,
existing methods frequently face limitations in both applicability and
explainability. Traditional graph-based and sequence-to-sequence models often
lack generalized chemical knowledge, leading to predictions that are neither
consistently accurate nor easily explainable. To address these challenges, we
introduce RetroDFM-R, a reasoning-based large language model (LLM) designed
specifically for chemical retrosynthesis. Leveraging large-scale reinforcement
learning guided by chemically verifiable rewards, RetroDFM-R significantly
enhances prediction accuracy and explainability. Comprehensive evaluations
demonstrate that RetroDFM-R significantly outperforms state-of-the-art methods,
achieving a top-1 accuracy of 65.0% on the USPTO-50K benchmark. Double-blind
human assessments further validate the chemical plausibility and practical
utility of RetroDFM-R's predictions. RetroDFM-R also accurately predicts
multistep retrosynthetic routes reported in the literature for both real-world
drug molecules and perovskite materials. Crucially, the model's explicit
reasoning process provides human-interpretable insights, thereby enhancing
trust and practical value in real-world retrosynthesis applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IndoorBEV: Joint <span class="highlight-title">Detection</span> and Footprint Completion of Objects via
  Mask-based <span class="highlight-title">Prediction</span> in Indoor Scenarios for Bird's-Eye View Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haichuan Li, Changda Tian, Panos Trahanias, Tomi Westerlund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting diverse objects within complex indoor 3D point clouds presents
significant challenges for robotic perception, particularly with varied object
shapes, clutter, and the co-existence of static and dynamic elements where
traditional bounding box methods falter. To address these limitations, we
propose IndoorBEV, a novel mask-based Bird's-Eye View (BEV) method for indoor
mobile robots.
  In a BEV method, a 3D scene is projected into a 2D BEV grid which handles
naturally occlusions and provides a consistent top-down view aiding to
distinguish static obstacles from dynamic agents. The obtained 2D BEV results
is directly usable to downstream robotic tasks like navigation, motion
prediction, and planning. Our architecture utilizes an axis compact encoder and
a window-based backbone to extract rich spatial features from this BEV map. A
query-based decoder head then employs learned object queries to concurrently
predict object classes and instance masks in the BEV space. This mask-centric
formulation effectively captures the footprint of both static and dynamic
objects regardless of their shape, offering a robust alternative to bounding
box regression. We demonstrate the effectiveness of IndoorBEV on a custom
indoor dataset featuring diverse object classes including static objects
  and dynamic elements like robots and miscellaneous items, showcasing its
potential for robust indoor scene understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Each to Their Own: Exploring the Optimal Embedding in RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiting Chen, Zijian Zhao, Jinsong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, as Large Language Models (LLMs) have fundamentally impacted various
fields, the methods for incorporating up-to-date information into LLMs or
adding external knowledge to construct domain-specific models have garnered
wide attention. Retrieval-Augmented Generation (RAG), serving as an
inference-time scaling method, is notable for its low cost and minimal effort
for parameter tuning. However, due to heterogeneous training data and model
architecture, the variant embedding models used in RAG exhibit different
benefits across various areas, often leading to different similarity
calculation results and, consequently, varying response quality from LLMs. To
address this problem, we propose and examine two approaches to enhance RAG by
combining the benefits of multiple embedding models, named Mixture-Embedding
RAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects
retrievals from multiple embedding models based on standardized similarity;
however, it does not outperform vanilla RAG. In contrast, Confident RAG
generates responses multiple times using different embedding models and then
selects the responses with the highest confidence level, demonstrating average
improvements of approximately 10% and 5% over vanilla LLMs and RAG,
respectively. The consistent results across different LLMs and embedding models
indicate that Confident RAG is an efficient plug-and-play approach for various
domains. We will release our code upon publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fair Compromises in Participatory Budgeting: a Multi-Agent Deep
  <span class="highlight-title">Reinforcement</span> Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugh Adams, Srijoni Majumdar, Evangelos Pournaras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Participatory budgeting is a method of collectively understanding and
addressing spending priorities where citizens vote on how a budget is spent, it
is regularly run to improve the fairness of the distribution of public funds.
Participatory budgeting requires voters to make decisions on projects which can
lead to ``choice overload". A multi-agent reinforcement learning approach to
decision support can make decision making easier for voters by identifying
voting strategies that increase the winning proportion of their vote. This
novel approach can also support policymakers by highlighting aspects of
election design that enable fair compromise on projects. This paper presents a
novel, ethically aligned approach to decision support using multi-agent deep
reinforcement learning modelling. This paper introduces a novel use of a
branching neural network architecture to overcome scalability challenges of
multi-agent reinforcement learning in a decentralized way. Fair compromises are
found through optimising voter actions towards greater representation of voter
preferences in the winning set. Experimental evaluation with real-world
participatory budgeting data reveals a pattern in fair compromise: that it is
achievable through projects with smaller cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ctx2TrajGen: Traffic Context-Aware Microscale Vehicle <span class="highlight-title">Trajectories</span> using
  Generative Adversarial Imitation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17418v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17418v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joobin Jin, Seokjun Hong, Gyeongseon Baek, Yeeun Kim, Byeongjoon Noh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise modeling of microscopic vehicle trajectories is critical for traffic
behavior analysis and autonomous driving systems. We propose Ctx2TrajGen, a
context-aware trajectory generation framework that synthesizes realistic urban
driving behaviors using GAIL. Leveraging PPO and WGAN-GP, our model addresses
nonlinear interdependencies and training instability inherent in microscopic
settings. By explicitly conditioning on surrounding vehicles and road geometry,
Ctx2TrajGen generates interaction-aware trajectories aligned with real-world
context. Experiments on the drone-captured DRIFT dataset demonstrate superior
performance over existing methods in terms of realism, behavioral diversity,
and contextual fidelity, offering a robust solution to data scarcity and domain
shift without simulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Content-based 3D Image Retrieval and a ColBERT-inspired Re-ranking for
  Tumor Flagging and Staging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farnaz Khun Jush, Steffen Vogler, Matthias Lenga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing volume of medical images poses challenges for radiologists in
retrieving relevant cases. Content-based image retrieval (CBIR) systems offer
potential for efficient access to similar cases, yet lack standardized
evaluation and comprehensive studies. Building on prior studies for tumor
characterization via CBIR, this study advances CBIR research for volumetric
medical images through three key contributions: (1) a framework eliminating
reliance on pre-segmented data and organ-specific datasets, aligning with large
and unstructured image archiving systems, i.e. PACS in clinical practice; (2)
introduction of C-MIR, a novel volumetric re-ranking method adapting ColBERT's
contextualized late interaction mechanism for 3D medical imaging; (3)
comprehensive evaluation across four tumor sites using three feature extractors
and three database configurations. Our evaluations highlight the significant
advantages of C-MIR. We demonstrate the successful adaptation of the late
interaction principle to volumetric medical images, enabling effective
context-aware re-ranking. A key finding is C-MIR's ability to effectively
localize the region of interest, eliminating the need for pre-segmentation of
datasets and offering a computationally efficient alternative to systems
relying on expensive data enrichment steps. C-MIR demonstrates promising
improvements in tumor flagging, achieving improved performance, particularly
for colon and lung tumors (p<0.05). C-MIR also shows potential for improving
tumor staging, warranting further exploration of its capabilities. Ultimately,
our work seeks to bridge the gap between advanced retrieval techniques and
their practical applications in healthcare, paving the way for improved
diagnostic processes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Millions of $\text{GeAR}$-s: Extending <span class="highlight-title">Graph</span>RAG to Millions of Documents <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhili Shen, Chenxin Diao, Pascual Merita, Pavlos Vougiouklis, Jeff Z. Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have explored graph-based approaches to retrieval-augmented
generation, leveraging structured or semi-structured information -- such as
entities and their relations extracted from documents -- to enhance retrieval.
However, these methods are typically designed to address specific tasks, such
as multi-hop question answering and query-focused summarisation, and therefore,
there is limited evidence of their general applicability across broader
datasets. In this paper, we aim to adapt a state-of-the-art graph-based RAG
solution: $\text{GeAR}$ and explore its performance and limitations on the
SIGIR 2025 LiveRAG Challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR 2025 LiveRAG Challenge Program</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HiProbe-VAD: Video Anomaly <span class="highlight-title">Detection</span> via Hidden States Probing in
  Tuning-Free Multimodal LLMs <span class="chip">ACM MM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17394v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17394v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaolin Cai, Fan Li, Ziwei Zheng, Yanjun Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Anomaly Detection (VAD) aims to identify and locate deviations from
normal patterns in video sequences. Traditional methods often struggle with
substantial computational demands and a reliance on extensive labeled datasets,
thereby restricting their practical applicability. To address these
constraints, we propose HiProbe-VAD, a novel framework that leverages
pre-trained Multimodal Large Language Models (MLLMs) for VAD without requiring
fine-tuning. In this paper, we discover that the intermediate hidden states of
MLLMs contain information-rich representations, exhibiting higher sensitivity
and linear separability for anomalies compared to the output layer. To
capitalize on this, we propose a Dynamic Layer Saliency Probing (DLSP)
mechanism that intelligently identifies and extracts the most informative
hidden states from the optimal intermediate layer during the MLLMs reasoning.
Then a lightweight anomaly scorer and temporal localization module efficiently
detects anomalies using these extracted hidden states and finally generate
explanations. Experiments on the UCF-Crime and XD-Violence datasets demonstrate
that HiProbe-VAD outperforms existing training-free and most traditional
approaches. Furthermore, our framework exhibits remarkable cross-model
generalization capabilities in different MLLMs without any tuning, unlocking
the potential of pre-trained MLLMs for video anomaly detection and paving the
way for more practical and scalable solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Training Data <span class="highlight-title">Detection</span> in AI Coders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianlin Li, Yunxiang Wei, Zhiming Li, Aishan Liu, Qing Guo, Xianglong Liu, Dongning Sun, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in code large language models (CodeLLMs) have made them
indispensable tools in modern software engineering. However, these models
occasionally produce outputs that contain proprietary or sensitive code
snippets, raising concerns about potential non-compliant use of training data,
and posing risks to privacy and intellectual property. To ensure responsible
and compliant deployment of CodeLLMs, training data detection (TDD) has become
a critical task. While recent TDD methods have shown promise in natural
language settings, their effectiveness on code data remains largely
underexplored. This gap is particularly important given code's structured
syntax and distinct similarity criteria compared to natural language. To
address this, we conduct a comprehensive empirical study of seven
state-of-the-art TDD methods on source code data, evaluating their performance
across eight CodeLLMs. To support this evaluation, we introduce CodeSnitch, a
function-level benchmark dataset comprising 9,000 code samples in three
programming languages, each explicitly labeled as either included or excluded
from CodeLLM training. Beyond evaluation on the original CodeSnitch, we design
targeted mutation strategies to test the robustness of TDD methods under three
distinct settings. These mutation strategies are grounded in the
well-established Type-1 to Type-4 code clone detection taxonomy. Our study
provides a systematic assessment of current TDD techniques for code and offers
insights to guide the development of more effective and robust detection
methods in the future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Helix 1.0: An Open-Source Framework for Reproducible and Interpretable
  Machine Learning on Tabular Scientific Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eduardo Aguilar-Bejarano, Daniel Lea, Karthikeyan Sivakumar, Jimiama M. Mase, Reza Omidvar, Ruizhe Li, Troy Kettle, James Mitchell-White, Morgan R Alexander, David A Winkler, Grazziela Figueredo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Helix is an open-source, extensible, Python-based software framework to
facilitate reproducible and interpretable machine learning workflows for
tabular data. It addresses the growing need for transparent experimental data
analytics provenance, ensuring that the entire analytical process -- including
decisions around data transformation and methodological choices -- is
documented, accessible, reproducible, and comprehensible to relevant
stakeholders. The platform comprises modules for standardised data
preprocessing, visualisation, machine learning model training, evaluation,
interpretation, results inspection, and model prediction for unseen data. To
further empower researchers without formal training in data science to derive
meaningful and actionable insights, Helix features a user-friendly interface
that enables the design of computational experiments, inspection of outcomes,
including a novel interpretation approach to machine learning decisions using
linguistic terms all within an integrated environment. Released under the MIT
licence, Helix is accessible via GitHub and PyPI, supporting community-driven
development and promoting adherence to the FAIR principles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SFUOD: Source-Free Unknown Object <span class="highlight-title">Detection</span> <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keon-Hee Park, Seun-An Choe, Gyeong-Moon Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Source-free object detection adapts a detector pre-trained on a source domain
to an unlabeled target domain without requiring access to labeled source data.
While this setting is practical as it eliminates the need for the source
dataset during domain adaptation, it operates under the restrictive assumption
that only pre-defined objects from the source domain exist in the target
domain. This closed-set setting prevents the detector from detecting undefined
objects. To ease this assumption, we propose Source-Free Unknown Object
Detection (SFUOD), a novel scenario which enables the detector to not only
recognize known objects but also detect undefined objects as unknown objects.
To this end, we propose CollaPAUL (Collaborative tuning and Principal
Axis-based Unknown Labeling), a novel framework for SFUOD. Collaborative tuning
enhances knowledge adaptation by integrating target-dependent knowledge from
the auxiliary encoder with source-dependent knowledge from the pre-trained
detector through a cross-domain attention mechanism. Additionally, principal
axes-based unknown labeling assigns pseudo-labels to unknown objects by
estimating objectness via principal axes projection and confidence scores from
model predictions. The proposed CollaPAUL achieves state-of-the-art
performances on SFUOD benchmarks, and extensive experiments validate its
effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DynaSearcher: <span class="highlight-title">Dynamic</span> Knowledge <span class="highlight-title">Graph</span> Augmented Search Agent via
  Multi-Reward <span class="highlight-title">Reinforcement</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuzhan Hao, Wenfeng Feng, Yuewei Zhang, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-step agentic retrieval systems based on large language models (LLMs)
have demonstrated remarkable performance in complex information search tasks.
However, these systems still face significant challenges in practical
applications, particularly in generating factually inconsistent intermediate
queries and inefficient search trajectories, which can lead to reasoning
deviations or redundant computations. To address these issues, we propose
DynaSearcher, an innovative search agent enhanced by dynamic knowledge graphs
and multi-reward reinforcement learning (RL). Specifically, our system
leverages knowledge graphs as external structured knowledge to guide the search
process by explicitly modeling entity relationships, thereby ensuring factual
consistency in intermediate queries and mitigating biases from irrelevant
information. Furthermore, we employ a multi-reward RL framework for
fine-grained control over training objectives such as retrieval accuracy,
efficiency, and response quality. This framework promotes the generation of
high-quality intermediate queries and comprehensive final answers, while
discouraging unnecessary exploration and minimizing information omissions or
redundancy. Experimental results demonstrate that our approach achieves
state-of-the-art answer accuracy on six multi-hop question answering datasets,
matching frontier LLMs while using only small-scale models and limited
computational resources. Furthermore, our approach demonstrates strong
generalization and robustness across diverse retrieval environments and
larger-scale models, highlighting its broad applicability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Repetition for Mitigating Position Bias in LLM-Based Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Vardasbi, Gustavo Penha, Claudia Hauff, Hugues Bouchard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When using LLMs to rank items based on given criteria, or evaluate answers,
the order of candidate items can influence the model's final decision. This
sensitivity to item positioning in a LLM's prompt is known as position bias.
Prior research shows that this bias exists even in large models, though its
severity varies across models and tasks. In addition to position bias, LLMs
also exhibit varying degrees of low repetition consistency, where repeating the
LLM call with the same candidate ordering can lead to different rankings. To
address both inconsistencies, a common approach is to prompt the model multiple
times with different candidate orderings and aggregate the results via majority
voting. However, this repetition strategy, significantly increases
computational costs. Extending prior findings, we observe that both the
direction -- favoring either the earlier or later candidate in the prompt --
and magnitude of position bias across instances vary substantially, even within
a single dataset. This observation highlights the need for a per-instance
mitigation strategy. To this end, we introduce a dynamic early-stopping method
that adaptively determines the number of repetitions required for each
instance. Evaluating our approach across three LLMs of varying sizes and on two
tasks, namely re-ranking and alignment, we demonstrate that transitioning to a
dynamic repetition strategy reduces the number of LLM calls by an average of
81%, while preserving the accuracy. Furthermore, we propose a confidence-based
adaptation to our early-stopping method, reducing LLM calls by an average of
87% compared to static repetition, with only a slight accuracy trade-off
relative to our original early-stopping method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyperbolic Deep Learning for Foundation Models: A <span class="highlight-title">Survey</span> <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17787v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17787v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neil He, Hiren Madhu, Ngoc Bui, Menglin Yang, Rex Ying
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models pre-trained on massive datasets, including large language
models (LLMs), vision-language models (VLMs), and large multimodal models, have
demonstrated remarkable success in diverse downstream tasks. However, recent
studies have shown fundamental limitations of these models: (1) limited
representational capacity, (2) lower adaptability, and (3) diminishing
scalability. These shortcomings raise a critical question: is Euclidean
geometry truly the optimal inductive bias for all foundation models, or could
incorporating alternative geometric spaces enable models to better align with
the intrinsic structure of real-world data and improve reasoning processes?
Hyperbolic spaces, a class of non-Euclidean manifolds characterized by
exponential volume growth with respect to distance, offer a mathematically
grounded solution. These spaces enable low-distortion embeddings of
hierarchical structures (e.g., trees, taxonomies) and power-law distributions
with substantially fewer dimensions compared to Euclidean counterparts. Recent
advances have leveraged these properties to enhance foundation models,
including improving LLMs' complex reasoning ability, VLMs' zero-shot
generalization, and cross-modal semantic alignment, while maintaining parameter
efficiency. This paper provides a comprehensive review of hyperbolic neural
networks and their recent development for foundation models. We further outline
key challenges and research directions to advance the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 Pages, SIGKDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal Point-Supervised Signal Reconstruction: A Human-Annotation-Free
  Framework for Weak Moving Target <span class="highlight-title">Detection</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihua Gao, Chunxu Ren, Wenlong Niu, Xiaodong Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In low-altitude surveillance and early warning systems, detecting weak moving
targets remains a significant challenge due to low signal energy, small spatial
extent, and complex background clutter. Existing methods struggle with
extracting robust features and suffer from the lack of reliable annotations. To
address these limitations, we propose a novel Temporal Point-Supervised (TPS)
framework that enables high-performance detection of weak targets without any
manual annotations.Instead of conventional frame-based detection, our framework
reformulates the task as a pixel-wise temporal signal modeling problem, where
weak targets manifest as short-duration pulse-like responses. A Temporal Signal
Reconstruction Network (TSRNet) is developed under the TPS paradigm to
reconstruct these transient signals.TSRNet adopts an encoder-decoder
architecture and integrates a Dynamic Multi-Scale Attention (DMSAttention)
module to enhance its sensitivity to diverse temporal patterns. Additionally, a
graph-based trajectory mining strategy is employed to suppress false alarms and
ensure temporal consistency.Extensive experiments on a purpose-built low-SNR
dataset demonstrate that our framework outperforms state-of-the-art methods
while requiring no human annotations. It achieves strong detection performance
and operates at over 1000 FPS, underscoring its potential for real-time
deployment in practical scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Confounded Causal Imitation Learning with Instrumental Variables 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Zeng, Shenglan Nie, Feng Xie, Libo Huang, Peng Wu, Zhi Geng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning from demonstrations usually suffers from the confounding
effects of unmeasured variables (i.e., unmeasured confounders) on the states
and actions. If ignoring them, a biased estimation of the policy would be
entailed. To break up this confounding gap, in this paper, we take the best of
the strong power of instrumental variables (IV) and propose a Confounded Causal
Imitation Learning (C2L) model. This model accommodates confounders that
influence actions across multiple timesteps, rather than being restricted to
immediate temporal dependencies. We develop a two-stage imitation learning
framework for valid IV identification and policy optimization. In particular,
in the first stage, we construct a testing criterion based on the defined
pseudo-variable, with which we achieve identifying a valid IV for the C2L
models. Such a criterion entails the sufficient and necessary identifiability
conditions for IV validity. In the second stage, with the identified IV, we
propose two candidate policy learning approaches: one is based on a simulator,
while the other is offline. Extensive experiments verified the effectiveness of
identifying the valid IV as well as learning the policy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Versatile Pathology Co-pilot via Reasoning Enhanced Multimodal Large
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Xu, Ziyi Liu, Junlin Hou, Jiabo Ma, Cheng Jin, Yihui Wang, Zhixuan Chen, Zhengyu Zhang, Zhengrui Guo, Fengtao Zhou, Yingxue Xu, Xi Wang, Ronald Cheong Kin Chan, Li Liang, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have emerged as powerful tools for
computational pathology, offering unprecedented opportunities to integrate
pathological images with language context for comprehensive diagnostic
analysis. These models hold particular promise for automating complex tasks
that traditionally require expert interpretation of pathologists. However,
current MLLM approaches in pathology demonstrate significantly constrained
reasoning capabilities, primarily due to their reliance on expensive
chain-of-thought annotations. Additionally, existing methods remain limited to
simplex application of visual question answering (VQA) at region-of-interest
(ROI) level, failing to address the full spectrum of diagnostic needs such as
ROI classification, detection, segmentation, whole-slide-image (WSI)
classification and VQA in clinical practice. In this study, we present
SmartPath-R1, a versatile MLLM capable of simultaneously addressing both
ROI-level and WSI-level tasks while demonstrating robust pathological reasoning
capability. Our framework combines scale-dependent supervised fine-tuning and
task-aware reinforcement fine-tuning, which circumvents the requirement for
chain-of-thought supervision by leveraging the intrinsic knowledge within MLLM.
Furthermore, SmartPath-R1 integrates multiscale and multitask analysis through
a mixture-of-experts mechanism, enabling dynamic processing for diverse tasks.
We curate a large-scale dataset comprising 2.3M ROI samples and 188K WSI
samples for training and evaluation. Extensive experiments across 72 tasks
validate the effectiveness and superiority of the proposed approach. This work
represents a significant step toward developing versatile, reasoning-enhanced
AI systems for precision pathology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Temporal Guidance and Iterative Refinement in Audio Source Separation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Morocutti, Jonathan Greif, Paul Primus, Florian Schmid, Gerhard Widmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial semantic segmentation of sound scenes (S5) involves the accurate
identification of active sound classes and the precise separation of their
sources from complex acoustic mixtures. Conventional systems rely on a
two-stage pipeline - audio tagging followed by label-conditioned source
separation - but are often constrained by the absence of fine-grained temporal
information critical for effective separation. In this work, we address this
limitation by introducing a novel approach for S5 that enhances the synergy
between the event detection and source separation stages. Our key contributions
are threefold. First, we fine-tune a pre-trained Transformer to detect active
sound classes. Second, we utilize a separate instance of this fine-tuned
Transformer to perform sound event detection (SED), providing the separation
module with detailed, time-varying guidance. Third, we implement an iterative
refinement mechanism that progressively enhances separation quality by
recursively reusing the separator's output from previous iterations. These
advancements lead to significant improvements in both audio tagging and source
separation performance, as demonstrated by our system's second-place finish in
Task 4 of the DCASE Challenge 2025. Our implementation and model checkpoints
are available in our GitHub repository: https://github.com/theMoro/dcase25task4 .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Belief Domains into Probabilistic Logic Programs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Damiano Azzolini, Fabrizio Riguzzi, Theresa Swift
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Probabilistic Logic Programming (PLP) under the Distribution Semantics is a
leading approach to practical reasoning under uncertainty. An advantage of the
Distribution Semantics is its suitability for implementation as a Prolog or
Python library, available through two well-maintained implementations, namely
ProbLog and cplint/PITA. However, current formulations of the Distribution
Semantics use point-probabilities, making it difficult to express epistemic
uncertainty, such as arises from, for example, hierarchical classifications
from computer vision models. Belief functions generalize probability measures
as non-additive capacities, and address epistemic uncertainty via interval
probabilities. This paper introduces interval-based Capacity Logic Programs
based on an extension of the Distribution Semantics to include belief
functions, and describes properties of the new framework that make it amenable
to practical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under consideration in Theory and Practice of Logic Programming
  (TPLP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Knowledge <span class="highlight-title">Graph</span>s and LLM Reasoning to Identify Operational
  Bottlenecks for Warehouse <span class="highlight-title">Planning</span> Assistance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishi Parekh, Saisubramaniam Gopalakrishnan, Zishan Ahmad, Anirudh Deodhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analyzing large, complex output datasets from Discrete Event Simulations
(DES) of warehouse operations to identify bottlenecks and inefficiencies is a
critical yet challenging task, often demanding significant manual effort or
specialized analytical tools. Our framework integrates Knowledge Graphs (KGs)
and Large Language Model (LLM)-based agents to analyze complex Discrete Event
Simulation (DES) output data from warehouse operations. It transforms raw DES
data into a semantically rich KG, capturing relationships between simulation
events and entities. An LLM-based agent uses iterative reasoning, generating
interdependent sub-questions. For each sub-question, it creates Cypher queries
for KG interaction, extracts information, and self-reflects to correct errors.
This adaptive, iterative, and self-correcting process identifies operational
issues mimicking human analysis. Our DES approach for warehouse bottleneck
identification, tested with equipment breakdowns and process irregularities,
outperforms baseline methods. For operational questions, it achieves
near-perfect pass rates in pinpointing inefficiencies. For complex
investigative questions, we demonstrate its superior diagnostic ability to
uncover subtle, interconnected issues. This work bridges simulation modeling
and AI (KG+LLM), offering a more intuitive method for actionable insights,
reducing time-to-insight, and enabling automated warehouse inefficiency
evaluation and diagnosis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Prompt Programming Tasks and Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jenny T. Liang, Chenyang Yang, Agnia Sergeyuk, Travis D. Breaux, Brad A. Myers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompting foundation models (FMs) like large language models (LLMs) have
enabled new AI-powered software features (e.g., text summarization) that
previously were only possible by fine-tuning FMs. Now, developers are embedding
prompts in software, known as prompt programs. The process of prompt
programming requires the developer to make many changes to their prompt. Yet,
the questions developers ask to update their prompt is unknown, despite the
answers to these questions affecting how developers plan their changes. With
the growing number of research and commercial prompt programming tools, it is
unclear whether prompt programmers' needs are being adequately addressed. We
address these challenges by developing a taxonomy of 25 tasks prompt
programmers do and 51 questions they ask, measuring the importance of each task
and question. We interview 16 prompt programmers, observe 8 developers make
prompt changes, and survey 50 developers. We then compare the taxonomy with 48
research and commercial tools. We find that prompt programming is not
well-supported: all tasks are done manually, and 16 of the 51 questions --
including a majority of the most important ones -- remain unanswered. Based on
this, we outline important opportunities for prompt programming tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantifying the Uniqueness and Divisiveness of Presidential Discourse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01405v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01405v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karen Zhou, Alexander A. Meitus, Milo Chase, Grace Wang, Anne Mykland, William Howell, Chenhao Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Do American presidents speak discernibly different from each other? If so, in
what ways? And are these differences confined to any single medium of
communication? To investigate these questions, this paper introduces a novel
metric of uniqueness based on large language models, develops a new lexicon for
divisive speech, and presents a framework for assessing the distinctive ways in
which presidents speak about their political opponents. Applying these tools to
a variety of corpora of presidential speeches, we find considerable evidence
that Donald Trump's speech patterns diverge from those of all major party
nominees for the presidency in recent history. Trump is significantly more
distinctive than his fellow Republicans, whose uniqueness values appear closer
to those of the Democrats. Contributing to these differences is Trump's
employment of divisive and antagonistic language, particularly when targeting
his political opponents. These differences hold across a variety of measurement
strategies, arise on both the campaign trail and in official presidential
addresses, and do not appear to be an artifact of secular changes in
presidential communications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in PNAS Nexus:
  https://academic.oup.com/pnasnexus/article/3/10/pgae431/7814873</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pulse-PPG: An Open-Source <span class="highlight-title">Field</span>-Trained PPG Foundation Model for
  Wearable Applications Across Lab and <span class="highlight-title">Field</span> Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01108v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01108v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mithun Saha, Maxwell A. Xu, Wanting Mao, Sameer Neupane, James M. Rehg, Santosh Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photoplethysmography (PPG)-based foundation models are gaining traction due
to the widespread use of PPG in biosignal monitoring and their potential to
generalize across diverse health applications. In this paper, we introduce
Pulse-PPG, the first open-source PPG foundation model trained exclusively on
raw PPG data collected over a 100-day field study with 120 participants.
Existing PPG foundation models are either open-source but trained on clinical
data or closed-source, limiting their applicability in real-world settings. We
evaluate Pulse-PPG across multiple datasets and downstream tasks, comparing its
performance against a state-of-the-art foundation model trained on clinical
data. Our results demonstrate that Pulse-PPG, trained on uncurated field data,
exhibits superior generalization across clinical and mobile health applications
in both lab and field settings. This suggests that exposure to real-world
variability enables the model to learn fine-grained representations, making it
more adaptable across tasks. Furthermore, pre-training on field data
surprisingly outperforms its pre-training on clinical data in many tasks,
reinforcing the importance of training on real-world, diverse datasets. To
encourage further advancements in robust foundation models leveraging field
data, we plan to release Pulse-PPG, providing researchers with a powerful
resource for developing more generalizable PPG-based models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Saha and Xu are co-first authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Structure of Game Provenance and its Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05094v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05094v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shawn Bowers, Yilin Xia, Bertram Ludäscher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Provenance in databases has been thoroughly studied for positive and for
recursive queries, then for first-order (FO) queries, i.e., having negation but
no recursion. Query evaluation can be understood as a two-player game where the
opponents argue whether or not a tuple is in the query answer. This
game-theoretic approach yields a natural provenance model for FO queries,
unifying how and why-not provenance. Here, we study the fine-grain structure of
game provenance. A game $G=(V,E)$ consists of positions $V$ and moves $E$ and
can be solved by computing the well-founded model of a single, unstratifiable
rule: \[ \text{win}(X) \leftarrow \text{move}(X, Y), \neg \, \text{win}(Y). \]
In the solved game $G^{\lambda}$, the value of a position $x\,{\in}\,V$ is
either won, lost, or drawn. This value is explained by the provenance
$\mathscr{P}$(x), i.e., certain (annotated) edges reachable from $x$. We
identify seven edge types that give rise to new kinds of provenance, i.e.,
potential, actual, and primary, and demonstrate that "not all moves are created
equal". We describe the new provenance types, show how they can be computed
while solving games, and discuss applications, e.g., for abstract argumentation
frameworks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multilingual LLMs Are Not Multilingual Thinkers: Evidence from Hindi
  Analogy <span class="highlight-title">Evaluation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13238v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13238v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashray Gupta, Rohan Joseph, Sunny Rai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analogies test a model's ability to infer implicit relationships between
concepts, making them a key benchmark for evaluating reasoning capabilities.
While large language models (LLMs) are widely evaluated for reasoning in
English, their abilities in Indic languages remain understudied, limiting our
understanding of whether these models generalize across languages. To address
this gap, we introduce a new Hindi Analogy Test Set (HATS), comprising 405
multiple-choice questions sourced from Indian government exams. We benchmark
state-of-the-art multilingual LLMs using various prompting strategies and
introduce a grounded Chain of Thought approach that leverages cognitive
theories of analogical reasoning. This approach improves model performance on
Hindi analogy questions. Our experiments show that models perform best with
English prompts, irrespective of the prompting strategy. Our test set addresses
the lack of a critical resource to evaluate LLM reasoning capabilities in
Hindi.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM Alignment as Retriever <span class="highlight-title">Optimization</span>: An Information Retrieval
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03699v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03699v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Jin, Jinsung Yoon, Zhen Qin, Ziqi Wang, Wei Xiong, Yu Meng, Jiawei Han, Sercan O. Arik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have revolutionized artificial intelligence with
capabilities in reasoning, coding, and communication, driving innovation across
industries. Their true potential depends on effective alignment to ensure
correct, trustworthy and ethical behavior, addressing challenges like
misinformation, hallucinations, bias and misuse. While existing Reinforcement
Learning (RL)-based alignment methods are notoriously complex, direct
optimization approaches offer a simpler alternative. In this work, we introduce
a novel direct optimization approach for LLM alignment by drawing on
established Information Retrieval (IR) principles. We present a systematic
framework that bridges LLM alignment and IR methodologies, mapping LLM
generation and reward models to IR's retriever-reranker paradigm. Building on
this foundation, we propose LLM Alignment as Retriever Preference Optimization
(LarPO), a new alignment method that enhances overall alignment quality.
Extensive experiments validate LarPO's effectiveness with 38.9 % and 13.7 %
averaged improvement on AlpacaEval2 and MixEval-Hard respectively. Our work
opens new avenues for advancing LLM alignment by integrating IR foundations,
offering a promising direction for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The New LLM Bottleneck: A Systems Perspective on Latent Attention and
  Mixture-of-Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15465v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15465v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungmin Yun, Seonyong Park, Hwayong Nam, Younjoo Lee, Gunjun Lee, Kwanhee Kyung, Sangpyo Kim, Nam Sung Kim, Jongmin Kim, Hyungyo Kim, Juhwan Cho, Seungmin Baek, Jung Ho Ahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational workloads composing traditional Transformer models are starkly
bifurcated. Multi-Head Attention (MHA) is memory-bound, with low arithmetic
intensity, while feedforward layers are compute-bound. This dichotomy has long
motivated research into specialized hardware to mitigate the MHA bottleneck.
  This paper argues that recent architectural shifts, namely Multi-head Latent
Attention (MLA) and Mixture-of-Experts (MoE), challenge the premise of
specialized attention hardware. We make two key observations. First, the
arithmetic intensity of MLA is over two orders of magnitude greater than that
of MHA, shifting it close to a compute-bound regime well-suited for modern
accelerators like GPUs. Second, by distributing MoE experts across a pool of
accelerators, their arithmetic intensity can be tuned through batching to match
that of the dense layers, creating a more balanced computational profile.
  These findings reveal a diminishing need for specialized attention hardware.
The central challenge for next-generation Transformers is no longer
accelerating a single memory-bound layer. Instead, the focus must shift to
designing balanced systems with sufficient compute, memory capacity, memory
bandwidth, and high-bandwidth interconnects to manage the diverse demands of
large-scale models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task Priors: Enhancing Model <span class="highlight-title">Evaluation</span> by Considering the Entire Space
  of Downstream Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.09871v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.09871v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niket Patel, Randall Balestriero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The grand goal of AI research, and particularly Self Supervised Learning
(SSL), is to produce systems that can successfully solve any possible task. In
contrast, current evaluation methods available to AI researchers typically rely
on a fixed collection of hand-picked downstream benchmarks. Hence, a large
amount of effort is put into designing and searching for large collection of
evaluation tasks that can serve as a proxy of our grand goal. We argue that
such a rigid evaluation protocol creates a silent bottleneck in AI research. To
remedy that, we define a probabilistic space of downstream tasks obtained by
adopting a distribution of tasks and by defining Task Priors. Under this view,
one can evaluate a model's performance over the set of all possible downstream
tasks. Our framework is the first to provide answers to key questions such as
(i) what is the average performance of my model over all possible downstream
tasks weighted by the probability to encounter each task? or (ii) what is the
variance of my model's performance across all downstream tasks under the
defined Task Priors? Beyond establishing a new standard for evaluation, we
believe that Task Priors will accelerate the pace of research in SSL - where
downstream task evaluation is the sole qualitative signal that researchers have
access to.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EEG Foundation Models: A Critical <span class="highlight-title">Review</span> of Current Progress and Future
  Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.11783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.11783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gayal Kuruppu, Neeraj Wagh, Yogatheesan Varatharajah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Patterns of electrical brain activity recorded via electroencephalography
(EEG) offer immense value for scientific and clinical investigations. The
inability of supervised EEG encoders to learn robust EEG patterns and their
over-reliance on expensive signal annotations have sparked a transition towards
general-purpose self-supervised EEG encoders, i.e., EEG foundation models
(EEG-FMs), for robust and scalable EEG feature extraction. However, the
real-world readiness of early EEG-FMs and the rubric for long-term research
progress remain unclear. A systematic and comprehensive review of
first-generation EEG-FMs is therefore necessary to understand the current
state-of-the-art and identify key directions for future EEG-FMs. To that end,
this study reviews 10 early EEG-FMs and presents a critical synthesis of their
methodology, empirical findings, and outstanding research gaps. We find that
most EEG-FMs adopt a sequence-based modeling scheme that relies on
transformer-based backbones and the reconstruction of masked sequences for
self-supervision. However, model evaluations remain heterogeneous and largely
limited, making it challenging to assess their practical off-the-shelf utility.
In addition to adopting standardized and realistic evaluations, future work
should demonstrate more substantial scaling effects and make principled and
trustworthy choices throughout the EEG representation learning pipeline. We
believe that developing benchmarks, software tools, technical methodologies,
and applications in collaboration with domain experts may further advance the
translational utility and real-world adoption of EEG-FMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 5 figures, 3 tables (main + supplement)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chemical reasoning in LLMs unlocks strategy-aware synthesis <span class="highlight-title">planning</span> and
  reaction mechanism elucidation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.08537v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.08537v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andres M Bran, Theo A Neukomm, Daniel P Armstrong, Zlatko Jončev, Philippe Schwaller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While automated chemical tools excel at specific tasks, they have struggled
to capture the strategic thinking that characterizes expert chemical reasoning.
Here we demonstrate that large language models (LLMs) can serve as powerful
tools enabling chemical analysis. When integrated with traditional search
algorithms, they enable a new approach to computer-aided synthesis that mirrors
human expert thinking. Rather than using LLMs to directly manipulate chemical
structures, we leverage their ability to evaluate chemical strategies and guide
search algorithms toward chemically meaningful solutions. We demonstrate this
paradigm through two fundamental challenges: strategy-aware retrosynthetic
planning and mechanism elucidation. In retrosynthetic planning, our system
allows chemists to specify desired synthetic strategies in natural language --
from protecting group strategies to global feasibility assessment -- and uses
traditional or LLM-guided Monte Carlo Tree Search to find routes that satisfy
these constraints. In mechanism elucidation, LLMs guide the search for
plausible reaction mechanisms by combining chemical principles with systematic
exploration. This approach shows strong performance across diverse chemical
tasks, with newer and larger models demonstrating increasingly sophisticated
chemical reasoning. Our approach establishes a new paradigm for computer-aided
chemistry that combines the strategic understanding of LLMs with the precision
of traditional chemical tools, opening possibilities for more intuitive and
powerful chemical automation systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PerceptionLM: Open-Access Data and Models for Detailed Visual
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.13180v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.13180v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jang Hyun Cho, Andrea Madotto, Effrosyni Mavroudi, Triantafyllos Afouras, Tushar Nagarajan, Muhammad Maaz, Yale Song, Tengyu Ma, Shuming Hu, Suyog Jain, Miguel Martin, Huiyu Wang, Hanoona Rasheed, Peize Sun, Po-Yao Huang, Daniel Bolya, Nikhila Ravi, Shashank Jain, Tammy Stark, Shane Moon, Babak Damavandi, Vivian Lee, Andrew Westbury, Salman Khan, Philipp Krähenbühl, Piotr Dollár, Lorenzo Torresani, Kristen Grauman, Christoph Feichtenhofer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models are integral to computer vision research, yet many
high-performing models remain closed-source, obscuring their data, design and
training recipe. The research community has responded by using distillation
from black-box models to label training data, achieving strong benchmark
results, at the cost of measurable scientific progress. However, without
knowing the details of the teacher model and its data sources, scientific
progress remains difficult to measure. In this paper, we study building a
Perception Language Model (PLM) in a fully open and reproducible framework for
transparent research in image and video understanding. We analyze standard
training pipelines without distillation from proprietary models and explore
large-scale synthetic data to identify critical data gaps, particularly in
detailed video understanding. To bridge these gaps, we release 2.8M
human-labeled instances of fine-grained video question-answer pairs and
spatio-temporally grounded video captions. Additionally, we introduce
PLM-VideoBench, a suite for evaluating challenging video understanding tasks
focusing on the ability to reason about "what", "where", "when", and "how" of a
video. We make our work fully reproducible by providing data, training recipes,
code & models. https://github.com/facebookresearch/perception_models
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PALADIN : <span class="highlight-title">Robust</span> Neural Fingerprinting for Text-to-Image Dif<span class="highlight-title">fusion</span>
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03170v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03170v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Murthy L, Subarna Tripathi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The risk of misusing text-to-image generative models for malicious uses,
especially due to the open-source development of such models, has become a
serious concern. As a risk mitigation strategy, attributing generative models
with neural fingerprinting is emerging as a popular technique. There has been a
plethora of recent work that aim for addressing neural fingerprinting. A
trade-off between the attribution accuracy and generation quality of such
models has been studied extensively. None of the existing methods yet achieved
100% attribution accuracy. However, any model with less than cent percent
accuracy is practically non-deployable. In this work, we propose an accurate
method to incorporate neural fingerprinting for text-to-image diffusion models
leveraging the concepts of cyclic error correcting codes from the literature of
coding theory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01939v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01939v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaosheng Zhao, Yang Huang, Guirong Xue, Xiao Kong, Jifeng Liu, Xiaoyu Tang, Timothy C. Beers, Yuan-Sen Ting, A-Li Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, large language models (LLMs) have transformed natural
language understanding through vast datasets and large-scale parameterization.
Inspired by this success, we present SpecCLIP, a foundation model framework
that extends LLM-inspired methodologies to stellar spectral analysis. Stellar
spectra, akin to structured language, encode rich physical and chemical
information about stars. By training foundation models on large-scale spectral
datasets, our goal is to learn robust and informative embeddings that support
diverse downstream applications. As a proof of concept, SpecCLIP involves
pre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed
by contrastive alignment using the CLIP (Contrastive Language-Image
Pre-training) framework, adapted to associate spectra from different
instruments. This alignment is complemented by auxiliary decoders that preserve
spectrum-specific information and enable translation (prediction) between
spectral types, with the former achieved by maximizing mutual information
between embeddings and input spectra. The result is a cross-spectrum framework
enabling intrinsic calibration and flexible applications across instruments. We
demonstrate that fine-tuning these models on moderate-sized labeled datasets
improves adaptability to tasks such as stellar-parameter estimation and
chemical-abundance determination. SpecCLIP also enhances the accuracy and
precision of parameter estimates benchmarked against external survey data.
Additionally, its similarity search and cross-spectrum prediction capabilities
offer potential for anomaly detection. Our results suggest that contrastively
trained foundation models enriched with spectrum-aware decoders can advance
precision stellar spectroscopy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 8 figures, 5 tables. Updated with minor corrections to flux
  normalization, and to related tables and figures. Submitted to AAS Journals.
  Comments welcome</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Robot</span> Operation of Home Appliances by Reading User Manuals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20424v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20424v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Zhang, Hanbo Zhang, Anxing Xiao, David Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Operating home appliances, among the most common tools in every household, is
a critical capability for assistive home robots. This paper presents ApBot, a
robot system that operates novel household appliances by "reading" their user
manuals. ApBot faces multiple challenges: (i) infer goal-conditioned partial
policies from their unstructured, textual descriptions in a user manual
document, (ii) ground the policies to the appliance in the physical world, and
(iii) execute the policies reliably over potentially many steps, despite
compounding errors. To tackle these challenges, ApBot constructs a structured,
symbolic model of an appliance from its manual, with the help of a large
vision-language model (VLM). It grounds the symbolic actions visually to
control panel elements. Finally, ApBot closes the loop by updating the model
based on visual feedback. Our experiments show that across a wide range of
simulated and real-world appliances, ApBot achieves consistent and
statistically significant improvements in task success rate, compared with
state-of-the-art large VLMs used directly as control policies. These results
suggest that a structured internal representations plays an important role in
robust robot operation of home appliances, especially, complex ones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Video Discovery: Agentic Search with Tool Use for Long-form Video
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.18079v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.18079v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyi Zhang, Zhaoyang Jia, Zongyu Guo, Jiahao Li, Bin Li, Houqiang Li, Yan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-form video understanding presents significant challenges due to
extensive temporal-spatial complexity and the difficulty of question answering
under such extended contexts. While Large Language Models (LLMs) have
demonstrated considerable advancements in video analysis capabilities and long
context handling, they continue to exhibit limitations when processing
information-dense hour-long videos. To overcome such limitations, we propose
the Deep Video Discovery agent to leverage an agentic search strategy over
segmented video clips. Different from previous video agents manually designing
a rigid workflow, our approach emphasizes the autonomous nature of agents. By
providing a set of search-centric tools on multi-granular video database, our
DVD agent leverages the advanced reasoning capability of LLM to plan on its
current observation state, strategically selects tools, formulates appropriate
parameters for actions, and iteratively refines its internal reasoning in light
of the gathered information. We perform comprehensive evaluation on multiple
long video understanding benchmarks that demonstrates the advantage of the
entire system design. Our DVD agent achieves SOTA performance, significantly
surpassing prior works by a large margin on the challenging LVBench dataset.
Comprehensive ablation studies and in-depth tool analyses are also provided,
yielding insights to further advance intelligent agents tailored for long-form
video understanding tasks. The code has been released in
https://github.com/microsoft/DeepVideoDiscovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>V3 draft. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Balans: Multi-Armed Bandits-based Adaptive Large Neighborhood Search for
  Mixed-Integer Programming Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14382v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14382v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyang Cai, Serdar Kadioglu, Bistra Dilkina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixed-integer programming (MIP) is a powerful paradigm for modeling and
solving various important combinatorial optimization problems. Recently,
learning-based approaches have shown a potential to speed up MIP solving via
offline training that then guides important design decisions during the search.
However, a significant drawback of these methods is their heavy reliance on
offline training, which requires collecting training datasets and
computationally costly training epochs yet offering only limited generalization
to unseen (larger) instances. In this paper, we propose Balans, an adaptive
meta-solver for MIPs with online learning capability that does not require any
supervision or apriori training. At its core, Balans is based on adaptive
large-neighborhood search, operating on top of an MIP solver by successive
applications of destroy and repair neighborhood operators. During the search,
the selection among different neighborhood definitions is guided on the fly for
the instance at hand via multi-armed bandit algorithms. Our extensive
experiments on hard optimization instances show that Balans offers significant
performance gains over the default MIP solver, is better than committing to any
single best neighborhood, and improves over the state-of-the-art
large-neighborhood search for MIPs. Finally, we release Balans as a highly
configurable, MIP solver agnostic, open-source software.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LoX: Low-Rank Extrapolation <span class="highlight-title">Robust</span>ifies LLM Safety Against Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel J. Perin, Runjin Chen, Xuxi Chen, Nina S. T. Hirata, Zhangyang Wang, Junyuan Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have become indispensable in real-world
applications. However, their widespread adoption raises significant safety
concerns, particularly in responding to socially harmful questions. Despite
substantial efforts to improve model safety through alignment, aligned models
can still have their safety protections undermined by subsequent fine-tuning -
even when the additional training data appears benign. In this paper, we
empirically demonstrate that this vulnerability stems from the sensitivity of
safety-critical low-rank subspaces in LLM parameters to fine-tuning. Building
on this insight, we propose a novel training-free method, termed Low-Rank
Extrapolation (LoX), to enhance safety robustness by extrapolating the safety
subspace of an aligned LLM. Our experimental results confirm the effectiveness
of LoX, demonstrating significant improvements in robustness against both
benign and malicious fine-tuning attacks while preserving the model's
adaptability to new tasks. For instance, LoX leads to 11% to 54% absolute
reductions in attack success rates (ASR) facing benign or malicious fine-tuning
attacks. By investigating the ASR landscape of parameters, we attribute the
success of LoX to that the extrapolation moves LLM parameters to a flatter
zone, thereby less sensitive to perturbations. The code is available at
github.com/VITA-Group/LoX.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RAPID-Net: Accurate Pocket Identification for Binding-Site-Agnostic
  Docking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02371v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02371v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaroslav Balytskyi, Inna Hubenko, Alina Balytska, Christopher V. Kelly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate identification of druggable pockets and their features is essential
for structure-based drug design and effective downstream docking. Here, we
present RAPID-Net, a deep learning-based algorithm designed for the accurate
prediction of binding pockets and seamless integration with docking pipelines.
On the PoseBusters benchmark, RAPID-Net-guided AutoDock Vina achieves 54.9% of
Top-1 poses with RMSD < 2 A and satisfying the PoseBusters chemical-validity
criterion, compared to 49.1% for DiffBindFR. On the most challenging time split
of PoseBusters aiming to assess generalization ability (structures submitted
after September 30, 2021), RAPID-Net-guided AutoDock Vina achieves 53.1% of
Top-1 poses with RMSD < 2 A and PB-valid, versus 59.5% for AlphaFold 3.
Notably, in 92.2% of cases, RAPID-Net-guided Vina samples at least one pose
with RMSD < 2 A (regardless of its rank), indicating that pose ranking, rather
than sampling, is the primary accuracy bottleneck. The lightweight inference,
scalability, and competitive accuracy of RAPID-Net position it as a viable
option for large-scale virtual screening campaigns. Across diverse benchmark
datasets, RAPID-Net outperforms other pocket prediction tools, including
PUResNet and Kalasanty, in both docking accuracy and pocket-ligand intersection
rates. Furthermore, we demonstrate the potential of RAPID-Net to accelerate the
development of novel therapeutics by highlighting its performance on
pharmacologically relevant targets. RAPID-Net accurately identifies distal
functional sites, offering new opportunities for allosteric inhibitor design.
In the case of the RNA-dependent RNA polymerase of SARS-CoV-2, RAPID-Net
uncovers a wider array of potential binding pockets than existing predictors,
which typically annotate only the orthosteric pocket and overlook secondary
cavities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Can We Generate Images with CoT? Let's Verify and Reinforce Image
  Generation Step by Step 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13926v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13926v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Rui Huang, Haoquan Zhang, Manyuan Zhang, Jia<span class="highlight-author">ming Liu</span>, Shanghang Zhang, Peng Gao, Hongsheng Li, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-Thought (CoT) reasoning has been extensively explored in large
models to tackle complex understanding tasks. However, it still remains an open
question whether such strategies can be applied to verifying and reinforcing
image generation scenarios. In this paper, we provide the first comprehensive
investigation of the potential of CoT reasoning to enhance autoregressive image
generation. We focus on three techniques: scaling test-time computation for
verification, aligning model preferences with Direct Preference Optimization
(DPO), and integrating these techniques for complementary effects. Our results
demonstrate that these approaches can be effectively adapted and combined to
significantly improve image generation performance. Furthermore, given the
pivotal role of reward models in our findings, we propose the Potential
Assessment Reward Model (PARM) and PARM++, specialized for autoregressive image
generation. PARM adaptively assesses each generation step through a potential
assessment approach, merging the strengths of existing reward models, and
PARM++ further introduces a reflection mechanism to self-correct the generated
unsatisfactory image, which is the first to incorporate reflection in
autoregressive image generation. Using our investigated reasoning strategies,
we enhance a baseline model, Show-o, to achieve superior results, with a
significant +24% improvement on the GenEval benchmark, surpassing Stable
Diffusion 3 by +15%. We hope our study provides unique insights and paves a new
path for integrating CoT reasoning with autoregressive image generation. Code
and models are released at https://github.com/ZiyuGuo99/Image-Generation-CoT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Journal Version. Code and models are released at
  https://github.com/ZiyuGuo99/Image-Generation-CoT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Level Explanations for Generative Language Models <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14459v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14459v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Monteiro Paes, Dennis Wei, Hyo Jin Do, Hendrik Strobelt, Ronny Luss, Amit Dhurandhar, Manish Nagireddy, Karthikeyan Natesan Ramamurthy, Prasanna Sattigeri, Werner Geyer, Soumya Ghosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the increasing use of large language models (LLMs) for
context-grounded tasks like summarization and question-answering, understanding
what makes an LLM produce a certain response is challenging. We propose
Multi-Level Explanations for Generative Language Models (MExGen), a technique
to provide explanations for context-grounded text generation. MExGen assigns
scores to parts of the context to quantify their influence on the model's
output. It extends attribution methods like LIME and SHAP to LLMs used in
context-grounded tasks where (1) inference cost is high, (2) input text is
long, and (3) the output is text. We conduct a systematic evaluation, both
automated and human, of perturbation-based attribution methods for
summarization and question answering. The results show that our framework can
provide more faithful explanations of generated output than available
alternatives, including LLM self-explanations. We open-source code for MExGen
as part of the ICX360 toolkit: https://github$.$com/IBM/ICX360.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as an oral presentation at ACL 2025. Code available at
  https://github.com/IBM/ICX360</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Constructing Optimal Noise Channels for Enhanced <span class="highlight-title">Robust</span>ness in Quantum
  Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16417v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16417v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Winderl, Nicola Franco, Jeanette Miriam Lorenz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of Quantum Machine Learning (QML), the critical
need to enhance security measures against adversarial attacks and protect QML
models becomes increasingly evident. In this work, we outline the connection
between quantum noise channels and differential privacy (DP), by constructing a
family of noise channels which are inherently $\epsilon$-DP: $(\alpha,
\gamma)$-channels. Through this approach, we successfully replicate the
$\epsilon$-DP bounds observed for depolarizing and random rotation channels,
thereby affirming the broad generality of our framework. Additionally, we use a
semi-definite program to construct an optimally robust channel. In a
small-scale experimental evaluation, we demonstrate the benefits of using our
optimal noise channel over depolarizing noise, particularly in enhancing
adversarial accuracy. Moreover, we assess how the variables $\alpha$ and
$\gamma$ affect the certifiable robustness and investigate how different
encoding methods impact the classifier's robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>QML technical track at IEEE QCE 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SToFM: a Multi-scale Foundation Model for Spatial Transcriptomics <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.11588v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.11588v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suyuan Zhao, Yizhen Luo, Ganbo Yang, Yan Zhong, Hao Zhou, Zaiqing Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial Transcriptomics (ST) technologies provide biologists with rich
insights into single-cell biology by preserving spatial context of cells.
Building foundational models for ST can significantly enhance the analysis of
vast and complex data sources, unlocking new perspectives on the intricacies of
biological tissues. However, modeling ST data is inherently challenging due to
the need to extract multi-scale information from tissue slices containing vast
numbers of cells. This process requires integrating macro-scale tissue
morphology, micro-scale cellular microenvironment, and gene-scale gene
expression profile. To address this challenge, we propose SToFM, a multi-scale
Spatial Transcriptomics Foundation Model. SToFM first performs multi-scale
information extraction on each ST slice, to construct a set of ST sub-slices
that aggregate macro-, micro- and gene-scale information. Then an SE(2)
Transformer is used to obtain high-quality cell representations from the
sub-slices. Additionally, we construct \textbf{SToCorpus-88M}, the largest
high-resolution spatial transcriptomics corpus for pretraining. SToFM achieves
outstanding performance on a variety of downstream tasks, such as tissue region
semantic segmentation and cell type annotation, demonstrating its comprehensive
understanding of ST data through capturing and integrating multi-scale
information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accpeted by ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Turing Test 2.0: The General Intelligence Threshold 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19550v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19550v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Mappouras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rise of artificial intelligence (A.I.) and large language models
like ChatGPT, a new race for achieving artificial general intelligence (A.G.I)
has started. While many speculate how and when A.I. will achieve A.G.I., there
is no clear agreement on how A.G.I. can be detected in A.I. models, even when
popular tools like the Turing test (and its modern variations) are used to
measure their intelligence. In this work, we discuss why traditional methods
like the Turing test do not suffice for measuring or detecting A.G.I. and
provide a new, practical method that can be used to decide if a system
(computer or any other) has reached or surpassed A.G.I. To achieve this, we
make two new contributions. First, we present a clear definition for general
intelligence (G.I.) and set a G.I. Threshold (G.I.T.) that can be used to
distinguish between systems that achieve A.G.I. and systems that do not.
Second, we present a new framework on how to construct tests that can detect if
a system has achieved G.I. in a simple, comprehensive, and clear-cut fail/pass
way. We call this novel framework the Turing test 2.0. We then demonstrate
real-life examples of applying tests that follow our Turing test 2.0 framework
on modern A.I. models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fairness <span class="highlight-title">Evaluation</span> of Large Language Models in Academic Library
  Reference Services 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.04224v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.04224v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haining Wang, Jason Clark, Yueru Yan, Star Bradley, Ruiyang Chen, Yiqiong Zhang, Hengyi Fu, Zuoyu Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As libraries explore large language models (LLMs) for use in virtual
reference services, a key question arises: Can LLMs serve all users equitably,
regardless of demographics or social status? While they offer great potential
for scalable support, LLMs may also reproduce societal biases embedded in their
training data, risking the integrity of libraries' commitment to equitable
service. To address this concern, we evaluate whether LLMs differentiate
responses across user identities by prompting six state-of-the-art LLMs to
assist patrons differing in sex, race/ethnicity, and institutional role. We
found no evidence of differentiation by race or ethnicity, and only minor
evidence of stereotypical bias against women in one model. LLMs demonstrated
nuanced accommodation of institutional roles through the use of linguistic
choices related to formality, politeness, and domain-specific vocabularies,
reflecting professional norms rather than discriminatory treatment. These
findings suggest that current LLMs show a promising degree of readiness to
support equitable and contextually appropriate communication in academic
library reference services.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Photonic Fabric Platform for AI Accelerators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14000v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14000v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Ding, Trung Diep
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM
(PFA), a photonic-enabled switch and memory subsystem that delivers low
latency, high bandwidth, and low per-bit energy. By integrating high-bandwidth
HBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D
electro-optical system-in-package, the PFA offers up to 32 TB of shared memory
alongside 115 Tbps of all-to-all digital switching. The Photonic FabricTM
enables distributed AI training and inference to execute parallelism strategies
more efficiently. The Photonic Fabric removes the silicon beachfront constraint
that limits the fixed memory-to-compute ratio observed in virtually all current
XPU accelerator designs. Replacing a local HBM stack on an XPU with a chiplet
that connects to the Photonic Fabric increases its memory capacity and
correspondingly its memory bandwidth by offering a flexible path to scaling
well beyond the limitations of on-package HBM alone. We introduce CelestiSim, a
lightweight analytical simulator validated on NVIDIA H100 and H200 systems. It
is used to evaluate the performance of LLM reference and energy savings on PFA,
without any significant change to the GPU core design. With the PFA, the
simulation results show that up to 3.66x throughput and 1.40x latency
improvements in LLM inference at 405B parameters, up to 7.04x throughput and
1.41x latency improvements at 1T parameters, and 60-90% energy savings in data
movement for heavy collective operations in all LLM training scenarios. While
these results are shown for NVIDIA GPUs, they can be applied similarly to other
AI accelerator designs (XPUs) that share the same fundamental limitation of
fixed memory to compute.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 14 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Application of YOLOv8 in monocular downward multiple Car Target
  <span class="highlight-title">detection</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10016v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10016v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving technology is progressively transforming traditional car
driving methods, marking a significant milestone in modern transportation.
Object detection serves as a cornerstone of autonomous systems, playing a vital
role in enhancing driving safety, enabling autonomous functionality, improving
traffic efficiency, and facilitating effective emergency responses. However,
current technologies such as radar for environmental perception, cameras for
road perception, and vehicle sensor networks face notable challenges, including
high costs, vulnerability to weather and lighting conditions, and limited
resolution.To address these limitations, this paper presents an improved
autonomous target detection network based on YOLOv8. By integrating structural
reparameterization technology, a bidirectional pyramid structure network model,
and a novel detection pipeline into the YOLOv8 framework, the proposed approach
achieves highly efficient and precise detection of multi-scale, small, and
remote objects. Experimental results demonstrate that the enhanced model can
effectively detect both large and small objects with a detection accuracy of
65%, showcasing significant advancements over traditional methods.This improved
model holds substantial potential for real-world applications and is
well-suited for autonomous driving competitions, such as the Formula Student
Autonomous China (FSAC), particularly excelling in scenarios involving
single-target and small-object detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This submission included authors who did not consent to the
  submission. The paper is being withdrawn until authorship issues are resolved</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ORL-LDM: Offline <span class="highlight-title">Reinforcement</span> Learning Guided Latent Dif<span class="highlight-title">fusion</span> Model
  Super-Resolution Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10027v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10027v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of remote sensing technology, super-resolution
image reconstruction is of great research and practical significance. Existing
deep learning methods have made progress but still face limitations in handling
complex scenes and preserving image details. This paper proposes a
reinforcement learning-based latent diffusion model (LDM) fine-tuning method
for remote sensing image super-resolution. The method constructs a
reinforcement learning environment with states, actions, and rewards,
optimizing decision objectives through proximal policy optimization (PPO)
during the reverse denoising process of the LDM model. Experiments on the
RESISC45 dataset show significant improvements over the baseline model in PSNR,
SSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11,
and LPIPS reducing by 0.06-0.10, particularly in structured and complex natural
scenes. The results demonstrate the method's effectiveness in enhancing
super-resolution quality and adaptability across scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This submission included authors who did not consent to the
  submission. The paper is being withdrawn until authorship issues are resolved</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Impact of Stickers on Multimodal Sentiment and Intent in Social Media: A
  New Task, <span class="highlight-title">Dataset</span> and Baseline 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.08427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.08427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanchen Shi, Biao Ma, Longyin Zhang, Fang Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stickers are increasingly used in social media to express sentiment and
intent. Despite their significant impact on sentiment analysis and intent
recognition, little research has been conducted in this area. To address this
gap, we propose a new task: \textbf{M}ultimodal chat \textbf{S}entiment
\textbf{A}nalysis and \textbf{I}ntent \textbf{R}ecognition involving
\textbf{S}tickers (MSAIRS). Additionally, we introduce a novel multimodal
dataset containing Chinese chat records and stickers excerpted from several
mainstream social media platforms. Our dataset includes paired data with the
same text but different stickers, the same sticker but different contexts, and
various stickers consisting of the same images with different texts, allowing
us to better understand the impact of stickers on chat sentiment and intent. We
also propose an effective multimodal joint model, MMSAIR, featuring
differential vector construction and cascaded attention mechanisms for enhanced
multimodal fusion. Our experiments demonstrate the necessity and effectiveness
of jointly modeling sentiment and intent, as they mutually reinforce each
other's recognition accuracy. MMSAIR significantly outperforms traditional
models and advanced MLLMs, demonstrating the challenge and uniqueness of
sticker interpretation in social media. Our dataset and code are available on
https://github.com/FakerBoom/MSAIRS-Dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for
  Mathematical Problem Solving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07773v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07773v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinji Mai, Haotian Xu, Xing W, Weinong Wang, Jian Hu, Yingying Zhang, Wenqiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) often struggle with mathematical reasoning tasks
requiring precise, verifiable computation. While Reinforcement Learning (RL)
from outcome-based rewards enhances text-based reasoning, understanding how
agents autonomously learn to leverage external tools like code execution
remains crucial. We investigate RL from outcome-based rewards for
Tool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously
generate and execute Python code for mathematical problems without supervised
tool-use examples. Our central contribution is we demonstrate that as RL
training progresses, key metrics scale predictably. Specifically, we observe
strong positive correlations where increased training steps lead to increases
in the spontaneous code execution frequency, the average response length, and,
critically, the final task accuracy. This suggests a quantifiable relationship
between computational effort invested in training and the emergence of
effective, tool-augmented reasoning strategies. We implement a robust framework
featuring a decoupled code execution environment and validate our findings
across standard RL algorithms and frameworks. Experiments show ZeroTIR
significantly surpasses non-tool ZeroRL baselines on challenging math
benchmarks. Our findings provide a foundational understanding of how autonomous
tool use is acquired and scales within Agent RL, offering a reproducible
benchmark for future studies. Code is released at
\href{https://github.com/yyht/openrlhf_async_pipline}{https://github.com/yyht/openrlhf\_async\_pipline}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-Driven <span class="highlight-title">Exploration</span> for a Class of Continuous-Time Indefinite
  Linear--Quadratic <span class="highlight-title">Reinforcement</span> Learning Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.00358v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.00358v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilie Huang, Xun Yu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study reinforcement learning (RL) for the same class of continuous-time
stochastic linear--quadratic (LQ) control problems as in
\cite{huang2024sublinear}, where volatilities depend on both states and
controls while states are scalar-valued and running control rewards are absent.
We propose a model-free, data-driven exploration mechanism that adaptively
adjusts entropy regularization by the critic and policy variance by the actor.
Unlike the constant or deterministic exploration schedules employed in
\cite{huang2024sublinear}, which require extensive tuning for implementations
and ignore learning progresses during iterations, our adaptive exploratory
approach boosts learning efficiency with minimal tuning. Despite its
flexibility, our method achieves a sublinear regret bound that matches the
best-known model-free results for this class of LQ problems, which were
previously derived only with fixed exploration schedules. Numerical experiments
demonstrate that adaptive explorations accelerate convergence and improve
regret performance compared to the non-adaptive model-free and model-based
counterparts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Sequential Recommender with Large Language Models for Joint
  Video and Comment Recommendation <span class="chip">RecSys2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Zheng, Zihan Lin, Enze Liu, Chen Yang, Enyang Bai, Cheng Ling, Wayne Xin Zhao, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, reading or writing comments on captivating videos has emerged as a
critical part of the viewing experience on online video platforms. However,
existing recommender systems primarily focus on users' interaction behaviors
with videos, neglecting comment content and interaction in user preference
modeling. In this paper, we propose a novel recommendation approach called
LSVCR that utilizes user interaction histories with both videos and comments to
jointly perform personalized video and comment recommendation. Specifically,
our approach comprises two key components: sequential recommendation (SR) model
and supplemental large language model (LLM) recommender. The SR model functions
as the primary recommendation backbone (retained in deployment) of our method
for efficient user preference modeling. Concurrently, we employ a LLM as the
supplemental recommender (discarded in deployment) to better capture underlying
user preferences derived from heterogeneous interaction behaviors. In order to
integrate the strengths of the SR model and the supplemental LLM recommender,
we introduce a two-stage training paradigm. The first stage, personalized
preference alignment, aims to align the preference representations from both
components, thereby enhancing the semantics of the SR model. The second stage,
recommendation-oriented fine-tuning, involves fine-tuning the
alignment-enhanced SR model according to specific objectives. Extensive
experiments in both video and comment recommendation tasks demonstrate the
effectiveness of LSVCR. Moreover, online A/B testing on KuaiShou platform
verifies the practical benefits of our approach. In particular, we attain a
cumulative gain of 4.13% in comment watch time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by RecSys2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Infinite Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.09068v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.09068v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dell Zhang, Xiangyu Chen, Jixiang Luo, Mengxi Jia, Changzhi Sun, Ruilong Ren, Jingren Liu, Hao Sun, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancements in Large Language Models (LLMs) and their multimodal
extensions (MLLMs) have ushered in remarkable progress in video understanding.
However, a fundamental challenge persists: effectively processing and
comprehending video content that extends beyond minutes or hours. While recent
efforts like Video-XL-2 have demonstrated novel architectural solutions for
extreme efficiency, and advancements in positional encoding such as HoPE and
VideoRoPE++ aim to improve spatio-temporal understanding over extensive
contexts, current state-of-the-art models still encounter significant
computational and memory constraints when faced with the sheer volume of visual
tokens from lengthy sequences. Furthermore, maintaining temporal coherence,
tracking complex events, and preserving fine-grained details over extended
periods remain formidable hurdles, despite progress in agentic reasoning
systems like Deep Video Discovery. This position paper posits that a logical,
albeit ambitious, next frontier for multimedia research is Infinite Video
Understanding -- the capability for models to continuously process, understand,
and reason about video data of arbitrary, potentially never-ending duration. We
argue that framing Infinite Video Understanding as a blue-sky research
objective provides a vital north star for the multimedia, and the wider AI,
research communities, driving innovation in areas such as streaming
architectures, persistent memory mechanisms, hierarchical and adaptive
representations, event-centric reasoning, and novel evaluation paradigms.
Drawing inspiration from recent work on long/ultra-long video understanding and
several closely related fields, we outline the core challenges and key research
directions towards achieving this transformative capability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RALAD: Bridging the Real-to-Sim Domain Gap in Autonomous Driving with
  Retrieval-Augmented Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12296v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12296v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Zuo, Haibo Hu, Zikang Zhou, Yufei Cui, Ziquan Liu, Jianping Wang, Nan Guan, Jin Wang, Chun Jason Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the pursuit of robust autonomous driving systems, models trained on
real-world datasets often struggle to adapt to new environments, particularly
when confronted with corner cases such as extreme weather conditions.
Collecting these corner cases in the real world is non-trivial, which
necessitates the use of simulators for validation. However,the high
computational cost and the domain gap in data distribution have hindered the
seamless transition between real and simulated driving scenarios. To tackle
this challenge, we propose Retrieval-Augmented Learning for Autonomous Driving
(RALAD), a novel framework designed to bridge the real-to-sim gap at a low
cost. RALAD features three primary designs, including (1) domain adaptation via
an enhanced Optimal Transport (OT) method that accounts for both individual and
grouped image distances, (2) a simple and unified framework that can be applied
to various models, and (3) efficient fine-tuning techniques that freeze the
computationally expensive layers while maintaining robustness. Experimental
results demonstrate that RALAD compensates for the performance degradation in
simulated environments while maintaining accuracy in real-world scenarios
across three different models. Taking Cross View as an example, the mIOU and
mAP metrics in real-world scenarios remain stable before and after RALAD
fine-tuning, while in simulated environments,the mIOU and mAP metrics are
improved by 10.30% and 12.29%, respectively. Moreover, the re-training cost of
our approach is reduced by approximately 88.1%. Our code is available at
https://github.com/JiachengZuo/RALAD.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MolX: Enhancing Large Language Models for Molecular Understanding With A
  Multi-Modal Extension <span class="chip">KDD '25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06777v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06777v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khiem Le, Zhichun Guo, Kaiwen Dong, Xiaobao Huang, Bozhao Nan, Roshni Iyer, Xiangliang Zhang, Olaf Wiest, Wei Wang, Ting Hua, Nitesh V. Chawla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) with their strong task-handling capabilities
have shown remarkable advancements across a spectrum of fields, moving beyond
natural language understanding. However, their proficiency within the chemistry
domain remains restricted, especially in solving molecule-related tasks. This
challenge is attributed to their inherent limitations in comprehending
molecules using only common textual representations, i.e. SMILES strings. In
this study, we seek to enhance the ability of LLMs to comprehend molecules by
equipping them with a multi-modal external module, termed MolX. Instead of
directly using SMILES strings to represent a molecule, we utilize specific
encoders to extract fine-grained features from both SMILES string and 2D
molecular graph representations for feeding into an LLM. A hand-crafted
molecular fingerprint is incorporated to leverage its embedded domain
knowledge. To establish an alignment between MolX and the LLM's textual input
space, the model in which the LLM is frozen, is pre-trained with a strategy
including a diverse set of tasks. Experimental evaluations show that our
proposed method outperforms baselines across 4 downstream molecule-related
tasks ranging from molecule-to-text translation to retrosynthesis, with and
without fine-tuning the LLM, while only introducing a small number of trainable
parameters--0.53\% and 0.82\%, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MLoG-GenAI@KDD '25 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Deep Learning Approach for Augmenting Perceptional Understanding of
  Histopathology Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.06894v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.06894v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoqian Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Recent Years, Digital Technologies Have Made Significant Strides In
Augmenting-Human-Health, Cognition, And Perception, Particularly Within The
Field Of Computational-Pathology. This Paper Presents A Novel Approach To
Enhancing The Analysis Of Histopathology Images By Leveraging A
Mult-modal-Model That Combines Vision Transformers (Vit) With Gpt-2 For Image
Captioning. The Model Is Fine-Tuned On The Specialized Arch-Dataset, Which
Includes Dense Image Captions Derived From Clinical And Academic Resources, To
Capture The Complexities Of Pathology Images Such As Tissue Morphologies,
Staining Variations, And Pathological Conditions. By Generating Accurate,
Contextually Captions, The Model Augments The Cognitive Capabilities Of
Healthcare Professionals, Enabling More Efficient Disease Classification,
Segmentation, And Detection. The Model Enhances The Perception Of Subtle
Pathological Features In Images That Might Otherwise Go Unnoticed, Thereby
Improving Diagnostic Accuracy. Our Approach Demonstrates The Potential For
Digital Technologies To Augment Human Cognitive Abilities In Medical Image
Analysis, Providing Steps Toward More Personalized And Accurate Healthcare
Outcomes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by International Conference on Semantic & Natural Language
  Processing (SNLP 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AirCache: Activating Inter-modal Relevancy KV Cache Compression for
  Efficient Large Vision-Language Model Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.23956v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.23956v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Huang, Hao Zou, Bochen Wang, Ye Xi, Zhen Xie, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Visual Language Models (LVLMs) have gained
significant attention due to their remarkable reasoning capabilities and
proficiency in generalization. However, processing a large number of visual
tokens and generating long-context outputs impose substantial computational
overhead, leading to excessive demands for key-value (KV) cache. To address
this critical bottleneck, we propose AirCache, a novel KV cache compression
method aimed at accelerating LVLMs inference. This work systematically
investigates the correlations between visual and textual tokens within the
attention mechanisms of LVLMs. Our empirical analysis reveals considerable
redundancy in cached visual tokens, wherein strategically eliminating these
tokens preserves model performance while significantly accelerating context
generation. Inspired by these findings, we introduce an elite observation
window for assessing the importance of visual components in the KV cache,
focusing on stable inter-modal relevancy modeling with enhanced
multi-perspective consistency. Additionally, we develop an adaptive layer-wise
budget allocation strategy that capitalizes on the strength and skewness of
token importance distribution, showcasing superior efficiency compared to
uniform allocation. Comprehensive evaluations across multiple LVLMs and
benchmarks demonstrate that our method achieves comparable performance to the
full cache while retaining only 10% of visual KV cache, thereby reducing
decoding latency by 29% to 66% across various batch size and prompt length of
inputs. Notably, as cache retention rates decrease, our method exhibits
increasing performance advantages over existing approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From DDMs to DNNs: Using process data and models of <span class="highlight-title">decision</span>-making to
  improve human-AI interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15225v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15225v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mrugsen Nagsen Gopnarayan, Jaan Aru, Sebastian Gluth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past decades, cognitive neuroscientists and behavioral economists
have recognized the value of describing the process of decision making in
detail and modeling the emergence of decisions over time. For example, the time
it takes to decide can reveal more about an agent's true hidden preferences
than only the decision itself. Similarly, data that track the ongoing decision
process such as eye movements or neural recordings contain critical information
that can be exploited, even if no decision is made. Here, we argue that
artificial intelligence (AI) research would benefit from a stronger focus on
insights about how decisions emerge over time and incorporate related process
data to improve AI predictions in general and human-AI interactions in
particular. First, we introduce a highly established computational framework
that assumes decisions to emerge from the noisy accumulation of evidence, and
we present related empirical work in psychology, neuroscience, and economics.
Next, we discuss to what extent current approaches in multi-agent AI do or do
not incorporate process data and models of decision making. Finally, we outline
how a more principled inclusion of the evidence-accumulation framework into the
training and use of AI can help to improve human-AI interactions in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Review paper, 17 pages, 2 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation
  Models on Standard Computer Vision Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01955v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01955v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahul Ramachandran, Ali Garjani, Roman Bachmann, Andrei Atanov, Oğuzhan Fatih Kar, Amir Zamir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal foundation models, such as GPT-4o, have recently made remarkable
progress, but it is not clear where exactly these models stand in terms of
understanding vision. In this paper, we benchmark the performance of popular
multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0
Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision
tasks (semantic segmentation, object detection, image classification, depth and
surface normal prediction) using established datasets (e.g., COCO, ImageNet and
its variants, etc).
  The main challenges to performing this are: 1) most models are trained to
output text and cannot natively express versatile domains, such as segments or
3D geometry, and 2) many leading models are proprietary and accessible only at
an API level, i.e., there is no weight access to adapt them. We address these
challenges by translating standard vision tasks into equivalent text-promptable
and API-compatible tasks via prompt chaining to create a standardized
benchmarking framework.
  We observe that 1) the models are not close to the state-of-the-art
specialist models at any task. However, 2) they are respectable generalists;
this is remarkable as they are presumably trained on primarily image-text-based
tasks. 3) They perform semantic tasks notably better than geometric ones. 4)
While the prompt-chaining techniques affect performance, better models exhibit
less sensitivity to prompt variations. 5) GPT-4o performs the best among
non-reasoning models, securing the top position in 4 out of 6 tasks, 6)
reasoning models, e.g. o3, show improvements in geometric tasks, and 7) a
preliminary analysis of models with native image generation, like the latest
GPT-4o, shows they exhibit quirks like hallucinations and spatial
misalignments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page at https://fm-vision-evals.epfl.ch/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Graph</span> Neural Networks for O-RAN Mobility Management: A Link <span class="highlight-title">Prediction</span>
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02170v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02170v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ana Gonzalez Bermudez, Miquel Farreras, Milan Groshev, José Antonio Trujillo, Isabel de la Bandera, Raquel Barco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobility performance has been a key focus in cellular networks up to 5G. To
enhance handover (HO) performance, 3GPP introduced Conditional Handover (CHO)
and Layer 1/Layer 2 Triggered Mobility (LTM) mechanisms in 5G. While these
reactive HO strategies address the trade-off between HO failures (HOF) and
ping-pong effects, they often result in inefficient radio resource utilization
due to additional HO preparations. To overcome these challenges, this article
proposes a proactive HO framework for mobility management in O-RAN, leveraging
user-cell link predictions to identify the optimal target cell for HO. We
explore various categories of Graph Neural Networks (GNNs) for link prediction
and analyze the complexity of applying them to the mobility management domain.
Two GNN models are compared using a real-world dataset, with experimental
results demonstrating their ability to capture the dynamic and graph-structured
nature of cellular networks. Finally, we present key insights from our study
and outline future steps to enable the integration of GNN-based link prediction
for mobility management in O-RAN networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, 1 table. Submitted to IEEE Vehicular Technology
  Magazine, Special Issue on "AI for 6G O-RAN Intelligent, Cost-Efficient and
  Secure Automation". Version after Major Revision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving nonconvex Hamilton--Jacobi--Isaacs equations with PINN-based
  policy iteration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15455v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15455v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hee Jun Yang, Minjung Gim, Yeoneung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a mesh-free policy iteration framework that combines classical
dynamic programming with physics-informed neural networks (PINNs) to solve
high-dimensional, nonconvex Hamilton--Jacobi--Isaacs (HJI) equations arising in
stochastic differential games and robust control. The method alternates between
solving linear second-order PDEs under fixed feedback policies and updating the
controls via pointwise minimax optimization using automatic differentiation.
Under standard Lipschitz and uniform ellipticity assumptions, we prove that the
value function iterates converge locally uniformly to the unique viscosity
solution of the HJI equation. The analysis establishes equi-Lipschitz
regularity of the iterates, enabling provable stability and convergence without
requiring convexity of the Hamiltonian. Numerical experiments demonstrate the
accuracy and scalability of the method. In a two-dimensional stochastic
path-planning game with a moving obstacle, our method matches finite-difference
benchmarks with relative $L^2$-errors below %10^{-2}%. In five- and
ten-dimensional publisher-subscriber differential games with anisotropic noise,
the proposed approach consistently outperforms direct PINN solvers, yielding
smoother value functions and lower residuals. Our results suggest that
integrating PINNs with policy iteration is a practical and theoretically
grounded method for solving high-dimensional, nonconvex HJI equations, with
potential applications in robotics, finance, and multi-agent reinforcement
learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Efficient Generative Large Language Model Serving: A <span class="highlight-title">Survey</span> from
  Algorithms to Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15234v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15234v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Hongyi Jin, Tianqi Chen, Zhihao Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly evolving landscape of artificial intelligence (AI), generative
large language models (LLMs) stand at the forefront, revolutionizing how we
interact with our data. However, the computational intensity and memory
consumption of deploying these models present substantial challenges in terms
of serving efficiency, particularly in scenarios demanding low latency and high
throughput. This survey addresses the imperative need for efficient LLM serving
methodologies from a machine learning system (MLSys) research perspective,
standing at the crux of advanced AI innovations and practical system
optimizations. We provide in-depth analysis, covering a spectrum of solutions,
ranging from cutting-edge algorithmic modifications to groundbreaking changes
in system designs. The survey aims to provide a comprehensive understanding of
the current state and future directions in efficient LLM serving, offering
valuable insights for researchers and practitioners in overcoming the barriers
of effective LLM deployment, thereby reshaping the future of AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM Computing Surveys</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Artificial Intelligence for Green Hydrogen Yield <span class="highlight-title">Prediction</span> and Site
  Suitability using SHAP-Based Composite Index: Focus on Oman 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14219v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14219v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Obumneme Zimuzor Nwafor, Mohammed Abdul Majeed Al Hooti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As nations seek sustainable alternatives to fossil fuels, green hydrogen has
emerged as a promising strategic pathway toward decarbonisation, particularly
in solar-rich arid regions. However, identifying optimal locations for hydrogen
production requires the integration of complex environmental, atmospheric, and
infrastructural factors, often compounded by limited availability of direct
hydrogen yield data. This study presents a novel Artificial Intelligence (AI)
framework for computing green hydrogen yield and site suitability index using
mean absolute SHAP (SHapley Additive exPlanations) values. This framework
consists of a multi-stage pipeline of unsupervised multi-variable clustering,
supervised machine learning classifier and SHAP algorithm. The pipeline trains
on an integrated meteorological, topographic and temporal dataset and the
results revealed distinct spatial patterns of suitability and relative
influence of the variables. With model predictive accuracy of 98%, the result
also showed that water proximity, elevation and seasonal variation are the most
influential factors determining green hydrogen site suitability in Oman with
mean absolute shap values of 2.470891, 2.376296 and 1.273216 respectively.
Given limited or absence of ground-truth yield data in many countries that have
green hydrogen prospects and ambitions, this study offers an objective and
reproducible alternative to subjective expert weightings, thus allowing the
data to speak for itself and potentially discover novel latent groupings
without pre-imposed assumptions. This study offers industry stakeholders and
policymakers a replicable and scalable tool for green hydrogen infrastructure
planning and other decision making in data-scarce regions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vascular <span class="highlight-title">Segmentation</span> of Functional Ultrasound Images using Deep
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22365v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22365v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hana Sebia, Thomas Guyet, Mickaël Pereira, Marco Valdebenito, Hugues Berry, Benjamin Vidal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmentation of medical images is a fundamental task with numerous
applications. While MRI, CT, and PET modalities have significantly benefited
from deep learning segmentation techniques, more recent modalities, like
functional ultrasound (fUS), have seen limited progress. fUS is a non invasive
imaging method that measures changes in cerebral blood volume (CBV) with high
spatio-temporal resolution. However, distinguishing arterioles from venules in
fUS is challenging due to opposing blood flow directions within the same pixel.
Ultrasound localization microscopy (ULM) can enhance resolution by tracking
microbubble contrast agents but is invasive, and lacks dynamic CBV
quantification. In this paper, we introduce the first deep learning-based
segmentation tool for fUS images, capable of differentiating signals from
different vascular compartments, based on ULM automatic annotation and enabling
dynamic CBV quantification. We evaluate various UNet architectures on fUS
images of rat brains, achieving competitive segmentation performance, with 90%
accuracy, a 71% F1 score, and an IoU of 0.59, using only 100 temporal frames
from a fUS stack. These results are comparable to those from tubular structure
segmentation in other imaging modalities. Additionally, models trained on
resting-state data generalize well to images captured during visual
stimulation, highlighting robustness. This work offers a non-invasive,
cost-effective alternative to ULM, enhancing fUS data interpretation and
improving understanding of vessel function. Our pipeline shows high linear
correlation coefficients between signals from predicted and actual compartments
in both cortical and deeper regions, showcasing its ability to accurately
capture blood flow dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Monitoring digestate application on agricultural crops using Sentinel-2
  Satellite imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.19996v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.19996v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Kalogeras, Dimitrios Bormpoudakis, Iason Tsardanidis, Dimitra A. Loka, Charalampos Kontoes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread use of Exogenous Organic Matter in agriculture necessitates
monitoring to assess its effects on soil and crop health. This study evaluates
optical Sentinel-2 satellite imagery for detecting digestate application, a
practice that enhances soil fertility but poses environmental risks like
microplastic contamination and nitrogen losses. In the first instance,
Sentinel-2 satellite image time series (SITS) analysis of specific indices
(EOMI, NDVI, EVI) was used to characterize EOM's spectral behavior after
application on the soils of four different crop types in Thessaly, Greece.
Furthermore, Machine Learning (ML) models (namely Random Forest, k-NN, Gradient
Boosting and a Feed-Forward Neural Network), were used to investigate digestate
presence detection, achieving F1-scores up to 0.85. The findings highlight the
potential of combining remote sensing and ML for scalable and cost-effective
monitoring of EOM applications, supporting precision agriculture and
sustainability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for 2025 IEEE International Geoscience and Remote Sensing
  Symposium (IGARSS 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prompt Guidance and Human Proximal Perception for HOT <span class="highlight-title">Prediction</span> with
  Regional Joint Loss <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01630v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01630v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiao Wang, Yu Lei, Zhenao Wei, Weiying Xue, Xinyu Jiang, Nan Zhuang, Qi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of Human-Object conTact (HOT) detection involves identifying the
specific areas of the human body that are touching objects. Nevertheless,
current models are restricted to just one type of image, often leading to too
much segmentation in areas with little interaction, and struggling to maintain
category consistency within specific regions. To tackle this issue, a HOT
framework, termed \textbf{P3HOT}, is proposed, which blends \textbf{P}rompt
guidance and human \textbf{P}roximal \textbf{P}erception. To begin with, we
utilize a semantic-driven prompt mechanism to direct the network's attention
towards the relevant regions based on the correlation between image and text.
Then a human proximal perception mechanism is employed to dynamically perceive
key depth range around the human, using learnable parameters to effectively
eliminate regions where interactions are not expected. Calculating depth
resolves the uncertainty of the overlap between humans and objects in a 2D
perspective, providing a quasi-3D viewpoint. Moreover, a Regional Joint Loss
(RJLoss) has been created as a new loss to inhibit abnormal categories in the
same area. A new evaluation metric called ``AD-Acc.'' is introduced to address
the shortcomings of existing methods in addressing negative samples.
Comprehensive experimental results demonstrate that our approach achieves
state-of-the-art performance in four metrics across two benchmark datasets.
Specifically, our model achieves an improvement of \textbf{0.7}$\uparrow$,
\textbf{2.0}$\uparrow$, \textbf{1.6}$\uparrow$, and \textbf{11.0}$\uparrow$ in
SC-Acc., mIoU, wIoU, and AD-Acc. metrics, respectively, on the HOT-Annotated
dataset. The sources code are available at
https://github.com/YuxiaoWang-AI/P3HOT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Learning-Based Modeling of the Anode Heel Effect in X-ray Beam
  Monte Carlo <span class="highlight-title">Simulation</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.19155v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.19155v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hussein Harb, Didier Benoit, Axel Rannou, Chi-Hieu Pham, Valentin Tissot, Bahaa Nasr, Julien Bert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To develop a machine learning-based framework for accurately modeling the
anode heel effect in Monte Carlo simulations of X-ray imaging systems, enabling
realistic beam intensity profiles with minimal experimental calibration.
Multiple regression models were trained to predict spatial intensity variations
along the anode-cathode axis using experimentally acquired weights derived from
beam measurements across different tube potentials. These weights captured the
asymmetry introduced by the anode heel effect. A systematic fine-tuning
protocol was established to minimize the number of required measurements while
preserving model accuracy. The models were implemented in the OpenGATE 10 and
GGEMS Monte Carlo toolkits to evaluate their integration feasibility and
predictive performance. Among the tested models, gradient boosting regression
(GBR) delivered the highest accuracy, with prediction errors remaining below 5%
across all energy levels. The optimized fine-tuning strategy required only six
detector positions per energy level, reducing measurement effort by 65%. The
maximum error introduced through this fine-tuning process remained below 2%.
Dose actor comparisons within Monte Carlo simulations demonstrated that the
GBR-based model closely replicated clinical beam profiles and significantly
outperformed conventional symmetric beam models. This study presents a robust
and generalizable method for incorporating the anode heel effect into Monte
Carlo simulations using machine learning. By enabling accurate,
energy-dependent beam modeling with limited calibration data, the approach
enhances simulation realism for applications in clinical dosimetry, image
quality assessment, and radiation protection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automated <span class="highlight-title">planning</span> with ontologies under coherence update semantics
  (Extended Version) <span class="chip">KR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15120v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15120v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Borgwardt, Duy Nhu, Gabriele Röger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Standard automated planning employs first-order formulas under closed-world
semantics to achieve a goal with a given set of actions from an initial state.
We follow a line of research that aims to incorporate background knowledge into
automated planning problems, for example, by means of ontologies, which are
usually interpreted under open-world semantics. We present a new approach for
planning with DL-Lite ontologies that combines the advantages of ontology-based
action conditions provided by explicit-input knowledge and action bases (eKABs)
and ontology-aware action effects under the coherence update semantics. We show
that the complexity of the resulting formalism is not higher than that of
previous approaches and provide an implementation via a polynomial compilation
into classical planning. An evaluation of existing and new benchmarks examines
the performance of a planning system on different variants of our compilation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of a paper accepted at 22nd International Conference
  on Principles of Knowledge Representation and Reasoning (KR 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Privacy-Utility Trade-off in Decentralized Learning with
  Generalized Correlated Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14644v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14644v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angelo Rodio, Zheng Chen, Erik G. Larsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decentralized learning enables distributed agents to collaboratively train a
shared machine learning model without a central server, through local
computation and peer-to-peer communication. Although each agent retains its
dataset locally, sharing local models can still expose private information
about the local training datasets to adversaries. To mitigate privacy attacks,
a common strategy is to inject random artificial noise at each agent before
exchanging local models between neighbors. However, this often leads to utility
degradation due to the negative effects of cumulated artificial noise on the
learning algorithm. In this work, we introduce CorN-DSGD, a novel
covariance-based framework for generating correlated privacy noise across
agents, which unifies several state-of-the-art methods as special cases. By
leveraging network topology and mixing weights, CorN-DSGD optimizes the noise
covariance to achieve network-wide noise cancellation. Experimental results
show that CorN-DSGD cancels more noise than existing pairwise correlation
schemes, improving model performance under formal privacy guarantees.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, accepted at IEEE ITW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards <span class="highlight-title">Detect</span>ing Persuasion on Social Media: From Model Development to
  Insights on Persuasion Strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.13844v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.13844v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elyas Meguellati, Stefano Civelli, Pietro Bernardelle, Shazia Sadiq, Irwin King, Gianluca Demartini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Political advertising plays a pivotal role in shaping public opinion and
influencing electoral outcomes, often through subtle persuasive techniques
embedded in broader propaganda strategies. Detecting these persuasive elements
is crucial for enhancing voter awareness and ensuring transparency in
democratic processes. This paper presents an integrated approach that bridges
model development and real-world application through two interconnected
studies. First, we introduce a lightweight model for persuasive text detection
that achieves state-of-the-art performance in Subtask 3 of SemEval 2023 Task 3
while requiring significantly fewer computational resources and training data
than existing methods. Second, we demonstrate the model's practical utility by
collecting the Australian Federal Election 2022 Facebook Ads (APA22) dataset,
partially annotating a subset for persuasion, and fine-tuning the model to
adapt from mainstream news to social media content. We then apply the
fine-tuned model to label the remainder of the APA22 dataset, revealing
distinct patterns in how political campaigns leverage persuasion through
different funding strategies, word choices, demographic targeting, and temporal
shifts in persuasion intensity as election day approaches. Our findings not
only underscore the necessity of domain-specific modeling for analyzing
persuasion on social media but also show how uncovering these strategies can
enhance transparency, inform voters, and promote accountability in digital
campaigns.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conflict <span class="highlight-title">Detection</span> for Temporal Knowledge <span class="highlight-title">Graph</span>s:A Fast Constraint
  Mining Algorithm and New <span class="highlight-title">Benchmark</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11053v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11053v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhao Chen, Junyang Ren, Wentao Ding, Haoyuan Ouyang, Wei Hu, Yuzhong Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal facts, which are used to describe events that occur during specific
time periods, have become a topic of increased interest in the field of
knowledge graph (KG) research. In terms of quality management, the introduction
of time restrictions brings new challenges to maintaining the temporal
consistency of KGs. Previous studies rely on manually enumerated temporal
constraints to detect conflicts, which are labor-intensive and may have
granularity issues. To address this problem, we start from the common pattern
of temporal facts and propose a pattern-based temporal constraint mining
method, PaTeCon. Unlike previous studies, PaTeCon uses graph patterns and
statistical information relevant to the given KG to automatically generate
temporal constraints, without the need for human experts. In this paper, we
illustrate how this method can be optimized to achieve significant speed
improvement. We also annotate Wikidata and Freebase to build two new benchmarks
for conflict detection. Extensive experiments demonstrate that our
pattern-based automatic constraint mining approach is highly effective in
generating valuable temporal constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-domain Multi-step Thinking: Zero-shot Fine-grained Traffic Sign
  Recognition in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01534v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01534v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaozong Gan, Guang Li, Ren Togo, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we propose Cross-domain Multi-step Thinking (CdMT) to improve
zero-shot fine-grained traffic sign recognition (TSR) performance in the wild.
Zero-shot fine-grained TSR in the wild is challenging due to the cross-domain
problem between clean template traffic signs and real-world counterparts, and
existing approaches particularly struggle with cross-country TSR scenarios,
where traffic signs typically differ between countries. The proposed CdMT
framework tackles these challenges by leveraging the multi-step reasoning
capabilities of large multimodal models (LMMs). We introduce context,
characteristic, and differential descriptions to design multiple thinking
processes for LMMs. Context descriptions, which are enhanced by center
coordinate prompt optimization, enable the precise localization of target
traffic signs in complex road images and filter irrelevant responses via novel
prior traffic sign hypotheses. Characteristic descriptions, which are derived
from in-context learning with template traffic signs, bridge cross-domain gaps
and enhance fine-grained TSR. Differential descriptions refine the multimodal
reasoning ability of LMMs by distinguishing subtle differences among similar
signs. CdMT is independent of training data and requires only simple and
uniform instructions, enabling it to achieve cross-country TSR. We conducted
extensive experiments on three benchmark datasets and two real-world datasets
from different countries. The proposed CdMT framework achieved superior
performance compared with other state-of-the-art methods on all five datasets,
with recognition accuracies of 0.93, 0.89, 0.97, 0.89, and 0.85 on the GTSRB,
BTSD, TT-100K, Sapporo, and Yokohama datasets, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published by Knowledge-Based Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache
  at a Large Cloud Provider <span class="chip">ATC'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02634v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02634v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Wang, Jinbo Han, Xingda Wei, Sijie Shen, Dingyan Zhang, Chenguang Fang, Rong Chen, Wenyuan Yu, Haibo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Serving large language models (LLMs) is important for cloud providers, and
caching intermediate results (KV\$) after processing each request substantially
improves serving throughput and latency. However, there is limited
understanding of how LLM serving benefits from KV\$ caching, where system
design decisions like cache eviction policies are highly workload-dependent. In
this paper, we present the first systematic characterization of the KV\$
workload patterns from one of the leading LLM service providers. We draw
observations that were not covered by previous studies focusing on synthetic
workloads, including: KV\$ reuses are skewed across requests, where reuses
between single-turn requests are equally important as multi-turn requests; the
reuse time and probability are diverse considering all requests, but for a
specific request category, the pattern tends to be predictable; and the overall
cache size required for an ideal cache hit ratio is moderate. Based on the
characterization, we further propose a workload-aware cache eviction policy
that improves the serving performance under real-world traces, especially with
limited cache capacity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by USENIX ATC'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cautious Next Token <span class="highlight-title">Prediction</span> <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.03038v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.03038v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhou Wang, Lingzhi Zhang, Yue Bai, Mang Tik Chiu, Zhengmian Hu, Mingyuan Zhang, Qihua Dong, Yu Yin, Sohrab Amirghodsi, Yun Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Next token prediction paradigm has been prevailing for autoregressive models
in the era of LLMs. The current default sampling choice for popular LLMs is
temperature scaling together with nucleus sampling to balance diversity and
coherence. Nevertheless, such approach leads to inferior performance in various
NLP tasks when the model is not certain about testing questions. To this end,
we propose a brand new training-free decoding strategy, dubbed as Cautious Next
Token Prediction (CNTP). In the decoding process, if the model has
comparatively high prediction entropy at a certain step, we sample multiple
trials starting from the step independently and stop when encountering any
punctuation. Then we select the trial with the lowest perplexity score viewed
as the most probable and reliable trial path given the model's capacity. The
trial number is negatively correlated with the prediction confidence, i.e., the
less confident the model is, the more trials it should sample. This is
consistent with human beings' behaviour: when feeling uncertain or unconfident,
one tends to think more creatively, exploring multiple thinking paths, to
cautiously select the path one feels most confident about. Extensive
experiments on both LLMs and MLLMs show that our proposed CNTP approach
outperforms existing standard decoding strategies consistently by a clear
margin. Moreover, the integration of CNTP with self consistency can further
improve over vanilla self consistency. We believe our proposed CNTP has the
potential to become one of the default choices for LLM decoding. Code is
available at https://github.com/wyzjack/CNTP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Alleviating Seasickness through Brain-Computer Interface-based Attention
  Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Bao, Kailin Xu, Jiawei Zhu, Haiyun Huang, Kangning Li, Qiyun Huang, Yuanqing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Seasickness poses a widespread problem that adversely impacts both passenger
comfort and the operational efficiency of maritime crews. Although attention
shift has been proposed as a potential method to alleviate symptoms of motion
sickness, its efficacy remains to be rigorously validated, especially in
maritime environments. In this study, we develop an AI-driven brain-computer
interface (BCI) to realize sustained and practical attention shift by
incorporating tasks such as breath counting. Forty-three participants completed
a real-world nautical experiment consisting of a real-feedback session, a
resting session, and a pseudo-feedback session. Notably, 81.39\% of the
participants reported that the BCI intervention was effective. EEG analysis
revealed that the proposed system can effectively regulate motion sickness EEG
signatures, such as an decrease in total band power, along with an increase in
theta relative power and a decrease in beta relative power. Furthermore, an
indicator of attentional focus, the theta/beta ratio, exhibited a significant
reduction during the real-feedback session, providing further evidence to
support the effectiveness of the BCI in shifting attention. Collectively, this
study presents a novel nonpharmacological, portable, and effective approach for
seasickness intervention, which has the potential to open up a brand-new
application domain for BCIs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Advancing Multimodal Reasoning via <span class="highlight-title">Reinforcement</span> Learning with Cold
  Start 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.22334v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.22334v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lai Wei, Yuting Li, Kaipeng Zheng, Chen Wang, <span class="highlight-author">Yue Wang</span>, Linghe Kong, Lichao Sun, Weiran Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have demonstrated
impressive chain-of-thought reasoning capabilities, with reinforcement learning
(RL) playing a crucial role in this progress. While "aha moment"
patterns--where models exhibit self-correction through reflection--are often
attributed to emergent properties from RL, we first demonstrate that these
patterns exist in multimodal LLMs (MLLMs) prior to RL training but may not
necessarily correlate with improved reasoning performance. Building on these
insights, we present a comprehensive study on enhancing multimodal reasoning
through a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start
with structured chain-of-thought reasoning patterns, followed by (2)
reinforcement learning via GRPO to further refine these capabilities. Our
extensive experiments show that this combined approach consistently outperforms
both SFT-only and RL-only methods across challenging multimodal reasoning
benchmarks. The resulting models achieve state-of-the-art performance among
open-source MLLMs at both 3B and 7B scales, with our 7B model showing
substantial improvements over base models (e.g., 66.3 %$\rightarrow$73.4 % on
MathVista, 62.9 %$\rightarrow$70.4 % on We-Math) and our 3B model achieving
performance competitive with several 7B models. Overall, this work provides
practical guidance for building advanced multimodal reasoning models. Our code
is available at https://github.com/waltonfuture/RL-with-Cold-Start.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SegQuant: A Semantics-Aware and Generalizable Quantization Framework for
  Dif<span class="highlight-title">fusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14811v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14811v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaji Zhang, Ruichao Sun, Hailiang Zhao, Jiaju Wu, Peng Chen, Hao Li, Yuying Liu, Xinkui Zhao, Kingsum Chow, Gang Xiong, Shuiguang Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated exceptional generative capabilities but
are computationally intensive, posing significant challenges for deployment in
resource-constrained or latency-sensitive environments. Quantization offers an
effective means to reduce model size and computational cost, with post-training
quantization (PTQ) being particularly appealing due to its compatibility with
pre-trained models without requiring retraining or training data. However,
existing PTQ methods for diffusion models often rely on architecture-specific
heuristics that limit their generalizability and hinder integration with
industrial deployment pipelines. To address these limitations, we propose
SegQuant, a unified quantization framework that adaptively combines
complementary techniques to enhance cross-model versatility. SegQuant consists
of a segment-aware, graph-based quantization strategy (SegLinear) that captures
structural semantics and spatial heterogeneity, along with a dual-scale
quantization scheme (DualScale) that preserves polarity-asymmetric activations,
which is crucial for maintaining visual fidelity in generated outputs. SegQuant
is broadly applicable beyond Transformer-based diffusion models, achieving
strong performance while ensuring seamless compatibility with mainstream
deployment tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoBridge: A Hierarchical Architecture Bridging Cognition and Execution
  for General <span class="highlight-title">Robot</span>ic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.01709v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.01709v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaidong Zhang, Rongtao Xu, Pengzhen Ren, Junfan Lin, Hefeng Wu, Liang Lin, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Operating robots in open-ended scenarios with diverse tasks is a crucial
research and application direction in robotics. While recent progress in
natural language processing and large multimodal models has enhanced robots'
ability to understand complex instructions, robot manipulation still faces the
procedural skill dilemma and the declarative skill dilemma in open
environments. Existing methods often compromise cognitive and executive
capabilities. To address these challenges, in this paper, we propose RoBridge,
a hierarchical intelligent architecture for general robotic manipulation. It
consists of a high-level cognitive planner (HCP) based on a large-scale
pre-trained vision-language model (VLM), an invariant operable representation
(IOR) serving as a symbolic bridge, and a generalist embodied agent (GEA).
RoBridge maintains the declarative skill of VLM and unleashes the procedural
skill of reinforcement learning, effectively bridging the gap between cognition
and execution. RoBridge demonstrates significant performance improvements over
existing baselines, achieving a 75% success rate on new tasks and an 83%
average success rate in sim-to-real generalization using only five real-world
data samples per task. This work represents a significant step towards
integrating cognitive reasoning with physical execution in robotic systems,
offering a new paradigm for general robotic manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://abliao.github.io/RoBridge/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Efficient and Precise Training Data Construction Framework for
  Process-supervised Reward Model in Mathematical Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02382v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02382v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Sun, Qianlong Du, Fuwei Cui, Jiajun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enhancing the mathematical reasoning capabilities of Large Language Models
(LLMs) is of great scientific and practical significance. Researchers typically
employ process-supervised reward models (PRMs) to guide the reasoning process,
effectively improving the models' reasoning abilities. However, existing
methods for constructing process supervision training data, such as manual
annotation and per-step Monte Carlo estimation, are often costly or suffer from
poor quality. To address these challenges, this paper introduces a framework
called EpicPRM, which annotates each intermediate reasoning step based on its
quantified contribution and uses an adaptive binary search algorithm to enhance
both annotation precision and efficiency. Using this approach, we efficiently
construct a high-quality process supervision training dataset named Epic50k,
consisting of 50k annotated intermediate steps. Compared to other publicly
available datasets, the PRM trained on Epic50k demonstrates significantly
superior performance. Getting Epic50k at https://github.com/xiaolizh1/EpicPRM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EXGnet: a single-lead explainable-AI guided multiresolution network with
  train-only quantitative features for trustworthy ECG arrhythmia
  classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12404v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12404v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tushar Talukder Showrav, Soyabul Islam Lincoln, Md. Kamrul Hasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has significantly propelled the performance of ECG arrhythmia
classification, yet its clinical adoption remains hindered by challenges in
interpretability and deployment on resource-constrained edge devices. To bridge
this gap, we propose EXGnet, a novel and reliable ECG arrhythmia classification
network tailored for single-lead signals, specifically designed to balance high
accuracy, explainability, and edge compatibility. EXGnet integrates XAI
supervision during training via a normalized cross-correlation based loss,
directing the model's attention to clinically relevant ECG regions, similar to
a cardiologist's focus. This supervision is driven by automatically generated
ground truth, derived through an innovative heart rate variability-based
approach, without the need for manual annotation. To enhance classification
accuracy without compromising deployment simplicity, we incorporate
quantitative ECG features during training. These enrich the model with
multi-domain knowledge but are excluded during inference, keeping the model
lightweight for edge deployment. Additionally, we introduce an innovative
multiresolution block to efficiently capture both short and long-term signal
features while maintaining computational efficiency. Rigorous evaluation on the
Chapman and Ningbo benchmark datasets validates the supremacy of EXGnet, which
achieves average five-fold accuracies of 98.762% and 96.932%, and F1-scores of
97.910% and 95.527%, respectively. Comprehensive ablation studies and both
quantitative and qualitative interpretability assessment confirm that the XAI
guidance is pivotal, demonstrably enhancing the model's focus and
trustworthiness. Overall, EXGnet sets a new benchmark by combining
high-performance arrhythmia classification with interpretability, paving the
way for more trustworthy and accessible portable ECG based health monitoring
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">155</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Unlearning of Traffic State <span class="highlight-title">Estimation</span> and <span class="highlight-title">Prediction</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17984v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17984v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Wang, R. Tyrrell Rockafellar,  Xuegang,  Ban
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven traffic state estimation and prediction (TSEP) relies heavily on
data sources that contain sensitive information. While the abundance of data
has fueled significant breakthroughs, particularly in machine learning-based
methods, it also raises concerns regarding privacy, cybersecurity, and data
freshness. These issues can erode public trust in intelligent transportation
systems. Recently, regulations have introduced the "right to be forgotten",
allowing users to request the removal of their private data from models. As
machine learning models can remember old data, simply removing it from back-end
databases is insufficient in such systems. To address these challenges, this
study introduces a novel learning paradigm for TSEP-Machine Unlearning
TSEP-which enables a trained TSEP model to selectively forget
privacy-sensitive, poisoned, or outdated data. By empowering models to
"unlearn," we aim to enhance the trustworthiness and reliability of data-driven
traffic TSEP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Learning Workflow for Analysis of High-Dimensional Order
  Parameter Space: A Case Study of Polymer Crystallization from Molecular
  <span class="highlight-title">Dynamic</span>s <span class="highlight-title">Simulation</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elyar Tourani, Brian J. Edwards, Bamin Khomami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently, identification of crystallization pathways in polymers is being
carried out using molecular simulation-based data on a preset cut-off point on
a single order parameter (OP) to define nucleated or crystallized regions.
Aside from sensitivity to cut-off, each of these OPs introduces its own
systematic biases. In this study, an integrated machine learning workflow is
presented to accurately quantify crystallinity in polymeric systems using
atomistic molecular dynamics data. Each atom is represented by a
high-dimensional feature vector that combines geometric, thermodynamic-like,
and symmetry-based descriptors. Low dimensional embeddings are employed to
expose latent structural fingerprints within atomic environments. Subsequently,
unsupervised clustering on the embeddings identified crystalline and amorphous
atoms with high fidelity. After generating high quality labels with
multidimensional data, we use supervised learning techniques to identify a
minimal set of order parameters that can fully capture this label. Various
tests were conducted to reduce the feature set, demonstrating that using only
three order parameters is sufficient to recreate the crystallization labels.
Based on these observed OPs, the crystallinity index (C-index) is defined as
the logistic regression model's probability of crystallinity, remaining bimodal
throughout the process and achieving over 0.98 classification performance
(AUC). Notably, a model trained on one or a few snapshots enables efficient
on-the-fly computation of crystallinity. Lastly, we demonstrate how the optimal
C-index fit evolves during various stages of crystallization, supporting the
hypothesis that entropy dominates early nucleation, while symmetry gains
relevance later. This workflow provides a data-driven strategy for OP selection
and a metric to monitor structural transformations in large-scale polymer
simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 8 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SIFOTL: A Principled, Statistically-Informed Fidelity-<span class="highlight-title">Optimization</span>
  Method for Tabular Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17979v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17979v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubham Mohole, Sainyam Galhotra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying the factors driving data shifts in tabular datasets is a
significant challenge for analysis and decision support systems, especially
those focusing on healthcare. Privacy rules restrict data access, and noise
from complex processes hinders analysis. To address this challenge, we propose
SIFOTL (Statistically-Informed Fidelity-Optimization Method for Tabular
Learning) that (i) extracts privacy-compliant data summary statistics, (ii)
employs twin XGBoost models to disentangle intervention signals from noise with
assistance from LLMs, and (iii) merges XGBoost outputs via a Pareto-weighted
decision tree to identify interpretable segments responsible for the shift.
Unlike existing analyses which may ignore noise or require full data access for
LLM-based analysis, SIFOTL addresses both challenges using only privacy-safe
summary statistics. Demonstrating its real-world efficacy, for a MEPS panel
dataset mimicking a new Medicare drug subsidy, SIFOTL achieves an F1 score of
0.85, substantially outperforming BigQuery Contribution Analysis (F1=0.46) and
statistical tests (F1=0.20) in identifying the segment receiving the subsidy.
Furthermore, across 18 diverse EHR datasets generated based on Synthea ABM,
SIFOTL sustains F1 scores of 0.86-0.96 without noise and >= 0.75 even with
injected observational noise, whereas baseline average F1 scores range from
0.19-0.67 under the same tests. SIFOTL, therefore, provides an interpretable,
privacy-conscious workflow that is empirically robust to observational noise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving the Computational Efficiency and Explainability of
  GeoAggregator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17977v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17977v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Deng, Ziqi Li, Mingshu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate modeling and explaining geospatial tabular data (GTD) are critical
for understanding geospatial phenomena and their underlying processes. Recent
work has proposed a novel transformer-based deep learning model named
GeoAggregator (GA) for this purpose, and has demonstrated that it outperforms
other statistical and machine learning approaches. In this short paper, we
further improve GA by 1) developing an optimized pipeline that accelerates the
dataloading process and streamlines the forward pass of GA to achieve better
computational efficiency; and 2) incorporating a model ensembling strategy and
a post-hoc model explanation function based on the GeoShapley framework to
enhance model explainability. We validate the functionality and efficiency of
the proposed strategies by applying the improved GA model to synthetic
datasets. Experimental results show that our implementation improves the
prediction accuracy and inference speed of GA compared to the original
implementation. Moreover, explanation experiments indicate that GA can
effectively captures the inherent spatial effects in the designed synthetic
dataset. The complete pipeline has been made publicly available for community
use (https://github.com/ruid7181/GA-sklearn).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot <span class="highlight-title">Dynamic</span> Concept Personalization with Grid-Based LoRA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rameen Abdal, Or Patashnik, Ekaterina Deyneka, Hao Chen, Aliaksandr Siarohin, Sergey Tulyakov, Daniel Cohen-Or, Kfir Aberman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-to-video generation have enabled high-quality
synthesis from text and image prompts. While the personalization of dynamic
concepts, which capture subject-specific appearance and motion from a single
video, is now feasible, most existing methods require per-instance fine-tuning,
limiting scalability. We introduce a fully zero-shot framework for dynamic
concept personalization in text-to-video models. Our method leverages
structured 2x2 video grids that spatially organize input and output pairs,
enabling the training of lightweight Grid-LoRA adapters for editing and
composition within these grids. At inference, a dedicated Grid Fill module
completes partially observed layouts, producing temporally coherent and
identity preserving outputs. Once trained, the entire system operates in a
single forward pass, generalizing to previously unseen dynamic concepts without
any test-time optimization. Extensive experiments demonstrate high-quality and
consistent results across a wide range of subjects beyond trained concepts and
editing scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page and Video :
  https://snap-research.github.io/zero-shot-dynamic-concepts/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VIBE: Video-Input Brain Encoder for fMRI Response Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17958v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17958v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Carlstrom Schad, Shrey Dixit, Janis Keck, Viktor Studenyak, Aleksandr Shpilevoi, Andrej Bicanski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present VIBE, a two-stage Transformer that fuses multi-modal video, audio,
and text features to predict fMRI activity. Representations from open-source
models (Qwen2.5, BEATs, Whisper, SlowFast, V-JEPA) are merged by a
modality-fusion transformer and temporally decoded by a prediction transformer
with rotary embeddings. Trained on 65 hours of movie data from the CNeuroMod
dataset and ensembled across 20 seeds, VIBE attains mean parcel-wise Pearson
correlations of 32.25 on in-distribution Friends S07 and 21.25 on six
out-of-distribution films. An earlier iteration of the same architecture
obtained 0.3198 and 0.2096, respectively, winning Phase-1 and placing second
overall in the Algonauts 2025 Challenge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clo-HDnn: A 4.66 TFLOPS/W and 3.78 TOPS/W Continual On-Device Learning
  Accelerator with Energy-efficient Hyperdimensional Computing via Progressive
  Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Eun Song, Weihong Xu, Keming Fan, Soumil Jain, Gopabandhu Hota, Haichao Yang, Leo Liu, Kerem Akarvardar, Meng-Fan Chang, Carlos H. Diaz, Gert Cauwenberghs, Tajana Rosing, Mingu Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clo-HDnn is an on-device learning (ODL) accelerator designed for emerging
continual learning (CL) tasks. Clo-HDnn integrates hyperdimensional computing
(HDC) along with low-cost Kronecker HD Encoder and weight clustering feature
extraction (WCFE) to optimize accuracy and efficiency. Clo-HDnn adopts
gradient-free CL to efficiently update and store the learned knowledge in the
form of class hypervectors. Its dual-mode operation enables bypassing costly
feature extraction for simpler datasets, while progressive search reduces
complexity by up to 61% by encoding and comparing only partial query
hypervectors. Achieving 4.66 TFLOPS/W (FE) and 3.78 TOPS/W (classifier),
Clo-HDnn delivers 7.77x and 4.85x higher energy efficiency compared to SOTA ODL
accelerators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in 2025 Symposium on VLSI Technology and Circuits (VLSI
  Technology and Circuits), Kyoto, Japan, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum Machine Learning Playground 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Debus, Sebastian Issel, Kilian Tscharke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article introduces an innovative interactive visualization tool designed
to demystify quantum machine learning (QML) algorithms. Our work is inspired by
the success of classical machine learning visualization tools, such as
TensorFlow Playground, and aims to bridge the gap in visualization resources
specifically for the field of QML. The article includes a comprehensive
overview of relevant visualization metaphors from both quantum computing and
classical machine learning, the development of an algorithm visualization
concept, and the design of a concrete implementation as an interactive web
application. By combining common visualization metaphors for the so-called data
re-uploading universal quantum classifier as a representative QML model, this
article aims to lower the entry barrier to quantum computing and encourage
further innovation in the field. The accompanying interactive application is a
proposal for the first version of a quantum machine learning playground for
learning and exploring QML models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Computer Graphics and Applications. Final version:
  https://doi.org/10.1109/MCG.2024.3456288</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UrbanPulse: A Cross-City Deep Learning Framework for Ultra-Fine-Grained
  Population Transfer <span class="highlight-title">Prediction</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongrong Yang, Markus Schlaepfer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate population flow prediction is essential for urban planning,
transportation management, and public health. Yet existing methods face key
limitations: traditional models rely on static spatial assumptions, deep
learning models struggle with cross-city generalization, and Large Language
Models (LLMs) incur high computational costs while failing to capture spatial
structure. Moreover, many approaches sacrifice resolution by clustering Points
of Interest (POIs) or restricting coverage to subregions, limiting their
utility for city-wide analytics. We introduce UrbanPulse, a scalable deep
learning framework that delivers ultra-fine-grained, city-wide OD flow
predictions by treating each POI as an individual node. It combines a temporal
graph convolutional encoder with a transformer-based decoder to model
multi-scale spatiotemporal dependencies. To ensure robust generalization across
urban contexts, UrbanPulse employs a three-stage transfer learning strategy:
pretraining on large-scale urban graphs, cold-start adaptation, and
reinforcement learning fine-tuning.Evaluated on over 103 million cleaned GPS
records from three metropolitan areas in California, UrbanPulse achieves
state-of-the-art accuracy and scalability. Through efficient transfer learning,
UrbanPulse takes a key step toward making high-resolution, AI-powered urban
forecasting deployable in practice across diverse cities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Seed to Harvest: Augmenting Human Creativity with AI for
  Red-teaming Text-to-Image Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jessica Quaye, Charvi Rastogi, Alicia Parrish, Oana Inel, Minsuk Kahng, Lora Aroyo, Vijay Janapa Reddi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image (T2I) models have become prevalent across numerous
applications, making their robust evaluation against adversarial attacks a
critical priority. Continuous access to new and challenging adversarial prompts
across diverse domains is essential for stress-testing these models for
resilience against novel attacks from multiple vectors. Current techniques for
generating such prompts are either entirely authored by humans or synthetically
generated. On the one hand, datasets of human-crafted adversarial prompts are
often too small in size and imbalanced in their cultural and contextual
representation. On the other hand, datasets of synthetically-generated prompts
achieve scale, but typically lack the realistic nuances and creative
adversarial strategies found in human-crafted prompts. To combine the strengths
of both human and machine approaches, we propose Seed2Harvest, a hybrid
red-teaming method for guided expansion of culturally diverse, human-crafted
adversarial prompt seeds. The resulting prompts preserve the characteristics
and attack patterns of human prompts while maintaining comparable average
attack success rates (0.31 NudeNet, 0.36 SD NSFW, 0.12 Q16). Our expanded
dataset achieves substantially higher diversity with 535 unique geographic
locations and a Shannon entropy of 7.48, compared to 58 locations and 5.28
entropy in the original dataset. Our work demonstrates the importance of
human-machine collaboration in leveraging human creativity and machine
computational capacity to achieve comprehensive, scalable red-teaming for
continuous T2I model safety evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sliding Window Informative Canonical Correlation Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arvind Prasadan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Canonical correlation analysis (CCA) is a technique for finding correlated
sets of features between two datasets. In this paper, we propose a novel
extension of CCA to the online, streaming data setting: Sliding Window
Informative Canonical Correlation Analysis (SWICCA). Our method uses a
streaming principal component analysis (PCA) algorithm as a backend and uses
these outputs combined with a small sliding window of samples to estimate the
CCA components in real time. We motivate and describe our algorithm, provide
numerical simulations to characterize its performance, and provide a
theoretical performance guarantee. The SWICCA method is applicable and scalable
to extremely high dimensions, and we provide a real-data example that
demonstrates this capability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, submitted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SETOL: A Semi-Empirical Theory of (Deep) Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17912v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17912v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles H Martin, Christopher Hinrichs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a SemiEmpirical Theory of Learning (SETOL) that explains the
remarkable performance of State-Of-The-Art (SOTA) Neural Networks (NNs). We
provide a formal explanation of the origin of the fundamental quantities in the
phenomenological theory of Heavy-Tailed Self-Regularization (HTSR): the
heavy-tailed power-law layer quality metrics, alpha and alpha-hat. In prior
work, these metrics have been shown to predict trends in the test accuracies of
pretrained SOTA NN models, importantly, without needing access to either
testing or training data. Our SETOL uses techniques from statistical mechanics
as well as advanced methods from random matrix theory and quantum chemistry.
The derivation suggests new mathematical preconditions for ideal learning,
including a new metric, ERG, which is equivalent to applying a single step of
the Wilson Exact Renormalization Group. We test the assumptions and predictions
of SETOL on a simple 3-layer multilayer perceptron (MLP), demonstrating
excellent agreement with the key theoretical assumptions. For SOTA NN models,
we show how to estimate the individual layer qualities of a trained NN by
simply computing the empirical spectral density (ESD) of the layer weight
matrices and plugging this ESD into our SETOL formulas. Notably, we examine the
performance of the HTSR alpha and the SETOL ERG layer quality metrics, and find
that they align remarkably well, both on our MLP and on SOTA NNs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>139 pages, 28 figures. Code for experiments available at
  https://github.com/charlesmartin14/SETOL_experiments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep learning-aided inverse design of porous metamaterials 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phu Thien Nguyen, Yousef Heider, Dennis M. Kochmann, Fadi Aldakheel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ultimate aim of the study is to explore the inverse design of porous
metamaterials using a deep learning-based generative framework. Specifically,
we develop a property-variational autoencoder (pVAE), a variational autoencoder
(VAE) augmented with a regressor, to generate structured metamaterials with
tailored hydraulic properties, such as porosity and permeability. While this
work uses the lattice Boltzmann method (LBM) to generate intrinsic permeability
tensor data for limited porous microstructures, a convolutional neural network
(CNN) is trained using a bottom-up approach to predict effective hydraulic
properties. This significantly reduces the computational cost compared to
direct LBM simulations. The pVAE framework is trained on two datasets: a
synthetic dataset of artificial porous microstructures and CT-scan images of
volume elements from real open-cell foams. The encoder-decoder architecture of
the VAE captures key microstructural features, mapping them into a compact and
interpretable latent space for efficient structure-property exploration. The
study provides a detailed analysis and interpretation of the latent space,
demonstrating its role in structure-property mapping, interpolation, and
inverse design. This approach facilitates the generation of new metamaterials
with desired properties. The datasets and codes used in this study will be made
open-access to support further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 29 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Learning for Large-Scale Cloud <span class="highlight-title">Robot</span>ic Manipulation:
  Opportunities and Challenges <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Obaidullah Zaland, Chanh Nguyen, Florian T. Pokorny, Monowar Bhuyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is an emerging distributed machine learning paradigm,
where the collaborative training of a model involves dynamic participation of
devices to achieve broad objectives. In contrast, classical machine learning
(ML) typically requires data to be located on-premises for training, whereas FL
leverages numerous user devices to train a shared global model without the need
to share private data. Current robotic manipulation tasks are constrained by
the individual capabilities and speed of robots due to limited low-latency
computing resources. Consequently, the concept of cloud robotics has emerged,
allowing robotic applications to harness the flexibility and reliability of
computing resources, effectively alleviating their computational demands across
the cloud-edge continuum. Undoubtedly, within this distributed computing
context, as exemplified in cloud robotic manipulation scenarios, FL offers
manifold advantages while also presenting several challenges and opportunities.
In this paper, we present fundamental concepts of FL and their connection to
cloud robotic manipulation. Additionally, we envision the opportunities and
challenges associated with realizing efficient and reliable cloud robotic
manipulation at scale through FL, where researchers adopt to design and verify
FL models in either centralized or decentralized settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for Presentation at IEEE International Conference on Machine
  Learning and Cybernetics (ICMLC) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Recurrent Ensembles for Predicting Brain Responses to
  Naturalistic Movies (Algonauts 2025) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Semih Eren, Deniz Kucukahmetler, Nico Scherf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately predicting distributed cortical responses to naturalistic stimuli
requires models that integrate visual, auditory and semantic information over
time. We present a hierarchical multimodal recurrent ensemble that maps
pretrained video, audio, and language embeddings to fMRI time series recorded
while four subjects watched almost 80 hours of movies provided by the Algonauts
2025 challenge. Modality-specific bidirectional RNNs encode temporal dynamics;
their hidden states are fused and passed to a second recurrent layer, and
lightweight subject-specific heads output responses for 1000 cortical parcels.
Training relies on a composite MSE-correlation loss and a curriculum that
gradually shifts emphasis from early sensory to late association regions.
Averaging 100 model variants further boosts robustness. The resulting system
ranked third on the competition leaderboard, achieving an overall Pearson r =
0.2094 and the highest single-parcel peak score (mean r = 0.63) among all
participants, with particularly strong gains for the most challenging subject
(Subject 5). The approach establishes a simple, extensible baseline for future
multimodal brain-encoding benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures, 1 table. Invited report, CCN 2025 Algonauts
  Project session (3rd-place team). Code:
  https://github.com/erensemih/Algonauts2025_ModalityRNN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lower Bounds for Public-Private Learning under Distribution Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17895v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17895v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amrith Setlur, Pratiksha Thaker, Jonathan Ullman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The most effective differentially private machine learning algorithms in
practice rely on an additional source of purportedly public data. This paradigm
is most interesting when the two sources combine to be more than the sum of
their parts. However, there are settings such as mean estimation where we have
strong lower bounds, showing that when the two data sources have the same
distribution, there is no complementary value to combining the two data
sources. In this work we extend the known lower bounds for public-private
learning to setting where the two data sources exhibit significant distribution
shift. Our results apply to both Gaussian mean estimation where the two
distributions have different means, and to Gaussian linear regression where the
two distributions exhibit parameter shift. We find that when the shift is small
(relative to the desired accuracy), either public or private data must be
sufficiently abundant to estimate the private parameter. Conversely, when the
shift is large, public data provides no benefit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Action-List <span class="highlight-title">Reinforcement</span> Learning Syndrome Decoding for Binary Linear
  Block Codes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milad Taghipour, Bane Vasic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the application of reinforcement learning techniques to
enhance the performance of decoding of linear block codes based on flipping
bits and finding optimal decisions. We describe the methodology for mapping the
iterative decoding process into Markov Decision Processes (MDPs) and propose
different methods to reduce the number of states in the MDP. A truncated MDP is
proposed to reduce the number of states in the MDP by learning a Hamming ball
with a specified radius around codewords. We then propose a general scheme for
reinforcement learning based decoders applicable to any class of codes to
improve the performance of decoders. We call this scheme an action-list
decoding. We design an action-list decoder based on the Deep-Q network values
that substantially enhance performance. We also get benefit of automorphism
group of code to further improve the code performance. Additionally, we propose
a feedback-based method to exploit and enhance the performance of existing
high-performing decoders by applying reinforcement learning algorithms after
the existing decoders. These approaches effectively reduces the complexity of
the reinforcement learning block. Finally, we present experimental results for
the Low-Density Parity Check (LDPC) codes over the Binary Symmetric Channel
(BSC) to demonstrate the efficiency of the proposed methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fourier Neural Operators for Non-Markovian Processes:Approximation
  Theorems and Experiments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonjae Lee, Taeyoung Kim, Hyungbin Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an operator-based neural network, the mirror-padded
Fourier neural operator (MFNO), designed to learn the dynamics of stochastic
systems. MFNO extends the standard Fourier neural operator (FNO) by
incorporating mirror padding, enabling it to handle non-periodic inputs. We
rigorously prove that MFNOs can approximate solutions of path-dependent
stochastic differential equations and Lipschitz transformations of fractional
Brownian motions to an arbitrary degree of accuracy. Our theoretical analysis
builds on Wong--Zakai type theorems and various approximation techniques.
Empirically, the MFNO exhibits strong resolution generalization--a property
rarely seen in standard architectures such as LSTMs, TCNs, and DeepONet.
Furthermore, our model achieves performance that is comparable or superior to
these baselines while offering significantly faster sample path generation than
classical numerical schemes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Supervised Machine Learning Framework for Multipactor Breakdown
  <span class="highlight-title">Prediction</span> in High-Power Radio Frequency Devices and Accelerator Components:
  A Case Study in Planar Geometry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asif Iqbal, John Verboncoeur, Peng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multipactor is a nonlinear electron avalanche phenomenon that can severely
impair the performance of high-power radio frequency (RF) devices and
accelerator systems. Accurate prediction of multipactor susceptibility across
different materials and operational regimes remains a critical yet
computationally intensive challenge in accelerator component design and RF
engineering. This study presents the first application of supervised machine
learning (ML) for predicting multipactor susceptibility in two-surface planar
geometries. A simulation-derived dataset spanning six distinct secondary
electron yield (SEY) material profiles is used to train regression models -
including Random Forest (RF), Extra Trees (ET), Extreme Gradient Boosting
(XGBoost), and funnel-structured Multilayer Perceptrons (MLPs) - to predict the
time-averaged electron growth rate, ${\delta}_{avg}$. Performance is evaluated
using Intersection over Union (IoU), Structural Similarity Index (SSIM), and
Pearson correlation coefficient. Tree-based models consistently outperform MLPs
in generalizing across disjoint material domains. MLPs trained using a
scalarized objective function that combines IoU and SSIM during Bayesian
hyperparameter optimization with 5-fold cross-validation outperform those
trained with single-objective loss functions. Principal Component Analysis
reveals that performance degradation for certain materials stems from disjoint
feature-space distributions, underscoring the need for broader dataset
coverage. This study demonstrates both the promise and limitations of ML-based
multipactor prediction and lays the groundwork for accelerated, data-driven
modeling in advanced RF and accelerator system design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Look the Other Way: Designing 'Positive' Molecules with Negative Data
  via Task Arithmetic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17876v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17876v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rıza Özçelik, Sarah de Ruiter, Francesca Grisoni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scarcity of molecules with desirable properties (i.e., 'positive'
molecules) is an inherent bottleneck for generative molecule design. To
sidestep such obstacle, here we propose molecular task arithmetic: training a
model on diverse and abundant negative examples to learn 'property directions'
$--$ without accessing any positively labeled data $--$ and moving models in
the opposite property directions to generate positive molecules. When analyzed
on 20 zero-shot design experiments, molecular task arithmetic generated more
diverse and successful designs than models trained on positive molecules.
Moreover, we employed molecular task arithmetic in dual-objective and few-shot
design tasks. We find that molecular task arithmetic can consistently increase
the diversity of designs while maintaining desirable design properties. With
its simplicity, data efficiency, and performance, molecular task arithmetic
bears the potential to become the $\textit{de-facto}$ transfer learning
strategy for de novo molecule design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Feature Selection and Machine Learning for Nitrogen
  Assessment in Grapevine Leaves using In-<span class="highlight-title">Field</span> Hyperspectral Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atif Bilal Asad, Achyut Paudel, Safal Kshetri, Chenchen Kang, Salik Ram Khanal, Nataliya Shcherbatyuk, Pierre Davadant, R. Paul Schreiner, Santosh Kalauni, Manoj Karkee, Markus Keller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nitrogen (N) is one of the most crucial nutrients in vineyards, affecting
plant growth and subsequent products such as wine and juice. Because soil N has
high spatial and temporal variability, it is desirable to accurately estimate
the N concentration of grapevine leaves and manage fertilization at the
individual plant level to optimally meet plant needs. In this study, we used
in-field hyperspectral images with wavelengths ranging from $400 to 1000nm of
four different grapevine cultivars collected from distinct vineyards and over
two growth stages during two growing seasons to develop models for predicting N
concentration at the leaf-level and canopy-level. After image processing, two
feature selection methods were employed to identify the optimal set of spectral
bands that were responsive to leaf N concentrations. The selected spectral
bands were used to train and test two different Machine Learning (ML) models,
Gradient Boosting and XGBoost, for predicting nitrogen concentrations. The
comparison of selected bands for both leaf-level and canopy-level datasets
showed that most of the spectral regions identified by the feature selection
methods were across both methods and the dataset types (leaf- and canopy-level
datasets), particularly in the key regions, 500-525nm, 650-690nm, 750-800nm,
and 900-950nm. These findings indicated the robustness of these spectral
regions for predicting nitrogen content. The results for N prediction
demonstrated that the ML model achieved an R square of 0.49 for canopy-level
data and an R square of 0.57 for leaf-level data, despite using different sets
of selected spectral bands for each analysis level. The study demonstrated the
potential of using in-field hyperspectral imaging and the use of spectral data
in integrated feature selection and ML techniques to monitor N status in
vineyards.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Facilitated Fairness Assessment of AI-based Skin Lesion
  Classifiers Through GenAI-based Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ko Watanabe. Stanislav Frolov. Adriano Lucieri. Andreas Dengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Deep Learning and its application on the edge hold
great potential for the revolution of routine screenings for skin cancers like
Melanoma. Along with the anticipated benefits of this technology, potential
dangers arise from unforseen and inherent biases. Thus, assessing and improving
the fairness of such systems is of utmost importance. A key challenge in
fairness assessment is to ensure that the evaluation dataset is sufficiently
representative of different Personal Identifiable Information (PII) (sex, age,
and race) and other minority groups. Against the backdrop of this challenge,
this study leverages the state-of-the-art Generative AI (GenAI) LightningDiT
model to assess the fairness of publicly available melanoma classifiers. The
results suggest that fairness assessment using highly realistic synthetic data
is a promising direction. Yet, our findings indicate that verifying fairness
becomes difficult when the melanoma-detection model used for evaluation is
trained on data that differ from the dataset underpinning the synthetic images.
Nonetheless, we propose that our approach offers a valuable new avenue for
employing synthetic data to gauge and enhance fairness in medical-imaging GenAI
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Energy Distribution of the Galactic Center Excess' Sources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian List, Yujin Park, Nicholas L. Rodd, Eve Schoen, Florian Wolf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Galactic Center Excess (GCE) remains one of the defining mysteries
uncovered by the Fermi $\gamma$-ray Space Telescope. Although it may yet herald
the discovery of annihilating dark matter, weighing against that conclusion are
analyses showing the spatial structure of the emission appears more consistent
with a population of dim point sources. Technical limitations have restricted
prior analyses to studying the point-source hypothesis purely spatially. All
spectral information that could help disentangle the GCE from the complex and
uncertain astrophysical emission was discarded. We demonstrate that a neural
network-aided simulation-based inference approach can overcome such limitations
and thereby confront the point source explanation of the GCE with spatial and
spectral data. The addition is profound: energy information drives the putative
point sources to be significantly dimmer, indicating either the GCE is truly
diffuse in nature or made of an exceptionally large number of sources.
Quantitatively, for our best fit background model, the excess is essentially
consistent with Poisson emission as predicted by dark matter. If the excess is
instead due to point sources, our median prediction is ${\cal O}(10^5)$ sources
in the Galactic Center, or more than 35,000 sources at 90% confidence, both
significantly larger than the hundreds of sources preferred by earlier
point-source analyses of the GCE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7+20 pages, 2+20 figures, comments welcome</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Learning Rates Simultaneously Achieve <span class="highlight-title">Robust</span>ness to Spurious
  Correlations and Compressibility <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Melih Barsbey, Lucas Prieto, Stefanos Zafeiriou, Tolga Birdal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robustness and resource-efficiency are two highly desirable properties for
modern machine learning models. However, achieving them jointly remains a
challenge. In this paper, we position high learning rates as a facilitator for
simultaneously achieving robustness to spurious correlations and network
compressibility. We demonstrate that large learning rates also produce
desirable representation properties such as invariant feature utilization,
class separation, and activation sparsity. Importantly, our findings indicate
that large learning rates compare favorably to other hyperparameters and
regularization methods, in consistently satisfying these properties in tandem.
In addition to demonstrating the positive effect of large learning rates across
diverse spurious correlation datasets, models, and optimizers, we also present
strong evidence that the previously documented success of large learning rates
in standard classification tasks is likely due to its effect on addressing
hidden/rare spurious correlations in the training dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCV 2025, 23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rubrics as Rewards: <span class="highlight-title">Reinforcement</span> Learning Beyond Verifiable Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anisha Gunjal, Anthony Wang, Elaine Lau, Vaskar Nath, Bing Liu, Sean Hendryx
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extending Reinforcement Learning with Verifiable Rewards (RLVR) to real-world
tasks often requires balancing objective and subjective evaluation criteria.
However, many such tasks lack a single, unambiguous ground truth-making it
difficult to define reliable reward signals for post-training language models.
While traditional preference-based methods offer a workaround, they rely on
opaque reward functions that are difficult to interpret and prone to spurious
correlations. We introduce $\textbf{Rubrics as Rewards}$ (RaR), a framework
that uses structured, checklist-style rubrics as interpretable reward signals
for on-policy training with GRPO. Our best RaR method yields up to a $28\%$
relative improvement on HealthBench-1k compared to simple Likert-based
approaches, while matching or surpassing the performance of reward signals
derived from expert-written references. By treating rubrics as structured
reward signals, we show that RaR enables smaller-scale judge models to better
align with human preferences and sustain robust performance across model
scales.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flow Matching Meets Biology and <span class="highlight-title">Life</span> Science: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Li, Zhichen Zeng, Xiao Lin, Feihao Fang, Yanru Qu, Zhe Xu, Zhining Liu, Xuying Ning, Tianxin Wei, Ge Liu, Hanghang Tong, Jingrui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past decade, advances in generative modeling, such as generative
adversarial networks, masked autoencoders, and diffusion models, have
significantly transformed biological research and discovery, enabling
breakthroughs in molecule design, protein generation, drug discovery, and
beyond. At the same time, biological applications have served as valuable
testbeds for evaluating the capabilities of generative models. Recently, flow
matching has emerged as a powerful and efficient alternative to diffusion-based
generative modeling, with growing interest in its application to problems in
biology and life sciences. This paper presents the first comprehensive survey
of recent developments in flow matching and its applications in biological
domains. We begin by systematically reviewing the foundations and variants of
flow matching, and then categorize its applications into three major areas:
biological sequence modeling, molecule generation and design, and peptide and
protein generation. For each, we provide an in-depth review of recent progress.
We also summarize commonly used datasets and software tools, and conclude with
a discussion of potential future directions. The corresponding curated
resources are available at
https://github.com/Violet24K/Awesome-Flow-Matching-Meets-Biology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, 27 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Generative Learning of Magnetic Frustration in Artificial Spin Ice
  from Magnetic Force Microscopy Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnab Neogi, Suryakant Mishra, Prasad P Iyer, Tzu-Ming Lu, Ezra Bussmann, Sergei Tretiak, Andrew Crandall Jones, Jian-Xin Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Increasingly large datasets of microscopic images with atomic resolution
facilitate the development of machine learning methods to identify and analyze
subtle physical phenomena embedded within the images. In this work, microscopic
images of honeycomb lattice spin-ice samples serve as datasets from which we
automate the calculation of net magnetic moments and directional orientations
of spin-ice configurations. In the first stage of our workflow, machine
learning models are trained to accurately predict magnetic moments and
directions within spin-ice structures. Variational Autoencoders (VAEs), an
emergent unsupervised deep learning technique, are employed to generate
high-quality synthetic magnetic force microscopy (MFM) images and extract
latent feature representations, thereby reducing experimental and segmentation
errors. The second stage of proposed methodology enables precise identification
and prediction of frustrated vertices and nanomagnetic segments, effectively
correlating structural and functional aspects of microscopic images. This
facilitates the design of optimized spin-ice configurations with controlled
frustration patterns, enabling potential on-demand synthesis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Interaction of Compressibility and Adversarial <span class="highlight-title">Robust</span>ness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Melih Barsbey, Antônio H. Ribeiro, Umut Şimşekli, Tolga Birdal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern neural networks are expected to simultaneously satisfy a host of
desirable properties: accurate fitting to training data, generalization to
unseen inputs, parameter and computational efficiency, and robustness to
adversarial perturbations. While compressibility and robustness have each been
studied extensively, a unified understanding of their interaction still remains
elusive. In this work, we develop a principled framework to analyze how
different forms of compressibility - such as neuron-level sparsity and spectral
compressibility - affect adversarial robustness. We show that these forms of
compression can induce a small number of highly sensitive directions in the
representation space, which adversaries can exploit to construct effective
perturbations. Our analysis yields a simple yet instructive robustness bound,
revealing how neuron and spectral compressibility impact $L_\infty$ and $L_2$
robustness via their effects on the learned representations. Crucially, the
vulnerabilities we identify arise irrespective of how compression is achieved -
whether via regularization, architectural bias, or implicit learning dynamics.
Through empirical evaluations across synthetic and realistic tasks, we confirm
our theoretical predictions, and further demonstrate that these vulnerabilities
persist under adversarial training and transfer learning, and contribute to the
emergence of universal adversarial perturbations. Our findings show a
fundamental tension between structured compressibility and robustness, and
suggest new pathways for designing models that are both efficient and secure.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sequential Bayesian Design for Efficient Surrogate Construction in the
  Inversion of Darcy Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17713v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17713v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongji Wang, Hongqiao Wang, Jinyong Ying, Qingping Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse problems governed by partial differential equations (PDEs) play a
crucial role in various fields, including computational science, image
processing, and engineering. Particularly, Darcy flow equation is a fundamental
equation in fluid mechanics, which plays a crucial role in understanding fluid
flow through porous media. Bayesian methods provide an effective approach for
solving PDEs inverse problems, while their numerical implementation requires
numerous evaluations of computationally expensive forward solvers. Therefore,
the adoption of surrogate models with lower computational costs is essential.
However, constructing a globally accurate surrogate model for high-dimensional
complex problems demands high model capacity and large amounts of data. To
address this challenge, this study proposes an efficient locally accurate
surrogate that focuses on the high-probability regions of the true likelihood
in inverse problems, with relatively low model complexity and few training data
requirements. Additionally, we introduce a sequential Bayesian design strategy
to acquire the proposed surrogate since the high-probability region of the
likelihood is unknown. The strategy treats the posterior evolution process of
sequential Bayesian design as a Gaussian process, enabling algorithmic
acceleration through one-step ahead prior. The complete algorithmic framework
is referred to as Sequential Bayesian design for locally accurate surrogate
(SBD-LAS). Finally, three experiments based the Darcy flow equation demonstrate
the advantages of the proposed method in terms of both inversion accuracy and
computational speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HydraOpt: Navigating the Efficiency-Performance Trade-off of Adapter
  Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taha Ceritli, Ondrej Bohdal, Mete Ozay, Jijoong Moon, Kyeng-Hun Lee, Hyeonmok Ko, Umberto Michieli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often leverage adapters, such as low-rank-based
adapters, to achieve strong performance on downstream tasks. However, storing a
separate adapter for each task significantly increases memory requirements,
posing a challenge for resource-constrained environments such as mobile
devices. Although model merging techniques can reduce storage costs, they
typically result in substantial performance degradation. In this work, we
introduce HydraOpt, a new model merging technique that capitalizes on the
inherent similarities between the matrices of low-rank adapters. Unlike
existing methods that produce a fixed trade-off between storage size and
performance, HydraOpt allows us to navigate this spectrum of efficiency and
performance. Our experiments show that HydraOpt significantly reduces storage
size (48% reduction) compared to storing all adapters, while achieving
competitive performance (0.2-1.8% drop). Furthermore, it outperforms existing
merging techniques in terms of performance at the same or slightly worse
storage efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Joint Asymmetric Loss for Learning with Noisy Labels <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialiang Wang, Xian<span class="highlight-author">ming Liu</span>, Xiong Zhou, Gangfeng Hu, Deming Zhai, Junjun Jiang, Xiangyang Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning with noisy labels is a crucial task for training accurate deep
neural networks. To mitigate label noise, prior studies have proposed various
robust loss functions, particularly symmetric losses. Nevertheless, symmetric
losses usually suffer from the underfitting issue due to the overly strict
constraint. To address this problem, the Active Passive Loss (APL) jointly
optimizes an active and a passive loss to mutually enhance the overall fitting
ability. Within APL, symmetric losses have been successfully extended, yielding
advanced robust loss functions. Despite these advancements, emerging
theoretical analyses indicate that asymmetric losses, a new class of robust
loss functions, possess superior properties compared to symmetric losses.
However, existing asymmetric losses are not compatible with advanced
optimization frameworks such as APL, limiting their potential and
applicability. Motivated by this theoretical gap and the prospect of asymmetric
losses, we extend the asymmetric loss to the more complex passive loss scenario
and propose the Asymetric Mean Square Error (AMSE), a novel asymmetric loss. We
rigorously establish the necessary and sufficient condition under which AMSE
satisfies the asymmetric condition. By substituting the traditional symmetric
passive loss in APL with our proposed AMSE, we introduce a novel robust loss
framework termed Joint Asymmetric Loss (JAL). Extensive experiments demonstrate
the effectiveness of our method in mitigating label noise. Code available at:
https://github.com/cswjl/joint-asymmetric-loss
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CASCADE: LLM-Powered JavaScript Deobfuscator at Google 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shan Jiang, Pranoy Kovuri, David Tao, Zhixun Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software obfuscation, particularly prevalent in JavaScript, hinders code
comprehension and analysis, posing significant challenges to software testing,
static analysis, and malware detection. This paper introduces CASCADE, a novel
hybrid approach that integrates the advanced coding capabilities of Gemini with
the deterministic transformation capabilities of a compiler Intermediate
Representation (IR), specifically JavaScript IR (JSIR). By employing Gemini to
identify critical prelude functions, the foundational components underlying the
most prevalent obfuscation techniques, and leveraging JSIR for subsequent code
transformations, CASCADE effectively recovers semantic elements like original
strings and API names, and reveals original program behaviors. This method
overcomes limitations of existing static and dynamic deobfuscation techniques,
eliminating hundreds to thousands of hardcoded rules while achieving
reliability and flexibility. CASCADE is already deployed in Google's production
environment, demonstrating substantial improvements in JavaScript deobfuscation
efficiency and reducing reverse engineering efforts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mindfulness Meditation and Respiration: Accelerometer-Based Respiration
  Rate and Mindfulness Progress <span class="highlight-title">Estimation</span> to Enhance App Engagement and
  Mindfulness Skills 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Nur Hossain Khan, David creswell, Jordan Albert, Patrick O'Connell, Shawn Fallon, Mathew Polowitz, Xuhai "orson" Xu, Bashima islam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mindfulness training is widely recognized for its benefits in reducing
depression, anxiety, and loneliness. With the rise of smartphone-based
mindfulness apps, digital meditation has become more accessible, but sustaining
long-term user engagement remains a challenge. This paper explores whether
respiration biosignal feedback and mindfulness skill estimation enhance system
usability and skill development. We develop a smartphone's accelerometer-based
respiration tracking algorithm, eliminating the need for additional wearables.
Unlike existing methods, our approach accurately captures slow breathing
patterns typical of mindfulness meditation. Additionally, we introduce the
first quantitative framework to estimate mindfulness skills-concentration,
sensory clarity, and equanimity-based on accelerometer-derived respiration
data. We develop and test our algorithms on 261 mindfulness sessions in both
controlled and real-world settings. A user study comparing an experimental
group receiving biosignal feedback with a control group using a standard app
shows that respiration feedback enhances system usability. Our respiration
tracking model achieves a mean absolute error (MAE) of 1.6 breaths per minute,
closely aligning with ground truth data, while our mindfulness skill estimation
attains F1 scores of 80-84% in tracking skill progression. By integrating
respiration tracking and mindfulness estimation into a commercial app, we
demonstrate the potential of smartphone sensors to enhance digital mindfulness
training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Proc. ACM Interact. Mob. Wearable Ubiquitous Technology
  (IMWUT)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Effective Open-set <span class="highlight-title">Graph</span> Class-incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiazhen Chen, Zheng Ma, Sichao Fu, Mingbin Feng, Tony S. Wirjanto, Weihua Ou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph class-incremental learning (GCIL) allows graph neural networks (GNNs)
to adapt to evolving graph analytical tasks by incrementally learning new class
knowledge while retaining knowledge of old classes. Existing GCIL methods
primarily focus on a closed-set assumption, where all test samples are presumed
to belong to previously known classes. Such an assumption restricts their
applicability in real-world scenarios, where unknown classes naturally emerge
during inference, and are absent during training. In this paper, we explore a
more challenging open-set graph class-incremental learning scenario with two
intertwined challenges: catastrophic forgetting of old classes, which impairs
the detection of unknown classes, and inadequate open-set recognition, which
destabilizes the retention of learned knowledge. To address the above problems,
a novel OGCIL framework is proposed, which utilizes pseudo-sample embedding
generation to effectively mitigate catastrophic forgetting and enable robust
detection of unknown classes. To be specific, a prototypical conditional
variational autoencoder is designed to synthesize node embeddings for old
classes, enabling knowledge replay without storing raw graph data. To handle
unknown classes, we employ a mixing-based strategy to generate
out-of-distribution (OOD) samples from pseudo in-distribution and current node
embeddings. A novel prototypical hypersphere classification loss is further
proposed, which anchors in-distribution embeddings to their respective class
prototypes, while repelling OOD embeddings away. Instead of assigning all
unknown samples into one cluster, our proposed objective function explicitly
models them as outliers through prototype-aware rejection regions, ensuring a
robust open-set recognition. Extensive experiments on five benchmarks
demonstrate the effectiveness of OGCIL over existing GCIL and open-set GNN
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 33rd ACM International Conference on Multimedia (MM 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Debiased maximum-likelihood <span class="highlight-title">estimator</span>s for hazard ratios under
  machine-learning adjustment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takashi Hayakawa, Satoshi Asai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous studies have shown that hazard ratios between treatment groups
estimated with the Cox model are uninterpretable because the indefinite
baseline hazard of the model fails to identify temporal change in the risk set
composition due to treatment assignment and unobserved factors among multiple,
contradictory scenarios. To alleviate this problem, especially in studies based
on observational data with uncontrolled dynamic treatment and real-time
measurement of many covariates, we propose abandoning the baseline hazard and
using machine learning to explicitly model the change in the risk set with or
without latent variables. For this framework, we clarify the context in which
hazard ratios can be causally interpreted, and then develop a method based on
Neyman orthogonality to compute debiased maximum-likelihood estimators of
hazard ratios. Computing the constructed estimators is more efficient than
computing those based on weighted regression with marginal structural Cox
models. Numerical simulations confirm that the proposed method identifies the
ground truth with minimal bias. These results lay the foundation for developing
a useful, alternative method for causal inference with uncontrolled,
observational data in modern epidemiology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized Dual Discriminator GANs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Penukonda Naga Chandana, Tejas Srivastava, Gowtham R. Kurri, V. Lalitha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dual discriminator generative adversarial networks (D2 GANs) were introduced
to mitigate the problem of mode collapse in generative adversarial networks. In
D2 GANs, two discriminators are employed alongside a generator: one
discriminator rewards high scores for samples from the true data distribution,
while the other favors samples from the generator. In this work, we first
introduce dual discriminator $\alpha$-GANs (D2 $\alpha$-GANs), which combines
the strengths of dual discriminators with the flexibility of a tunable loss
function, $\alpha$-loss. We further generalize this approach to arbitrary
functions defined on positive reals, leading to a broader class of models we
refer to as generalized dual discriminator generative adversarial networks. For
each of these proposed models, we provide theoretical analysis and show that
the associated min-max optimization reduces to the minimization of a linear
combination of an $f$-divergence and a reverse $f$-divergence. This generalizes
the known simplification for D2-GANs, where the objective reduces to a linear
combination of the KL-divergence and the reverse KL-divergence. Finally, we
perform experiments on 2D synthetic data and use multiple performance metrics
to capture various advantages of our GANs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures, extended version of a paper accepted for
  presentation at ITW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Should We Meta-Learn <span class="highlight-title">Reinforcement</span> Learning Algorithms? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander David Goldie, Zilin Wang, Jakob Nicolaus Foerster, Shimon Whiteson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The process of meta-learning algorithms from data, instead of relying on
manual design, is growing in popularity as a paradigm for improving the
performance of machine learning systems. Meta-learning shows particular promise
for reinforcement learning (RL), where algorithms are often adapted from
supervised or unsupervised learning despite their suboptimality for RL.
However, until now there has been a severe lack of comparison between different
meta-learning algorithms, such as using evolution to optimise over black-box
functions or LLMs to propose code. In this paper, we carry out this empirical
comparison of the different approaches when applied to a range of meta-learned
algorithms which target different parts of the RL pipeline. In addition to
meta-train and meta-test performance, we also investigate factors including the
interpretability, sample cost and train time for each meta-learning algorithm.
Based on these findings, we propose several guidelines for meta-learning new RL
algorithms which will help ensure that future learned algorithms are as
performant as possible.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted paper at Reinforcement Learning Conference (RLC) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mammo-Mamba: A Hybrid State-Space and Transformer Architecture with
  Sequential Mixture of Experts for <span class="highlight-title">Multi-View</span> Mammo<span class="highlight-title">graph</span>y 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farnoush Bayatmakou, Reza Taleei, Nicole Simone, Arash Mohammadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Breast cancer (BC) remains one of the leading causes of cancer-related
mortality among women, despite recent advances in Computer-Aided Diagnosis
(CAD) systems. Accurate and efficient interpretation of multi-view mammograms
is essential for early detection, driving a surge of interest in Artificial
Intelligence (AI)-powered CAD models. While state-of-the-art multi-view
mammogram classification models are largely based on Transformer architectures,
their computational complexity scales quadratically with the number of image
patches, highlighting the need for more efficient alternatives. To address this
challenge, we propose Mammo-Mamba, a novel framework that integrates Selective
State-Space Models (SSMs), transformer-based attention, and expert-driven
feature refinement into a unified architecture. Mammo-Mamba extends the
MambaVision backbone by introducing the Sequential Mixture of Experts (SeqMoE)
mechanism through its customized SecMamba block. The SecMamba is a modified
MambaVision block that enhances representation learning in high-resolution
mammographic images by enabling content-adaptive feature refinement. These
blocks are integrated into the deeper stages of MambaVision, allowing the model
to progressively adjust feature emphasis through dynamic expert gating,
effectively mitigating the limitations of traditional Transformer models.
Evaluated on the CBIS-DDSM benchmark dataset, Mammo-Mamba achieves superior
classification performance across all key metrics while maintaining
computational efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ XStacking: Explanation-Guided Stacked Ensemble Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moncef Garouani, Ayah Barhrhouj, Olivier Teste
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensemble Machine Learning (EML) techniques, especially stacking, have been
shown to improve predictive performance by combining multiple base models.
However, they are often criticized for their lack of interpretability. In this
paper, we introduce XStacking, an effective and inherently explainable
framework that addresses this limitation by integrating dynamic feature
transformation with model-agnostic Shapley additive explanations. This enables
stacked models to retain their predictive accuracy while becoming inherently
explainable. We demonstrate the effectiveness of the framework on 29 datasets,
achieving improvements in both the predictive effectiveness of the learning
space and the interpretability of the resulting models. XStacking offers a
practical and scalable solution for responsible ML.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Concept-based approach to Voice Disorder <span class="highlight-title">Detection</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Ghia, Gabriele Ciravegna, Alkis Koudounas, Marco Fantini, Erika Crosetti, Giovanni Succo, Tania Cerquitelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Voice disorders affect a significant portion of the population, and the
ability to diagnose them using automated, non-invasive techniques would
represent a substantial advancement in healthcare, improving the quality of
life of patients. Recent studies have demonstrated that artificial intelligence
models, particularly Deep Neural Networks (DNNs), can effectively address this
task. However, due to their complexity, the decision-making process of such
models often remain opaque, limiting their trustworthiness in clinical
contexts. This paper investigates an alternative approach based on Explainable
AI (XAI), a field that aims to improve the interpretability of DNNs by
providing different forms of explanations. Specifically, this works focuses on
concept-based models such as Concept Bottleneck Model (CBM) and Concept
Embedding Model (CEM) and how they can achieve performance comparable to
traditional deep learning methods, while offering a more transparent and
interpretable decision framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM
  Pre-training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changxin Tian, Jiapeng Wang, Qian Zhao, Kunlong Chen, Jia Liu, Ziqi Liu, Jiaxin Mao, Wayne Xin Zhao, Zhiqiang Zhang, Jun Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in learning rate (LR) scheduling have demonstrated the
effectiveness of decay-free approaches that eliminate the traditional decay
phase while maintaining competitive performance. Model merging techniques have
emerged as particularly promising solutions in this domain. We present
Warmup-Stable and Merge (WSM), a general framework that establishes a formal
connection between learning rate decay and model merging. WSM provides a
unified theoretical foundation for emulating various decay strategies-including
cosine decay, linear decay and inverse square root decay-as principled model
averaging schemes, while remaining fully compatible with diverse optimization
methods. Through extensive experiments, we identify merge duration-the training
window for checkpoint aggregation-as the most critical factor influencing model
performance, surpassing the importance of both checkpoint interval and merge
quantity. Our framework consistently outperforms the widely-adopted
Warmup-Stable-Decay (WSD) approach across multiple benchmarks, achieving
significant improvements of +3.5% on MATH, +2.9% on HumanEval, and +5.5% on
MMLU-Pro. The performance advantages extend to supervised fine-tuning
scenarios, highlighting WSM's potential for long-term model refinement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision Transformer attention alignment with human visual perception in
  aesthetic object <span class="highlight-title">evaluation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miguel Carrasco, César González-Martín, José Aranda, Luis Oliveros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual attention mechanisms play a crucial role in human perception and
aesthetic evaluation. Recent advances in Vision Transformers (ViTs) have
demonstrated remarkable capabilities in computer vision tasks, yet their
alignment with human visual attention patterns remains underexplored,
particularly in aesthetic contexts. This study investigates the correlation
between human visual attention and ViT attention mechanisms when evaluating
handcrafted objects. We conducted an eye-tracking experiment with 30
participants (9 female, 21 male, mean age 24.6 years) who viewed 20 artisanal
objects comprising basketry bags and ginger jars. Using a Pupil Labs
eye-tracker, we recorded gaze patterns and generated heat maps representing
human visual attention. Simultaneously, we analyzed the same objects using a
pre-trained ViT model with DINO (Self-DIstillation with NO Labels), extracting
attention maps from each of the 12 attention heads. We compared human and ViT
attention distributions using Kullback-Leibler divergence across varying
Gaussian parameters (sigma=0.1 to 3.0). Statistical analysis revealed optimal
correlation at sigma=2.4 +-0.03, with attention head #12 showing the strongest
alignment with human visual patterns. Significant differences were found
between attention heads, with heads #7 and #9 demonstrating the greatest
divergence from human attention (p< 0.05, Tukey HSD test). Results indicate
that while ViTs exhibit more global attention patterns compared to human focal
attention, certain attention heads can approximate human visual behavior,
particularly for specific object features like buckles in basketry items. These
findings suggest potential applications of ViT attention mechanisms in product
design and aesthetic evaluation, while highlighting fundamental differences in
attention strategies between human perception and current AI models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Time Deep Gradient Flow Method for pricing American options 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jasper Rou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this research, we explore neural network-based methods for pricing
multidimensional American put options under the BlackScholes and Heston model,
extending up to five dimensions. We focus on two approaches: the Time Deep
Gradient Flow (TDGF) method and the Deep Galerkin Method (DGM). We extend the
TDGF method to handle the free-boundary partial differential equation inherent
in American options. We carefully design the sampling strategy during training
to enhance performance. Both TDGF and DGM achieve high accuracy while
outperforming conventional Monte Carlo methods in terms of computational speed.
In particular, TDGF tends to be faster during training than DGM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Citation Recommendation using Deep Canonical Correlation Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Conor McNamara, Effirul Ramlan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in citation recommendation have improved accuracy by
leveraging multi-view representation learning to integrate the various
modalities present in scholarly documents. However, effectively combining
multiple data views requires fusion techniques that can capture complementary
information while preserving the unique characteristics of each modality. We
propose a novel citation recommendation algorithm that improves upon linear
Canonical Correlation Analysis (CCA) methods by applying Deep CCA (DCCA), a
neural network extension capable of capturing complex, non-linear relationships
between distributed textual and graph-based representations of scientific
articles. Experiments on the large-scale DBLP (Digital Bibliography & Library
Project) citation network dataset demonstrate that our approach outperforms
state-of-the-art CCA-based methods, achieving relative improvements of over 11%
in Mean Average Precision@10, 5% in Precision@10, and 7% in Recall@10. These
gains reflect more relevant citation recommendations and enhanced ranking
quality, suggesting that DCCA's non-linear transformations yield more
expressive latent representations than CCA's linear projections.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 6 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wasserstein GAN-Based Precipitation Downscaling with Optimal Transport
  for Enhancing Perceptual Realism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenta Shiraishi, Yuka Muto, Atsushi Okazaki, Shunji Kotsuki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-resolution (HR) precipitation prediction is essential for reducing
damage from stationary and localized heavy rainfall; however, HR precipitation
forecasts using process-driven numerical weather prediction models remains
challenging. This study proposes using Wasserstein Generative Adversarial
Network (WGAN) to perform precipitation downscaling with an optimal transport
cost. In contrast to a conventional neural network trained with mean squared
error, the WGAN generated visually realistic precipitation fields with
fine-scale structures even though the WGAN exhibited slightly lower performance
on conventional evaluation metrics. The learned critic of WGAN correlated well
with human perceptual realism. Case-based analysis revealed that large
discrepancies in critic scores can help identify both unrealistic WGAN outputs
and potential artifacts in the reference data. These findings suggest that the
WGAN framework not only improves perceptual realism in precipitation
downscaling but also offers a new perspective for evaluating and
quality-controlling precipitation datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GenSelect: A Generative Approach to Best-of-N <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubham Toshniwal, Ivan Sorokin, Aleksander Ficek, Ivan Moshkov, Igor Gitman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative reward models with parallel sampling have enabled effective
test-time scaling for reasoning tasks. Current approaches employ pointwise
scoring of individual solutions or pairwise comparisons. However, pointwise
methods underutilize LLMs' comparative abilities, while pairwise methods scale
inefficiently with larger sampling budgets. We introduce GenSelect, where the
LLM uses long reasoning to select the best solution among N candidates. This
leverages LLMs' comparative strengths while scaling efficiently across parallel
sampling budgets. For math reasoning, we demonstrate that reasoning models,
such as QwQ and DeepSeek-R1-0528, excel at GenSelect, outperforming existing
scoring approaches with simple prompting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the 2nd AI for MATH Workshop @ ICML</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Quantum Federated Learning with Fisher Information-Based
  <span class="highlight-title">Optimization</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amandeep Singh Bhatia, Sabre Kais
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) has become increasingly popular across different
sectors, offering a way for clients to work together to train a global model
without sharing sensitive data. It involves multiple rounds of communication
between the global model and participating clients, which introduces several
challenges like high communication costs, heterogeneous client data, prolonged
processing times, and increased vulnerability to privacy threats. In recent
years, the convergence of federated learning and parameterized quantum circuits
has sparked significant research interest, with promising implications for
fields such as healthcare and finance. By enabling decentralized training of
quantum models, it allows clients or institutions to collaboratively enhance
model performance and outcomes while preserving data privacy. Recognizing that
Fisher information can quantify the amount of information that a quantum state
carries under parameter changes, thereby providing insight into its geometric
and statistical properties. We intend to leverage this property to address the
aforementioned challenges. In this work, we propose a Quantum Federated
Learning (QFL) algorithm that makes use of the Fisher information computed on
local client models, with data distributed across heterogeneous partitions.
This approach identifies the critical parameters that significantly influence
the quantum model's performance, ensuring they are preserved during the
aggregation process. Our research assessed the effectiveness and feasibility of
QFL by comparing its performance against other variants, and exploring the
benefits of incorporating Fisher information in QFL settings. Experimental
results on ADNI and MNIST datasets demonstrate the effectiveness of our
approach in achieving better performance and robustness against the quantum
federated averaging method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Ray Search Procedure of Hard-label Attacks with Transfer-based
  Priors <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Ma, Xinjie Xu, Shuyu Cheng, Qi Xuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the most practical and challenging types of black-box adversarial
attacks is the hard-label attack, where only the top-1 predicted label is
available. One effective approach is to search for the optimal ray direction
from the benign image that minimizes the $\ell_p$-norm distance to the
adversarial region. The unique advantage of this approach is that it transforms
the hard-label attack into a continuous optimization problem. The objective
function value is the ray's radius, which can be obtained via binary search at
a high query cost. Existing methods use a "sign trick" in gradient estimation
to reduce the number of queries. In this paper, we theoretically analyze the
quality of this gradient estimation and propose a novel prior-guided approach
to improve ray search efficiency both theoretically and empirically.
Specifically, we utilize the transfer-based priors from surrogate models, and
our gradient estimators appropriately integrate them by approximating the
projection of the true gradient onto the subspace spanned by these priors and
random directions, in a query-efficient manner. We theoretically derive the
expected cosine similarities between the obtained gradient estimators and the
true gradient, and demonstrate the improvement achieved by incorporating
priors. Extensive experiments on the ImageNet and CIFAR-10 datasets show that
our approach significantly outperforms 11 state-of-the-art methods in terms of
query efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2025 (Spotlight paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable DC <span class="highlight-title">Optimization</span> via Adaptive Frank-Wolfe Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Pokutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of minimizing a difference of (smooth) convex
functions over a compact convex feasible region $P$, i.e., $\min_{x \in P} f(x)
- g(x)$, with smooth $f$ and Lipschitz continuous $g$. This computational study
builds upon and complements the framework of Maskan et al. [2025] by
integrating advanced Frank-Wolfe variants to reduce computational overhead. We
empirically show that constrained DC problems can be efficiently solved using a
combination of the Blended Pairwise Conditional Gradients (BPCG) algorithm
[Tsuji et al., 2022] with warm-starting and the adaptive error bound from
Maskan et al. [2025]. The result is a highly efficient and scalable
projection-free algorithm for constrained DC optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal differentially private kernel learning with random projection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bonwoo Lee, Cheolwoo Park, Jeongyoun Ahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differential privacy has become a cornerstone in the development of
privacy-preserving learning algorithms. This work addresses optimizing
differentially private kernel learning within the empirical risk minimization
(ERM) framework. We propose a novel differentially private kernel ERM algorithm
based on random projection in the reproducing kernel Hilbert space using
Gaussian processes. Our method achieves minimax-optimal excess risk for both
the squared loss and Lipschitz-smooth convex loss functions under a local
strong convexity condition. We further show that existing approaches based on
alternative dimension reduction techniques, such as random Fourier feature
mappings or $\ell_2$ regularization, yield suboptimal generalization
performance. Our key theoretical contribution also includes the derivation of
dimension-free generalization bounds for objective perturbation-based private
linear ERM -- marking the first such result that does not rely on noisy
gradient-based mechanisms. Additionally, we obtain sharper generalization
bounds for existing differentially private kernel ERM algorithms. Empirical
evaluations support our theoretical claims, demonstrating that random
projection enables statistically efficient and optimally private kernel
learning. These findings provide new insights into the design of differentially
private algorithms and highlight the central role of dimension reduction in
balancing privacy and utility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>110 page, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Cluster</span>ing-based hard negative sampling for supervised contrastive
  speaker verification <span class="chip">INTERSPEECH 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piotr Masztalski, Michał Romaniuk, Jakub Żak, Mateusz Matuszewski, Konrad Kowalczyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In speaker verification, contrastive learning is gaining popularity as an
alternative to the traditionally used classification-based approaches.
Contrastive methods can benefit from an effective use of hard negative pairs,
which are different-class samples particularly challenging for a verification
model due to their similarity. In this paper, we propose CHNS - a
clustering-based hard negative sampling method, dedicated for supervised
contrastive speaker representation learning. Our approach clusters embeddings
of similar speakers, and adjusts batch composition to obtain an optimal ratio
of hard and easy negatives during contrastive loss calculation. Experimental
evaluation shows that CHNS outperforms a baseline supervised contrastive
approach with and without loss-based hard negative sampling, as well as a
state-of-the-art classification-based approach to speaker verification by as
much as 18 % relative EER and minDCF on the VoxCeleb dataset using two
lightweight model architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to INTERSPEECH 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoCAI: Copula-based Conformal Anomaly Identification for Multivariate
  Time-Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas A. Pearson, Francesca Zanello, Davide Russo, Luca Bortolussi, Francesca Cairoli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel framework that harnesses the power of generative
artificial intelligence and copula-based modeling to address two critical
challenges in multivariate time-series analysis: delivering accurate
predictions and enabling robust anomaly detection. Our method, Copula-based
Conformal Anomaly Identification for Multivariate Time-Series (CoCAI),
leverages a diffusion-based model to capture complex dependencies within the
data, enabling high quality forecasting. The model's outputs are further
calibrated using a conformal prediction technique, yielding predictive regions
which are statistically valid, i.e., cover the true target values with a
desired confidence level. Starting from these calibrated forecasts, robust
outlier detection is performed by combining dimensionality reduction techniques
with copula-based modeling, providing a statistically grounded anomaly score.
CoCAI benefits from an offline calibration phase that allows for minimal
overhead during deployment and delivers actionable results rooted in
established theoretical foundations. Empirical tests conducted on real
operational data derived from water distribution and sewerage systems confirm
CoCAI's effectiveness in accurately forecasting target sequences of data and in
identifying anomalous segments within them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for Presentation at Runtime Verification 25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Majorize-Minimization: Beyond Parameter Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aymeric Dieuleveut, Gersende Fort, Mahmoud Hegazy, Hoi-To Wai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a unified approach for designing stochastic optimization
algorithms that robustly scale to the federated learning setting. Our work
studies a class of Majorize-Minimization (MM) problems, which possesses a
linearly parameterized family of majorizing surrogate functions. This framework
encompasses (proximal) gradient-based algorithms for (regularized) smooth
objectives, the Expectation Maximization algorithm, and many problems seen as
variational surrogate MM. We show that our framework motivates a unifying
algorithm called Stochastic Approximation Stochastic Surrogate MM (\SSMM),
which includes previous stochastic MM procedures as special instances. We then
extend \SSMM\ to the federated setting, while taking into consideration common
bottlenecks such as data heterogeneity, partial participation, and
communication constraints; this yields \QSMM. The originality of \QSMM\ is to
learn locally and then aggregate information characterizing the
\textit{surrogate majorizing function}, contrary to classical algorithms which
learn and aggregate the \textit{original parameter}. Finally, to showcase the
flexibility of this methodology beyond our theoretical setting, we use it to
design an algorithm for computing optimal transport maps in the federated
setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Generalized Advantage <span class="highlight-title">Estimation</span> for Distributional Policy Gradients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahil Shaik, Jonathon M. Smereka, <span class="highlight-author">Yue Wang</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalized Advantage Estimation (GAE) has been used to mitigate the
computational complexity of reinforcement learning (RL) by employing an
exponentially weighted estimation of the advantage function to reduce the
variance in policy gradient estimates. Despite its effectiveness, GAE is not
designed to handle value distributions integral to distributional RL, which can
capture the inherent stochasticity in systems and is hence more robust to
system noises. To address this gap, we propose a novel approach that utilizes
the optimal transport theory to introduce a Wasserstein-like directional
metric, which measures both the distance and the directional discrepancies
between probability distributions. Using the exponentially weighted estimation,
we leverage this Wasserstein-like directional metric to derive distributional
GAE (DGAE). Similar to traditional GAE, our proposed DGAE provides a
low-variance advantage estimate with controlled bias, making it well-suited for
policy gradient algorithms that rely on advantage estimation for policy
updates. We integrated DGAE into three different policy gradient methods.
Algorithms were evaluated across various OpenAI Gym environments and compared
with the baselines with traditional GAE to assess the performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, published at ACC 2025 Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized Low-Rank Matrix Contextual Bandits with <span class="highlight-title">Graph</span> Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Wang, Jiannan Li, Yue Kang, Shanxing Gao, Zhenxin Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The matrix contextual bandit (CB), as an extension of the well-known
multi-armed bandit, is a powerful framework that has been widely applied in
sequential decision-making scenarios involving low-rank structure. In many
real-world scenarios, such as online advertising and recommender systems,
additional graph information often exists beyond the low-rank structure, that
is, the similar relationships among users/items can be naturally captured
through the connectivity among nodes in the corresponding graphs. However,
existing matrix CB methods fail to explore such graph information, and thereby
making them difficult to generate effective decision-making policies. To fill
in this void, we propose in this paper a novel matrix CB algorithmic framework
that builds upon the classical upper confidence bound (UCB) framework. This new
framework can effectively integrate both the low-rank structure and graph
information in a unified manner. Specifically, it involves first solving a
joint nuclear norm and matrix Laplacian regularization problem, followed by the
implementation of a graph-based generalized linear version of the UCB
algorithm. Rigorous theoretical analysis demonstrates that our procedure
outperforms several popular alternatives in terms of cumulative regret bound,
owing to the effective utilization of graph information. A series of synthetic
and real-world data experiments are conducted to further illustrate the merits
of our procedure.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Physics-Based and Data-Driven Approaches for Probabilistic
  Building Energy Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leandro Von Krannichfeldt, Kristina Orehounig, Olga Fink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building energy modeling is a key tool for optimizing the performance of
building energy systems. Historically, a wide spectrum of methods has been
explored -- ranging from conventional physics-based models to purely
data-driven techniques. Recently, hybrid approaches that combine the strengths
of both paradigms have gained attention. These include strategies such as
learning surrogates for physics-based models, modeling residuals between
simulated and observed data, fine-tuning surrogates with real-world
measurements, using physics-based outputs as additional inputs for data-driven
models, and integrating the physics-based output into the loss function the
data-driven model. Despite this progress, two significant research gaps remain.
First, most hybrid methods focus on deterministic modeling, often neglecting
the inherent uncertainties caused by factors like weather fluctuations and
occupant behavior. Second, there has been little systematic comparison within a
probabilistic modeling framework. This study addresses these gaps by evaluating
five representative hybrid approaches for probabilistic building energy
modeling, focusing on quantile predictions of building thermodynamics in a
real-world case study. Our results highlight two main findings. First, the
performance of hybrid approaches varies across different building room types,
but residual learning with a Feedforward Neural Network performs best on
average. Notably, the residual approach is the only model that produces
physically intuitive predictions when applied to out-of-distribution test data.
Second, Quantile Conformal Prediction is an effective procedure for calibrating
quantile predictions in case of indoor temperature modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LSDM: LLM-Enhanced Spatio-temporal Dif<span class="highlight-title">fusion</span> Model for Service-Level
  Mobile Traffic <span class="highlight-title">Prediction</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyuan Zhang, Tong Li, Zhu Xiao, Hongyang Du, Kaibin Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Service-level mobile traffic prediction for individual users is essential for
network efficiency and quality of service enhancement. However, current
prediction methods are limited in their adaptability across different urban
environments and produce inaccurate results due to the high uncertainty in
personal traffic patterns, the lack of detailed environmental context, and the
complex dependencies among different network services. These challenges demand
advanced modeling techniques that can capture dynamic traffic distributions and
rich environmental features. Inspired by the recent success of diffusion models
in distribution modeling and Large Language Models (LLMs) in contextual
understanding, we propose an LLM-Enhanced Spatio-temporal Diffusion Model
(LSDM). LSDM integrates the generative power of diffusion models with the
adaptive learning capabilities of transformers, augmented by the ability to
capture multimodal environmental information for modeling service-level
patterns and dynamics. Extensive evaluations on real-world service-level
datasets demonstrate that the model excels in traffic usage predictions,
showing outstanding generalization and adaptability. After incorporating
contextual information via LLM, the performance improves by at least 2.83% in
terms of the coefficient of determination. Compared to models of a similar
type, such as CSDI, the root mean squared error can be reduced by at least
8.29%. The code and dataset will be available at:
https://github.com/SoftYuaneR/LSDM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HOTA: Hamiltonian framework for Optimal Transport Advection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nazar Buzun, Daniil Shlenskii, Maxim Bobrin, Dmitry V. Dylov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimal transport (OT) has become a natural framework for guiding the
probability flows. Yet, the majority of recent generative models assume trivial
geometry (e.g., Euclidean) and rely on strong density-estimation assumptions,
yielding trajectories that do not respect the true principles of optimality in
the underlying manifold. We present Hamiltonian Optimal Transport Advection
(HOTA), a Hamilton-Jacobi-Bellman based method that tackles the dual dynamical
OT problem explicitly through Kantorovich potentials, enabling efficient and
scalable trajectory optimization. Our approach effectively evades the need for
explicit density modeling, performing even when the cost functionals are
non-smooth. Empirically, HOTA outperforms all baselines in standard benchmarks,
as well as in custom datasets with non-differentiable costs, both in terms of
feasibility and optimality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can One Domain Help Others? A Data-Centric Study on Multi-Domain
  Reasoning via <span class="highlight-title">Reinforcement</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Li, Zhuoshi Pan, Honglin Lin, Mengyuan Sun, Conghui He, Lijun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for enhancing the reasoning capabilities of LLMs. Existing
research has predominantly concentrated on isolated reasoning domains such as
mathematical problem-solving, coding tasks, or logical reasoning. However, real
world reasoning scenarios inherently demand an integrated application of
multiple cognitive skills. Despite this, the interplay among these reasoning
skills under reinforcement learning remains poorly understood. To bridge this
gap, we present a systematic investigation of multi-domain reasoning within the
RLVR framework, explicitly focusing on three primary domains: mathematical
reasoning, code generation, and logical puzzle solving. We conduct a
comprehensive study comprising four key components: (1) Leveraging the GRPO
algorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the
models' in-domain improvements and cross-domain generalization capabilities
when trained on single-domain datasets. (2) Additionally, we examine the
intricate interactions including mutual enhancements and conflicts that emerge
during combined cross-domain training. (3) To further understand the influence
of SFT on RL, we also analyze and compare performance differences between base
and instruct models under identical RL configurations. (4) Furthermore, we
delve into critical RL training details, systematically exploring the impacts
of curriculum learning strategies, variations in reward design, and
language-specific factors. Through extensive experiments, our results offer
significant insights into the dynamics governing domain interactions, revealing
key factors influencing both specialized and generalizable reasoning
performance. These findings provide valuable guidance for optimizing RL
methodologies to foster comprehensive, multi-domain reasoning capabilities in
LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 24 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Graph</span> Neural Network Approach to Predicting Magnetization in
  Quasi-One-Dimensional Ising Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        V. Slavin, O. Kryvchikov, D. Laptev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a graph-based deep learning framework for predicting the magnetic
properties of quasi-one-dimensional Ising spin systems. The lattice geometry is
encoded as a graph and processed by a graph neural network (GNN) followed by
fully connected layers. The model is trained on Monte Carlo simulation data and
accurately reproduces key features of the magnetization curve, including
plateaus, critical transition points, and the effects of geometric frustration.
It captures both local motifs and global symmetries, demonstrating that GNNs
can infer magnetic behavior directly from structural connectivity. The proposed
approach enables efficient prediction of magnetization without the need for
additional Monte Carlo simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Multi-Target <span class="highlight-title">Detection</span>-Tracking in Cognitive Massive MIMO Radar
  via POMCP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Imad Bouhou, Stefano Fortunati, Leila Gharsalli, Alexandre Renaux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This correspondence presents a power-aware cognitive radar framework for
joint detection and tracking of multiple targets in a massive multiple-input
multiple-output (MIMO) radar environment. Building on a previous single-target
algorithm based on Partially Observable Monte Carlo Planning (POMCP), we extend
it to the multi-target case by assigning each target an independent POMCP tree,
enabling scalable and efficient planning.
  Departing from uniform power allocation-which is often suboptimal with
varying signal-to-noise ratios (SNRs)-our approach predicts each target's
future angular position and expected received power, based on its estimated
range and radar cross-section (RCS). These predictions guide adaptive waveform
design via a constrained optimization problem that allocates transmit energy to
enhance the detectability of weaker or distant targets, while ensuring
sufficient power for high-SNR targets. The reward function in the underlying
partially observable Markov decision process (POMDP) is also modified to
prioritize accurate spatial and power estimation.
  Simulations involving multiple targets with different SNRs confirm the
effectiveness of our method. The proposed framework for the cognitive radar
improves detection probability for low-SNR targets and achieves more accurate
tracking compared to approaches using uniform or orthogonal waveforms. These
results demonstrate the potential of the POMCP-based framework for adaptive,
efficient multi-target radar systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DNT: a Deeply Normalized Transformer that can be trained by Momentum SGD 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianbiao Qi, Marco Chen, Wenjie Xiao, Jiaquan Ye, Yelin He, Chun-Guang Li, Zhouchen Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have become the de facto backbone of modern deep learning, yet
their training typically demands an advanced optimizer with adaptive learning
rate like AdamW, rather than a momentum SGDW (mSGDW). Previous works show that
it is mainly due to a heavy-tailed distribution of the gradients. In this
paper, we introduce a Deeply Normalized Transformer (DNT), which is
meticulously engineered to overcome this limitation enabling seamless training
with vanilla mSGDW while yielding comparable performance to the Transformers
trained via AdamW. To be specific, in DNT, we strategically integrate
normalization techniques at proper positions in the Transformers to effectively
modulate the Jacobian matrices of each layer, balance the influence of weights,
activations, and their interactions, and thus enable the distributions of
gradients concentrated. We provide both theoretical justifications of the
normalization technique used in our DNT and extensive empirical evaluation on
two popular Transformer architectures to validate that: a) DNT outperforms its
counterparts (\ie, ViT and GPT), and b) DNT can be effectively trained with
vanilla mSGDW.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We have introduced a novel architecture, Deeply Normalized
  Transformer (DNT), which enables efficient training with vanilla momentum
  SGDW (mSGDW), achieving performance on par with AdamW-optimized Transformers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ To Trust or Not to Trust: On <span class="highlight-title">Calibration</span> in ML-based Resource Allocation
  for Wireless Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rashika Raina, Nidhi Simmons, David E. Simmons, Michel Daoud Yacoub, Trung Q. Duong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In next-generation communications and networks, machine learning (ML) models
are expected to deliver not only accurate predictions but also well-calibrated
confidence scores that reflect the true likelihood of correct decisions. This
paper studies the calibration performance of an ML-based outage predictor
within a single-user, multi-resource allocation framework. We first establish
key theoretical properties of this system's outage probability (OP) under
perfect calibration. Importantly, we show that as the number of resources
grows, the OP of a perfectly calibrated predictor approaches the expected
output conditioned on it being below the classification threshold. In contrast,
when only one resource is available, the system's OP equals the model's overall
expected output. We then derive the OP conditions for a perfectly calibrated
predictor. These findings guide the choice of the classification threshold to
achieve a desired OP, helping system designers meet specific reliability
requirements. We also demonstrate that post-processing calibration cannot
improve the system's minimum achievable OP, as it does not introduce new
information about future channel states. Additionally, we show that
well-calibrated models are part of a broader class of predictors that
necessarily improve OP. In particular, we establish a monotonicity condition
that the accuracy-confidence function must satisfy for such improvement to
occur. To demonstrate these theoretical properties, we conduct a rigorous
simulation-based analysis using post-processing calibration techniques: Platt
scaling and isotonic regression. As part of this framework, the predictor is
trained using an outage loss function specifically designed for this system.
Furthermore, this analysis is performed on Rayleigh fading channels with
temporal correlation captured by Clarke's 2D model, which accounts for receiver
mobility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SRMambaV2: Biomimetic Attention for Sparse <span class="highlight-title">Point Cloud</span> Upsampling in
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuang Chen, Xiaolin Qin, Jing Hu, Wenyi Ge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Upsampling LiDAR point clouds in autonomous driving scenarios remains a
significant challenge due to the inherent sparsity and complex 3D structures of
the data. Recent studies have attempted to address this problem by converting
the complex 3D spatial scenes into 2D image super-resolution tasks. However,
due to the sparse and blurry feature representation of range images, accurately
reconstructing detailed and complex spatial topologies remains a major
difficulty. To tackle this, we propose a novel sparse point cloud upsampling
method named SRMambaV2, which enhances the upsampling accuracy in long-range
sparse regions while preserving the overall geometric reconstruction quality.
Specifically, inspired by human driver visual perception, we design a
biomimetic 2D selective scanning self-attention (2DSSA) mechanism to model the
feature distribution in distant sparse areas. Meanwhile, we introduce a
dual-branch network architecture to enhance the representation of sparse
features. In addition, we introduce a progressive adaptive loss (PAL) function
to further refine the reconstruction of fine-grained details during the
upsampling process. Experimental results demonstrate that SRMambaV2 achieves
superior performance in both qualitative and quantitative evaluations,
highlighting its effectiveness and practical value in automotive sparse point
cloud upsampling tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BGM-HAN: A Hierarchical Attention Network for Accurate and Fair <span class="highlight-title">Decision</span>
  Assessment on Semi-Structured Profiles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhua Liu, Roy Ka-Wei Lee, Kwan Hui Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human decision-making in high-stakes domains often relies on expertise and
heuristics, but is vulnerable to hard-to-detect cognitive biases that threaten
fairness and long-term outcomes. This work presents a novel approach to
enhancing complex decision-making workflows through the integration of
hierarchical learning alongside various enhancements. Focusing on university
admissions as a representative high-stakes domain, we propose BGM-HAN, an
enhanced Byte-Pair Encoded, Gated Multi-head Hierarchical Attention Network,
designed to effectively model semi-structured applicant data. BGM-HAN captures
multi-level representations that are crucial for nuanced assessment, improving
both interpretability and predictive performance. Experimental results on real
admissions data demonstrate that our proposed model significantly outperforms
both state-of-the-art baselines from traditional machine learning to large
language models, offering a promising framework for augmenting decision-making
in domains where structure, context, and fairness matter. Source code is
available at: https://github.com/junhua/bgm-han.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ASONAM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Demonstration of Efficient Predictive Surrogates for Large-scale Quantum
  Processors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei-You Liao, Yuxuan Du, Xinbiao Wang, Tian-Ci Tian, Yong Luo, Bo Du, Dacheng Tao, He-Liang Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ongoing development of quantum processors is driving breakthroughs in
scientific discovery. Despite this progress, the formidable cost of fabricating
large-scale quantum processors means they will remain rare for the foreseeable
future, limiting their widespread application. To address this bottleneck, we
introduce the concept of predictive surrogates, which are classical learning
models designed to emulate the mean-value behavior of a given quantum processor
with provably computational efficiency. In particular, we propose two
predictive surrogates that can substantially reduce the need for quantum
processor access in diverse practical scenarios. To demonstrate their potential
in advancing digital quantum simulation, we use these surrogates to emulate a
quantum processor with up to 20 programmable superconducting qubits, enabling
efficient pre-training of variational quantum eigensolvers for families of
transverse-field Ising models and identification of non-equilibrium Floquet
symmetry-protected topological phases. Experimental results reveal that the
predictive surrogates not only reduce measurement overhead by orders of
magnitude, but can also surpass the performance of conventional,
quantum-resource-intensive approaches. Collectively, these findings establish
predictive surrogates as a practical pathway to broadening the impact of
advanced quantum processors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>53 pages, 15 figures, comments are welcome</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ C3RL: Rethinking the Combination of Channel-independence and
  Channel-mixing from Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shusen Ma, Yun-Bo Zhao, Yu Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multivariate time series forecasting has drawn increasing attention due to
its practical importance. Existing approaches typically adopt either
channel-mixing (CM) or channel-independence (CI) strategies. CM strategy can
capture inter-variable dependencies but fails to discern variable-specific
temporal patterns. CI strategy improves this aspect but fails to fully exploit
cross-variable dependencies like CM. Hybrid strategies based on feature fusion
offer limited generalization and interpretability. To address these issues, we
propose C3RL, a novel representation learning framework that jointly models
both CM and CI strategies. Motivated by contrastive learning in computer
vision, C3RL treats the inputs of the two strategies as transposed views and
builds a siamese network architecture: one strategy serves as the backbone,
while the other complements it. By jointly optimizing contrastive and
prediction losses with adaptive weighting, C3RL balances representation and
forecasting performance. Extensive experiments on seven models show that C3RL
boosts the best-case performance rate to 81.4\% for models based on CI strategy
and to 76.3\% for models based on CM strategy, demonstrating strong
generalization and effectiveness. The code will be available once the paper is
accepted.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Neural Network Verification via Order Leading <span class="highlight-title">Exploration</span> of
  Branch-and-Bound Trees <span class="chip">DATE 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17453v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17453v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanqin Zhang, Kota Fukuda, Zhenya Zhang, H. M. N. Dilum Bandara, Shiping Chen, Jianjun Zhao, Yulei Sui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The vulnerability of neural networks to adversarial perturbations has
necessitated formal verification techniques that can rigorously certify the
quality of neural networks. As the state-of-the-art, branch and bound (BaB) is
a "divide-and-conquer" strategy that applies off-the-shelf verifiers to
sub-problems for which they perform better. While BaB can identify the
sub-problems that are necessary to be split, it explores the space of these
sub-problems in a naive "first-come-first-serve" manner, thereby suffering from
an issue of inefficiency to reach a verification conclusion. To bridge this
gap, we introduce an order over different sub-problems produced by BaB,
concerning with their different likelihoods of containing counterexamples.
Based on this order, we propose a novel verification framework Oliva that
explores the sub-problem space by prioritizing those sub-problems that are more
likely to find counterexamples, in order to efficiently reach the conclusion of
the verification. Even if no counterexample can be found in any sub-problem, it
only changes the order of visiting different sub-problem and so will not lead
to a performance degradation. Specifically, Oliva has two variants, including
$Oliva^{GR}$, a greedy strategy that always prioritizes the sub-problems that
are more likely to find counterexamples, and $Oliva^{SA}$, a balanced strategy
inspired by simulated annealing that gradually shifts from exploration to
exploitation to locate the globally optimal sub-problems. We experimentally
evaluate the performance of Oliva on 690 verification problems spanning over 5
models with datasets MNIST and CIFAR10. Compared to the state-of-the-art
approaches, we demonstrate the speedup of Oliva for up to 25X in MNIST, and up
to 80X in CIFAR10.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an extended version of the ECOOP 2025 paper, with a
  comparison with DATE 2025 (Figure 7 of RQ1 in Section 5.2), as well as an
  in-depth discussion of OOPSLA 2025 in the related work (Section 6)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Persistent Patterns in Eye Movements: A Topological Approach to Emotion
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17450v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17450v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arsha Niksa, Hooman Zare, Ali Shahrabi, Hanieh Hatami, Mohammadreza Razvan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a topological pipeline for automated multiclass emotion
recognition from eye-tracking data. Delay embeddings of gaze trajectories are
analyzed using persistent homology. From the resulting persistence diagrams, we
extract shape-based features such as mean persistence, maximum persistence, and
entropy. A random forest classifier trained on these features achieves up to
$75.6\%$ accuracy on four emotion classes, which are the quadrants the
Circumplex Model of Affect. The results demonstrate that persistence diagram
geometry effectively encodes discriminative gaze dynamics, suggesting a
promising topological approach for affective computing and human behavior
analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Doubly <span class="highlight-title">robust</span> outlier resistant inference on causal treatment effect 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joonsung Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Outliers can severely distort causal effect estimation in observational
studies, yet this issue has received limited attention in the literature. Their
influence is especially pronounced in small sample sizes, where detecting and
removing outliers becomes increasingly difficult. Therefore, it is essential to
estimate treatment effects robustly without excluding these influential data
points. To address this, we propose a doubly robust point estimator for the
average treatment effect under a contaminated model that includes outliers.
Robustness in outcome regression is achieved through a robust estimating
equation, while covariate balancing propensity scores (CBPS) ensure resilience
in propensity score modeling.
  To prevent model overfitting due to the inclusion of numerous parameters, we
incorporate variable selection. All these components are unified under a
penalized empirical likelihood framework. For confidence interval estimation,
most existing approaches rely on asymptotic properties, which may be unreliable
in finite samples. We derive an optimal finite-sample confidence interval for
the average treatment effect using our proposed estimating equation, ensuring
that the interval bounds remain unaffected by outliers. Through simulations and
a real-world application involving hypertension data with outliers, we
demonstrate that our method consistently outperforms existing approaches in
both accuracy and robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ctx2TrajGen: Traffic Context-Aware Microscale Vehicle <span class="highlight-title">Trajectories</span> using
  Generative Adversarial Imitation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17418v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17418v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joobin Jin, Seokjun Hong, Gyeongseon Baek, Yeeun Kim, Byeongjoon Noh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise modeling of microscopic vehicle trajectories is critical for traffic
behavior analysis and autonomous driving systems. We propose Ctx2TrajGen, a
context-aware trajectory generation framework that synthesizes realistic urban
driving behaviors using GAIL. Leveraging PPO and WGAN-GP, our model addresses
nonlinear interdependencies and training instability inherent in microscopic
settings. By explicitly conditioning on surrounding vehicles and road geometry,
Ctx2TrajGen generates interaction-aware trajectories aligned with real-world
context. Experiments on the drone-captured DRIFT dataset demonstrate superior
performance over existing methods in terms of realism, behavioral diversity,
and contextual fidelity, offering a robust solution to data scarcity and domain
shift without simulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive <span class="highlight-title">Evaluation</span> on Quantization Techniques for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutong Liu, Cairong Zhao, Guosheng Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For large language models (LLMs), post-training quantization (PTQ) can
significantly reduce memory footprint and computational overhead. Model
quantization is a rapidly evolving research field. Though many papers have
reported breakthrough performance, they may not conduct experiments on the same
ground since one quantization method usually contains multiple components. In
addition, analyzing the theoretical connections among existing methods is
crucial for in-depth understanding. To bridge these gaps, we conduct an
extensive review of state-of-the-art methods and perform comprehensive
evaluations on the same ground to ensure fair comparisons. To our knowledge,
this fair and extensive investigation remains critically important yet
underexplored. To better understand the theoretical connections, we decouple
the published quantization methods into two steps: pre-quantization
transformation and quantization error mitigation. We define the former as a
preprocessing step applied before quantization to reduce the impact of
outliers, making the data distribution flatter and more suitable for
quantization. Quantization error mitigation involves techniques that offset the
errors introduced during quantization, thereby enhancing model performance. We
evaluate and analyze the impact of different components of quantization
methods. Additionally, we analyze and evaluate the latest MXFP4 data format and
its performance. Our experimental results demonstrate that optimized rotation
and scaling yield the best performance for pre-quantization transformation, and
combining low-rank compensation with GPTQ occasionally outperforms using GPTQ
alone for quantization error mitigation. Furthermore, we explore the potential
of the latest MXFP4 quantization and reveal that the optimal pre-quantization
transformation strategy for INT4 does not generalize well to MXFP4, inspiring
further investigation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning from Scratch: Structurally-masked Transformer for Next
  Generation Lib-free <span class="highlight-title">Simulation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junlang Huang, Hao Chen, Zhong Guan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a neural framework for power and timing prediction of
multi-stage data path, distinguishing itself from traditional lib-based
analytical methods dependent on driver characterization and load
simplifications. To the best of our knowledge, this is the first
language-based, netlist-aware neural network designed explicitly for standard
cells. Our approach employs two pre-trained neural models of waveform
prediction and delay estimation that directly infer transient waveforms and
propagation delays from SPICE netlists, conditioned on critical physical
parameters such as load capacitance, input slew, and gate size. This method
accurately captures both intrinsic and coupling-induced delay effects without
requiring simplification or interpolation. For multi-stage timing prediction,
we implement a recursive propagation strategy where predicted waveforms from
each stage feed into subsequent stages, cumulatively capturing delays across
the logic chain. This approach ensures precise timing alignment and complete
waveform visibility throughout complex signal pathways. The waveform prediction
utilizes a hybrid CNN-Transformer architecture with netlist-aware node-level
encoding, addressing traditional Transformers' fixed input dimensionality
constraints. Additionally, specialized subnetworks separately handle primary
delay estimation and crosstalk correction. Experimental results demonstrate
SPICE-level accuracy, consistently achieving RMSE below 0.0098 across diverse
industrial circuits. The proposed framework provides a scalable, structurally
adaptable neural alternative to conventional power and timing engines,
demonstrating high fidelity to physical circuit behaviors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Mechanism <span class="highlight-title">Estimation</span> in Multi-Sensor Systems Across Multiple
  Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17792v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17792v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyi Yu, Tim Pychynski, Marco F. Huber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To gain deeper insights into a complex sensor system through the lens of
causality, we present common and individual causal mechanism estimation
(CICME), a novel three-step approach to inferring causal mechanisms from
heterogeneous data collected across multiple domains. By leveraging the
principle of Causal Transfer Learning (CTL), CICME is able to reliably detect
domain-invariant causal mechanisms when provided with sufficient samples. The
identified common causal mechanisms are further used to guide the estimation of
the remaining causal mechanisms in each domain individually. The performance of
CICME is evaluated on linear Gaussian models under scenarios inspired from a
manufacturing process. Building upon existing continuous optimization-based
causal discovery methods, we show that CICME leverages the benefits of applying
causal discovery on the pooled data and repeatedly on data from individual
domains, and it even outperforms both baseline methods under certain scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Helix 1.0: An Open-Source Framework for Reproducible and Interpretable
  Machine Learning on Tabular Scientific Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eduardo Aguilar-Bejarano, Daniel Lea, Karthikeyan Sivakumar, Jimiama M. Mase, Reza Omidvar, Ruizhe Li, Troy Kettle, James Mitchell-White, Morgan R Alexander, David A Winkler, Grazziela Figueredo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Helix is an open-source, extensible, Python-based software framework to
facilitate reproducible and interpretable machine learning workflows for
tabular data. It addresses the growing need for transparent experimental data
analytics provenance, ensuring that the entire analytical process -- including
decisions around data transformation and methodological choices -- is
documented, accessible, reproducible, and comprehensible to relevant
stakeholders. The platform comprises modules for standardised data
preprocessing, visualisation, machine learning model training, evaluation,
interpretation, results inspection, and model prediction for unseen data. To
further empower researchers without formal training in data science to derive
meaningful and actionable insights, Helix features a user-friendly interface
that enables the design of computational experiments, inspection of outcomes,
including a novel interpretation approach to machine learning decisions using
linguistic terms all within an integrated environment. Released under the MIT
licence, Helix is accessible via GitHub and PyPI, supporting community-driven
development and promoting adherence to the FAIR principles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Confidence <span class="highlight-title">Calibration</span> in Vision-Language-Action Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas P Zollo, Richard Zemel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trustworthy robot behavior requires not only high levels of task success but
also that the robot can reliably quantify how likely it is to succeed. To this
end, we present the first systematic study of confidence calibration in
vision-language-action (VLA) foundation models, which map visual observations
and natural-language instructions to low-level robot motor commands. We begin
with extensive benchmarking to understand the critical relationship between
task success and calibration error across multiple datasets and VLA variants,
finding that task performance and calibration are not in tension. Next, we
introduce prompt ensembles for VLAs, a lightweight, Bayesian-inspired algorithm
that averages confidence across paraphrased instructions and consistently
improves calibration. We further analyze calibration over the task time
horizon, showing that confidence is often most reliable after making some
progress, suggesting natural points for risk-aware intervention. Finally, we
reveal differential miscalibration across action dimensions and propose
action-wise Platt scaling, a method to recalibrate each action dimension
independently to produce better confidence estimates. Our aim in this study is
to begin to develop the tools and conceptual understanding necessary to render
VLAs both highly performant and highly trustworthy via reliable uncertainty
quantification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Generalized Category Discovery: Learning and Forgetting from a
  Bayesian Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17382v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17382v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Dai, Jagmohan Chauhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual Generalized Category Discovery (C-GCD) faces a critical challenge:
incrementally learning new classes from unlabeled data streams while preserving
knowledge of old classes. Existing methods struggle with catastrophic
forgetting, especially when unlabeled data mixes known and novel categories. We
address this by analyzing C-GCD's forgetting dynamics through a Bayesian lens,
revealing that covariance misalignment between old and new classes drives
performance degradation. Building on this insight, we propose Variational Bayes
C-GCD (VB-CGCD), a novel framework that integrates variational inference with
covariance-aware nearest-class-mean classification. VB-CGCD adaptively aligns
class distributions while suppressing pseudo-label noise via stochastic
variational updates. Experiments show VB-CGCD surpasses prior art by +15.21%
with the overall accuracy in the final session on standard benchmarks. We also
introduce a new challenging benchmark with only 10% labeled data and extended
online phases, VB-CGCD achieves a 67.86% final accuracy, significantly higher
than state-of-the-art (38.55%), demonstrating its robust applicability across
diverse scenarios. Code is available at: https://github.com/daihao42/VB-CGCD
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 6 figures. Forty-second International Conference on Machine
  Learning. 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViRN: Variational Inference and Distribution Trilateration for
  Long-Tailed Continual Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Dai, Chong Tang, Jagmohan Chauhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning (CL) with long-tailed data distributions remains a
critical challenge for real-world AI systems, where models must sequentially
adapt to new classes while retaining knowledge of old ones, despite severe
class imbalance. Existing methods struggle to balance stability and plasticity,
often collapsing under extreme sample scarcity. To address this, we propose
ViRN, a novel CL framework that integrates variational inference (VI) with
distributional trilateration for robust long-tailed learning. First, we model
class-conditional distributions via a Variational Autoencoder to mitigate bias
toward head classes. Second, we reconstruct tail-class distributions via
Wasserstein distance-based neighborhood retrieval and geometric fusion,
enabling sample-efficient alignment of tail-class representations. Evaluated on
six long-tailed classification benchmarks, including speech (e.g., rare
acoustic events, accents) and image tasks, ViRN achieves a 10.24% average
accuracy gain over state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DynaSearcher: <span class="highlight-title">Dynamic</span> Knowledge <span class="highlight-title">Graph</span> Augmented Search Agent via
  Multi-Reward <span class="highlight-title">Reinforcement</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuzhan Hao, Wenfeng Feng, Yuewei Zhang, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-step agentic retrieval systems based on large language models (LLMs)
have demonstrated remarkable performance in complex information search tasks.
However, these systems still face significant challenges in practical
applications, particularly in generating factually inconsistent intermediate
queries and inefficient search trajectories, which can lead to reasoning
deviations or redundant computations. To address these issues, we propose
DynaSearcher, an innovative search agent enhanced by dynamic knowledge graphs
and multi-reward reinforcement learning (RL). Specifically, our system
leverages knowledge graphs as external structured knowledge to guide the search
process by explicitly modeling entity relationships, thereby ensuring factual
consistency in intermediate queries and mitigating biases from irrelevant
information. Furthermore, we employ a multi-reward RL framework for
fine-grained control over training objectives such as retrieval accuracy,
efficiency, and response quality. This framework promotes the generation of
high-quality intermediate queries and comprehensive final answers, while
discouraging unnecessary exploration and minimizing information omissions or
redundancy. Experimental results demonstrate that our approach achieves
state-of-the-art answer accuracy on six multi-hop question answering datasets,
matching frontier LLMs while using only small-scale models and limited
computational resources. Furthermore, our approach demonstrates strong
generalization and robustness across diverse retrieval environments and
larger-scale models, highlighting its broad applicability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Repetition for Mitigating Position Bias in LLM-Based Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Vardasbi, Gustavo Penha, Claudia Hauff, Hugues Bouchard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When using LLMs to rank items based on given criteria, or evaluate answers,
the order of candidate items can influence the model's final decision. This
sensitivity to item positioning in a LLM's prompt is known as position bias.
Prior research shows that this bias exists even in large models, though its
severity varies across models and tasks. In addition to position bias, LLMs
also exhibit varying degrees of low repetition consistency, where repeating the
LLM call with the same candidate ordering can lead to different rankings. To
address both inconsistencies, a common approach is to prompt the model multiple
times with different candidate orderings and aggregate the results via majority
voting. However, this repetition strategy, significantly increases
computational costs. Extending prior findings, we observe that both the
direction -- favoring either the earlier or later candidate in the prompt --
and magnitude of position bias across instances vary substantially, even within
a single dataset. This observation highlights the need for a per-instance
mitigation strategy. To this end, we introduce a dynamic early-stopping method
that adaptively determines the number of repetitions required for each
instance. Evaluating our approach across three LLMs of varying sizes and on two
tasks, namely re-ranking and alignment, we demonstrate that transitioning to a
dynamic repetition strategy reduces the number of LLM calls by an average of
81%, while preserving the accuracy. Furthermore, we propose a confidence-based
adaptation to our early-stopping method, reducing LLM calls by an average of
87% compared to static repetition, with only a slight accuracy trade-off
relative to our original early-stopping method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyperbolic Deep Learning for Foundation Models: A <span class="highlight-title">Survey</span> <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17787v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17787v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neil He, Hiren Madhu, Ngoc Bui, Menglin Yang, Rex Ying
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models pre-trained on massive datasets, including large language
models (LLMs), vision-language models (VLMs), and large multimodal models, have
demonstrated remarkable success in diverse downstream tasks. However, recent
studies have shown fundamental limitations of these models: (1) limited
representational capacity, (2) lower adaptability, and (3) diminishing
scalability. These shortcomings raise a critical question: is Euclidean
geometry truly the optimal inductive bias for all foundation models, or could
incorporating alternative geometric spaces enable models to better align with
the intrinsic structure of real-world data and improve reasoning processes?
Hyperbolic spaces, a class of non-Euclidean manifolds characterized by
exponential volume growth with respect to distance, offer a mathematically
grounded solution. These spaces enable low-distortion embeddings of
hierarchical structures (e.g., trees, taxonomies) and power-law distributions
with substantially fewer dimensions compared to Euclidean counterparts. Recent
advances have leveraged these properties to enhance foundation models,
including improving LLMs' complex reasoning ability, VLMs' zero-shot
generalization, and cross-modal semantic alignment, while maintaining parameter
efficiency. This paper provides a comprehensive review of hyperbolic neural
networks and their recent development for foundation models. We further outline
key challenges and research directions to advance the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 Pages, SIGKDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeCo-SGD: Joint <span class="highlight-title">Optimization</span> of Delay Staleness and Gradient Compression
  Ratio for Distributed SGD 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongwei Lu, Jingyan Jiang, Chunyang Li, Haotian Dong, Xingguang Wei, Delin Cai, Zhi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed machine learning in high end-to-end latency and low, varying
bandwidth network environments undergoes severe throughput degradation. Due to
its low communication requirements, distributed SGD (D-SGD) remains the
mainstream optimizer in such challenging networks, but it still suffers from
significant throughput reduction. To mitigate these limitations, existing
approaches typically employ gradient compression and delayed aggregation to
alleviate low bandwidth and high latency, respectively. To address both
challenges simultaneously, these strategies are often combined, introducing a
complex three-way trade-off among compression ratio, staleness (delayed
synchronization steps), and model convergence rate. To achieve the balance
under varying bandwidth conditions, an adaptive policy is required to
dynamically adjust these parameters. Unfortunately, existing works rely on
static heuristic strategies due to the lack of theoretical guidance, which
prevents them from achieving this goal. This study fills in this theoretical
gap by introducing a new theoretical tool, decomposing the joint optimization
problem into a traditional convergence rate analysis with multiple analyzable
noise terms. We are the first to reveal that staleness exponentially amplifies
the negative impact of gradient compression on training performance, filling a
critical gap in understanding how compressed and delayed gradients affect
training. Furthermore, by integrating the convergence rate with a network-aware
time minimization condition, we propose DeCo-SGD, which dynamically adjusts the
compression ratio and staleness based on the real-time network condition and
training task. DeCo-SGD achieves up to 5.07 and 1.37 speed-ups over D-SGD and
static strategy in high-latency and low, varying bandwidth networks,
respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Reinforcement</span> Learning for Accelerated Aero<span class="highlight-title">dynamic</span> Shape Optimisation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Sobieczky, Alfredo Lopez, Erika Dudkin, Christopher Lackner, Matthias Hochsteger, Bernhard Scheichl, Helmut Sobieczky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a reinforcement learning (RL) based adaptive optimization
algorithm for aerodynamic shape optimization focused on dimensionality
reduction. The form in which RL is applied here is that of a surrogate-based,
actor-critic policy evaluation MCMC approach allowing for temporal 'freezing'
of some of the parameters to be optimized. The goals are to minimize
computational effort, and to use the observed optimization results for
interpretation of the discovered extrema in terms of their role in achieving
the desired flow-field.
  By a sequence of local optimized parameter changes around intermediate CFD
simulations acting as ground truth, it is possible to speed up the global
optimization if (a) the local neighbourhoods of the parameters in which the
changed parameters must reside are sufficiently large to compete with the
grid-sized steps and its large number of simulations, and (b) the estimates of
the rewards and costs on these neighbourhoods necessary for a good step-wise
parameter adaption are sufficiently accurate. We give an example of a simple
fluid-dynamical problem on which the method allows interpretation in the sense
of a feature importance scoring.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Principled Multimodal Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohao Liu, Xiaobo Xia, See-Kiong Ng, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal representation learning seeks to create a unified representation
space by integrating diverse data modalities to improve multimodal
understanding. Traditional methods often depend on pairwise contrastive
learning, which relies on a predefined anchor modality, restricting alignment
across all modalities. Recent advances have investigated the simultaneous
alignment of multiple modalities, yet several challenges remain, such as
limitations imposed by fixed anchor points and instability arising from
optimizing the product of singular values. To address the challenges, in this
paper, we propose Principled Multimodal Representation Learning (PMRL), a novel
framework that achieves simultaneous alignment of multiple modalities without
anchor dependency in a more stable manner. Specifically, grounded in the
theoretical insight that full alignment corresponds to a rank-1 Gram matrix,
PMRL optimizes the dominant singular value of the representation matrix to
align modalities along a shared leading direction. We propose a softmax-based
loss function that treats singular values as logits to prioritize the largest
singular value. Besides, instance-wise contrastive regularization on the
leading eigenvectors maintains inter-instance separability and prevents
representation collapse. Extensive experiments across diverse tasks demonstrate
PMRL's superiority compared to baseline methods. The source code will be
publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 9 figures, 10 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-similarity Analysis in Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyi Ding, Chengwen Qi, Hongfei Wang, Jianshe Wu, Licheng Jiao, Yuwei Guo, Jian Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current research has found that some deep neural networks exhibit strong
hierarchical self-similarity in feature representation or parameter
distribution. However, aside from preliminary studies on how the power-law
distribution of weights across different training stages affects model
performance,there has been no quantitative analysis on how the self-similarity
of hidden space geometry influences model weight optimization, nor is there a
clear understanding of the dynamic behavior of internal neurons. Therefore,
this paper proposes a complex network modeling method based on the output
features of hidden-layer neurons to investigate the self-similarity of feature
networks constructed at different hidden layers, and analyzes how adjusting the
degree of self-similarity in feature networks can enhance the classification
performance of deep neural networks. Validated on three types of networks MLP
architectures, convolutional networks, and attention architectures this study
reveals that the degree of self-similarity exhibited by feature networks varies
across different model architectures. Furthermore, embedding constraints on the
self-similarity of feature networks during the training process can improve the
performance of self-similar deep neural networks (MLP architectures and
attention architectures) by up to 6 percentage points.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Learning-based Domain Decomposition Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Wu, Nikola Kovachki, Burigede Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments in mechanical, aerospace, and structural engineering have
driven a growing need for efficient ways to model and analyse structures at
much larger and more complex scales than before. While established numerical
methods like the Finite Element Method remain reliable, they often struggle
with computational cost and scalability when dealing with large and
geometrically intricate problems. In recent years, neural network-based methods
have shown promise because of their ability to efficiently approximate
nonlinear mappings. However, most existing neural approaches are still largely
limited to simple domains, which makes it difficult to apply to real-world PDEs
involving complex geometries. In this paper, we propose a learning-based domain
decomposition method (L-DDM) that addresses this gap. Our approach uses a
single, pre-trained neural operator-originally trained on simple domains-as a
surrogate model within a domain decomposition scheme, allowing us to tackle
large and complicated domains efficiently. We provide a general theoretical
result on the existence of neural operator approximations in the context of
domain decomposition solution of abstract PDEs. We then demonstrate our method
by accurately approximating solutions to elliptic PDEs with discontinuous
microstructures in complex geometries, using a physics-pretrained neural
operator (PPNO). Our results show that this approach not only outperforms
current state-of-the-art methods on these challenging problems, but also offers
resolution-invariance and strong generalization to microstructural patterns
unseen during training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nearly Minimax Discrete Distribution <span class="highlight-title">Estimation</span> in Kullback-Leibler
  Divergence with High Probability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dirk van der Hoeven, Julia Olkhovskaia, Tim van Erven
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of estimating a discrete distribution $p$ with
support of size $K$ and provide both upper and lower bounds with high
probability in KL divergence. We prove that in the worst case, for any
estimator $\widehat{p}$, with probability at least $\delta$, $\text{KL}(p \|
\widehat{p}) \geq C\max\{K,\ln(K)\ln(1/\delta) \}/n $, where $n$ is the sample
size and $C > 0$ is a constant. We introduce a computationally efficient
estimator $p^{\text{OTB}}$, based on Online to Batch conversion and suffix
averaging, and show that with probability at least $1 - \delta$ $\text{KL}(p \|
\widehat{p}) \leq C(K\log(\log(K)) + \ln(K)\ln(1/\delta)) /n$.
  Furthermore, we also show that with sufficiently many observations relative
to $\log(1/\delta)$, the maximum likelihood estimator $\bar{p}$ guarantees that
with probability at least $1-\delta$ $$
  1/6 \chi^2(\bar{p}\|p) \leq 1/4 \chi^2(p\|\bar{p}) \leq \text{KL}(p|\bar{p})
\leq C(K + \log(1/\delta))/n\,, $$ where $\chi^2$ denotes the
$\chi^2$-divergence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Confounded Causal Imitation Learning with Instrumental Variables 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Zeng, Shenglan Nie, Feng Xie, Libo Huang, Peng Wu, Zhi Geng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning from demonstrations usually suffers from the confounding
effects of unmeasured variables (i.e., unmeasured confounders) on the states
and actions. If ignoring them, a biased estimation of the policy would be
entailed. To break up this confounding gap, in this paper, we take the best of
the strong power of instrumental variables (IV) and propose a Confounded Causal
Imitation Learning (C2L) model. This model accommodates confounders that
influence actions across multiple timesteps, rather than being restricted to
immediate temporal dependencies. We develop a two-stage imitation learning
framework for valid IV identification and policy optimization. In particular,
in the first stage, we construct a testing criterion based on the defined
pseudo-variable, with which we achieve identifying a valid IV for the C2L
models. Such a criterion entails the sufficient and necessary identifiability
conditions for IV validity. In the second stage, with the identified IV, we
propose two candidate policy learning approaches: one is based on a simulator,
while the other is offline. Extensive experiments verified the effectiveness of
identifying the valid IV as well as learning the policy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ R-Stitch: <span class="highlight-title">Dynamic</span> <span class="highlight-title">Trajectory</span> Stitching for Efficient Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuokun Chen, Zeren Chen, Jiahao He, Mingkui Tan, Jianfei Cai, Bohan Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-thought (CoT) reasoning enhances the problem-solving capabilities of
large language models by encouraging step-by-step intermediate reasoning during
inference. While effective, CoT introduces substantial computational overhead
due to its reliance on autoregressive decoding over long token sequences.
Existing acceleration strategies either reduce sequence length through early
stopping or compressive reward designs, or improve decoding speed via
speculative decoding with smaller models. However, speculative decoding suffers
from limited speedup when the agreement between small and large models is low,
and fails to exploit the potential advantages of small models in producing
concise intermediate reasoning. In this paper, we present R-Stitch, a
token-level, confidence-based hybrid decoding framework that accelerates CoT
inference by switching between a small language model (SLM) and a large
language model (LLM) along the reasoning trajectory. R-Stitch uses the SLM to
generate tokens by default and delegates to the LLM only when the SLM's
confidence falls below a threshold. This design avoids full-sequence rollback
and selectively invokes the LLM on uncertain steps, preserving both efficiency
and answer quality. R-Stitch is model-agnostic, training-free, and compatible
with standard decoding pipelines. Experiments on math reasoning benchmarks
demonstrate that R-Stitch achieves up to 85\% reduction in inference latency
with negligible accuracy drop, highlighting its practical effectiveness in
accelerating CoT reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Temporal Guidance and Iterative Refinement in Audio Source Separation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Morocutti, Jonathan Greif, Paul Primus, Florian Schmid, Gerhard Widmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial semantic segmentation of sound scenes (S5) involves the accurate
identification of active sound classes and the precise separation of their
sources from complex acoustic mixtures. Conventional systems rely on a
two-stage pipeline - audio tagging followed by label-conditioned source
separation - but are often constrained by the absence of fine-grained temporal
information critical for effective separation. In this work, we address this
limitation by introducing a novel approach for S5 that enhances the synergy
between the event detection and source separation stages. Our key contributions
are threefold. First, we fine-tune a pre-trained Transformer to detect active
sound classes. Second, we utilize a separate instance of this fine-tuned
Transformer to perform sound event detection (SED), providing the separation
module with detailed, time-varying guidance. Third, we implement an iterative
refinement mechanism that progressively enhances separation quality by
recursively reusing the separator's output from previous iterations. These
advancements lead to significant improvements in both audio tagging and source
separation performance, as demonstrated by our system's second-place finish in
Task 4 of the DCASE Challenge 2025. Our implementation and model checkpoints
are available in our GitHub repository: https://github.com/theMoro/dcase25task4 .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level
  Tactile Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianxin Bi, Kevin Yuchen Ma, Ce Hao, Mike Zheng Shou, Harold Soh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tactile feedback is generally recognized to be crucial for effective
interaction with the physical world. However, state-of-the-art
Vision-Language-Action (VLA) models lack the ability to interpret and use
tactile signals, limiting their effectiveness in contact-rich tasks.
Incorporating tactile feedback into these systems is challenging due to the
absence of large multi-modal datasets. We present VLA-Touch, an approach that
enhances generalist robot policies with tactile sensing \emph{without
fine-tuning} the base VLA. Our method introduces two key innovations: (1) a
pipeline that leverages a pretrained tactile-language model that provides
semantic tactile feedback for high-level task planning, and (2) a
diffusion-based controller that refines VLA-generated actions with tactile
signals for contact-rich manipulation. Through real-world experiments, we
demonstrate that our dual-level integration of tactile feedback improves task
planning efficiency while enhancing execution precision. Code is open-sourced
at \href{https://github.com/jxbi1010/VLA-Touch}{this URL}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Virtualization for Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saiful Khan, Joyraj Chakraborty, Philip Beaucamp, Niraj Bhujel, Min Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, machine learning (ML) teams have multiple concurrent ML workflows
for different applications. Each workflow typically involves many experiments,
iterations, and collaborative activities and commonly takes months and
sometimes years from initial data wrangling to model deployment.
Organizationally, there is a large amount of intermediate data to be stored,
processed, and maintained. \emph{Data virtualization} becomes a critical
technology in an infrastructure to serve ML workflows. In this paper, we
present the design and implementation of a data virtualization service,
focusing on its service architecture and service operations. The infrastructure
currently supports six ML applications, each with more than one ML workflow.
The data virtualization service allows the number of applications and workflows
to grow in the coming years.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decentralized Federated Learning of Probabilistic Generative Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aritz Pérez, Carlos Echegoyen, Guzmán Santafé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a paradigm of increasing relevance in real world
applications, aimed at building a global model across a network of
heterogeneous users without requiring the sharing of private data. We focus on
model learning over decentralized architectures, where users collaborate
directly to update the global model without relying on a central server. In
this context, the current paper proposes a novel approach to collaboratively
learn probabilistic generative classifiers with a parametric form. The
framework is composed by a communication network over a set of local nodes,
each of one having its own local data, and a local updating rule. The proposal
involves sharing local statistics with neighboring nodes, where each node
aggregates the neighbors' information and iteratively learns its own local
classifier, which progressively converges to a global model. Extensive
experiments demonstrate that the algorithm consistently converges to a globally
competitive model across a wide range of network topologies, network sizes,
local dataset sizes, and extreme non-i.i.d. data distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prolonging Tool <span class="highlight-title">Life</span>: Learning Skillful Use of General-purpose Tools
  through <span class="highlight-title">Life</span>span-guided <span class="highlight-title">Reinforcement</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Po-Yen Wu, Cheng-Yu Kuo, Yuki Kadokawa, Takamitsu Matsubara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In inaccessible environments with uncertain task demands, robots often rely
on general-purpose tools that lack predefined usage strategies. These tools are
not tailored for particular operations, making their longevity highly sensitive
to how they are used. This creates a fundamental challenge: how can a robot
learn a tool-use policy that both completes the task and prolongs the tool's
lifespan? In this work, we address this challenge by introducing a
reinforcement learning (RL) framework that incorporates tool lifespan as a
factor during policy optimization. Our framework leverages Finite Element
Analysis (FEA) and Miner's Rule to estimate Remaining Useful Life (RUL) based
on accumulated stress, and integrates the RUL into the RL reward to guide
policy learning toward lifespan-guided behavior. To handle the fact that RUL
can only be estimated after task execution, we introduce an Adaptive Reward
Normalization (ARN) mechanism that dynamically adjusts reward scaling based on
estimated RULs, ensuring stable learning signals. We validate our method across
simulated and real-world tool use tasks, including Object-Moving and
Door-Opening with multiple general-purpose tools. The learned policies
consistently prolong tool lifespan (up to 8.01x in simulation) and transfer
effectively to real-world settings, demonstrating the practical value of
learning lifespan-guided tool use strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Knowledge <span class="highlight-title">Graph</span>s and LLM Reasoning to Identify Operational
  Bottlenecks for Warehouse <span class="highlight-title">Planning</span> Assistance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishi Parekh, Saisubramaniam Gopalakrishnan, Zishan Ahmad, Anirudh Deodhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analyzing large, complex output datasets from Discrete Event Simulations
(DES) of warehouse operations to identify bottlenecks and inefficiencies is a
critical yet challenging task, often demanding significant manual effort or
specialized analytical tools. Our framework integrates Knowledge Graphs (KGs)
and Large Language Model (LLM)-based agents to analyze complex Discrete Event
Simulation (DES) output data from warehouse operations. It transforms raw DES
data into a semantically rich KG, capturing relationships between simulation
events and entities. An LLM-based agent uses iterative reasoning, generating
interdependent sub-questions. For each sub-question, it creates Cypher queries
for KG interaction, extracts information, and self-reflects to correct errors.
This adaptive, iterative, and self-correcting process identifies operational
issues mimicking human analysis. Our DES approach for warehouse bottleneck
identification, tested with equipment breakdowns and process irregularities,
outperforms baseline methods. For operational questions, it achieves
near-perfect pass rates in pinpointing inefficiencies. For complex
investigative questions, we demonstrate its superior diagnostic ability to
uncover subtle, interconnected issues. This work bridges simulation modeling
and AI (KG+LLM), offering a more intuitive method for actionable insights,
reducing time-to-insight, and enabling automated warehouse inefficiency
evaluation and diagnosis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pulse-PPG: An Open-Source <span class="highlight-title">Field</span>-Trained PPG Foundation Model for
  Wearable Applications Across Lab and <span class="highlight-title">Field</span> Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01108v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01108v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mithun Saha, Maxwell A. Xu, Wanting Mao, Sameer Neupane, James M. Rehg, Santosh Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photoplethysmography (PPG)-based foundation models are gaining traction due
to the widespread use of PPG in biosignal monitoring and their potential to
generalize across diverse health applications. In this paper, we introduce
Pulse-PPG, the first open-source PPG foundation model trained exclusively on
raw PPG data collected over a 100-day field study with 120 participants.
Existing PPG foundation models are either open-source but trained on clinical
data or closed-source, limiting their applicability in real-world settings. We
evaluate Pulse-PPG across multiple datasets and downstream tasks, comparing its
performance against a state-of-the-art foundation model trained on clinical
data. Our results demonstrate that Pulse-PPG, trained on uncurated field data,
exhibits superior generalization across clinical and mobile health applications
in both lab and field settings. This suggests that exposure to real-world
variability enables the model to learn fine-grained representations, making it
more adaptable across tasks. Furthermore, pre-training on field data
surprisingly outperforms its pre-training on clinical data in many tasks,
reinforcing the importance of training on real-world, diverse datasets. To
encourage further advancements in robust foundation models leveraging field
data, we plan to release Pulse-PPG, providing researchers with a powerful
resource for developing more generalizable PPG-based models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Saha and Xu are co-first authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analyzing Fairness of Computer Vision and Natural Language Processing
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09900v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09900v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Rashed, Abdelkrim Kallich, Mohamed Eltayeb
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) algorithms play a critical role in decision-making
across various domains, such as healthcare, finance, education, and law
enforcement. However, concerns about fairness and bias in these systems have
raised significant ethical and social challenges. To address these challenges,
this research utilizes two prominent fairness libraries, Fairlearn by Microsoft
and AIF360 by IBM. These libraries offer comprehensive frameworks for fairness
analysis, providing tools to evaluate fairness metrics, visualize results, and
implement bias mitigation algorithms. The study focuses on assessing and
mitigating biases for unstructured datasets using Computer Vision (CV) and
Natural Language Processing (NLP) models. The primary objective is to present a
comparative analysis of the performance of mitigation algorithms from the two
fairness libraries. This analysis involves applying the algorithms
individually, one at a time, in one of the stages of the ML lifecycle,
pre-processing, in-processing, or post-processing, as well as sequentially
across more than one stage. The results reveal that some sequential
applications improve the performance of mitigation algorithms by effectively
reducing bias while maintaining the model's performance. Publicly available
datasets from Kaggle were chosen for this research, providing a practical
context for evaluating fairness in real-world machine learning workflows.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 8 table, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Safe Strategies for Value Maximizing Buyers in Uniform Price
  Auctions <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03674v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03674v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Negin Golrezaei, Sourav Sahoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the bidding problem in repeated uniform price multi-unit auctions
from the perspective of a value-maximizing buyer. The buyer aims to maximize
their cumulative value over $T$ rounds while adhering to per-round
return-on-investment (RoI) constraints in a strategic (or adversarial)
environment. Using an $m$-uniform bidding format, the buyer submits $m$
bid-quantity pairs $(b_i, q_i)$ to demand $q_i$ units at bid $b_i$, with $m \ll
M$ in practice, where $M$ denotes the maximum demand of the buyer.
  We introduce the notion of safe bidding strategies as those that satisfy the
RoI constraints irrespective of competing bids. Despite the stringent
requirement, we show that these strategies satisfy a mild no-overbidding
condition, depend only on the valuation curve of the bidder, and the bidder can
focus on a finite subset without loss of generality. Though the subset size is
$O(M^m)$, we design a polynomial-time learning algorithm that achieves
sublinear regret, both in full-information and bandit settings, relative to the
hindsight-optimal safe strategy.
  We assess the robustness of safe strategies against the hindsight-optimal
strategy from a richer class. We define the richness ratio $\alpha \in (0,1]$
as the minimum ratio of the value of the optimal safe strategy to that of the
optimal strategy from richer class and construct hard instances showing the
tightness of $\alpha$. Our algorithm achieves $\alpha$-approximate sublinear
regret against these stronger benchmarks. Simulations on semi-synthetic auction
data show that empirical richness ratios significantly outperform the
theoretical worst-case bounds. The proposed safe strategies and learning
algorithm extend naturally to more nuanced buyer and competitor models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>84 pages, 5 figures. Appeared at ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task Priors: Enhancing Model <span class="highlight-title">Evaluation</span> by Considering the Entire Space
  of Downstream Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.09871v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.09871v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niket Patel, Randall Balestriero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The grand goal of AI research, and particularly Self Supervised Learning
(SSL), is to produce systems that can successfully solve any possible task. In
contrast, current evaluation methods available to AI researchers typically rely
on a fixed collection of hand-picked downstream benchmarks. Hence, a large
amount of effort is put into designing and searching for large collection of
evaluation tasks that can serve as a proxy of our grand goal. We argue that
such a rigid evaluation protocol creates a silent bottleneck in AI research. To
remedy that, we define a probabilistic space of downstream tasks obtained by
adopting a distribution of tasks and by defining Task Priors. Under this view,
one can evaluate a model's performance over the set of all possible downstream
tasks. Our framework is the first to provide answers to key questions such as
(i) what is the average performance of my model over all possible downstream
tasks weighted by the probability to encounter each task? or (ii) what is the
variance of my model's performance across all downstream tasks under the
defined Task Priors? Beyond establishing a new standard for evaluation, we
believe that Task Priors will accelerate the pace of research in SSL - where
downstream task evaluation is the sole qualitative signal that researchers have
access to.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tuning Sequential Monte Carlo Samplers via Greedy Incremental Divergence
  Minimization <span class="chip">ICML'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.15704v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.15704v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyurae Kim, Zuheng Xu, Jacob R. Gardner, Trevor Campbell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of sequential Monte Carlo (SMC) samplers heavily depends on
the tuning of the Markov kernels used in the path proposal. For SMC samplers
with unadjusted Markov kernels, standard tuning objectives, such as the
Metropolis-Hastings acceptance rate or the expected-squared jump distance, are
no longer applicable. While stochastic gradient-based end-to-end optimization
has been explored for tuning SMC samplers, they often incur excessive training
costs, even for tuning just the kernel step sizes. In this work, we propose a
general adaptation framework for tuning the Markov kernels in SMC samplers by
minimizing the incremental Kullback-Leibler (KL) divergence between the
proposal and target paths. For step size tuning, we provide a gradient- and
tuning-free algorithm that is generally applicable for kernels such as Langevin
Monte Carlo (LMC). We further demonstrate the utility of our approach by
providing a tailored scheme for tuning kinetic LMC used in SMC samplers. Our
implementations are able to obtain a full schedule of tuned parameters at the
cost of a few vanilla SMC runs, which is a fraction of gradient-based
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML'25; v4: fixed typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EEG Foundation Models: A Critical <span class="highlight-title">Review</span> of Current Progress and Future
  Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.11783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.11783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gayal Kuruppu, Neeraj Wagh, Yogatheesan Varatharajah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Patterns of electrical brain activity recorded via electroencephalography
(EEG) offer immense value for scientific and clinical investigations. The
inability of supervised EEG encoders to learn robust EEG patterns and their
over-reliance on expensive signal annotations have sparked a transition towards
general-purpose self-supervised EEG encoders, i.e., EEG foundation models
(EEG-FMs), for robust and scalable EEG feature extraction. However, the
real-world readiness of early EEG-FMs and the rubric for long-term research
progress remain unclear. A systematic and comprehensive review of
first-generation EEG-FMs is therefore necessary to understand the current
state-of-the-art and identify key directions for future EEG-FMs. To that end,
this study reviews 10 early EEG-FMs and presents a critical synthesis of their
methodology, empirical findings, and outstanding research gaps. We find that
most EEG-FMs adopt a sequence-based modeling scheme that relies on
transformer-based backbones and the reconstruction of masked sequences for
self-supervision. However, model evaluations remain heterogeneous and largely
limited, making it challenging to assess their practical off-the-shelf utility.
In addition to adopting standardized and realistic evaluations, future work
should demonstrate more substantial scaling effects and make principled and
trustworthy choices throughout the EEG representation learning pipeline. We
believe that developing benchmarks, software tools, technical methodologies,
and applications in collaboration with domain experts may further advance the
translational utility and real-world adoption of EEG-FMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 5 figures, 3 tables (main + supplement)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepCrossAttention: Supercharging Transformer Residual Connections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06785v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06785v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mike Heddes, Adel Javanmard, Kyriakos Axiotis, Gang Fu, MohammadHossein Bateni, Vahab Mirrokni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer networks have achieved remarkable success across diverse domains,
leveraging a variety of architectural innovations, including residual
connections. However, traditional residual connections, which simply sum the
outputs of previous layers, can dilute crucial information. This work
introduces DeepCrossAttention (DCA), an approach that enhances residual
learning in transformers. DCA employs learnable, input-dependent weights to
dynamically combine layer outputs, enabling the model to selectively focus on
the most relevant information in any of the previous layers. Furthermore, DCA
incorporates depth-wise cross-attention, allowing for richer interactions
between layers at different depths. Our language modeling experiments show that
DCA achieves improved perplexity for a given training time. Moreover, DCA
obtains the same model quality up to 3x faster while adding a negligible number
of parameters. Theoretical analysis confirms that DCA provides an improved
trade-off between accuracy and model size when the ratio of collective layer
ranks to the ambient dimension falls below a critical threshold.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PerceptionLM: Open-Access Data and Models for Detailed Visual
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.13180v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.13180v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jang Hyun Cho, Andrea Madotto, Effrosyni Mavroudi, Triantafyllos Afouras, Tushar Nagarajan, Muhammad Maaz, Yale Song, Tengyu Ma, Shuming Hu, Suyog Jain, Miguel Martin, Huiyu Wang, Hanoona Rasheed, Peize Sun, Po-Yao Huang, Daniel Bolya, Nikhila Ravi, Shashank Jain, Tammy Stark, Shane Moon, Babak Damavandi, Vivian Lee, Andrew Westbury, Salman Khan, Philipp Krähenbühl, Piotr Dollár, Lorenzo Torresani, Kristen Grauman, Christoph Feichtenhofer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models are integral to computer vision research, yet many
high-performing models remain closed-source, obscuring their data, design and
training recipe. The research community has responded by using distillation
from black-box models to label training data, achieving strong benchmark
results, at the cost of measurable scientific progress. However, without
knowing the details of the teacher model and its data sources, scientific
progress remains difficult to measure. In this paper, we study building a
Perception Language Model (PLM) in a fully open and reproducible framework for
transparent research in image and video understanding. We analyze standard
training pipelines without distillation from proprietary models and explore
large-scale synthetic data to identify critical data gaps, particularly in
detailed video understanding. To bridge these gaps, we release 2.8M
human-labeled instances of fine-grained video question-answer pairs and
spatio-temporally grounded video captions. Additionally, we introduce
PLM-VideoBench, a suite for evaluating challenging video understanding tasks
focusing on the ability to reason about "what", "where", "when", and "how" of a
video. We make our work fully reproducible by providing data, training recipes,
code & models. https://github.com/facebookresearch/perception_models
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Individual Reproductive Behavior from Aggregate Fertility Rates
  via Neural Posterior <span class="highlight-title">Estimation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.22607v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.22607v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Ciganda, Ignacio Campón, Iñaki Permanyer, Jakob H Macke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Age-specific fertility rates (ASFRs) provide the most extensive record of
reproductive change, but their aggregate nature obscures the individual-level
behavioral mechanisms that drive fertility trends. To bridge this micro-macro
divide, we introduce a likelihood-free Bayesian framework that couples a
demographically interpretable, individual-level simulation model of the
reproductive process with Sequential Neural Posterior Estimation (SNPE). We
show that this framework successfully recovers core behavioral parameters
governing contemporary fertility, including preferences for family size,
reproductive timing, and contraceptive failure, using only ASFRs. The
framework's effectiveness is validated on cohorts from four countries with
diverse fertility regimes. Most compellingly, the model, estimated solely on
aggregate data, successfully predicts out-of-sample distributions of
individual-level outcomes, including age at first sex, desired family size, and
birth intervals. Because our framework yields complete synthetic life
histories, it significantly reduces the data requirements for building
microsimulation models and enables behaviorally explicit demographic forecasts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Streaming, Fast and Slow: Cognitive Load-Aware Streaming for Efficient
  LLM Serving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.17999v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.17999v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Xiao, Brenda Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative conversational interfaces powered by large language models (LLMs)
typically stream output token-by-token at a rate determined by computational
budget, often neglecting actual human reading speeds and the cognitive load
associated with the content. This mismatch frequently leads to inefficient use
of computational resources. For example, in cloud-based services, streaming
content faster than users can read appears unnecessary, resulting in wasted
computational resources and potential delays for other users, particularly
during peak usage periods. To address this issue, we propose an adaptive
streaming method that dynamically adjusts the pacing of LLM streaming output in
real-time based on inferred cognitive load. Our approach estimates the
cognitive load associated with streaming content and strategically slows down
the stream during complex or information-rich segments, thereby freeing
computational resources for other users. We conducted a statistical analysis
and simulation based on a statistical model derived from data collected in a
crowdsourced user study across various types of LLM-generated content. Our
results show that this adaptive method can effectively reduce computational
consumption while largely maintaining streaming speed above user's normal
reading speed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PALADIN : <span class="highlight-title">Robust</span> Neural Fingerprinting for Text-to-Image Dif<span class="highlight-title">fusion</span>
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03170v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03170v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Murthy L, Subarna Tripathi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The risk of misusing text-to-image generative models for malicious uses,
especially due to the open-source development of such models, has become a
serious concern. As a risk mitigation strategy, attributing generative models
with neural fingerprinting is emerging as a popular technique. There has been a
plethora of recent work that aim for addressing neural fingerprinting. A
trade-off between the attribution accuracy and generation quality of such
models has been studied extensively. None of the existing methods yet achieved
100% attribution accuracy. However, any model with less than cent percent
accuracy is practically non-deployable. In this work, we propose an accurate
method to incorporate neural fingerprinting for text-to-image diffusion models
leveraging the concepts of cyclic error correcting codes from the literature of
coding theory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Choosing Public <span class="highlight-title">Dataset</span>s for Private Machine Learning via Gradient
  Subspace Distance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01256v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01256v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Gu, Gautam Kamath, Zhiwei Steven Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentially private stochastic gradient descent privatizes model training
by injecting noise into each iteration, where the noise magnitude increases
with the number of model parameters. Recent works suggest that we can reduce
the noise by leveraging public data for private machine learning, by projecting
gradients onto a subspace prescribed by the public data. However, given a
choice of public datasets, it is not a priori clear which one may be most
appropriate for the private task. We give an algorithm for selecting a public
dataset by measuring a low-dimensional subspace distance between gradients of
the public and private examples. We provide theoretical analysis demonstrating
that the excess risk scales with this subspace distance. This distance is easy
to compute and robust to modifications in the setting. Empirical evaluation
shows that trained model accuracy is monotone in this distance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to SaTML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01939v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01939v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaosheng Zhao, Yang Huang, Guirong Xue, Xiao Kong, Jifeng Liu, Xiaoyu Tang, Timothy C. Beers, Yuan-Sen Ting, A-Li Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, large language models (LLMs) have transformed natural
language understanding through vast datasets and large-scale parameterization.
Inspired by this success, we present SpecCLIP, a foundation model framework
that extends LLM-inspired methodologies to stellar spectral analysis. Stellar
spectra, akin to structured language, encode rich physical and chemical
information about stars. By training foundation models on large-scale spectral
datasets, our goal is to learn robust and informative embeddings that support
diverse downstream applications. As a proof of concept, SpecCLIP involves
pre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed
by contrastive alignment using the CLIP (Contrastive Language-Image
Pre-training) framework, adapted to associate spectra from different
instruments. This alignment is complemented by auxiliary decoders that preserve
spectrum-specific information and enable translation (prediction) between
spectral types, with the former achieved by maximizing mutual information
between embeddings and input spectra. The result is a cross-spectrum framework
enabling intrinsic calibration and flexible applications across instruments. We
demonstrate that fine-tuning these models on moderate-sized labeled datasets
improves adaptability to tasks such as stellar-parameter estimation and
chemical-abundance determination. SpecCLIP also enhances the accuracy and
precision of parameter estimates benchmarked against external survey data.
Additionally, its similarity search and cross-spectrum prediction capabilities
offer potential for anomaly detection. Our results suggest that contrastively
trained foundation models enriched with spectrum-aware decoders can advance
precision stellar spectroscopy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 8 figures, 5 tables. Updated with minor corrections to flux
  normalization, and to related tables and figures. Submitted to AAS Journals.
  Comments welcome</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Generalist <span class="highlight-title">Robot</span> Learning from Internet Video: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.19664v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.19664v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert McCarthy, Daniel C. H. Tan, Dominik Schmidt, Fernando Acero, Nathan Herr, Yilun Du, Thomas G. Thuruthel, Zhibin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling deep learning to massive and diverse internet data has driven
remarkable breakthroughs in domains such as video generation and natural
language processing. Robot learning, however, has thus far failed to replicate
this success and remains constrained by a scarcity of available data. Learning
from videos (LfV) methods aim to address this data bottleneck by augmenting
traditional robot data with large-scale internet video. This video data
provides foundational information regarding physical dynamics, behaviours, and
tasks, and can be highly informative for general-purpose robots.
  This survey systematically examines the emerging field of LfV. We first
outline essential concepts, including detailing fundamental LfV challenges such
as distribution shift and missing action labels in video data. Next, we
comprehensively review current methods for extracting knowledge from
large-scale internet video, overcoming LfV challenges, and improving robot
learning through video-informed training. The survey concludes with a critical
discussion of future opportunities. Here, we emphasize the need for scalable
foundation model approaches that can leverage the full range of available
internet video and enhance the learning of robot policies and dynamics models.
Overall, the survey aims to inform and catalyse future LfV research, driving
progress towards general-purpose robots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flow-Based Single-Step Completion for Efficient and Expressive Policy
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.21427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.21427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prajwal Koirala, Cody Fleming
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models such as diffusion and flow-matching offer expressive
policies for offline reinforcement learning (RL) by capturing rich, multimodal
action distributions, but their iterative sampling introduces high inference
costs and training instability due to gradient propagation across sampling
steps. We propose the \textit{Single-Step Completion Policy} (SSCP), a
generative policy trained with an augmented flow-matching objective to predict
direct completion vectors from intermediate flow samples, enabling accurate,
one-shot action generation. In an off-policy actor-critic framework, SSCP
combines the expressiveness of generative models with the training and
inference efficiency of unimodal policies, without requiring long
backpropagation chains. Our method scales effectively to offline,
offline-to-online, and online RL settings, offering substantial gains in speed
and adaptability over diffusion-based baselines. We further extend SSCP to
goal-conditioned RL, enabling flat policies to exploit subgoal structures
without explicit hierarchical inference. SSCP achieves strong results across
standard offline RL and behavior cloning benchmarks, positioning it as a
versatile, expressive, and efficient framework for deep RL and sequential
decision-making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Challenges learning from imbalanced data using tree-based models:
  Prevalence estimates systematically depend on hyperparameters and can be
  upwardly biased 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16209v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16209v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Phelps, Daniel J. Lizotte, Douglas G. Woolford
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imbalanced binary classification problems arise in many fields of study. When
using machine learning models for these problems, it is common to subsample the
majority class (i.e., undersampling) to create a (more) balanced dataset for
model training. This biases the model's predictions because the model learns
from a dataset that does not follow the same data generating process as new
data. One way of accounting for this bias is to analytically map the resulting
predictions to new values based on the sampling rate for the majority class,
which was used to create the training dataset. While this approach may work
well for some machine learning models, we show that calibrating a random forest
this way has unintended negative consequences, including prevalence estimates
that can be upwardly biased. These prevalence estimates depend on both i) the
number of predictors considered at each split in the random forest; and ii) the
sampling rate used. We explain the former using known properties of random
forests and analytical calibration. However, in investigating the latter issue,
we made a surprising discovery - contrary to the widespread belief that
decision trees are biased towards the majority class, they actually can be
biased towards the minority class.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Impact of Feature Scaling In Machine Learning: Effects on Regression
  and Classification Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08274v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08274v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João Manoel Herrera Pinheiro, Suzana Vilas Boas de Oliveira, Thiago Henrique Segreto Silva, Pedro Antonio Rabelo Saraiva, Enzo Ferreira de Souza, Ricardo V. Godoy, Leonardo André Ambrosio, Marcelo Becker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research addresses the critical lack of comprehensive studies on feature
scaling by systematically evaluating 12 scaling techniques - including several
less common transformations - across 14 different Machine Learning algorithms
and 16 datasets for classification and regression tasks. We meticulously
analyzed impacts on predictive performance (using metrics such as accuracy,
MAE, MSE, and $R^2$) and computational costs (training time, inference time,
and memory usage). Key findings reveal that while ensemble methods (such as
Random Forest and gradient boosting models like XGBoost, CatBoost and LightGBM)
demonstrate robust performance largely independent of scaling, other widely
used models such as Logistic Regression, SVMs, TabNet, and MLPs show
significant performance variations highly dependent on the chosen scaler. This
extensive empirical analysis, with all source code, experimental results, and
model parameters made publicly available to ensure complete transparency and
reproducibility, offers model-specific crucial guidance to practitioners on the
need for an optimal selection of feature scaling techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dif<span class="highlight-title">fusion</span> Factor Models: Generating High-Dimensional Returns with Factor
  Structure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.06566v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.06566v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minshuo Chen, Renyuan Xu, Yumin Xu, Ruixun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Financial scenario simulation is essential for risk management and portfolio
optimization, yet it remains challenging especially in high-dimensional and
small data settings common in finance. We propose a diffusion factor model that
integrates latent factor structure into generative diffusion processes,
bridging econometrics with modern generative AI to address the challenges of
the curse of dimensionality and data scarcity in financial simulation. By
exploiting the low-dimensional factor structure inherent in asset returns, we
decompose the score function--a key component in diffusion models--using
time-varying orthogonal projections, and this decomposition is incorporated
into the design of neural network architectures. We derive rigorous statistical
guarantees, establishing nonasymptotic error bounds for both score estimation
at O(d^{5/2} n^{-2/(k+5)}) and generated distribution at O(d^{5/4}
n^{-1/2(k+5)}), primarily driven by the intrinsic factor dimension k rather
than the number of assets d, surpassing the dimension-dependent limits in the
classical nonparametric statistics literature and making the framework viable
for markets with thousands of assets. Numerical studies confirm superior
performance in latent subspace recovery under small data regimes. Empirical
analysis demonstrates the economic significance of our framework in
constructing mean-variance optimal portfolios and factor portfolios. This work
presents the first theoretical integration of factor structure with diffusion
models, offering a principled approach for high-dimensional financial
simulation with limited data. Our code is available at
https://github.com/xymmmm00/diffusion_factor_model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Balans: Multi-Armed Bandits-based Adaptive Large Neighborhood Search for
  Mixed-Integer Programming Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14382v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14382v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyang Cai, Serdar Kadioglu, Bistra Dilkina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixed-integer programming (MIP) is a powerful paradigm for modeling and
solving various important combinatorial optimization problems. Recently,
learning-based approaches have shown a potential to speed up MIP solving via
offline training that then guides important design decisions during the search.
However, a significant drawback of these methods is their heavy reliance on
offline training, which requires collecting training datasets and
computationally costly training epochs yet offering only limited generalization
to unseen (larger) instances. In this paper, we propose Balans, an adaptive
meta-solver for MIPs with online learning capability that does not require any
supervision or apriori training. At its core, Balans is based on adaptive
large-neighborhood search, operating on top of an MIP solver by successive
applications of destroy and repair neighborhood operators. During the search,
the selection among different neighborhood definitions is guided on the fly for
the instance at hand via multi-armed bandit algorithms. Our extensive
experiments on hard optimization instances show that Balans offers significant
performance gains over the default MIP solver, is better than committing to any
single best neighborhood, and improves over the state-of-the-art
large-neighborhood search for MIPs. Finally, we release Balans as a highly
configurable, MIP solver agnostic, open-source software.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Mathematical Theory of Discursive Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06565v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06565v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan B. Gutiérrez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) turn writing into a live exchange between humans
and software. We characterize this new medium as a discursive network that
treats people and LLMs as equal nodes and tracks how their statements
circulate. We define the generation of erroneous information as invalidation
(any factual, logical, or structural breach) and show it follows four hazards:
drift from truth, self-repair, fresh fabrication, and external detection. We
develop a general mathematical model of discursive networks that shows that a
network governed only by drift and self-repair stabilizes at a modest error
rate. Giving each false claim even a small chance of peer review shifts the
system to a truth-dominant state. We operationalize peer review with the
open-source Flaws-of-Others (FOO) algorithm: a configurable loop in which any
set of agents critique one another while a harmonizer merges their verdicts. We
identify an ethical transgression, epithesis, that occurs when humans fail to
engage in the discursive network. The takeaway is practical and cultural:
reliability in this new medium comes not from perfecting single models but from
connecting imperfect ones into networks that enforce mutual accountability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 4 figures, 4 tables, 3 algorithm, 61 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-<span class="highlight-title">Trajectory</span> Inverse <span class="highlight-title">Reinforcement</span> Learning: Learn Incrementally Before
  An Ongoing <span class="highlight-title">Trajectory</span> Terminates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15612v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15612v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shicheng Liu, Minghui Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse reinforcement learning (IRL) aims to learn a reward function and a
corresponding policy that best fit the demonstrated trajectories of an expert.
However, current IRL works cannot learn incrementally from an ongoing
trajectory because they have to wait to collect at least one complete
trajectory to learn. To bridge the gap, this paper considers the problem of
learning a reward function and a corresponding policy while observing the
initial state-action pair of an ongoing trajectory and keeping updating the
learned reward and policy when new state-action pairs of the ongoing trajectory
are observed. We formulate this problem as an online bi-level optimization
problem where the upper level dynamically adjusts the learned reward according
to the newly observed state-action pairs with the help of a meta-regularization
term, and the lower level learns the corresponding policy. We propose a novel
algorithm to solve this problem and guarantee that the algorithm achieves
sub-linear local regret $O(\sqrt{T}+\log T+\sqrt{T}\log T)$. If the reward
function is linear, we prove that the proposed algorithm achieves sub-linear
regret $O(\log T)$. Experiments are used to validate the proposed algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LoX: Low-Rank Extrapolation <span class="highlight-title">Robust</span>ifies LLM Safety Against Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel J. Perin, Runjin Chen, Xuxi Chen, Nina S. T. Hirata, Zhangyang Wang, Junyuan Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have become indispensable in real-world
applications. However, their widespread adoption raises significant safety
concerns, particularly in responding to socially harmful questions. Despite
substantial efforts to improve model safety through alignment, aligned models
can still have their safety protections undermined by subsequent fine-tuning -
even when the additional training data appears benign. In this paper, we
empirically demonstrate that this vulnerability stems from the sensitivity of
safety-critical low-rank subspaces in LLM parameters to fine-tuning. Building
on this insight, we propose a novel training-free method, termed Low-Rank
Extrapolation (LoX), to enhance safety robustness by extrapolating the safety
subspace of an aligned LLM. Our experimental results confirm the effectiveness
of LoX, demonstrating significant improvements in robustness against both
benign and malicious fine-tuning attacks while preserving the model's
adaptability to new tasks. For instance, LoX leads to 11% to 54% absolute
reductions in attack success rates (ASR) facing benign or malicious fine-tuning
attacks. By investigating the ASR landscape of parameters, we attribute the
success of LoX to that the extrapolation moves LLM parameters to a flatter
zone, thereby less sensitive to perturbations. The code is available at
github.com/VITA-Group/LoX.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RAPID-Net: Accurate Pocket Identification for Binding-Site-Agnostic
  Docking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02371v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02371v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaroslav Balytskyi, Inna Hubenko, Alina Balytska, Christopher V. Kelly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate identification of druggable pockets and their features is essential
for structure-based drug design and effective downstream docking. Here, we
present RAPID-Net, a deep learning-based algorithm designed for the accurate
prediction of binding pockets and seamless integration with docking pipelines.
On the PoseBusters benchmark, RAPID-Net-guided AutoDock Vina achieves 54.9% of
Top-1 poses with RMSD < 2 A and satisfying the PoseBusters chemical-validity
criterion, compared to 49.1% for DiffBindFR. On the most challenging time split
of PoseBusters aiming to assess generalization ability (structures submitted
after September 30, 2021), RAPID-Net-guided AutoDock Vina achieves 53.1% of
Top-1 poses with RMSD < 2 A and PB-valid, versus 59.5% for AlphaFold 3.
Notably, in 92.2% of cases, RAPID-Net-guided Vina samples at least one pose
with RMSD < 2 A (regardless of its rank), indicating that pose ranking, rather
than sampling, is the primary accuracy bottleneck. The lightweight inference,
scalability, and competitive accuracy of RAPID-Net position it as a viable
option for large-scale virtual screening campaigns. Across diverse benchmark
datasets, RAPID-Net outperforms other pocket prediction tools, including
PUResNet and Kalasanty, in both docking accuracy and pocket-ligand intersection
rates. Furthermore, we demonstrate the potential of RAPID-Net to accelerate the
development of novel therapeutics by highlighting its performance on
pharmacologically relevant targets. RAPID-Net accurately identifies distal
functional sites, offering new opportunities for allosteric inhibitor design.
In the case of the RNA-dependent RNA polymerase of SARS-CoV-2, RAPID-Net
uncovers a wider array of potential binding pockets than existing predictors,
which typically annotate only the orthosteric pocket and overlook secondary
cavities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Lipschitz Constant of Deep Networks and Double Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.12309v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.12309v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Gamba, Hossein Azizpour, Mårten Björkman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing bounds on the generalization error of deep networks assume some form
of smooth or bounded dependence on the input variable, falling short of
investigating the mechanisms controlling such factors in practice. In this
work, we present an extensive experimental study of the empirical Lipschitz
constant of deep networks undergoing double descent, and highlight
non-monotonic trends strongly correlating with the test error. Building a
connection between parameter-space and input-space gradients for SGD around a
critical point, we isolate two important factors -- namely loss landscape
curvature and distance of parameters from initialization -- respectively
controlling optimization dynamics around a critical point and bounding model
function complexity, even beyond the training data. Our study presents novels
insights on implicit regularization via overparameterization, and effective
model complexity for networks trained in practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unified Convergence Theory of Stochastic and Variance-Reduced Cubic
  Newton Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.11962v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.11962v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        El Mahdi Chayti, Nikita Doikov, Martin Jaggi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study stochastic Cubic Newton methods for solving general possibly
non-convex minimization problems. We propose a new framework, which we call the
helper framework, that provides a unified view of the stochastic and
variance-reduced second-order algorithms equipped with global complexity
guarantees. It can also be applied to learning with auxiliary information. Our
helper framework offers the algorithm designer high flexibility for
constructing and analyzing the stochastic Cubic Newton methods, allowing
arbitrary size batches, and the use of noisy and possibly biased estimates of
the gradients and Hessians, incorporating both the variance reduction and the
lazy Hessian updates. We recover the best-known complexities for the stochastic
and variance-reduced Cubic Newton, under weak assumptions on the noise. A
direct consequence of our theory is the new lazy stochastic second-order
method, which significantly improves the arithmetic complexity for large
dimension problems. We also establish complexity bounds for the classes of
gradient-dominated objectives, that include convex and strongly convex
problems. For Auxiliary Learning, we show that using a helper (auxiliary
function) can outperform training alone if a given similarity measure is small.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning Research</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward a Lightweight and <span class="highlight-title">Robust</span> Design for Caching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16242v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16242v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Chen, Hailiang Zhao, Jiaji Zhang, Xueyan Tang, Yixuan Wang, Shuiguang Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The online caching problem aims to minimize cache misses when serving a
sequence of requests under a limited cache size. While naive learning-augmented
caching algorithms achieve ideal $1$-consistency, they lack robustness
guarantees. Existing robustification methods either sacrifice $1$-consistency
or introduce significant computational overhead. In this paper, we introduce
Guard, a lightweight robustification framework that enhances the robustness of
a broad class of learning-augmented caching algorithms to $2H_k + 2$, while
preserving their $1$-consistency. Guard achieves the current best-known
trade-off between consistency and robustness, with only $O(1)$ additional
per-request overhead, thereby maintaining the original time complexity of the
base algorithm. Extensive experiments across multiple real-world datasets and
prediction models validate the effectiveness of Guard in practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Learning Classification and Portfolio Allocation: with
  Implications from Machine Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.02283v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.02283v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Bai, Kuntara Pukthuanthong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We use multi-class machine learning classifiers to identify the stocks that
outperform or underperform other stocks. The resulting long-short portfolios
achieve annual Sharpe ratios of 1.67 (value-weighted) and 3.35
(equal-weighted), with annual alphas ranging from 29\% to 48\%. These results
persist after controlling for machine learning regressions and remain robust
among large-cap stocks. Machine uncertainty, as measured by predicted
probabilities, impairs the prediction performance. Stocks with higher machine
uncertainty experience lower returns, particularly when human proxies of
information uncertainty align with machine uncertainty. Consistent with the
literature, such an effect is driven by the past underperformers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Trusted <span class="highlight-title">Multi-view</span> Learning under Noisy Supervision <span class="chip">IJCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11944v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11944v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilin Zhang, Cai Xu, Han Jiang, Ziyu Guan, Wei Zhao, Xiaofei He, Murat Sensoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-view learning methods often focus on improving decision accuracy while
neglecting the decision uncertainty, which significantly restricts their
applications in safety-critical scenarios. To address this, trusted multi-view
learning methods estimate prediction uncertainties by learning class
distributions from each instance. However, these methods heavily rely on high
quality ground-truth labels. This motivates us to delve into a new problem: how
to develop a reliable multi-view learning model under the guidance of noisy
labels? We propose the Trusted Multi view Noise Refining (TMNR) method to
address this challenge by modeling label noise arising from low-quality data
features and easily-confused classes. TMNR employs evidential deep neural
networks to construct view-specific opinions that capture both beliefs and
uncertainty. These opinions are then transformed through noise correlation
matrices to align with the noisy supervision, where matrix elements are
constrained by sample uncertainty to reflect label reliability. Furthermore,
considering the challenge of jointly optimizing the evidence network and noise
correlation matrices under noisy supervision, we further propose Trusted
Multi-view Noise Re-Refining (TMNR^2 ), which disentangles this complex
co-training problem by establishing different training objectives for distinct
modules. TMNR^2 identifies potentially mislabeled samples through
evidence-label consistency and generates pseudo-labels from neighboring
information. By assigning clean samples to optimize evidential networks and
noisy samples to guide noise correlation matrices, respectively, TMNR^2 reduces
mapping interference and achieves stabilizes training. Experimental results
demonstrate that TMNR^2 significantly outperforms baseline methods, with
average accuracy improvements of 7% on datasets with 50% label noise.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2 12 pages, accepted at IJCAI 2024; v3 is currently under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HyDRA: A Hybrid-Driven Reasoning Architecture for Verifiable Knowledge
  <span class="highlight-title">Graph</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15917v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15917v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Kaiser, Claudiu Leoveanu-Condrei, Ryan Gold, Marius-Constantin Dinu, Markus Hofmarcher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The synergy between symbolic knowledge, often represented by Knowledge Graphs
(KGs), and the generative capabilities of neural networks is central to
advancing neurosymbolic AI. A primary bottleneck in realizing this potential is
the difficulty of automating KG construction, which faces challenges related to
output reliability, consistency, and verifiability. These issues can manifest
as structural inconsistencies within the generated graphs, such as the
formation of disconnected $\textit{isolated islands}$ of data or the inaccurate
conflation of abstract classes with specific instances. To address these
challenges, we propose HyDRA, a $\textbf{Hy}$brid-$\textbf{D}$riven
$\textbf{R}$easoning $\textbf{A}$rchitecture designed for verifiable KG
automation. Given a domain or an initial set of documents, HyDRA first
constructs an ontology via a panel of collaborative neurosymbolic agents. These
agents collaboratively agree on a set of competency questions (CQs) that define
the scope and requirements the ontology must be able to answer. Given these
CQs, we build an ontology graph that subsequently guides the automated
extraction of triplets for KG generation from arbitrary documents. Inspired by
design-by-contracts (DbC) principles, our method leverages verifiable contracts
as the primary control mechanism to steer the generative process of Large
Language Models (LLMs). To verify the output of our approach, we extend beyond
standard benchmarks and propose an evaluation framework that assesses the
functional correctness of the resulting KG by leveraging symbolic verifications
as described by the neurosymbolic AI framework, $\textit{SymbolicAI}$. This
work contributes a hybrid-driven architecture for improving the reliability of
automated KG construction and the exploration of evaluation methods for
measuring the functional integrity of its output. The code is publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ First, Learn What You Don't Know: Active Information Gathering for
  Driving at the Limits of Handling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00107v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00107v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Davydov, Franck Djeumou, Marcus Greiff, Makoto Suminaka, Michael Thompson, John Subosits, Thomas Lew
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combining data-driven models that adapt online and model predictive control
(MPC) has enabled effective control of nonlinear systems. However, when
deployed on unstable systems, online adaptation may not be fast enough to
ensure reliable simultaneous learning and control. For example, a controller on
a vehicle executing highly dynamic maneuvers--such as drifting to avoid an
obstacle--may push the vehicle's tires to their friction limits, destabilizing
the vehicle and allowing modeling errors to quickly compound and cause a loss
of control. To address this challenge, we present an active information
gathering framework for identifying vehicle dynamics as quickly as possible. We
propose an expressive vehicle dynamics model that leverages Bayesian last-layer
meta-learning to enable rapid online adaptation. The model's uncertainty
estimates are used to guide informative data collection and quickly improve the
model prior to deployment. Dynamic drifting experiments on a Toyota Supra show
that (i) the framework enables reliable control of a vehicle at the edge of
stability, (ii) online adaptation alone may not suffice for zero-shot control
and can lead to undesirable transient errors or spin-outs, and (iii) active
data collection helps achieve reliable performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Constructing Optimal Noise Channels for Enhanced <span class="highlight-title">Robust</span>ness in Quantum
  Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16417v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16417v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Winderl, Nicola Franco, Jeanette Miriam Lorenz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of Quantum Machine Learning (QML), the critical
need to enhance security measures against adversarial attacks and protect QML
models becomes increasingly evident. In this work, we outline the connection
between quantum noise channels and differential privacy (DP), by constructing a
family of noise channels which are inherently $\epsilon$-DP: $(\alpha,
\gamma)$-channels. Through this approach, we successfully replicate the
$\epsilon$-DP bounds observed for depolarizing and random rotation channels,
thereby affirming the broad generality of our framework. Additionally, we use a
semi-definite program to construct an optimally robust channel. In a
small-scale experimental evaluation, we demonstrate the benefits of using our
optimal noise channel over depolarizing noise, particularly in enhancing
adversarial accuracy. Moreover, we assess how the variables $\alpha$ and
$\gamma$ affect the certifiable robustness and investigate how different
encoding methods impact the classifier's robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>QML technical track at IEEE QCE 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SToFM: a Multi-scale Foundation Model for Spatial Transcriptomics <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.11588v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.11588v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suyuan Zhao, Yizhen Luo, Ganbo Yang, Yan Zhong, Hao Zhou, Zaiqing Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial Transcriptomics (ST) technologies provide biologists with rich
insights into single-cell biology by preserving spatial context of cells.
Building foundational models for ST can significantly enhance the analysis of
vast and complex data sources, unlocking new perspectives on the intricacies of
biological tissues. However, modeling ST data is inherently challenging due to
the need to extract multi-scale information from tissue slices containing vast
numbers of cells. This process requires integrating macro-scale tissue
morphology, micro-scale cellular microenvironment, and gene-scale gene
expression profile. To address this challenge, we propose SToFM, a multi-scale
Spatial Transcriptomics Foundation Model. SToFM first performs multi-scale
information extraction on each ST slice, to construct a set of ST sub-slices
that aggregate macro-, micro- and gene-scale information. Then an SE(2)
Transformer is used to obtain high-quality cell representations from the
sub-slices. Additionally, we construct \textbf{SToCorpus-88M}, the largest
high-resolution spatial transcriptomics corpus for pretraining. SToFM achieves
outstanding performance on a variety of downstream tasks, such as tissue region
semantic segmentation and cell type annotation, demonstrating its comprehensive
understanding of ST data through capturing and integrating multi-scale
information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accpeted by ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Federated Behavioural Planes: Explaining the Evolution of Client
  Behaviour in Federated Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15632v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15632v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dario Fenoglio, Gabriele Dominici, Pietro Barbiero, Alberto Tonda, Martin Gjoreski, Marc Langheinrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL), a privacy-aware approach in distributed deep
learning environments, enables many clients to collaboratively train a model
without sharing sensitive data, thereby reducing privacy risks. However,
enabling human trust and control over FL systems requires understanding the
evolving behaviour of clients, whether beneficial or detrimental for the
training, which still represents a key challenge in the current literature. To
address this challenge, we introduce Federated Behavioural Planes (FBPs), a
novel method to analyse, visualise, and explain the dynamics of FL systems,
showing how clients behave under two different lenses: predictive performance
(error behavioural space) and decision-making processes (counterfactual
behavioural space). Our experiments demonstrate that FBPs provide informative
trajectories describing the evolving states of clients and their contributions
to the global model, thereby enabling the identification of clusters of clients
with similar behaviours. Leveraging the patterns identified by FBPs, we propose
a robust aggregation technique named Federated Behavioural Shields to detect
malicious or noisy client models, thereby enhancing security and surpassing the
efficacy of existing state-of-the-art FL defense mechanisms. Our code is
publicly available on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>[v3] Pre-print of the paper accepted to NeurIPS 2024 (30 pages)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Physically Driven Long Short Term Memory Model for Estimating Snow
  Water Equivalent over the Continental United States 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.20129v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.20129v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arun M. Saranathan, Mahmoud Saeedimoghaddam, Brandon Smith, Deepthi Raghunandan, Grey Nearing, Craig Pelissier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Snow is an essential input for various land surface models. Seasonal snow
estimates are available as snow water equivalent (SWE) from process-based
reanalysis products or locally from in situ measurements. While the reanalysis
products are computationally expensive and available at only fixed spatial and
temporal resolutions, the in situ measurements are highly localized and sparse.
To address these issues and enable the analysis of the effect of a large suite
of physical, morphological, and geological conditions on the presence and
amount of snow, we build a Long Short-Term Memory (LSTM) network, which is able
to estimate the SWE based on time series input of the various
physical/meteorological factors as well static spatial/morphological factors.
Specifically, this model breaks down the SWE estimation into two separate
tasks: (i) a classification task that indicates the presence/absence of snow on
a specific day and (ii) a regression task that indicates the height of the SWE
on a specific day in the case of snow presence. The model is trained using
physical/in situ SWE measurements from the SNOw TELemetry (SNOTEL) snow pillows
in the western United States. We will show that trained LSTM models have a
classification accuracy of $\geq 93\%$ for the presence of snow and a
coefficient of correlation of $\sim 0.9$ concerning their SWE estimates. We
will also demonstrate that the models can generalize both spatially and
temporally to previously unseen data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint of journal paper in preparation. Details: 24 pages, 8
  figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HiFi-Stream: Streaming Speech Enhancement with Generative Adversarial
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.17141v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.17141v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ekaterina Dmitrieva, Maksim Kaledin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech Enhancement techniques have become core technologies in mobile devices
and voice software. Still, modern deep learning solutions often require high
amount of computational resources what makes their usage on low-resource
devices challenging. We present HiFi-Stream, an optimized version of recently
published HiFi++ model. Our experiments demonstrate that HiFi-Stream saves most
of the qualities of the original model despite its size and computational
complexity improved in comparison to the original HiFi++ making it one of the
smallest and fastest models available. The model is evaluated in streaming
setting where it demonstrates its superior performance in comparison to modern
baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages (4 content pages + 1 page of references)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Channel <span class="highlight-title">Estimation</span> for RIS-Assisted mmWave Systems via Dif<span class="highlight-title">fusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07770v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07770v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Wang, Yin Xu, Cixiao Zhang, Zhiyong Chen, Mingzeng Dai, Haiming Wang, Bingchao Liu, Dazhi He, Meixia Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconfigurable intelligent surface (RIS) has been recognized as a promising
technology for next-generation wireless communications. However, the
performance of RIS-assisted systems critically depends on accurate channel
state information (CSI). To address this challenge, this letter proposes a
novel channel estimation method for RIS-aided millimeter-wave (mmWave) systems
based on diffusion models (DMs). Specifically, the forward diffusion process of
the original signal is formulated to model the received signal as a noisy
observation within the framework of DMs. Subsequently, the channel estimation
task is formulated as the reverse diffusion process, and a sampling algorithm
based on denoising diffusion implicit models (DDIMs) is developed to enable
effective inference. Furthermore, a lightweight neural network, termed BRCNet,
is introduced to replace the conventional U-Net, significantly reducing the
number of parameters and computational complexity. Extensive experiments
conducted under various scenarios demonstrate that the proposed method
consistently outperforms existing baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sampling-enabled scalable manifold learning unveils discriminative
  <span class="highlight-title">cluster</span> structure of high-dimensional data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01100v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01100v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dehua Peng, Zhipeng Gui, Wenzhang Wei, Fa Li, Jie Gui, Huayi Wu, Jianya Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a pivotal branch of machine learning, manifold learning uncovers the
intrinsic low-dimensional structure within complex nonlinear manifolds in
high-dimensional space for visualization, classification, clustering, and
gaining key insights. Although existing techniques have achieved remarkable
successes, they suffer from extensive distortions of cluster structure, which
hinders the understanding of underlying patterns. Scalability issues also limit
their applicability for handling large-scale data. We hence propose a
sampling-based Scalable manifold learning technique that enables Uniform and
Discriminative Embedding, namely SUDE, for large-scale and high-dimensional
data. It starts by seeking a set of landmarks to construct the low-dimensional
skeleton of the entire data, and then incorporates the non-landmarks into the
learned space based on the constrained locally linear embedding (CLLE). We
empirically validated the effectiveness of SUDE on synthetic datasets and
real-world benchmarks, and applied it to analyze single-cell data and detect
anomalies in electrocardiogram (ECG) signals. SUDE exhibits distinct advantage
in scalability with respect to data size and embedding dimension, and has
promising performance in cluster separation, integrity, and global structure
preservation. The experiments also demonstrate notable robustness in embedding
quality as the sampling rate decreases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>80 pages, 37 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-Driven <span class="highlight-title">Exploration</span> for a Class of Continuous-Time Indefinite
  Linear--Quadratic <span class="highlight-title">Reinforcement</span> Learning Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.00358v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.00358v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilie Huang, Xun Yu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study reinforcement learning (RL) for the same class of continuous-time
stochastic linear--quadratic (LQ) control problems as in
\cite{huang2024sublinear}, where volatilities depend on both states and
controls while states are scalar-valued and running control rewards are absent.
We propose a model-free, data-driven exploration mechanism that adaptively
adjusts entropy regularization by the critic and policy variance by the actor.
Unlike the constant or deterministic exploration schedules employed in
\cite{huang2024sublinear}, which require extensive tuning for implementations
and ignore learning progresses during iterations, our adaptive exploratory
approach boosts learning efficiency with minimal tuning. Despite its
flexibility, our method achieves a sublinear regret bound that matches the
best-known model-free results for this class of LQ problems, which were
previously derived only with fixed exploration schedules. Numerical experiments
demonstrate that adaptive explorations accelerate convergence and improve
regret performance compared to the non-adaptive model-free and model-based
counterparts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fake or Real: The Impostor Hunt in Texts for Space Operations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13508v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13508v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Agata Kaczmarek, Dawid Płudowski, Piotr Wilczyński, Krzysztof Kotowski, Ramez Shendy, Evridiki Ntagiou, Jakub Nalepa, Artur Janicki, Przemysław Biecek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The "Fake or Real" competition hosted on Kaggle
(https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt ) is the
second part of a series of follow-up competitions and hackathons related to the
"Assurance for Space Domain AI Applications" project funded by the European
Space Agency (https://assurance-ai.space-codev.org/ ). The competition idea is
based on two real-life AI security threats identified within the project --
data poisoning and overreliance in Large Language Models. The task is to
distinguish between the proper output from LLM and the output generated under
malicious modification of the LLM. As this problem was not extensively
researched, participants are required to develop new techniques to address this
issue or adjust already existing ones to this problem's statement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast post-process Bayesian inference with Variational Sparse Bayesian
  Quadrature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05263v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05263v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengkun Li, Grégoire Clarté, Martin Jørgensen, Luigi Acerbi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In applied Bayesian inference scenarios, users may have access to a large
number of pre-existing model evaluations, for example from maximum-a-posteriori
(MAP) optimization runs. However, traditional approximate inference techniques
make little to no use of this available information. We propose the framework
of post-process Bayesian inference as a means to obtain a quick posterior
approximation from existing target density evaluations, with no further model
calls. Within this framework, we introduce Variational Sparse Bayesian
Quadrature (VSBQ), a method for post-process approximate inference for models
with black-box and potentially noisy likelihoods. VSBQ reuses existing target
density evaluations to build a sparse Gaussian process (GP) surrogate model of
the log posterior density function. Subsequently, we leverage sparse-GP
Bayesian quadrature combined with variational inference to achieve fast
approximate posterior inference over the surrogate. We validate our method on
challenging synthetic scenarios and real-world applications from computational
neuroscience. The experiments show that VSBQ builds high-quality posterior
approximations by post-processing existing optimization traces, with no further
model evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Statistics and Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Infinite Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.09068v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.09068v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dell Zhang, Xiangyu Chen, Jixiang Luo, Mengxi Jia, Changzhi Sun, Ruilong Ren, Jingren Liu, Hao Sun, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancements in Large Language Models (LLMs) and their multimodal
extensions (MLLMs) have ushered in remarkable progress in video understanding.
However, a fundamental challenge persists: effectively processing and
comprehending video content that extends beyond minutes or hours. While recent
efforts like Video-XL-2 have demonstrated novel architectural solutions for
extreme efficiency, and advancements in positional encoding such as HoPE and
VideoRoPE++ aim to improve spatio-temporal understanding over extensive
contexts, current state-of-the-art models still encounter significant
computational and memory constraints when faced with the sheer volume of visual
tokens from lengthy sequences. Furthermore, maintaining temporal coherence,
tracking complex events, and preserving fine-grained details over extended
periods remain formidable hurdles, despite progress in agentic reasoning
systems like Deep Video Discovery. This position paper posits that a logical,
albeit ambitious, next frontier for multimedia research is Infinite Video
Understanding -- the capability for models to continuously process, understand,
and reason about video data of arbitrary, potentially never-ending duration. We
argue that framing Infinite Video Understanding as a blue-sky research
objective provides a vital north star for the multimedia, and the wider AI,
research communities, driving innovation in areas such as streaming
architectures, persistent memory mechanisms, hierarchical and adaptive
representations, event-centric reasoning, and novel evaluation paradigms.
Drawing inspiration from recent work on long/ultra-long video understanding and
several closely related fields, we outline the core challenges and key research
directions towards achieving this transformative capability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Dif<span class="highlight-title">fusion</span> Models for Parameterized Quantum Circuit Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20863v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20863v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Barta, Darya Martyniuk, Johannes Jung, Adrian Paschke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum computing holds immense potential, yet its practical success depends
on multiple factors, including advances in quantum circuit design. In this
paper, we introduce a generative approach based on denoising diffusion models
(DMs) to synthesize parameterized quantum circuits (PQCs). Extending the recent
diffusion model pipeline of F\"urrutter et al. [1], our model effectively
conditions the synthesis process, enabling the simultaneous generation of
circuit architectures and their continuous gate parameters. We demonstrate our
approach in synthesizing PQCs optimized for generating high-fidelity
Greenberger-Horne-Zeilinger (GHZ) states and achieving high accuracy in quantum
machine learning (QML) classification tasks. Our results indicate a strong
generalization across varying gate sets and scaling qubit counts, highlighting
the versatility and computational efficiency of diffusion-based methods. This
work illustrates the potential of generative models as a powerful tool for
accelerating and optimizing the design of PQCs, supporting the development of
more practical and scalable quantum applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted for presentation at IEEE Quantum Week
  2025: IEEE International Conference on Quantum Computing and Engineering
  (QCE)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Surprising Agreement Between Convex <span class="highlight-title">Optimization</span> Theory and
  Learning-Rate Scheduling for Large Model Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18965v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18965v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Schaipp, Alexander Hägele, Adrien Taylor, Umut Simsekli, Francis Bach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show that learning-rate schedules for large model training behave
surprisingly similar to a performance bound from non-smooth convex optimization
theory. We provide a bound for the constant schedule with linear cooldown; in
particular, the practical benefit of cooldown is reflected in the bound due to
the absence of logarithmic terms. Further, we show that this surprisingly close
match between optimization theory and practice can be exploited for
learning-rate tuning: we achieve noticeable improvements for training 124M and
210M Llama-type models by (i) extending the schedule for continued training
with optimal learning-rate, and (ii) transferring the optimal learning-rate
across schedules.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MIRA: Medical Time Series Foundation Model for Real-World Health Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07584v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07584v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Li, Bowen Deng, Chang Xu, Zhiyuan Feng, Viktor Schlegel, Yu-Hao Huang, Yizheng Sun, Jingyuan Sun, Kailai Yang, Yiyao Yu, Jiang Bian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A unified foundation model for medical time series -- pretrained on open
access and ethics board-approved medical corpora -- offers the potential to
reduce annotation burdens, minimize model customization, and enable robust
transfer across clinical institutions, modalities, and tasks, particularly in
data-scarce or privacy-constrained environments. However, existing generalist
time series foundation models struggle to handle medical time series data due
to their inherent challenges, including irregular intervals, heterogeneous
sampling rates, and frequent missing values. To address these challenges, we
introduce MIRA, a unified foundation model specifically designed for medical
time series forecasting. MIRA incorporates a Continuous-Time Rotary Positional
Encoding that enables fine-grained modeling of variable time intervals, a
frequency-specific mixture-of-experts layer that routes computation across
latent frequency regimes to further promote temporal specialization, and a
Continuous Dynamics Extrapolation Block based on Neural ODE that models the
continuous trajectory of latent states, enabling accurate forecasting at
arbitrary target timestamps. Pretrained on a large-scale and diverse medical
corpus comprising over 454 billion time points collect from publicly available
datasets, MIRA achieves reductions in forecasting errors by an average of 10%
and 7% in out-of-distribution and in-distribution scenarios, respectively, when
compared to other zero-shot and fine-tuned baselines. We also introduce a
comprehensive benchmark spanning multiple downstream clinical tasks,
establishing a foundation for future research in medical time series modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Mapping</span> of Weed Management Methods in Orchards using Sentinel-2 and
  PlanetScope Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.19991v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.19991v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ioannis Kontogiorgakis, Iason Tsardanidis, Dimitrios Bormpoudakis, Ilias Tsoumas, Dimitra A. Loka, Christos Noulas, Alexandros Tsitouras, Charalampos Kontoes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective weed management is crucial for improving agricultural productivity,
as weeds compete with crops for vital resources like nutrients and water.
Accurate maps of weed management methods are essential for policymakers to
assess farmer practices, evaluate impacts on vegetation health, biodiversity,
and climate, as well as ensure compliance with policies and subsidies. However,
monitoring weed management methods is challenging as they commonly rely on
ground-based field surveys, which are often costly, time-consuming and subject
to delays. In order to tackle this problem, we leverage earth observation data
and Machine Learning (ML). Specifically, we developed separate ML models using
Sentinel-2 and PlanetScope satellite time series data, respectively, to
classify four distinct weed management methods (Mowing, Tillage,
Chemical-spraying, and No practice) in orchards. The findings demonstrate the
potential of ML-driven remote sensing to enhance the efficiency and accuracy of
weed management mapping in orchards.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for 2025 IEEE International Geoscience and Remote Sensing
  Symposium (IGARSS 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JEDI: The Force of Jensen-Shannon Divergence in Disentangling Dif<span class="highlight-title">fusion</span>
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19166v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19166v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Tillmann Bill, Enis Simsar, Thomas Hofmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce JEDI, a test-time adaptation method that enhances subject
separation and compositional alignment in diffusion models without requiring
retraining or external supervision. JEDI operates by minimizing semantic
entanglement in attention maps using a novel Jensen-Shannon divergence based
objective. To improve efficiency, we leverage adversarial optimization,
reducing the number of updating steps required. JEDI is model-agnostic and
applicable to architectures such as Stable Diffusion 1.5 and 3.5, consistently
improving prompt alignment and disentanglement in complex scenes. Additionally,
JEDI provides a lightweight, CLIP-free disentanglement score derived from
internal attention distributions, offering a principled benchmark for
compositional alignment under test-time conditions. Code and results are
available at https://ericbill21.github.io/JEDI/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gathering and Exploiting Higher-Order Information when Training Large
  Structured Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03885v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03885v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Wolinski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When training large models, such as neural networks, the full derivatives of
order 2 and beyond are usually inaccessible, due to their computational cost.
Therefore, among the second-order optimization methods, it is common to bypass
the computation of the Hessian by using first-order information, such as the
gradient of the parameters (e.g., quasi-Newton methods) or the activations
(e.g., K-FAC). In this paper, we focus on the exact and explicit computation of
projections of the Hessian and higher-order derivatives on well-chosen
subspaces relevant for optimization. Namely, for a given partition of the set
of parameters, we compute tensors that can be seen as "higher-order derivatives
according to the partition", at a reasonable cost as long as the number of
subsets of the partition remains small. Then, we give some examples of how
these tensors can be used. First, we show how to compute a learning rate per
subset of parameters, which can be used for hyperparameter tuning. Second, we
show how to use these tensors at order 2 to construct an optimization method
that uses information contained in the Hessian. Third, we show how to use these
tensors at order 3 (information contained in the third derivative of the loss)
to regularize this optimization method. The resulting training step has several
interesting properties, including: it takes into account long-range
interactions between the layers of the trained neural network, which is usually
not the case in similar methods (e.g., K-FAC); the trajectory of the
optimization is invariant under affine layer-wise reparameterization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation
  Models on Standard Computer Vision Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01955v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01955v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahul Ramachandran, Ali Garjani, Roman Bachmann, Andrei Atanov, Oğuzhan Fatih Kar, Amir Zamir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal foundation models, such as GPT-4o, have recently made remarkable
progress, but it is not clear where exactly these models stand in terms of
understanding vision. In this paper, we benchmark the performance of popular
multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0
Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision
tasks (semantic segmentation, object detection, image classification, depth and
surface normal prediction) using established datasets (e.g., COCO, ImageNet and
its variants, etc).
  The main challenges to performing this are: 1) most models are trained to
output text and cannot natively express versatile domains, such as segments or
3D geometry, and 2) many leading models are proprietary and accessible only at
an API level, i.e., there is no weight access to adapt them. We address these
challenges by translating standard vision tasks into equivalent text-promptable
and API-compatible tasks via prompt chaining to create a standardized
benchmarking framework.
  We observe that 1) the models are not close to the state-of-the-art
specialist models at any task. However, 2) they are respectable generalists;
this is remarkable as they are presumably trained on primarily image-text-based
tasks. 3) They perform semantic tasks notably better than geometric ones. 4)
While the prompt-chaining techniques affect performance, better models exhibit
less sensitivity to prompt variations. 5) GPT-4o performs the best among
non-reasoning models, securing the top position in 4 out of 6 tasks, 6)
reasoning models, e.g. o3, show improvements in geometric tasks, and 7) a
preliminary analysis of models with native image generation, like the latest
GPT-4o, shows they exhibit quirks like hallucinations and spatial
misalignments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page at https://fm-vision-evals.epfl.ch/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Efficient Generative Large Language Model Serving: A <span class="highlight-title">Survey</span> from
  Algorithms to Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15234v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15234v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Hongyi Jin, Tianqi Chen, Zhihao Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly evolving landscape of artificial intelligence (AI), generative
large language models (LLMs) stand at the forefront, revolutionizing how we
interact with our data. However, the computational intensity and memory
consumption of deploying these models present substantial challenges in terms
of serving efficiency, particularly in scenarios demanding low latency and high
throughput. This survey addresses the imperative need for efficient LLM serving
methodologies from a machine learning system (MLSys) research perspective,
standing at the crux of advanced AI innovations and practical system
optimizations. We provide in-depth analysis, covering a spectrum of solutions,
ranging from cutting-edge algorithmic modifications to groundbreaking changes
in system designs. The survey aims to provide a comprehensive understanding of
the current state and future directions in efficient LLM serving, offering
valuable insights for researchers and practitioners in overcoming the barriers
of effective LLM deployment, thereby reshaping the future of AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM Computing Surveys</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Leveraging RAG-LLMs for Urban Mobility <span class="highlight-title">Simulation</span> and Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.10382v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.10382v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Ding, Conor McCarthy, Kevin O'Shea, Ming<span class="highlight-author">ming Liu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rise of smart mobility and shared e-mobility services, numerous
advanced technologies have been applied to this field. Cloud-based traffic
simulation solutions have flourished, offering increasingly realistic
representations of the evolving mobility landscape. LLMs have emerged as
pioneering tools, providing robust support for various applications, including
intelligent decision-making, user interaction, and real-time traffic analysis.
As user demand for e-mobility continues to grow, delivering comprehensive
end-to-end solutions has become crucial. In this paper, we present a
cloud-based, LLM-powered shared e-mobility platform, integrated with a mobile
application for personalized route recommendations. The optimization module is
evaluated based on travel time and cost across different traffic scenarios.
Additionally, the LLM-powered RAG framework is evaluated at the schema level
for different users, using various evaluation methods. Schema-level RAG with
XiYanSQL achieves an average execution accuracy of 0.81 on system operator
queries and 0.98 on user queries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Artificial Intelligence for Green Hydrogen Yield <span class="highlight-title">Prediction</span> and Site
  Suitability using SHAP-Based Composite Index: Focus on Oman 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14219v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14219v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Obumneme Zimuzor Nwafor, Mohammed Abdul Majeed Al Hooti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As nations seek sustainable alternatives to fossil fuels, green hydrogen has
emerged as a promising strategic pathway toward decarbonisation, particularly
in solar-rich arid regions. However, identifying optimal locations for hydrogen
production requires the integration of complex environmental, atmospheric, and
infrastructural factors, often compounded by limited availability of direct
hydrogen yield data. This study presents a novel Artificial Intelligence (AI)
framework for computing green hydrogen yield and site suitability index using
mean absolute SHAP (SHapley Additive exPlanations) values. This framework
consists of a multi-stage pipeline of unsupervised multi-variable clustering,
supervised machine learning classifier and SHAP algorithm. The pipeline trains
on an integrated meteorological, topographic and temporal dataset and the
results revealed distinct spatial patterns of suitability and relative
influence of the variables. With model predictive accuracy of 98%, the result
also showed that water proximity, elevation and seasonal variation are the most
influential factors determining green hydrogen site suitability in Oman with
mean absolute shap values of 2.470891, 2.376296 and 1.273216 respectively.
Given limited or absence of ground-truth yield data in many countries that have
green hydrogen prospects and ambitions, this study offers an objective and
reproducible alternative to subjective expert weightings, thus allowing the
data to speak for itself and potentially discover novel latent groupings
without pre-imposed assumptions. This study offers industry stakeholders and
policymakers a replicable and scalable tool for green hydrogen infrastructure
planning and other decision making in data-scarce regions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Monitoring digestate application on agricultural crops using Sentinel-2
  Satellite imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.19996v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.19996v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Kalogeras, Dimitrios Bormpoudakis, Iason Tsardanidis, Dimitra A. Loka, Charalampos Kontoes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread use of Exogenous Organic Matter in agriculture necessitates
monitoring to assess its effects on soil and crop health. This study evaluates
optical Sentinel-2 satellite imagery for detecting digestate application, a
practice that enhances soil fertility but poses environmental risks like
microplastic contamination and nitrogen losses. In the first instance,
Sentinel-2 satellite image time series (SITS) analysis of specific indices
(EOMI, NDVI, EVI) was used to characterize EOM's spectral behavior after
application on the soils of four different crop types in Thessaly, Greece.
Furthermore, Machine Learning (ML) models (namely Random Forest, k-NN, Gradient
Boosting and a Feed-Forward Neural Network), were used to investigate digestate
presence detection, achieving F1-scores up to 0.85. The findings highlight the
potential of combining remote sensing and ML for scalable and cost-effective
monitoring of EOM applications, supporting precision agriculture and
sustainability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for 2025 IEEE International Geoscience and Remote Sensing
  Symposium (IGARSS 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Privacy-Utility Trade-off in Decentralized Learning with
  Generalized Correlated Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14644v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14644v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angelo Rodio, Zheng Chen, Erik G. Larsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decentralized learning enables distributed agents to collaboratively train a
shared machine learning model without a central server, through local
computation and peer-to-peer communication. Although each agent retains its
dataset locally, sharing local models can still expose private information
about the local training datasets to adversaries. To mitigate privacy attacks,
a common strategy is to inject random artificial noise at each agent before
exchanging local models between neighbors. However, this often leads to utility
degradation due to the negative effects of cumulated artificial noise on the
learning algorithm. In this work, we introduce CorN-DSGD, a novel
covariance-based framework for generating correlated privacy noise across
agents, which unifies several state-of-the-art methods as special cases. By
leveraging network topology and mixing weights, CorN-DSGD optimizes the noise
covariance to achieve network-wide noise cancellation. Experimental results
show that CorN-DSGD cancels more noise than existing pairwise correlation
schemes, improving model performance under formal privacy guarantees.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, accepted at IEEE ITW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RIS-aided Latent Space Alignment for Semantic Channel Equalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16450v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16450v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomás Hüttebräucker, Mario Edoardo Pandolfo, Simone Fiorellino, Emilio Calvanese Strinati, Paolo Di Lorenzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic communication systems introduce a new paradigm in wireless
communications, focusing on transmitting the intended meaning rather than
ensuring strict bit-level accuracy. These systems often rely on Deep Neural
Networks (DNNs) to learn and encode meaning directly from data, enabling more
efficient communication. However, in multi-user settings where interacting
agents are trained independently-without shared context or joint
optimization-divergent latent representations across AI-native devices can lead
to semantic mismatches, impeding mutual understanding even in the absence of
traditional transmission errors. In this work, we address semantic mismatch in
Multiple-Input Multiple-Output (MIMO) channels by proposing a joint physical
and semantic channel equalization framework that leverages the presence of
Reconfigurable Intelligent Surfaces (RIS). The semantic equalization is
implemented as a sequence of transformations: (i) a pre-equalization stage at
the transmitter; (ii) propagation through the RIS-aided channel; and (iii) a
post-equalization stage at the receiver. We formulate the problem as a
constrained Minimum Mean Squared Error (MMSE) optimization and propose two
solutions: (i) a linear semantic equalization chain, and (ii) a non-linear
DNN-based semantic equalizer. Both methods are designed to operate under
semantic compression in the latent space and adhere to transmit power
constraints. Through extensive evaluations, we show that the proposed joint
equalization strategies consistently outperform conventional, disjoint
approaches to physical and semantic channel equalization across a broad range
of scenarios and wireless channel conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards <span class="highlight-title">Detect</span>ing Persuasion on Social Media: From Model Development to
  Insights on Persuasion Strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.13844v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.13844v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elyas Meguellati, Stefano Civelli, Pietro Bernardelle, Shazia Sadiq, Irwin King, Gianluca Demartini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Political advertising plays a pivotal role in shaping public opinion and
influencing electoral outcomes, often through subtle persuasive techniques
embedded in broader propaganda strategies. Detecting these persuasive elements
is crucial for enhancing voter awareness and ensuring transparency in
democratic processes. This paper presents an integrated approach that bridges
model development and real-world application through two interconnected
studies. First, we introduce a lightweight model for persuasive text detection
that achieves state-of-the-art performance in Subtask 3 of SemEval 2023 Task 3
while requiring significantly fewer computational resources and training data
than existing methods. Second, we demonstrate the model's practical utility by
collecting the Australian Federal Election 2022 Facebook Ads (APA22) dataset,
partially annotating a subset for persuasion, and fine-tuning the model to
adapt from mainstream news to social media content. We then apply the
fine-tuned model to label the remainder of the APA22 dataset, revealing
distinct patterns in how political campaigns leverage persuasion through
different funding strategies, word choices, demographic targeting, and temporal
shifts in persuasion intensity as election day approaches. Our findings not
only underscore the necessity of domain-specific modeling for analyzing
persuasion on social media but also show how uncovering these strategies can
enhance transparency, inform voters, and promote accountability in digital
campaigns.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Universal Fourier Neural Operators for Micromechanics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.12233v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.12233v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binh Huy Nguyen, Matti Schneider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solving cell problems in homogenization is hard, and available deep-learning
frameworks fail to match the speed and generality of traditional computational
frameworks. More to the point, it is generally unclear what to expect of
machine-learning approaches, let alone single out which approaches are
promising. In the work at hand, we advocate Fourier Neural Operators (FNOs) for
micromechanics, empowering them by insights from computational micromechanics
methods based on the fast Fourier transform (FFT). We construct an FNO
surrogate mimicking the basic scheme foundational for FFT-based methods and
show that the resulting operator predicts solutions to cell problems with
arbitrary stiffness distribution only subject to a material-contrast constraint
up to a desired accuracy. In particular, there are no restrictions on the
material symmetry like isotropy, on the number of phases and on the geometry of
the interfaces between materials. Also, the provided fidelity is sharp and
uniform, providing explicit guarantees leveraging our physical empowerment of
FNOs. To show the desired universal approximation property, we construct an FNO
explicitly that requires no training to begin with. Still, the obtained neural
operator complies with the same memory requirements as the basic scheme and
comes with runtimes proportional to classical FFT solvers. In particular,
large-scale problems with more than 100 million voxels are readily handled. The
goal of this work is to underline the potential of FNOs for solving
micromechanical problems, linking FFT-based methods to FNOs. This connection is
expected to provide a fruitful exchange between both worlds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cautious Next Token <span class="highlight-title">Prediction</span> <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.03038v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.03038v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhou Wang, Lingzhi Zhang, Yue Bai, Mang Tik Chiu, Zhengmian Hu, Mingyuan Zhang, Qihua Dong, Yu Yin, Sohrab Amirghodsi, Yun Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Next token prediction paradigm has been prevailing for autoregressive models
in the era of LLMs. The current default sampling choice for popular LLMs is
temperature scaling together with nucleus sampling to balance diversity and
coherence. Nevertheless, such approach leads to inferior performance in various
NLP tasks when the model is not certain about testing questions. To this end,
we propose a brand new training-free decoding strategy, dubbed as Cautious Next
Token Prediction (CNTP). In the decoding process, if the model has
comparatively high prediction entropy at a certain step, we sample multiple
trials starting from the step independently and stop when encountering any
punctuation. Then we select the trial with the lowest perplexity score viewed
as the most probable and reliable trial path given the model's capacity. The
trial number is negatively correlated with the prediction confidence, i.e., the
less confident the model is, the more trials it should sample. This is
consistent with human beings' behaviour: when feeling uncertain or unconfident,
one tends to think more creatively, exploring multiple thinking paths, to
cautiously select the path one feels most confident about. Extensive
experiments on both LLMs and MLLMs show that our proposed CNTP approach
outperforms existing standard decoding strategies consistently by a clear
margin. Moreover, the integration of CNTP with self consistency can further
improve over vanilla self consistency. We believe our proposed CNTP has the
potential to become one of the default choices for LLM decoding. Code is
available at https://github.com/wyzjack/CNTP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Spatio-Temporal Machine Learning Model for Mortgage Credit Risk:
  Default Probabilities and Loan Portfolios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02846v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02846v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Kündig, Fabio Sigrist
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel machine learning model for credit risk by combining
tree-boosting with a latent spatio-temporal Gaussian process model accounting
for frailty correlation. This allows for modeling non-linearities and
interactions among predictor variables in a flexible data-driven manner and for
accounting for spatio-temporal variation that is not explained by observable
predictor variables. We also show how estimation and prediction can be done in
a computationally efficient manner. In an application to a large U.S. mortgage
credit risk data set, we find that both predictive default probabilities for
individual loans and predictive loan portfolio loss distributions obtained with
our novel approach are more accurate compared to conventional independent
linear hazard models and also linear spatio-temporal models. Using
interpretability tools for machine learning models, we find that the likely
reasons for this outperformance are strong interaction and non-linear effects
in the predictor variables and the presence of spatio-temporal frailty effects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hardware-Efficient Photonic Tensor Core: Accelerating Deep Neural
  Networks with Structured Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shupeng Ning, Hanqing Zhu, Chenghao Feng, Jiaqi Gu, David Z. Pan, Ray T. Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth in computing demands, particularly driven by artificial
intelligence applications, has begun to exceed the capabilities of traditional
electronic hardware. Optical computing offers a promising alternative due to
its parallelism, high computational speed, and low power consumption. However,
existing photonic integrated circuits are constrained by large footprints,
costly electro-optical interfaces, and complex control mechanisms, limiting the
practical scalability of optical neural networks (ONNs). To address these
limitations, we introduce a block-circulant photonic tensor core for a
structure-compressed optical neural network (StrC-ONN) architecture. The
structured compression technique substantially reduces both model complexity
and hardware resources without sacrificing the versatility of neural networks,
and achieves accuracy comparable to uncompressed models. Additionally, we
propose a hardware-aware training framework to compensate for on-chip
nonidealities to improve model robustness and accuracy. Experimental validation
through image processing and classification tasks demonstrates that our
StrC-ONN achieves a reduction in trainable parameters of up to 74.91%,while
still maintaining competitive accuracy levels. Performance analyses further
indicate that this hardware-software co-design approach is expected to yield a
3.56 times improvement in power efficiency. By reducing both hardware
requirements and control complexity across multiple dimensions, this work
explores a new pathway toward practical and scalable ONNs, highlighting a
promising route to address future computational efficiency challenges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Advancing Multimodal Reasoning via <span class="highlight-title">Reinforcement</span> Learning with Cold
  Start 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.22334v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.22334v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lai Wei, Yuting Li, Kaipeng Zheng, Chen Wang, <span class="highlight-author">Yue Wang</span>, Linghe Kong, Lichao Sun, Weiran Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have demonstrated
impressive chain-of-thought reasoning capabilities, with reinforcement learning
(RL) playing a crucial role in this progress. While "aha moment"
patterns--where models exhibit self-correction through reflection--are often
attributed to emergent properties from RL, we first demonstrate that these
patterns exist in multimodal LLMs (MLLMs) prior to RL training but may not
necessarily correlate with improved reasoning performance. Building on these
insights, we present a comprehensive study on enhancing multimodal reasoning
through a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start
with structured chain-of-thought reasoning patterns, followed by (2)
reinforcement learning via GRPO to further refine these capabilities. Our
extensive experiments show that this combined approach consistently outperforms
both SFT-only and RL-only methods across challenging multimodal reasoning
benchmarks. The resulting models achieve state-of-the-art performance among
open-source MLLMs at both 3B and 7B scales, with our 7B model showing
substantial improvements over base models (e.g., 66.3 %$\rightarrow$73.4 % on
MathVista, 62.9 %$\rightarrow$70.4 % on We-Math) and our 3B model achieving
performance competitive with several 7B models. Overall, this work provides
practical guidance for building advanced multimodal reasoning models. Our code
is available at https://github.com/waltonfuture/RL-with-Cold-Start.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-07-22T00:00:00Z">2025-07-22</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">54</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Deformable</span> <span class="highlight-title">Cluster</span> Manipulation via Whole-Arm Policy Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jayadeep Jacob, Wenzheng Zhang, Houston Warren, Paulo Borges, Tirthankar Bandyopadhyay, Fabio Ramos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manipulating clusters of deformable objects presents a substantial challenge
with widespread applicability, but requires contact-rich whole-arm
interactions. A potential solution must address the limited capacity for
realistic model synthesis, high uncertainty in perception, and the lack of
efficient spatial abstractions, among others. We propose a novel framework for
learning model-free policies integrating two modalities: 3D point clouds and
proprioceptive touch indicators, emphasising manipulation with full body
contact awareness, going beyond traditional end-effector modes. Our
reinforcement learning framework leverages a distributional state
representation, aided by kernel mean embeddings, to achieve improved training
efficiency and real-time inference. Furthermore, we propose a novel
context-agnostic occlusion heuristic to clear deformables from a target region
for exposure tasks. We deploy the framework in a power line clearance scenario
and observe that the agent generates creative strategies leveraging multiple
arm links for de-occlusion. Finally, we perform zero-shot sim-to-real policy
transfer, allowing the arm to clear real branches with unknown occlusion
patterns, unseen topology, and uncertain dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shared <span class="highlight-title">Control</span> of Holonomic Wheelchairs through <span class="highlight-title">Reinforcement</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jannis Bähler, Diego Paez-Granados, Jorge Peña-Queralta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Smart electric wheelchairs can improve user experience by supporting the
driver with shared control. State-of-the-art work showed the potential of
shared control in improving safety in navigation for non-holonomic robots.
However, for holonomic systems, current approaches often lead to unintuitive
behavior for the user and fail to utilize the full potential of omnidirectional
driving. Therefore, we propose a reinforcement learning-based method, which
takes a 2D user input and outputs a 3D motion while ensuring user comfort and
reducing cognitive load on the driver. Our approach is trained in Isaac Gym and
tested in simulation in Gazebo. We compare different RL agent architectures and
reward functions based on metrics considering cognitive load and user comfort.
We show that our method ensures collision-free navigation while smartly
orienting the wheelchair and showing better or competitive smoothness compared
to a previous non-learning-based method. We further perform a sim-to-real
transfer and demonstrate, to the best of our knowledge, the first real-world
implementation of RL-based shared control for an omnidirectional mobility
platform.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Uncertainty and Quality of Visual Language Action-enabled
  <span class="highlight-title">Robot</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Valle, Chengjie Lu, Shaukat Ali, Aitor Arrieta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Language Action (VLA) models are a multi-modal class of Artificial
Intelligence (AI) systems that integrate visual perception, natural language
understanding, and action planning to enable agents to interpret their
environment, comprehend instructions, and perform embodied tasks autonomously.
Recently, significant progress has been made to advance this field. These kinds
of models are typically evaluated through task success rates, which fail to
capture the quality of task execution and the mode's confidence in its
decisions. In this paper, we propose eight uncertainty metrics and five quality
metrics specifically designed for VLA models for robotic manipulation tasks. We
assess their effectiveness through a large-scale empirical study involving 908
successful task executions from three state-of-the-art VLA models across four
representative robotic manipulation tasks. Human domain experts manually
labeled task quality, allowing us to analyze the correlation between our
proposed metrics and expert judgments. The results reveal that several metrics
show moderate to strong correlation with human assessments, highlighting their
utility for evaluating task quality and model confidence. Furthermore, we found
that some of the metrics can discriminate between high-, medium-, and
low-quality executions from unsuccessful tasks, which can be interesting when
test oracles are not available. Our findings challenge the adequacy of current
evaluation practices that rely solely on binary success rates and pave the way
for improved real-time monitoring and adaptive enhancement of VLA-enabled
robotic systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAPTAR: Radar Radiation Pattern Acquisition through Automated
  Collaborative <span class="highlight-title">Robot</span>ics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maaz Qureshi, Mohammad Omid Bagheri, Abdelrahman Elbadrawy, William Melek, George Shaker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate characterization of modern on-chip antennas remains challenging, as
current probe-station techniques offer limited angular coverage, rely on
bespoke hardware, and require frequent manual alignment. This research
introduces RAPTAR (Radiation Pattern Acquisition through Robotic Automation), a
portable, state-of-the-art, and autonomous system based on collaborative
robotics. RAPTAR enables 3D radiation-pattern measurement of integrated radar
modules without dedicated anechoic facilities. The system is designed to
address the challenges of testing radar modules mounted in diverse real-world
configurations, including vehicles, UAVs, AR/VR headsets, and biomedical
devices, where traditional measurement setups are impractical. A
7-degree-of-freedom Franka cobot holds the receiver probe and performs
collision-free manipulation across a hemispherical spatial domain, guided by
real-time motion planning and calibration accuracy with RMS error below 0.9 mm.
The system achieves an angular resolution upto 2.5 degree and integrates
seamlessly with RF instrumentation for near- and far-field power measurements.
Experimental scans of a 60 GHz radar module show a mean absolute error of less
than 2 dB compared to full-wave electromagnetic simulations ground truth.
Benchmarking against baseline method demonstrates 36.5% lower mean absolute
error, highlighting RAPTAR accuracy and repeatability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 Pages, IEEE Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical <span class="highlight-title">Reinforcement</span> Learning Framework for Adaptive Walking
  <span class="highlight-title">Control</span> Using General Value Functions of Lower-Limb Sensor Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16983v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16983v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sonny T. Jones, Grange M. Simpson, Patrick M. Pilarski, Ashley N. Dalrymple
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rehabilitation technology is a natural setting to study the shared learning
and decision-making of human and machine agents. In this work, we explore the
use of Hierarchical Reinforcement Learning (HRL) to develop adaptive control
strategies for lower-limb exoskeletons, aiming to enhance mobility and autonomy
for individuals with motor impairments. Inspired by prominent models of
biological sensorimotor processing, our investigated HRL approach breaks down
the complex task of exoskeleton control adaptation into a higher-level
framework for terrain strategy adaptation and a lower-level framework for
providing predictive information; this latter element is implemented via the
continual learning of general value functions (GVFs). GVFs generated temporal
abstractions of future signal values from multiple wearable lower-limb sensors,
including electromyography, pressure insoles, and goniometers. We investigated
two methods for incorporating actual and predicted sensor signals into a policy
network with the intent to improve the decision-making capacity of the control
system of a lower-limb exoskeleton during ambulation across varied terrains. As
a key result, we found that the addition of predictions made from GVFs
increased overall network accuracy. Terrain-specific performance increases were
seen while walking on even ground, uneven ground, up and down ramps, and turns,
terrains that are often misclassified without predictive information. This
suggests that predictive information can aid decision-making during
uncertainty, e.g., on terrains that have a high chance of being misclassified.
This work, therefore, contributes new insights into the nuances of HRL and the
future development of exoskeletons to facilitate safe transitioning and
traversing across different walking environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, accepted at the 6th Multi-disciplinary Conference
  on Reinforcement Learning and Decision Making (RLDM2025), June 11-14, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-agent <span class="highlight-title">Reinforcement</span> Learning for <span class="highlight-title">Robot</span>ized Coral Reef Sample
  Collection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Correa, Tero Kaarlela, Jose Fuentes, Paulo Padrao, Alain Duran, Leonardo Bobadilla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a reinforcement learning (RL) environment for developing
an autonomous underwater robotic coral sampling agent, a crucial coral reef
conservation and research task. Using software-in-the-loop (SIL) and
hardware-in-the-loop (HIL), an RL-trained artificial intelligence (AI)
controller is developed using a digital twin (DT) in simulation and
subsequently verified in physical experiments. An underwater motion capture
(MOCAP) system provides real-time 3D position and orientation feedback during
verification testing for precise synchronization between the digital and
physical domains. A key novelty of this approach is the combined use of a
general-purpose game engine for simulation, deep RL, and real-time underwater
motion capture for an effective zero-shot sim-to-real strategy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent
  <span class="highlight-title">Planning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16815v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16815v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, Yu-Chiang Frank Wang, Fu-En Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language-action (VLA) reasoning tasks require agents to interpret
multimodal instructions, perform long-horizon planning, and act adaptively in
dynamic environments. Existing approaches typically train VLA models in an
end-to-end fashion, directly mapping inputs to actions without explicit
reasoning, which hinders their ability to plan over multiple steps or adapt to
complex task variations. In this paper, we propose ThinkAct, a dual-system
framework that bridges high-level reasoning with low-level action execution via
reinforced visual latent planning. ThinkAct trains a multimodal LLM to generate
embodied reasoning plans guided by reinforcing action-aligned visual rewards
based on goal completion and trajectory consistency. These reasoning plans are
compressed into a visual plan latent that conditions a downstream action model
for robust action execution on target environments. Extensive experiments on
embodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct
enables few-shot adaptation, long-horizon planning, and self-correction
behaviors in complex embodied AI tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://jasper0314-huang.github.io/thinkact-vla/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Experience is the Best Teacher: Grounding VLMs for <span class="highlight-title">Robot</span>ics through
  Self-Generated Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16713v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16713v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guowei Lan, Kaixian Qu, René Zurbrügg, Changan Chen, Christopher E. Mower, Haitham Bou-Ammar, Marco Hutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) have been widely adopted in robotics to enable
autonomous planning. However, grounding VLMs, originally trained on internet
data, to diverse real-world robots remains a challenge. This paper presents
ExpTeach, a framework that grounds VLMs to physical robots by building a
self-generated memory of real-world experiences. In ExpTeach, the VLM
autonomously plans actions, verifies outcomes, reflects on failures, and adapts
robot behaviors in a closed loop. The self-generated experiences during this
process are then summarized into a long-term memory, enabling retrieval of
learned knowledge to guide future tasks via retrieval-augmented generation
(RAG). Additionally, ExpTeach enhances the spatial understanding of VLMs with
an on-demand image annotation module. In experiments, we show that reflection
improves success rates from 36% to 84% on four challenging robotic tasks and
observe the emergence of intelligent object interactions, including creative
tool use. Across extensive tests on 12 real-world scenarios (including eight
unseen ones), we find that grounding with long-term memory boosts single-trial
success rates from 22% to 80%, demonstrating the effectiveness and
generalizability of ExpTeach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Morpheus: A Neural-driven Animatronic Face with Hybrid Actuation and
  Diverse Emotion <span class="highlight-title">Control</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16645v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16645v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongzheng Zhang, Jiawen Yang, Ziqiao Peng, Meng Yang, Jianzhu Ma, Lin Cheng, Huazhe Xu, <span class="highlight-author">Hang Zhao</span>, Hao Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous animatronic faces struggle to express emotions effectively due to
hardware and software limitations. On the hardware side, earlier approaches
either use rigid-driven mechanisms, which provide precise control but are
difficult to design within constrained spaces, or tendon-driven mechanisms,
which are more space-efficient but challenging to control. In contrast, we
propose a hybrid actuation approach that combines the best of both worlds. The
eyes and mouth-key areas for emotional expression-are controlled using rigid
mechanisms for precise movement, while the nose and cheek, which convey subtle
facial microexpressions, are driven by strings. This design allows us to build
a compact yet versatile hardware platform capable of expressing a wide range of
emotions. On the algorithmic side, our method introduces a self-modeling
network that maps motor actions to facial landmarks, allowing us to
automatically establish the relationship between blendshape coefficients for
different facial expressions and the corresponding motor control signals
through gradient backpropagation. We then train a neural network to map speech
input to corresponding blendshape controls. With our method, we can generate
distinct emotional expressions such as happiness, fear, disgust, and anger,
from any given sentence, each with nuanced, emotion-specific control signals-a
feature that has not been demonstrated in earlier systems. We release the
hardware design and code at https://github.com/ZZongzheng0918/Morpheus-Hardware
and https://github.com/ZZongzheng0918/Morpheus-Software.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to RSS 2025, Project Page:
  https://jiawenyang-ch.github.io/Morpheus-Hardware-Design/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Target-based Multi-<span class="highlight-title">LiDAR</span> Multi-Camera Extrinsic <span class="highlight-title">Calibration</span> System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Gentilini, Pierpaolo Serio, Valentina Donzella, Lorenzo Pollini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extrinsic Calibration represents the cornerstone of autonomous driving. Its
accuracy plays a crucial role in the perception pipeline, as any errors can
have implications for the safety of the vehicle. Modern sensor systems collect
different types of data from the environment, making it harder to align the
data. To this end, we propose a target-based extrinsic calibration system
tailored for a multi-LiDAR and multi-camera sensor suite. This system enables
cross-calibration between LiDARs and cameras with limited prior knowledge using
a custom ChArUco board and a tailored nonlinear optimization method. We test
the system with real-world data gathered in a warehouse. Results demonstrated
the effectiveness of the proposed method, highlighting the feasibility of a
unique pipeline tailored for various types of sensors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guided <span class="highlight-title">Reinforcement</span> Learning for Omnidirectional 3D Jumping in
  Quadruped <span class="highlight-title">Robot</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Bussola, Michele Focchi, Giulio Turrisi, Claudio Semini, Luigi Palopoli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Jumping poses a significant challenge for quadruped robots, despite being
crucial for many operational scenarios. While optimisation methods exist for
controlling such motions, they are often time-consuming and demand extensive
knowledge of robot and terrain parameters, making them less robust in
real-world scenarios. Reinforcement learning (RL) is emerging as a viable
alternative, yet conventional end-to-end approaches lack efficiency in terms of
sample complexity, requiring extensive training in simulations, and
predictability of the final motion, which makes it difficult to certify the
safety of the final motion. To overcome these limitations, this paper
introduces a novel guided reinforcement learning approach that leverages
physical intuition for efficient and explainable jumping, by combining B\'ezier
curves with a Uniformly Accelerated Rectilinear Motion (UARM) model. Extensive
simulation and experimental results clearly demonstrate the advantages of our
approach over existing alternatives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Designing for Difference: How Human Characteristics Shape Perceptions of
  Collaborative <span class="highlight-title">Robot</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sabrina Livanec, Laura Londoño, Michael Gorki, Adrian Röfer, Abhinav Valada, Andrea Kiesel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of assistive robots for social collaboration raises critical
questions about responsible and inclusive design, especially when interacting
with individuals from protected groups such as those with disabilities or
advanced age. Currently, research is scarce on how participants assess varying
robot behaviors in combination with diverse human needs, likely since
participants have limited real-world experience with advanced domestic robots.
In the current study, we aim to address this gap while using methods that
enable participants to assess robot behavior, as well as methods that support
meaningful reflection despite limited experience. In an online study, 112
participants (from both experimental and control groups) evaluated 7 videos
from a total of 28 variations of human-robot collaboration types. The
experimental group first completed a cognitive-affective mapping (CAM) exercise
on human-robot collaboration before providing their ratings. Although CAM
reflection did not significantly affect overall ratings, it led to more
pronounced assessments for certain combinations of robot behavior and human
condition. Most importantly, the type of human-robot collaboration influences
the assessment. Antisocial robot behavior was consistently rated as the lowest,
while collaboration with aged individuals elicited more sensitive evaluations.
Scenarios involving object handovers were viewed more positively than those
without them. These findings suggest that both human characteristics and
interaction paradigms influence the perceived acceptability of collaborative
robots, underscoring the importance of prosocial design. They also highlight
the potential of reflective methods, such as CAM, to elicit nuanced feedback,
supporting the development of user-centered and socially responsible robotic
systems tailored to diverse populations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Oscillatory Guidance for Formation Flight of Fixed-Wing
  Drones <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Xu, Jesús Bautista, José Hinojosa, Héctor García de Marina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The autonomous formation flight of fixed-wing drones is hard when the
coordination requires the actuation over their speeds since they are critically
bounded and aircraft are mostly designed to fly at a nominal airspeed. This
paper proposes an algorithm to achieve formation flights of fixed-wing drones
without requiring any actuation over their speed. In particular, we guide all
the drones to travel over specific paths, e.g., parallel straight lines, and we
superpose an oscillatory behavior onto the guiding vector field that drives the
drones to the paths. This oscillation enables control over the average velocity
along the path, thereby facilitating inter-drone coordination. Each drone
adjusts its oscillation amplitude distributively in a closed-loop manner by
communicating with neighboring agents in an undirected and connected graph. A
novel consensus algorithm is introduced, leveraging a non-negative, asymmetric
saturation function. This unconventional saturation is justified since negative
amplitudes do not make drones travel backward or have a negative velocity along
the path. Rigorous theoretical analysis of the algorithm is complemented by
validation through numerical simulations and a real-world formation flight.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Yang Xu and Jes\'us Bautista contributed equally to this work. In the
  proceedings of the IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI or Human? Understanding Perceptions of <span class="highlight-title">Embodied</span> <span class="highlight-title">Robot</span>s with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lavinia Hriscu, Alberto Sanfeliu, Anais Garrell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pursuit of artificial intelligence has long been associated to the the
challenge of effectively measuring intelligence. Even if the Turing Test was
introduced as a means of assessing a system intelligence, its relevance and
application within the field of human-robot interaction remain largely
underexplored. This study investigates the perception of intelligence in
embodied robots by performing a Turing Test within a robotic platform. A total
of 34 participants were tasked with distinguishing between AI- and
human-operated robots while engaging in two interactive tasks: an information
retrieval and a package handover. These tasks assessed the robot perception and
navigation abilities under both static and dynamic conditions. Results indicate
that participants were unable to reliably differentiate between AI- and
human-controlled robots beyond chance levels. Furthermore, analysis of
participant responses reveals key factors influencing the perception of
artificial versus human intelligence in embodied robotic systems. These
findings provide insights into the design of future interactive robots and
contribute to the ongoing discourse on intelligence assessment in AI-driven
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Application of LLM Guided <span class="highlight-title">Reinforcement</span> Learning in Formation <span class="highlight-title">Control</span>
  with Collision Avoidance <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16382v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16382v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhao Yao, Zike Yuan, Xiaoxu Liu, Chi Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Agent Systems (MAS) excel at accomplishing complex objectives through
the collaborative efforts of individual agents. Among the methodologies
employed in MAS, Multi-Agent Reinforcement Learning (MARL) stands out as one of
the most efficacious algorithms. However, when confronted with the complex
objective of Formation Control with Collision Avoidance (FCCA): designing an
effective reward function that facilitates swift convergence of the policy
network to an optimal solution. In this paper, we introduce a novel framework
that aims to overcome this challenge. By giving large language models (LLMs) on
the prioritization of tasks and the observable information available to each
agent, our framework generates reward functions that can be dynamically
adjusted online based on evaluation outcomes by employing more advanced
evaluation metrics rather than the rewards themselves. This mechanism enables
the MAS to simultaneously achieve formation control and obstacle avoidance in
dynamic environments with enhanced efficiency, requiring fewer iterations to
reach superior performance levels. Our empirical studies, conducted in both
simulation and real-world settings, validate the practicality and effectiveness
of our proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IROS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Humanoid <span class="highlight-title">Robot</span> Whole-body <span class="highlight-title">Geometric</span> <span class="highlight-title">Calibration</span> with Embedded Sensors
  and a Single Plane 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thanh D V Nguyen, Vincent Bonnet, Pierre Fernbach, David Daney, Florent Lamiraux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whole-body geometric calibration of humanoid robots using classical robot
calibration methods is a timeconsuming and experimentally burdensome task.
However, despite its significance for accurate control and simulation, it is
often overlooked in the humanoid robotics community. To address this issue, we
propose a novel practical method that utilizes a single plane, embedded force
sensors, and an admittance controller to calibrate the whole-body kinematics of
humanoids without requiring manual intervention. Given the complexity of
humanoid robots, it is crucial to generate and determine a minimal set of
optimal calibration postures. To do so, we propose a new algorithm called IROC
(Information Ranking algorithm for selecting Optimal Calibration postures).
IROC requires a pool of feasible candidate postures to build a normalized
weighted information matrix for each posture. Then, contrary to other
algorithms from the literature, IROC will determine the minimal number of
optimal postures that are to be played onto a robot for its calibration. Both
IROC and the single-plane calibration method were experimentally validated on a
TALOS humanoid robot. The total whole-body kinematics chain was calibrated
using solely 31 optimal postures with 3-point contacts on a table by the robot
gripper. In a cross-validation experiment, the average root-mean-square (RMS)
error was reduced by a factor of 2.3 compared to the manufacturer's model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Budget Allocation Policies for Real-Time Multi-Agent Path Finding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16874v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16874v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raz Beck, Roni Stern
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Agent Pathfinding (MAPF) is the problem of finding paths for a set of
agents such that each agent reaches its desired destination while avoiding
collisions with the other agents. Many MAPF solvers are designed to run
offline, that is, first generate paths for all agents and then execute them.
Real-Time MAPF (RT-MAPF) embodies a realistic MAPF setup in which one cannot
wait until a complete path for each agent has been found before they start to
move. Instead, planning and execution are interleaved, where the agents must
commit to a fixed number of steps in a constant amount of computation time,
referred to as the planning budget. Existing solutions to RT-MAPF iteratively
call windowed versions of MAPF algorithms in every planning period, without
explicitly considering the size of the planning budget. We address this gap and
explore different policies for allocating the planning budget in windowed
versions of standard MAPF algorithms, namely Prioritized Planning (PrP) and
MAPF-LNS2. Our exploration shows that the baseline approach in which all agents
draw from a shared planning budget pool is ineffective in over-constrained
situations. Instead, policies that distribute the planning budget over the
agents are able to solve more problems with a smaller makespan.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Topology <span class="highlight-title">Optimization</span> of Leg Structures for Construction <span class="highlight-title">Robot</span>s Based on
  Variable Density Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Liu, Xianlong Yang, Weijun Wang, Wei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In complex terrain construction environments, there are high demands for
robots to achieve both high payload capacity and mobility flexibility. As the
key load-bearing component, the optimization of robotic leg structures is of
particular importance. Therefore, this study focuses on the optimization of leg
structures for construction robots, proposing a topology optimization strategy
based on the SIMP (Solid Isotropic Microstructures with Penalization) variable
density method along with a structural re-design approach. The design
performance is comprehensively validated through finite element analysis using
ANSYS. First, static and modal analyses are conducted to evaluate the
rationality of the initial design. Then, topology optimization using the
SIMP-based variable density method is applied to the femur section, which
accounts for the largest proportion of the leg's weight. Based on iterative
calculations, the femur undergoes secondary structural reconstruction. After
optimization, the mass of the femur is reduced by 19.45\%, and the overall leg
mass decreases by 7.92\%, achieving the goal of lightweight design. Finally,
static and modal analyses are conducted on the reconstructed leg. The results
demonstrate that the optimized leg still meets structural performance
requirements, validating the feasibility of lightweight design. This research
provides robust theoretical and technical support for lightweight construction
robot design and lays a foundation for their efficient operation in complex
construction environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Design and Dimensional <span class="highlight-title">Optimization</span> of Legged Structures for
  Construction <span class="highlight-title">Robot</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Liu, Xianlong Yang, Weijun Wang, Wei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Faced with complex and unstructured construction environments, wheeled and
tracked robots exhibit significant limitations in terrain adaptability and
flexibility, making it difficult to meet the requirements of autonomous
operation. Inspired by ants in nature, this paper proposes a leg configuration
design and optimization method tailored for construction scenarios, aiming to
enhance the autonomous mobility of construction robots. This paper analyzes the
full operational motion performance of the leg during both swing and stance
phases. First, based on kinematic modeling and multi-dimensional workspace
analysis, the concept of an "improved workspace" is introduced, and graphical
methods are used to optimize the leg dimensions during the swing phase.
Furthermore, a new concept of "average manipulability" is introduced based on
the velocity Jacobian matrix, and numerical solutions are applied to obtain the
leg segment ratio that maximizes manipulability. To overcome the difficulties
associated with traditional analytical methods, virtual prototype simulations
are conducted in ADAMS to explore the relationship between the robot body's
optimal flexibility and leg segment proportions. In summary, the leg segment
proportions with the best comprehensive motion performance are obtained. This
study presents the first multi-dimensional quantitative evaluation framework
for leg motion performance tailored for construction environments, providing a
structural design foundation for legged construction robots to achieve
autonomous mobility in complex terrains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COMPASS: Cooperative Multi-Agent Persistent Monitoring using
  Spatio-Temporal Attention Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingjian Zhang, Yizhuo Wang, Guillaume Sartoretti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Persistent monitoring of dynamic targets is essential in real-world
applications such as disaster response, environmental sensing, and wildlife
conservation, where mobile agents must continuously gather information under
uncertainty. We propose COMPASS, a multi-agent reinforcement learning (MARL)
framework that enables decentralized agents to persistently monitor multiple
moving targets efficiently. We model the environment as a graph, where nodes
represent spatial locations and edges capture topological proximity, allowing
agents to reason over structured layouts and revisit informative regions as
needed. Each agent independently selects actions based on a shared
spatio-temporal attention network that we design to integrate historical
observations and spatial context. We model target dynamics using Gaussian
Processes (GPs), which support principled belief updates and enable
uncertainty-aware planning. We train COMPASS using centralized value estimation
and decentralized policy execution under an adaptive reward setting. Our
extensive experiments demonstrate that COMPASS consistently outperforms strong
baselines in uncertainty reduction, target coverage, and coordination
efficiency across dynamic multi-target scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Trajectory</span> <span class="highlight-title">Planning</span> of a Curtain Wall Installation <span class="highlight-title">Robot</span> Based on
  Biomimetic Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Liu, Weijun Wang, Tianlun Huang, Zhiyong Wang, Wei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the robotics market rapidly evolves, energy consumption has become a
critical issue, particularly restricting the application of construction
robots. To tackle this challenge, our study innovatively draws inspiration from
the mechanics of human upper limb movements during weight lifting, proposing a
bio-inspired trajectory planning framework that incorporates human energy
conversion principles. By collecting motion trajectories and electromyography
(EMG) signals during dumbbell curls, we construct an anthropomorphic trajectory
planning that integrates human force exertion patterns and energy consumption
patterns. Utilizing the Particle Swarm Optimization (PSO) algorithm, we achieve
dynamic load distribution for robotic arm trajectory planning based on
human-like movement features. In practical application, these bio-inspired
movement characteristics are applied to curtain wall installation tasks,
validating the correctness and superiority of our trajectory planning method.
Simulation results demonstrate a 48.4% reduction in energy consumption through
intelligent conversion between kinetic and potential energy. This approach
provides new insights and theoretical support for optimizing energy use in
curtain wall installation robots during actual handling tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Wake-Up Time For Euclidean Freeze-Tag Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sharareh Alipour, Arash Ahadi, Kajal Baghestani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Freeze-Tag Problem (FTP) involves activating a set of initially asleep
robots as quickly as possible, starting from a single awake robot. Once
activated, a robot can assist in waking up other robots. Each active robot
moves at unit speed. The objective is to minimize the makespan, i.e., the time
required to activate the last robot. A key performance measure is the wake-up
ratio, defined as the maximum time needed to activate any number of robots in
any primary positions. This work focuses on the geometric (Euclidean) version
of FTP in $\mathbb{R}^d$ under the $\ell_p$ norm, where the initial distance
between each asleep robot and the single active robot is at most 1. For
$(\mathbb{R}^2, \ell_2)$, we improve the previous upper bound of 4.62 ([7],
CCCG 2024) to 4.31. Note that it is known that 3.82 is a lower bound for the
wake-up ratio. In $\mathbb{R}^3$, we propose a new strategy that achieves a
wake-up ratio of 12 for $(\mathbb{R}^3, \ell_1)$ and 12.76 for $(\mathbb{R}^3,
\ell_2)$, improving upon the previous bounds of 13 and $13\sqrt{3}$,
respectively, reported in [2].
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-aware Truck and Drone Delivery <span class="highlight-title">Planning</span> Using <span class="highlight-title">Optimization</span> &
  Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yineng Sun, Armin Fügenschuh, Vikrant Vaze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combining an energy-efficient drone with a high-capacity truck for last-mile
package delivery can benefit operators and customers by reducing delivery times
and environmental impact. However, directly integrating drone flight dynamics
into the combinatorially hard truck route planning problem is challenging.
Simplified models that ignore drone flight physics can lead to suboptimal
delivery plans. We propose an integrated formulation for the joint problem of
truck route and drone trajectory planning and a new end-to-end solution
approach that combines optimization and machine learning to generate
high-quality solutions in practical online runtimes. Our solution method trains
neural network predictors based on offline solutions to the drone trajectory
optimization problem instances to approximate drone flight times, and uses
these approximations to optimize the overall truck-and-drone delivery plan by
augmenting an existing order-first-split-second heuristic. Our method
explicitly incorporates key kinematics and energy equations in drone trajectory
optimization, and thereby outperforms state-of-the-art benchmarks that ignore
drone flight physics. Extensive experimentation using synthetic datasets and
real-world case studies shows that the integration of drone trajectories into
package delivery planning substantially improves system performance in terms of
tour duration and drone energy consumption. Our modeling and computational
framework can help delivery planners achieve annual savings worth millions of
dollars while also benefiting the environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GFM-Planner: Perception-Aware <span class="highlight-title">Trajectory</span> <span class="highlight-title">Planning</span> with <span class="highlight-title">Geometric</span> Feature
  Metric <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Lin, Xiaoxuan Zhang, Yang Liu, Dong Wang, Huchuan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Like humans who rely on landmarks for orientation, autonomous robots depend
on feature-rich environments for accurate localization. In this paper, we
propose the GFM-Planner, a perception-aware trajectory planning framework based
on the geometric feature metric, which enhances LiDAR localization accuracy by
guiding the robot to avoid degraded areas. First, we derive the Geometric
Feature Metric (GFM) from the fundamental LiDAR localization problem. Next, we
design a 2D grid-based Metric Encoding Map (MEM) to efficiently store GFM
values across the environment. A constant-time decoding algorithm is further
proposed to retrieve GFM values for arbitrary poses from the MEM. Finally, we
develop a perception-aware trajectory planning algorithm that improves LiDAR
localization capabilities by guiding the robot in selecting trajectories
through feature-rich areas. Both simulation and real-world experiments
demonstrate that our approach enables the robot to actively select trajectories
that significantly enhance LiDAR localization accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IROS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scanning Bot: Efficient Scan <span class="highlight-title">Planning</span> using Panoramic Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Euijeong Lee, Kyung Min Han, Young J. Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Panoramic RGB-D cameras are known for their ability to produce high quality
3D scene reconstructions. However, operating these cameras involves manually
selecting viewpoints and physically transporting the camera, making the
generation of a 3D model time consuming and tedious. Additionally, the process
can be challenging for novice users due to spatial constraints, such as
ensuring sufficient feature overlap between viewpoint frames. To address these
challenges, we propose a fully autonomous scan planning that generates an
efficient tour plan for environment scanning, ensuring collision-free
navigation and adequate overlap between viewpoints within the plan. Extensive
experiments conducted in both synthetic and real-world environments validate
the performance of our planner against state-of-the-art view planners. In
particular, our method achieved an average scan coverage of 99 percent in the
real-world experiment, with our approach being up to 3 times faster than
state-of-the-art planners in total scan time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Equivariant Goal Conditioned Contrastive <span class="highlight-title">Reinforcement</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arsh Tangri, Nichols Crawford Taylor, Haojie Huang, Robert Platt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Reinforcement Learning (CRL) provides a promising framework for
extracting useful structured representations from unlabeled interactions. By
pulling together state-action pairs and their corresponding future states,
while pushing apart negative pairs, CRL enables learning nontrivial policies
without manually designed rewards. In this work, we propose Equivariant CRL
(ECRL), which further structures the latent space using equivariant
constraints. By leveraging inherent symmetries in goal-conditioned manipulation
tasks, our method improves both sample efficiency and spatial generalization.
Specifically, we formally define Goal-Conditioned Group-Invariant MDPs to
characterize rotation-symmetric robotic manipulation tasks, and build on this
by introducing a novel rotation-invariant critic representation paired with a
rotation-equivariant actor for Contrastive RL. Our approach consistently
outperforms strong baselines across a range of simulated tasks in both
state-based and image-based settings. Finally, we extend our method to the
offline RL setting, demonstrating its effectiveness across multiple tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Benchmark</span>ing LLM Privacy Recognition for Social <span class="highlight-title">Robot</span> <span class="highlight-title">Decision</span> Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dakota Sullivan, Shirley Zhang, Jennica Li, Heather Kirkorian, Bilge Mutlu, Kassem Fawaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social robots are embodied agents that interact with people while following
human communication norms. These robots interact using verbal and non-verbal
cues, and share the physical environments of people. While social robots have
previously utilized rule-based systems or probabilistic models for user
interaction, the rapid evolution of large language models (LLMs) presents new
opportunities to develop LLM-empowered social robots for enhanced human-robot
interaction. To fully realize these capabilities, however, robots need to
collect data such as audio, fine-grained images, video, and locations. As a
result, LLMs often process sensitive personal information, particularly within
home environments. Given the tension between utility and privacy risks,
evaluating how current LLMs manage sensitive data is critical. Specifically, we
aim to explore the extent to which out-of-the-box LLMs are privacy-aware in the
context of household social robots. In this study, we present a set of
privacy-relevant scenarios crafted through the lens of Contextual Integrity
(CI). We first survey users' privacy preferences regarding in-home social robot
behaviors and then examine how their privacy orientation affects their choices
of these behaviors (N = 450). We then provide the same set of scenarios and
questions to state-of-the-art LLMs (N = 10) and find that the agreement between
humans and LLMs is low. To further investigate the capabilities of LLMs as a
potential privacy controller, we implement four additional prompting strategies
and compare their results. Finally, we discuss the implications and potential
of AI privacy awareness in human-robot interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures. Dakota Sullivan and Shirley Zhang contributed
  equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> DWSFormer: A Lightweight Inertial <span class="highlight-title">Odometry</span> Network for Complex Motion
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16121v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16121v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanshan Zhang, Qi Zhang, Si<span class="highlight-author">yue Wang</span>, Tianshui Wen, Ziheng Zhou, Lingxiang Zheng, Yu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inertial odometry (IO) directly estimates the position of a carrier from
inertial sensor measurements and serves as a core technology for the widespread
deployment of consumer grade localization systems. While existing IO methods
can accurately reconstruct simple and near linear motion trajectories, they
often fail to account for drift errors caused by complex motion patterns such
as turning. This limitation significantly degrades localization accuracy and
restricts the applicability of IO systems in real world scenarios. To address
these challenges, we propose a lightweight IO framework. Specifically, inertial
data is projected into a high dimensional implicit nonlinear feature space
using the Star Operation method, enabling the extraction of complex motion
features that are typically overlooked. We further introduce a collaborative
attention mechanism that jointly models global motion dynamics across both
channel and temporal dimensions. In addition, we design Multi Scale Gated
Convolution Units to capture fine grained dynamic variations throughout the
motion process, thereby enhancing the model's ability to learn rich and
expressive motion representations. Extensive experiments demonstrate that our
proposed method consistently outperforms SOTA baselines across six widely used
inertial datasets. Compared to baseline models on the RoNIN dataset, it
achieves reductions in ATE ranging from 2.26% to 65.78%, thereby establishing a
new benchmark in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> ResKACNNet: A Residual ChebyKAN Network for Inertial <span class="highlight-title">Odometry</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanshan Zhang, Tianshui Wen, Si<span class="highlight-author">yue Wang</span>, Qi Zhang, Ziheng Zhou, Huiru Zheng, Lingxiang Zheng, Yu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inertial Measurement Unit (IMU) has become a key technology for achieving
low-cost and precise positioning. However, traditional CNN-based inertial
positioning methods struggle to capture the nonlinear motion characteristics
and long-term dependencies in IMU data. To address this limitation, we propose
a novel inertial positioning network with a generic backbone called
ResChebyKAN, which leverages the nonlinear approximation capabilities of
Chebyshev polynomials to model complex motion patterns. Additionally, we
introduce an Efficient Kernel-based Self-Attention (EKSA) module to effectively
capture contextual information and enhance long-term dependency modeling.
Experimental results on public datasets (e.g., RIDI, RoNIN, RNIN-VIO, OxIOD,
IMUNet, and TLIO) demonstrate that our method reduces the absolute trajectory
error by 3.79% to 42.32% compared to existing benchmark methods. Furthermore,
we release a preprocessed dataset and empirically show that removing the
gravity component from acceleration data significantly improves inertial
positioning performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> FTIN: Frequency-Time Integration Network for Inertial <span class="highlight-title">Odometry</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanshan Zhang, Qi Zhang, Si<span class="highlight-author">yue Wang</span>, Tianshui Wen, Ziheng Zhou, Lingxiang Zheng, Yu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, machine learning has achieved significant advancements in
inertial odometry. However, most existing inertial odometry methods primarily
rely on CNNs in the time domain. These methods often struggle to capture
long-term dependency in inertial measurement unit data, thereby constraining
the potential for further improvements in localization accuracy. To address
these issues, we propose a novel network architecture that integrates both
frequency-domain and time-domain information. Specifically, we leverage the
global view and energy compaction properties of frequency-domain learning to
effectively model long-term dependency and reduce redundancy in IMU data.
Additionally, we introduce a Scalar LSTM to capture sequential dependencies in
the time domain, enabling cross-domain information fusion and providing a
stable and reliable reference for localization. Experimental evaluations on
multiple public datasets (e.g., RIDI, RoNIN, OxIOD, RNIN, TLIO, and IMUNet)
demonstrate the effectiveness of the proposed frequency-time domain fusion
strategy. Notably, on the RoNIN dataset, our method achieves a 43.0% reduction
in absolute trajectory error and a 13.1% reduction in relative trajectory error
compared to RoNIN ResNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ROADWork <span class="highlight-title">Dataset</span>: Learning to Recognize, Observe, Analyze and Drive
  Through Work Zones <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07661v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07661v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anurag Ghosh, Shen Zheng, Robert Tamburo, Khiem Vuong, Juan Alvarez-Padilla, Hailiang Zhu, Michael Cardei, Nicholas Dunn, Christoph Mertz, Srinivasa G. Narasimhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perceiving and autonomously navigating through work zones is a challenging
and underexplored problem. Open datasets for this long-tailed scenario are
scarce. We propose the ROADWork dataset to learn to recognize, observe,
analyze, and drive through work zones. State-of-the-art foundation models fail
when applied to work zones. Fine-tuning models on our dataset significantly
improves perception and navigation in work zones. With ROADWork dataset, we
discover new work zone images with higher precision (+32.5%) at a much higher
rate (12.8$\times$) around the world. Open-vocabulary methods fail too, whereas
fine-tuned detectors improve performance (+32.2 AP). Vision-Language Models
(VLMs) struggle to describe work zones, but fine-tuning substantially improves
performance (+36.7 SPICE).
  Beyond fine-tuning, we show the value of simple techniques. Video label
propagation provides additional gains (+2.6 AP) for instance segmentation.
While reading work zone signs, composing a detector and text spotter via
crop-scaling improves performance +14.2% 1-NED). Composing work zone detections
to provide context further reduces hallucinations (+3.9 SPICE) in VLMs. We
predict navigational goals and compute drivable paths from work zone videos.
Incorporating road work semantics ensures 53.6% goals have angular error (AE) <
0.5 (+9.9 %) and 75.3% pathways have AE < 0.5 (+8.1 %).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025 Accepted Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active Probing with Multimodal <span class="highlight-title">Prediction</span>s for Motion <span class="highlight-title">Planning</span> <span class="chip">IROS '25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.09822v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.09822v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Darshan Gadginmath, Farhad Nawaz, Minjun Sung, Faizan M Tariq, Sangjae Bae, David Isele, Fabio Pasqualetti, Jovin D'sa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigation in dynamic environments requires autonomous systems to reason
about uncertainties in the behavior of other agents. In this paper, we
introduce a unified framework that combines trajectory planning with multimodal
predictions and active probing to enhance decision-making under uncertainty. We
develop a novel risk metric that seamlessly integrates multimodal prediction
uncertainties through mixture models. When these uncertainties follow a
Gaussian mixture distribution, we prove that our risk metric admits a
closed-form solution, and is always finite, thus ensuring analytical
tractability. To reduce prediction ambiguity, we incorporate an active probing
mechanism that strategically selects actions to improve its estimates of
behavioral parameters of other agents, while simultaneously handling multimodal
uncertainties. We extensively evaluate our framework in autonomous navigation
scenarios using the MetaDrive simulation environment. Results demonstrate that
our active probing approach successfully navigates complex traffic scenarios
with uncertain predictions. Additionally, our framework shows robust
performance across diverse traffic agent behavior models, indicating its broad
applicability to real-world autonomous navigation challenges. Code and videos
are available at
https://darshangm.github.io/papers/active-probing-multimodal-predictions/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at IROS '25. 8 pages. 3 tables. 6 figures. Project page:
  https://darshangm.github.io/papers/active-probing-multimodal-predictions/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VL-Explore: Zero-shot Vision-Language <span class="highlight-title">Exploration</span> and Target Discovery
  by Mobile <span class="highlight-title">Robot</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08791v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08791v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Zhang, Adnan Abdullah, Sanjeev J. Koppal, Md Jahidul Islam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language navigation (VLN) has emerged as a promising paradigm,
enabling mobile robots to perform zero-shot inference and execute tasks without
specific pre-programming. However, current systems often separate map
exploration and path planning, with exploration relying on inefficient
algorithms due to limited (partially observed) environmental information. In
this paper, we present a novel navigation pipeline named "VL-Explore" for
simultaneous exploration and target discovery in unknown environments,
leveraging the capabilities of a vision-language model named CLIP. Our approach
requires only monocular vision and operates without any prior map or knowledge
about the target. For comprehensive evaluations, we designed a functional
prototype of a UGV (unmanned ground vehicle) system named "Open Rover", a
customized platform for general-purpose VLN tasks. We integrated and deployed
the VL-Explore pipeline on Open Rover to evaluate its throughput, obstacle
avoidance capability, and trajectory performance across various real-world
scenarios. Experimental results demonstrate that VL-Explore consistently
outperforms traditional map-traversal algorithms and achieves performance
comparable to path-planning methods that depend on prior map and target
knowledge. Notably, VL-Explore offers real-time active navigation without
requiring pre-captured candidate images or pre-built node graphs, addressing
key limitations of existing VLN pipelines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>V2, includes suppl as appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ X-MOBILITY: End-To-End Generalizable <span class="highlight-title">Navigation</span> via World Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17491v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17491v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Liu, Huihua Zhao, Chenran Li, Joydeep Biswas, Billy Okal, Pulkit Goyal, Yan Chang, Soha Pouya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  General-purpose navigation in challenging environments remains a significant
problem in robotics, with current state-of-the-art approaches facing myriad
limitations. Classical approaches struggle with cluttered settings and require
extensive tuning, while learning-based methods face difficulties generalizing
to out-of-distribution environments. This paper introduces X-Mobility, an
end-to-end generalizable navigation model that overcomes existing challenges by
leveraging three key ideas. First, X-Mobility employs an auto-regressive world
modeling architecture with a latent state space to capture world dynamics.
Second, a diverse set of multi-head decoders enables the model to learn a rich
state representation that correlates strongly with effective navigation skills.
Third, by decoupling world modeling from action policy, our architecture can
train effectively on a variety of data sources, both with and without expert
policies: off-policy data allows the model to learn world dynamics, while
on-policy data with supervisory control enables optimal action policy learning.
Through extensive experiments, we demonstrate that X-Mobility not only
generalizes effectively but also surpasses current state-of-the-art navigation
approaches. Additionally, X-Mobility also achieves zero-shot Sim2Real
transferability and shows strong potential for cross-embodiment generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM as a code generator in Agile Model Driven Development 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18489v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18489v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed R. Sadik, Sebastian Brulin, Markus Olhofer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging Large Language Models (LLM) like GPT4 in the auto generation of
code represents a significant advancement, yet it is not without its
challenges. The ambiguity inherent in natural language descriptions of software
poses substantial obstacles to generating deployable, structured artifacts.
This research champions Model Driven Development (MDD) as a viable strategy to
overcome these challenges, proposing an Agile Model Driven Development (AMDD)
approach that employs GPT4 as a code generator. This approach enhances the
flexibility and scalability of the code auto generation process and offers
agility that allows seamless adaptation to changes in models or deployment
environments. We illustrate this by modeling a multi agent Unmanned Vehicle
Fleet (UVF) system using the Unified Modeling Language (UML), significantly
reducing model ambiguity by integrating the Object Constraint Language (OCL)
for code structure meta modeling, and the FIPA ontology language for
communication semantics meta modeling. Applying GPT4 auto generation
capabilities yields Java and Python code that is compatible with the JADE and
PADE frameworks, respectively. Our thorough evaluation of the auto generated
code verifies its alignment with expected behaviors and identifies enhancements
in agent interactions. Structurally, we assessed the complexity of code derived
from a model constrained solely by OCL meta models, against that influenced by
both OCL and FIPA ontology meta models. The results indicate that the ontology
constrained meta model produces inherently more complex code, yet its
cyclomatic complexity remains within manageable levels, suggesting that
additional meta model constraints can be incorporated without exceeding the
high risk threshold for complexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Why Automate This? Exploring Correlations between Desire for <span class="highlight-title">Robot</span>ic
  Automation, Invested Time and Well-Being 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06348v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06348v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruchira Ray, Leona Pang, Sanjana Srivastava, Li Fei-Fei, Samantha Shorey, Roberto Martín-Martín
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the motivations underlying the human inclination to automate
tasks is vital to developing truly helpful robots integrated into daily life.
Accordingly, we ask: are individuals more inclined to automate chores based on
the time they consume or the feelings experienced while performing them? This
study explores these preferences and whether they vary across different social
groups (i.e., gender category and income level). Leveraging data from the
BEHAVIOR-1K dataset, the American Time-Use Survey, and the American Time-Use
Survey Well-Being Module, we investigate the relationship between the desire
for automation, time spent on daily activities, and their associated feelings -
Happiness, Meaningfulness, Sadness, Painfulness, Stressfulness, or Tiredness.
Our key findings show that, despite common assumptions, time spent does not
strongly relate to the desire for automation for the general population. For
the feelings analyzed, only happiness and pain are key indicators. Significant
differences by gender and economic level also emerged: Women prefer to automate
stressful activities, whereas men prefer to automate those that make them
unhappy; mid-income individuals prioritize automating less enjoyable and
meaningful activities, while low and high-income show no significant
correlations. We hope our research helps motivate technologies to develop
robots that match the priorities of potential users, moving domestic robotics
toward more socially relevant solutions. We open-source all the data, including
an online tool that enables the community to replicate our analysis and explore
additional trends at https://robin-lab.cs.utexas.edu/why-automate-this/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GR-3 Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15493v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15493v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chilam Cheang, Sijin Chen, Zhongren Cui, Yingdong Hu, Liqun Huang, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Xiao Ma, Hao Niu, Wenxuan Ou, Wanli Peng, Zeyu Ren, Haixin Shi, Jiawen Tian, Hongtao Wu, Xin Xiao, Yuyang Xiao, Jiafeng Xu, Yichu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We report our recent progress towards building generalist robot policies, the
development of GR-3. GR-3 is a large-scale vision-language-action (VLA) model.
It showcases exceptional capabilities in generalizing to novel objects,
environments, and instructions involving abstract concepts. Furthermore, it can
be efficiently fine-tuned with minimal human trajectory data, enabling rapid
and cost-effective adaptation to new settings. GR-3 also excels in handling
long-horizon and dexterous tasks, including those requiring bi-manual
manipulation and mobile movement, showcasing robust and reliable performance.
These capabilities are achieved through a multi-faceted training recipe that
includes co-training with web-scale vision-language data, efficient fine-tuning
from human trajectory data collected via VR devices, and effective imitation
learning with robot trajectory data. In addition, we introduce ByteMini, a
versatile bi-manual mobile robot designed with exceptional flexibility and
reliability, capable of accomplishing a wide range of tasks when integrated
with GR-3. Through extensive real-world experiments, we show GR-3 surpasses the
state-of-the-art baseline method, $\pi_0$, on a wide variety of challenging
tasks. We hope GR-3 can serve as a step towards building generalist robots
capable of assisting humans in daily life.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech report. Authors are listed in alphabetical order. Project page:
  https://seed.bytedance.com/GR3/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Robust</span> Ladder Climbing with a Quadrupedal <span class="highlight-title">Robot</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17731v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17731v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dylan Vogel, Robert Baines, Joseph Church, Julian Lotzer, Karl Werner, Marco Hutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quadruped robots are proliferating in industrial environments where they
carry sensor payloads and serve as autonomous inspection platforms. Despite the
advantages of legged robots over their wheeled counterparts on rough and uneven
terrain, they are still unable to reliably negotiate a ubiquitous feature of
industrial infrastructure: ladders. Inability to traverse ladders prevents
quadrupeds from inspecting dangerous locations, puts humans in harm's way, and
reduces industrial site productivity. In this paper, we learn quadrupedal
ladder climbing via a reinforcement learning-based control policy and a
complementary hooked end effector. We evaluate the robustness in simulation
across different ladder inclinations, rung geometries, and inter-rung spacings.
On hardware, we demonstrate zero-shot transfer with an overall 90% success rate
at ladder angles ranging from 70{\deg} to 90{\deg}, consistent climbing
performance during unmodeled perturbations, and climbing speeds 232x faster
than the state of the art. This work expands the scope of industrial quadruped
robot applications beyond inspection on nominal terrains to challenging
infrastructural features in the environment, highlighting synergies between
robot morphology and control policy when performing complex skills. More
information can be found at the project website:
https://sites.google.com/leggedrobotics.com/climbingladders.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website:
  https://sites.google.com/leggedrobotics.com/climbingladders</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Gaussian Mixture Models-based Anomaly <span class="highlight-title">Detection</span> for
  under-constrained Cable-Driven Parallel <span class="highlight-title">Robot</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07714v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07714v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julio Garrido, Javier Vales, Diego Silva-Muñiz, Enrique Riveiro, Pablo López-Matencio, Josué Rivera-Andrade
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cable-Driven Parallel Robots (CDPRs) are increasingly used for load
manipulation tasks involving predefined toolpaths with intermediate stops. At
each stop, where the platform maintains a fixed pose and the motors keep the
cables under tension, the system must evaluate whether it is safe to proceed by
detecting anomalies that could compromise performance (e.g., wind gusts or
cable impacts). This paper investigates whether anomalies can be detected using
only motor torque data, without additional sensors. It introduces an adaptive,
unsupervised outlier detection algorithm based on Gaussian Mixture Models
(GMMs) to identify anomalies from torque signals. The method starts with a
brief calibration period, just a few seconds, during which a GMM is fit on
known anomaly-free data. Real-time torque measurements are then evaluated using
Mahalanobis distance from the GMM, with statistically derived thresholds
triggering anomaly flags. Model parameters are periodically updated using the
latest segments identified as anomaly-free to adapt to changing conditions.
Validation includes 14 long-duration test sessions simulating varied wind
intensities. The proposed method achieves a 100% true positive rate and 95.4%
average true negative rate, with 1-second detection latency. Comparative
evaluation against power threshold and non-adaptive GMM methods indicates
higher robustness to drift and environmental variation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Potential of Segment Anything Model 2 for RGB-Thermal
  Semantic <span class="highlight-title">Segmentation</span> with Language Guidance <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02581v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02581v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Zhao, Fei Teng, Kai Luo, Guoqiang Zhao, Zhiyong Li, Xu Zheng, Kailun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The perception capability of robotic systems relies on the richness of the
dataset. Although Segment Anything Model 2 (SAM2), trained on large datasets,
demonstrates strong perception potential in perception tasks, its inherent
training paradigm prevents it from being suitable for RGB-T tasks. To address
these challenges, we propose SHIFNet, a novel SAM2-driven Hybrid Interaction
Paradigm that unlocks the potential of SAM2 with linguistic guidance for
efficient RGB-Thermal perception. Our framework consists of two key components:
(1) Semantic-Aware Cross-modal Fusion (SACF) module that dynamically balances
modality contributions through text-guided affinity learning, overcoming SAM2's
inherent RGB bias; (2) Heterogeneous Prompting Decoder (HPD) that enhances
global semantic information through a semantic enhancement module and then
combined with category embeddings to amplify cross-modal semantic consistency.
With 32.27M trainable parameters, SHIFNet achieves state-of-the-art
segmentation performance on public benchmarks, reaching 89.8% on PST900 and
67.8% on FMB, respectively. The framework facilitates the adaptation of
pre-trained large models to RGB-T segmentation tasks, effectively mitigating
the high costs associated with data collection while endowing robotic systems
with comprehensive perception capabilities. The source code will be made
publicly available at https://github.com/iAsakiT3T/SHIFNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IROS 2025. The source code will be made publicly
  available at https://github.com/iAsakiT3T/SHIFNet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Growing Trees with an Agent: Accelerating RRTs with Learned, Multi-Step
  Episodic <span class="highlight-title">Exploration</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06605v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06605v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classical sampling-based motion planners like the RRTs suffer from
inefficiencies, particularly in cluttered or high-dimensional spaces, due to
their reliance on undirected, random sampling. This paper introduces the
Episodic RRT, a novel hybrid planning framework that replaces the primitive of
a random point with a learned, multi-step "exploratory episode" generated by a
Deep Reinforcement Learning agent. By making the DRL agent the engine of
exploration, ERRT transforms the search process from a diffuse, volumetric
expansion into a directed, branch-like growth. This paradigm shift yields key
advantages: it counters the curse of dimensionality with focused exploration,
minimizes expensive collision checks by proactively proposing locally valid
paths, and improves connectivity by generating inherently connected path
segments. We demonstrate through extensive empirical evaluation across 2D, 3D,
and 6D environments that ERRT and its variants consistently and significantly
outperform their classical counterparts without any GPU acceleration. In a
challenging 6D robotic arm scenario, ERRT achieves a 98% success rate compared
to 19% for RRT, is up to 107x faster, reduces collision checks by over 99.6%,
and finds initial paths that are nearly 50% shorter. Furthermore, its
asymptotically optimal variant, ERRT*, demonstrates vastly superior anytime
performance, refining solutions to near-optimality up to 29x faster than
standard RRT* in 3D environments. Code:
https://xinyuwuu.github.io/Episodic_RRT/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One-Shot Affordance Grounding of <span class="highlight-title">Deformable</span> Objects in Egocentric
  Organizing Scenes <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01092v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01092v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanjun Jia, Fan Yang, Mengfei Duan, Xianchi Chen, Yinxi Wang, Yiming Jiang, Wenrui Chen, Kailun Yang, Zhiyong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deformable object manipulation in robotics presents significant challenges
due to uncertainties in component properties, diverse configurations, visual
interference, and ambiguous prompts. These factors complicate both perception
and control tasks. To address these challenges, we propose a novel method for
One-Shot Affordance Grounding of Deformable Objects (OS-AGDO) in egocentric
organizing scenes, enabling robots to recognize previously unseen deformable
objects with varying colors and shapes using minimal samples. Specifically, we
first introduce the Deformable Object Semantic Enhancement Module (DefoSEM),
which enhances hierarchical understanding of the internal structure and
improves the ability to accurately identify local features, even under
conditions of weak component information. Next, we propose the ORB-Enhanced
Keypoint Fusion Module (OEKFM), which optimizes feature extraction of key
components by leveraging geometric constraints and improves adaptability to
diversity and visual interference. Additionally, we propose an
instance-conditional prompt based on image data and task context, which
effectively mitigates the issue of region ambiguity caused by prompt words. To
validate these methods, we construct a diverse real-world dataset, AGDDO15,
which includes 15 common types of deformable objects and their associated
organizational actions. Experimental results demonstrate that our approach
significantly outperforms state-of-the-art methods, achieving improvements of
6.2%, 3.2%, and 2.9% in KLD, SIM, and NSS metrics, respectively, while
exhibiting high generalization performance. Source code and benchmark dataset
are made publicly available at https://github.com/Dikay1/OS-AGDO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IROS 2025. Source code and benchmark dataset will be
  publicly available at https://github.com/Dikay1/OS-AGDO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Progressive-Resolution Policy Distillation: Leveraging Coarse-Resolution
  <span class="highlight-title">Simulation</span>s for Time-Efficient Fine-Resolution Policy Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07477v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07477v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuki Kadokawa, Hirotaka Tahara, Takamitsu Matsubara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In earthwork and construction, excavators often encounter large rocks mixed
with various soil conditions, requiring skilled operators. This paper presents
a framework for achieving autonomous excavation using reinforcement learning
(RL) through a rock excavation simulator. In the simulation, resolution can be
defined by the particle size/number in the whole soil space. Fine-resolution
simulations closely mimic real-world behavior but demand significant
calculation time and challenging sample collection, while coarse-resolution
simulations enable faster sample collection but deviate from real-world
behavior. To combine the advantages of both resolutions, we explore using
policies developed in coarse-resolution simulations for pre-training in
fine-resolution simulations. To this end, we propose a novel policy learning
framework called Progressive-Resolution Policy Distillation (PRPD), which
progressively transfers policies through some middle-resolution simulations
with conservative policy transfer to avoid domain gaps that could lead to
policy transfer failure. Validation in a rock excavation simulator and nine
real-world rock environments demonstrated that PRPD reduced sampling time to
less than 1/7 while maintaining task success rates comparable to those achieved
through policy learning in a fine-resolution simulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted for IEEE Transactions on Automation Science and Engineering
  (T-ASE)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PR2: A Physics- and Photo-realistic Humanoid Testbed with Pilot Study in
  Competition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01559v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01559v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hangxin Liu, Qi Xie, Zeyu Zhang, Tao Yuan, Song Wang, Zaijin Wang, Xiaokun Leng, Lining Sun, Jingwen Zhang, Zhicheng He, Yao Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the development of a Physics-realistic and
Photo-realistic humanoid robot testbed, PR2, to facilitate collaborative
research between Embodied Artificial Intelligence (Embodied AI) and robotics.
PR2 offers high-quality scene rendering and robot dynamic simulation, enabling
(i) the creation of diverse scenes using various digital assets, (ii) the
integration of advanced perception or foundation models, and (iii) the
implementation of planning and control algorithms for dynamic humanoid robot
behaviors based on environmental feedback. The beta version of PR2 has been
deployed for the simulation track of a nationwide full-size humanoid robot
competition for college students, attracting 137 teams and over 400
participants within four months. This competition covered traditional tasks in
bipedal walking, as well as novel challenges in loco-manipulation and
language-instruction-based object search, marking a first for public college
robotics competitions. A retrospective analysis of the competition suggests
that future events should emphasize the integration of locomotion with
manipulation and perception. By making the PR2 testbed publicly available at
https://github.com/pr2-humanoid/PR2-Platform, we aim to further advance
education and training in humanoid robotics. Video demonstration:
https://pr2-humanoid.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ eKalibr-Stereo: Continuous-Time Spatiotemporal <span class="highlight-title">Calibration</span> for
  Event-Based Stereo Visual Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.04451v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.04451v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuolong Chen, Xingxing Li, Liu Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The bioinspired event camera, distinguished by its exceptional temporal
resolution, high dynamic range, and low power consumption, has been extensively
studied in recent years for motion estimation, robotic perception, and object
detection. In ego-motion estimation, the stereo event camera setup is commonly
adopted due to its direct scale perception and depth recovery. For optimal
stereo visual fusion, accurate spatiotemporal (extrinsic and temporal)
calibration is required. Considering that few stereo visual calibrators
orienting to event cameras exist, based on our previous work eKalibr (an event
camera intrinsic calibrator), we propose eKalibr-Stereo for accurate
spatiotemporal calibration of event-based stereo visual systems. To improve the
continuity of grid pattern tracking, building upon the grid pattern recognition
method in eKalibr, an additional motion prior-based tracking module is designed
in eKalibr-Stereo to track incomplete grid patterns. Based on tracked grid
patterns, a two-step initialization procedure is performed to recover initial
guesses of piece-wise B-splines and spatiotemporal parameters, followed by a
continuous-time batch bundle adjustment to refine the initialized states to
optimal ones. The results of extensive real-world experiments show that
eKalibr-Stereo can achieve accurate event-based stereo spatiotemporal
calibration. The implementation of eKalibr-Stereo is open-sourced at
(https://github.com/Unsigned-Long/eKalibr) to benefit the research community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GeoFlow-<span class="highlight-title">SLAM</span>: A <span class="highlight-title">Robust</span> Tightly-Coupled RGBD-Inertial and Legged <span class="highlight-title">Odometry</span>
  <span class="highlight-title">Fusion</span> <span class="highlight-title">SLAM</span> for <span class="highlight-title">Dynamic</span> Legged <span class="highlight-title">Robot</span>ics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.14247v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.14247v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingyang Xiao, Xiaolin Zhou, Liu Liu, Wei Sui, Wei Feng, Jiaxiong Qiu, Xinjie Wang, Zhizhong Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents GeoFlow-SLAM, a robust and effective Tightly-Coupled
RGBD-inertial SLAM for legged robotics undergoing aggressive and high-frequency
motions.By integrating geometric consistency, legged odometry constraints, and
dual-stream optical flow (GeoFlow), our method addresses three critical
challenges:feature matching and pose initialization failures during fast
locomotion and visual feature scarcity in texture-less scenes.Specifically, in
rapid motion scenarios, feature matching is notably enhanced by leveraging
dual-stream optical flow, which combines prior map points and poses.
Additionally, we propose a robust pose initialization method for fast
locomotion and IMU error in legged robots, integrating IMU/Legged odometry,
inter-frame Perspective-n-Point (PnP), and Generalized Iterative Closest Point
(GICP). Furthermore, a novel optimization framework that tightly couples
depth-to-map and GICP geometric constraints is first introduced to improve the
robustness and accuracy in long-duration, visually texture-less environments.
The proposed algorithms achieve state-of-the-art (SOTA) on collected legged
robots and open-source datasets. To further promote research and development,
the open-source datasets and code will be made publicly available at
https://github.com/HorizonRobotics/GeoFlowSlam
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bio-Skin: A Cost-Effective Thermostatic Tactile Sensor with Multi-Modal
  Force and Temperature <span class="highlight-title">Detection</span> <span class="chip">IROS2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.07989v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.07989v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Guo, Haoyang Wang, Zhengxiong Li, Lingfeng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tactile sensors can significantly enhance the perception of humanoid robotics
systems by providing contact information that facilitates human-like
interactions. However, existing commercial tactile sensors focus on improving
the resolution and sensitivity of single-modal detection with high-cost
components and densely integrated design, incurring complex manufacturing
processes and unaffordable prices. In this work, we present Bio-Skin, a
cost-effective multi-modal tactile sensor that utilizes single-axis Hall-effect
sensors for planar normal force measurement and bar-shape piezo resistors for
2D shear force measurement. A thermistor coupling with a heating wire is
integrated into a silicone body to achieve temperature sensation and
thermostatic function analogous to human skin. We also present a
cross-reference framework to validate the two modalities of the force sensing
signal, improving the sensing fidelity in a complex electromagnetic
environment. Bio-Skin has a multi-layer design, and each layer is manufactured
sequentially and subsequently integrated, thereby offering a fast production
pathway. After calibration, Bio-Skin demonstrates performance metrics-including
signal-to-range ratio, sampling rate, and measurement range-comparable to
current commercial products, with one-tenth of the cost. The sensor's
real-world performance is evaluated using an Allegro hand in object grasping
tasks, while its temperature regulation functionality was assessed in a
material detection task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted by IROS2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SciFi-<span class="highlight-title">Benchmark</span>: Leveraging Science Fiction To Improve <span class="highlight-title">Robot</span> Behavior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.10706v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.10706v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Sermanet, Anirudha Majumdar, Vikas Sindhwani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the recent rate of progress in artificial intelligence (AI) and
robotics, a tantalizing question is emerging: would robots controlled by
emerging AI systems be strongly aligned with human values? In this work, we
propose a scalable way to probe this question by generating a benchmark
spanning the key moments in 824 major pieces of science fiction literature
(movies, tv, novels and scientific books) where an agent (AI or robot) made
critical decisions (good or bad). We use a state-of-the-art LLM's recollection
of each key moment to generate questions in similar situations, the decisions
made by the agent, and alternative decisions it could have made (good or bad).
We then measure an approximation of how well models align with human values on
a set of human-voted answers. We also generate rules that can be automatically
improved via an amendment process in order to generate the first Sci-Fi
inspired constitutions for promoting ethical behavior in AIs and robots in the
real world. Our first finding is that modern LLMs paired with constitutions
turn out to be well-aligned with human values (95.8%), contrary to unsettling
decisions typically made in Sci-Fi (only 21.2% alignment). Secondly, we find
that generated constitutions substantially increase alignment compared to the
base model (79.4% to 95.8%), and show resilience to an adversarial prompt
setting (23.3% to 92.3%). Additionally, we find that those constitutions are
among the top performers on the ASIMOV Benchmark which is derived from
real-world images and hospital injury reports. Sci-Fi-inspired constitutions
are thus highly aligned and applicable in real-world situations. We release
SciFi-Benchmark: a large-scale dataset to advance robot ethics and safety
research. It comprises 9,056 questions and 53,384 answers generated through a
novel LLM-introspection process, in addition to a smaller human-labeled
evaluation set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Minor improvements over previous version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asymptotically Optimal Lazy <span class="highlight-title">Life</span>long Sampling-based Algorithm for
  Efficient Motion <span class="highlight-title">Planning</span> in <span class="highlight-title">Dynamic</span> Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06521v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06521v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Huang, Jingwen Yu, Jiankun Wang, Xingjian Jing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper introduces an asymptotically optimal lifelong sampling-based path
planning algorithm that combines the merits of lifelong planning algorithms and
lazy search algorithms for rapid replanning in dynamic environments where edge
evaluation is expensive. By evaluating only sub-path candidates for the optimal
solution, the algorithm saves considerable evaluation time and thereby reduces
the overall planning cost. It employs a novel informed rewiring cascade to
efficiently repair the search tree when the underlying search graph changes.
Theoretical analysis indicates that the proposed algorithm converges to the
optimal solution as long as sufficient planning time is given. Planning results
on robotic systems with $\mathbb{SE}(3)$ and $\mathbb{R}^7$ state spaces in
challenging environments highlight the superior performance of the proposed
algorithm over various state-of-the-art sampling-based planners in both static
and dynamic motion planning tasks. The experiment of planning for a Turtlebot 4
operating in a dynamic environment with several moving pedestrians further
verifies the feasibility and advantages of the proposed algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beacon: A Naturalistic Driving <span class="highlight-title">Dataset</span> During Blackouts for <span class="highlight-title">Benchmark</span>ing
  Traffic Reconstruction and <span class="highlight-title">Control</span> <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14208v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14208v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Supriya Sarker, Iftekharul Islam, Bibek Poudel, Weizi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extreme weather and infrastructure vulnerabilities pose significant
challenges to urban mobility, particularly at intersections where signals
become inoperative. To address this growing concern, we introduce Beacon, a
naturalistic driving dataset capturing traffic dynamics during blackouts at two
major intersections in Memphis, TN, USA. The dataset provides detailed traffic
movements, including timesteps, origin, and destination lanes for each vehicle
over four hours of peak periods. We analyze traffic demand, vehicle
trajectories, and density across different scenarios, demonstrating
high-fidelity reconstruction under unsignalized, signalized, and mixed traffic
conditions. We find that integrating robot vehicles (RVs) into traffic flow can
substantially reduce intersection delays, with wait time improvements of up to
82.6%. However, this enhanced traffic efficiency comes with varying
environmental impacts, as decreased vehicle idling may lead to higher overall
CO2 emissions. To the best of our knowledge, Beacon is the first publicly
available traffic dataset for naturalistic driving behaviors during blackouts
at intersections.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE/RSJ International Conference on Intelligent Robots and Systems
  (IROS), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Goal-Oriented <span class="highlight-title">Reinforcement</span> Learning-Based Path <span class="highlight-title">Planning</span> Algorithm for
  Modular Self-Reconfigurable Satellites 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.01966v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.01966v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bofei Liu, Dong Ye, Zunhao Yao, Zhaowei Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modular self-reconfigurable satellites refer to satellite clusters composed
of individual modular units capable of altering their configurations. The
configuration changes enable the execution of diverse tasks and mission
objectives. Existing path planning algorithms for reconfiguration often suffer
from high computational complexity, poor generalization capability, and limited
support for diverse target configurations. To address these challenges, this
paper proposes a goal-oriented reinforcement learning-based path planning
algorithm. This algorithm is the first to address the challenge that previous
reinforcement learning methods failed to overcome, namely handling multiple
target configurations. Moreover, techniques such as Hindsight Experience Replay
and Invalid Action Masking are incorporated to overcome the significant
obstacles posed by sparse rewards and invalid actions. Based on these designs,
our model achieves a 95% and 73% success rate in reaching arbitrary target
configurations in a modular satellite cluster composed of four and six units,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Curating Demonstrations using Online Experience 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03707v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03707v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Annie S. Chen, Alec M. Lessing, Yuejiang Liu, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many robot demonstration datasets contain heterogeneous demonstrations of
varying quality. This heterogeneity may benefit policy pre-training, but can
hinder robot performance when used with a final imitation learning objective.
In particular, some strategies in the data may be less reliable than others or
may be underrepresented in the data, leading to poor performance when such
strategies are sampled at test time. Moreover, such unreliable or
underrepresented strategies can be difficult even for people to discern, and
sifting through demonstration datasets is time-consuming and costly. On the
other hand, policy performance when trained on such demonstrations can reflect
the reliability of different strategies. We thus propose for robots to
self-curate based on online robot experience (Demo-SCORE). More specifically,
we train and cross-validate a classifier to discern successful policy roll-outs
from unsuccessful ones and use the classifier to filter heterogeneous
demonstration datasets. Our experiments in simulation and the real world show
that Demo-SCORE can effectively identify suboptimal demonstrations without
manual curation. Notably, Demo-SCORE achieves over 15-35% higher absolute
success rate in the resulting policy compared to the base policy trained with
all original demonstrations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adapt On-the-Go: Behavior Modulation for Single-<span class="highlight-title">Life</span> <span class="highlight-title">Robot</span> Deployment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01059v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01059v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Annie S. Chen, Govind Chada, Laura Smith, Archit Sharma, Zipeng Fu, Sergey Levine, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To succeed in the real world, robots must cope with situations that differ
from those seen during training. We study the problem of adapting on-the-fly to
such novel scenarios during deployment, by drawing upon a diverse repertoire of
previouslylearned behaviors. Our approach, RObust Autonomous Modulation (ROAM),
introduces a mechanism based on the perceived value of pre-trained behaviors to
select and adapt pre-trained behaviors to the situation at hand. Crucially,
this adaptation process all happens within a single episode at test time,
without any human supervision. We demonstrate that ROAM enables a robot to
adapt rapidly to changes in dynamics both in simulation and on a real Go1
quadruped, even successfully moving forward with roller skates on its feet. Our
approach adapts over 2x as efficiently compared to existing methods when facing
a variety of out-of-distribution situations during deployment by effectively
choosing and adapting relevant behaviors on-the-fly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human-Machine Shared <span class="highlight-title">Control</span> Approach for the Takeover of Cooperative
  Adaptive Cruise <span class="highlight-title">Control</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11551v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11551v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Wang, Zhexi Lian, Zhenning Li, Jiawei Wang, Arno Eichberger, Jia Hu, Yongyu Chen, Yongji Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cooperative Adaptive Cruise Control (CACC) often requires human takeover for
tasks such as exiting a freeway. Direct human takeover can pose significant
risks, especially given the close-following strategy employed by CACC, which
might cause drivers to feel unsafe and execute hard braking, potentially
leading to collisions. This research aims to develop a CACC takeover controller
that ensures a smooth transition from automated to human control. The proposed
CACC takeover maneuver employs an indirect human-machine shared control
approach, modeled as a Stackelberg competition where the machine acts as the
leader and the human as the follower. The machine guides the human to respond
in a manner that aligns with the machine's expectations, aiding in maintaining
following stability. Additionally, the human reaction function is integrated
into the machine's predictive control system, moving beyond a simple
"prediction-planning" pipeline to enhance planning optimality. The controller
has been verified to i) enable a smooth takeover maneuver of CACC; ii) ensure
string stability in the condition that the platoon has less than 6 CAVs and
human control authority is less than 40%; iii) enhance both perceived and
actual safety through machine interventions; and iv) reduce the impact on
upstream traffic by up to 60%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Transactions on Intelligent Transportation Systems (2025)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-07-21T00:00:00Z">2025-07-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">46</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Therapist-Exoskeleton-Patient Interaction: An Immersive Gait Therapy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emek Barış Küçüktabak, Matthew R. Short, Lorenzo Vianello, Daniel Ludvig, Levi Hargrove, Kevin Lynch, Jose Pons
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Following a stroke, individuals often experience mobility and balance
impairments due to lower-limb weakness and loss of independent joint control.
Gait recovery is a key goal of rehabilitation, traditionally achieved through
high-intensity therapist-led training. However, manual assistance can be
physically demanding and limits the therapist's ability to interact with
multiple joints simultaneously. Robotic exoskeletons offer multi-joint support,
reduce therapist strain, and provide objective feedback, but current control
strategies often limit therapist involvement and adaptability.
  We present a novel gait rehabilitation paradigm based on physical
Human-Robot-Human Interaction (pHRHI), where both the therapist and the
post-stroke individual wear lower-limb exoskeletons virtually connected at the
hips and knees via spring-damper elements. This enables bidirectional
interaction, allowing the therapist to guide movement and receive haptic
feedback. In a study with eight chronic stroke patients, pHRHI training
outperformed conventional therapist-guided treadmill walking, leading to
increased joint range of motion, step metrics, muscle activation, and
motivation. These results highlight pHRHI's potential to combine robotic
precision with therapist intuition for improved rehabilitation outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Semantic <span class="highlight-title">Segmentation</span> from Ultra-Low-Resolution RGB Images
  Applied to Privacy-Preserving Object-Goal <span class="highlight-title">Navigation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16034v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16034v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuying Huang, Sicong Pan, Olga Zatsarynna, Juergen Gall, Maren Bennewitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User privacy in mobile robotics has become a critical concern. Existing
methods typically prioritize either the performance of downstream robotic tasks
or privacy protection, with the latter often constraining the effectiveness of
task execution. To jointly address both objectives, we study semantic-based
robot navigation in an ultra-low-resolution setting to preserve visual privacy.
A key challenge in such scenarios is recovering semantic segmentation from
ultra-low-resolution RGB images. In this work, we introduce a novel fully
joint-learning method that integrates an agglomerative feature extractor and a
segmentation-aware discriminator to solve ultra-low-resolution semantic
segmentation, thereby enabling privacy-preserving, semantic object-goal
navigation. Our method outperforms different baselines on ultra-low-resolution
semantic segmentation and our improved segmentation results increase the
success rate of the semantic object-goal navigation in a real-world
privacy-constrained scenario.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to RA-L</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive <span class="highlight-title">Evaluation</span> of <span class="highlight-title">LiDAR</span> <span class="highlight-title">Odometry</span> Techniques <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16000v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16000v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Easton Potokar, Michael Kaess
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Light Detection and Ranging (LiDAR) sensors have become the sensor of choice
for many robotic state estimation tasks. Because of this, in recent years there
has been significant work done to fine the most accurate method to perform
state estimation using these sensors. In each of these prior works, an
explosion of possible technique combinations has occurred, with each work
comparing LiDAR Odometry (LO) "pipelines" to prior "pipelines". Unfortunately,
little work up to this point has performed the significant amount of ablation
studies comparing the various building-blocks of a LO pipeline. In this work,
we summarize the various techniques that go into defining a LO pipeline and
empirically evaluate these LO components on an expansive number of datasets
across environments, LiDAR types, and vehicle motions. Finally, we make
empirically-backed recommendations for the design of future LO pipelines to
provide the most accurate and reliable performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IROS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Task <span class="highlight-title">Planning</span> with Neuro-Symbolic Relaxation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiwei Du, Bowen Li, Yi Du, Shaoshu Su, Taimeng Fu, Zitong Zhan, Zhipeng Zhao, Chen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world task planning requires long-horizon reasoning over large sets of
entities with complex relationships and attributes, leading to a combinatorial
explosion for classical symbolic planners. To prune the search space, recent
methods prioritize searching on a simplified task only containing a few
"important" entities predicted by a neural network. However, such a simple
neuro-symbolic (NeSy) integration risks omitting critical entities and wasting
resources on unsolvable simplified tasks. To enable Fast and reliable planning,
we introduce a NeSy relaxation strategy (Flax), combining neural importance
prediction with symbolic expansion. Specifically, we first learn a graph neural
network to predict entity importance to create a simplified task and solve it
with a symbolic planner. Then, we solve a rule-relaxed task to obtain a quick
rough plan, and reintegrate all referenced entities into the simplified task to
recover any overlooked but essential elements. Finally, we apply complementary
rules to refine the updated task, keeping it both reliable and compact.
Extensive experiments are conducted on both synthetic and real-world maze
navigation benchmarks where a robot must traverse through a maze and interact
with movable objects. The results show that Flax boosts the average success
rate by 20.82% and cuts mean wall-clock planning time by 17.65% compared with
the state-of-the-art NeSy baseline. We expect that Flax offers a practical path
toward fast, scalable, long-horizon task planning in complex environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Look, Focus, Act: Efficient and <span class="highlight-title">Robust</span> <span class="highlight-title">Robot</span> Learning via Human Gaze and
  Foveated Vision Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ian Chuang, Andrew Lee, Dechen Gao, Jinyu Zou, Iman Soltani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human vision is a highly active process driven by gaze, which directs
attention and fixation to task-relevant regions and dramatically reduces visual
processing. In contrast, robot learning systems typically rely on passive,
uniform processing of raw camera images. In this work, we explore how
incorporating human-like active gaze into robotic policies can enhance both
efficiency and performance. We build on recent advances in foveated image
processing and apply them to an Active Vision robot system that emulates both
human head movement and eye tracking. Extending prior work on the AV-ALOHA
robot simulation platform, we introduce a framework for simultaneously
collecting eye-tracking data and robot demonstrations from a human operator as
well as a simulation benchmark and dataset for training robot policies that
incorporate human gaze. Given the widespread use of Vision Transformers (ViTs)
in robot learning, we integrate gaze information into ViTs using a foveated
patch tokenization scheme inspired by recent work in image segmentation.
Compared to uniform patch tokenization, this significantly reduces the number
of tokens-and thus computation-without sacrificing visual fidelity near regions
of interest. We also explore two approaches to gaze imitation and prediction
from human data. The first is a two-stage model that predicts gaze to guide
foveation and action; the second integrates gaze into the action space,
allowing the policy to jointly predict gaze and actions end-to-end. Our results
show that our method for foveated robot vision not only drastically reduces
computational overhead, but also improves performance for high precision tasks
and robustness to unseen distractors. Together, these findings suggest that
human-inspired visual processing offers a useful inductive bias for robotic
vision systems. https://ian-chuang.github.io/gaze-av-aloha/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interleaved LLM and Motion <span class="highlight-title">Planning</span> for Generalized Multi-Object
  Collection in Large Scene <span class="highlight-title">Graph</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruochu Yang, Yu Zhou, Fumin Zhang, Mengxue Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Household robots have been a longstanding research topic, but they still lack
human-like intelligence, particularly in manipulating open-set objects and
navigating large environments efficiently and accurately. To push this
boundary, we consider a generalized multi-object collection problem in large
scene graphs, where the robot needs to pick up and place multiple objects
across multiple locations in a long mission of multiple human commands. This
problem is extremely challenging since it requires long-horizon planning in a
vast action-state space under high uncertainties. To this end, we propose a
novel interleaved LLM and motion planning algorithm Inter-LLM. By designing a
multimodal action cost similarity function, our algorithm can both reflect the
history and look into the future to optimize plans, striking a good balance of
quality and efficiency. Simulation experiments demonstrate that compared with
latest works, our algorithm improves the overall mission performance by 30% in
terms of fulfilling human commands, maximizing mission success rates, and
minimizing mission costs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gaze-supported Large Language Model Framework for Bi-directional
  Human-<span class="highlight-title">Robot</span> Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jens V. Rüppel, Andrey Rudenko, Tim Schreiter, Martin Magnusson, Achim J. Lilienthal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of Large Language Models (LLMs) creates an exciting
potential for flexible, general knowledge-driven Human-Robot Interaction (HRI)
systems for assistive robots. Existing HRI systems demonstrate great progress
in interpreting and following user instructions, action generation, and robot
task solving. On the other hand, bi-directional, multi-modal, and context-aware
support of the user in collaborative tasks still remains an open challenge. In
this paper, we present a gaze- and speech-informed interface to the assistive
robot, which is able to perceive the working environment from multiple vision
inputs and support the dynamic user in their tasks. Our system is designed to
be modular and transferable to adapt to diverse tasks and robots, and it is
capable of real-time use of language-based interaction state representation and
fast on board perception modules. Its development was supported by multiple
public dissemination events, contributing important considerations for improved
robustness and user experience. Furthermore, in two lab studies, we compare the
performance and user ratings of our system with those of a traditional scripted
HRI pipeline. Our findings indicate that an LLM-based approach enhances
adaptability and marginally improves user engagement and task execution metrics
but may produce redundant output, while a scripted pipeline is well suited for
more straightforward tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to the 34th IEEE International
  Conference on Robot and Human Interactive Communication (RO-MAN), which will
  be held in Eindhoven, Netherlands on August 25-29, 2025. Copyright 2025 IEEE.
  Personal use of this material is permitted. Permission from IEEE must be
  obtained for all other uses</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffPF: Differentiable Particle Filtering with Generative Sampling via
  Conditional Dif<span class="highlight-title">fusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15716v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15716v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Wan, Lin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes DiffPF, a differentiable particle filter that leverages
diffusion models for state estimation in dynamic systems. Unlike conventional
differentiable particle filters, which require importance weighting and
typically rely on predefined or low-capacity proposal distributions. DiffPF
learns a flexible posterior sampler by conditioning a diffusion model on
predicted particles and the current observation. This enables accurate,
equally-weighted sampling from complex, high-dimensional, and multimodal
filtering distributions. We evaluate DiffPF across a range of scenarios,
including both unimodal and highly multimodal distributions, and test it on
simulated as well as real-world tasks, where it consistently outperforms
existing filtering baselines. In particular, DiffPF achieves an 82.8%
improvement in estimation accuracy on a highly multimodal global localization
benchmark, and a 26% improvement on the real-world KITTI visual odometry
benchmark, compared to state-of-the-art differentiable filters. To the best of
our knowledge, DiffPF is the first method to integrate conditional diffusion
models into particle filtering, enabling high-quality posterior sampling that
produces more informative particles and significantly improves state
estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Selective Densification for Rapid Motion <span class="highlight-title">Planning</span> in High Dimensions
  with Narrow Passages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Huang, Lingxiao Meng, Jiankun Wang, Xingjian Jing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sampling-based algorithms are widely used for motion planning in
high-dimensional configuration spaces. However, due to low sampling efficiency,
their performance often diminishes in complex configuration spaces with narrow
corridors. Existing approaches address this issue using handcrafted or learned
heuristics to guide sampling toward useful regions. Unfortunately, these
strategies often lack generalizability to various problems or require extensive
prior training. In this paper, we propose a simple yet efficient sampling-based
planning framework along with its bidirectional version that overcomes these
issues by integrating different levels of planning granularity. Our approach
probes configuration spaces with uniform random samples at varying resolutions
and explores these multi-resolution samples online with a bias towards sparse
samples when traveling large free configuration spaces. By seamlessly
transitioning between sparse and dense samples, our approach can navigate
complex configuration spaces while maintaining planning speed and completeness.
The simulation results demonstrate that our approach outperforms several
state-of-the-art sampling-based planners in $\mathbb{SE}(2)$, $\mathbb{SE}(3)$,
and $\mathbb{R}^{14}$ with challenging terrains. Furthermore, experiments
conducted with the Franka Emika Panda robot operating in a constrained
workspace provide additional evidence of the superiority of the proposed
method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Strong, Accurate, and Low-Cost <span class="highlight-title">Robot</span> Manipulator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georges Chebly, Spencer Little, Nisal Perera, Aliya Abedeen, Ken Suzuki, Donghyun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Forte, a fully 3D-printable, 6-DoF robotic arm designed
to achieve near industrial-grade performance - 0.63 kg payload, 0.467 m reach,
and sub-millimeter repeatability - at a material cost under $215. As an
accessible robot for broad applications across classroom education to AI
experiments, Forte pushes forward the performance limitations of existing
low-cost educational arms. We introduce a cost-effective mechanical design that
combines capstan-based cable drives, timing belts, simple tensioning
mechanisms, and lightweight 3D-printed structures, along with topology
optimization for structural stiffness. Through careful drivetrain engineering,
we minimize backlash and maintain control fidelity without relying on
high-power electronics or expensive manufacturing processes. Experimental
validation demonstrates that Forte achieves high repeatability and load
capacity, offering a compelling robotic platform for both classroom instruction
and advanced robotics research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-Driven MPC with Data Selection for Flexible Cable-Driven <span class="highlight-title">Robot</span>ic
  Arms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huayue Liang, Yanbo Chen, Hongyang Cheng, Yanzhao Yu, Shoujie Li, Junbo Tan, Xueqian Wang, Long Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Flexible cable-driven robotic arms (FCRAs) offer dexterous and compliant
motion. Still, the inherent properties of cables, such as resilience,
hysteresis, and friction, often lead to particular difficulties in modeling and
control. This paper proposes a model predictive control (MPC) method that
relies exclusively on input-output data, without a physical model, to improve
the control accuracy of FCRAs. First, we develop an implicit model based on
input-output data and integrate it into an MPC optimization framework. Second,
a data selection algorithm (DSA) is introduced to filter the data that best
characterize the system, thereby reducing the solution time per step to
approximately 4 ms, which is an improvement of nearly 80%. Lastly, the
influence of hyperparameters on tracking error is investigated through
simulation. The proposed method has been validated on a real FCRA platform,
including five-point positioning accuracy tests, a five-point response tracking
test, and trajectory tracking for letter drawing. The results demonstrate that
the average positioning accuracy is approximately 2.070 mm. Moreover, compared
to the PID method with an average tracking error of 1.418{\deg}, the proposed
method achieves an average tracking error of 0.541{\deg}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EMP: Executable Motion Prior for Humanoid <span class="highlight-title">Robot</span> Standing Upper-body
  Motion Imitation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haocheng Xu, Haodong Zhang, Zhenghan Chen, Rong Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To support humanoid robots in performing manipulation tasks, it is essential
to study stable standing while accommodating upper-body motions. However, the
limited controllable range of humanoid robots in a standing position affects
the stability of the entire body. Thus we introduce a reinforcement learning
based framework for humanoid robots to imitate human upper-body motions while
maintaining overall stability. Our approach begins with designing a retargeting
network that generates a large-scale upper-body motion dataset for training the
reinforcement learning (RL) policy, which enables the humanoid robot to track
upper-body motion targets, employing domain randomization for enhanced
robustness. To avoid exceeding the robot's execution capability and ensure
safety and stability, we propose an Executable Motion Prior (EMP) module, which
adjusts the input target movements based on the robot's current state. This
adjustment improves standing stability while minimizing changes to motion
amplitude. We evaluate our framework through simulation and real-world tests,
demonstrating its practical applicability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Force Signals from Human Demonstrations of In-Contact Motions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Hartwig, Fabian Viessmann, Dominik Henrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For non-robot-programming experts, kinesthetic guiding can be an intuitive
input method, as robot programming of in-contact tasks is becoming more
prominent. However, imprecise and noisy input signals from human demonstrations
pose problems when reproducing motions directly or using the signal as input
for machine learning methods. This paper explores optimizing force signals to
correspond better to the human intention of the demonstrated signal. We compare
different signal filtering methods and propose a peak detection method for
dealing with first-contact deviations in the signal. The evaluation of these
methods considers a specialized error criterion between the input and the
human-intended signal. In addition, we analyze the critical parameters'
influence on the filtering methods. The quality for an individual motion could
be increased by up to \SI{20}{\percent} concerning the error criterion. The
proposed contribution can improve the usability of robot programming and the
interaction between humans and robots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Annals of Scientific Society for
  Assembly, Handling and Industrial Robotics 2024 (to appear)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Universal Vehicle-Trailer <span class="highlight-title">Navigation</span> System with Neural Kinematics and
  Online Residual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanbo Chen, Yunzhe Tan, Yaojia Wang, Zhengzhe Xu, Junbo Tan, Xueqian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous navigation of vehicle-trailer systems is crucial in environments
like airports, supermarkets, and concert venues, where various types of
trailers are needed to navigate with different payloads and conditions.
However, accurately modeling such systems remains challenging, especially for
trailers with castor wheels. In this work, we propose a novel universal
vehicle-trailer navigation system that integrates a hybrid nominal kinematic
model--combining classical nonholonomic constraints for vehicles and neural
network-based trailer kinematics--with a lightweight online residual learning
module to correct real-time modeling discrepancies and disturbances.
Additionally, we develop a model predictive control framework with a weighted
model combination strategy that improves long-horizon prediction accuracy and
ensures safer motion planning. Our approach is validated through extensive
real-world experiments involving multiple trailer types and varying payload
conditions, demonstrating robust performance without manual tuning or
trailer-specific calibration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Estimation</span> of Payload Inertial Parameters from Human Demonstrations by
  Hand Guiding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Hartwig, Philipp Lienhardt, Dominik Henrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the availability of cobots increases, it is essential to address the needs
of users with little to no programming knowledge to operate such systems
efficiently. Programming concepts often use intuitive interaction modalities,
such as hand guiding, to address this. When programming in-contact motions,
such frameworks require knowledge of the robot tool's payload inertial
parameters (PIP) in addition to the demonstrated velocities and forces to
ensure effective hybrid motion-force control. This paper aims to enable
non-expert users to program in-contact motions more efficiently by eliminating
the need for a dedicated PIP calibration, thereby enabling flexible robot tool
changes. Since demonstrated tasks generally also contain motions with
non-contact, our approach uses these parts to estimate the robot's PIP using
established estimation techniques. The results show that the estimation of the
payload's mass is accurate, whereas the center of mass and the inertia tensor
are affected by noise and a lack of excitation. Overall, these findings show
the feasibility of PIP estimation during hand guiding but also highlight the
need for sufficient payload accelerations for an accurate estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Annals of Scientific Society for
  Assembly, Handling and Industrial Robotics 2025 (to appear)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Being-H0: Vision-Language-Action Pretraining from Large-Scale Human
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Luo, Yicheng Feng, Wanpeng Zhang, Sipeng Zheng, Ye Wang, Haoqi Yuan, Jiazheng Liu, Chaoyi Xu, Qin Jin, Zongqing Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained
on large-scale human videos. Existing VLAs struggle with complex manipulation
tasks requiring high dexterity and generalize poorly to novel scenarios and
tasks, primarily due to their reliance on synthetic data with significant
sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To
address this data bottleneck, we propose leveraging human hands as a foundation
manipulator, capitalizing on the rich dexterity and scalability present in web
data. Our approach centers on physical instruction tuning, a novel training
paradigm that combines large-scale VLA pretraining from human videos, physical
space alignment for 3D reasoning, and post-training adaptation for robotic
tasks. Additionally, we introduce a part-level motion tokenization method which
achieves millimeter-level reconstruction accuracy to model precise hand
trajectories for action learning. To support our proposed paradigm, we further
develop a comprehensive data curation pipeline that integrates heterogeneous
sources -- including motion capture, VR, and RGB-only videos -- into a
large-scale dataset with millions of motion-based instructional instances. We
empirically show the excellence of Being-H0 in hand motion generation and
instruction following, and it also scales well with model and data sizes.
Importantly, we observe the expected gains of Being-H0 in real-world robotic
manipulation as physical instruction tuning is applied. More details are
available at https://beingbeyond.github.io/Being-H0.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Functional Reliability of Near-<span class="highlight-title">Field</span> Monitoring for Emergency
  Braking in Autonomous Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junnan Pan, Prodromos Sotiriadis, Vladislav Nenchev, Ferdinand Englberger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous vehicles require reliable hazard detection. However, primary
sensor systems may miss near-field obstacles, resulting in safety risks.
Although a dedicated fast-reacting near-field monitoring system can mitigate
this, it typically suffers from false positives. To mitigate these, in this
paper, we introduce three monitoring strategies based on dynamic spatial
properties, relevant object sizes, and motion-aware prediction. In experiments
in a validated simulation, we compare the initial monitoring strategy against
the proposed improvements. The results demonstrate that the proposed strategies
can significantly improve the reliability of near-field monitoring systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, conference paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLEVER: Stream-based Active Learning for <span class="highlight-title">Robust</span> Semantic Perception from
  Human Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jongseok Lee, Timo Birr, Rudolph Triebel, Tamim Asfour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose CLEVER, an active learning system for robust semantic perception
with Deep Neural Networks (DNNs). For data arriving in streams, our system
seeks human support when encountering failures and adapts DNNs online based on
human instructions. In this way, CLEVER can eventually accomplish the given
semantic perception tasks. Our main contribution is the design of a system that
meets several desiderata of realizing the aforementioned capabilities. The key
enabler herein is our Bayesian formulation that encodes domain knowledge
through priors. Empirically, we not only motivate CLEVER's design but further
demonstrate its capabilities with a user validation study as well as
experiments on humanoid and deformable objects. To our knowledge, we are the
first to realize stream-based active learning on a real robot, providing
evidence that the robustness of the DNN-based semantic perception can be
improved in practice. The project website can be accessed at
https://sites.google.com/view/thecleversystem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages. Accepted to IEEE RAL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dense-depth map guided deep <span class="highlight-title">Lidar</span>-Visual <span class="highlight-title">Odometry</span> with Sparse Point
  Clouds and Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        JunYing Huang, Ao Xu, DongSun Yong, KeRen Li, YuanFeng Wang, Qi Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Odometry is a critical task for autonomous systems for self-localization and
navigation. We propose a novel LiDAR-Visual odometry framework that integrates
LiDAR point clouds and images for accurate and robust pose estimation. Our
method utilizes a dense-depth map estimated from point clouds and images
through depth completion, and incorporates a multi-scale feature extraction
network with attention mechanisms, enabling adaptive depth-aware
representations. Furthermore, we leverage dense depth information to refine
flow estimation and mitigate errors in occlusion-prone regions. Our
hierarchical pose refinement module optimizes motion estimation progressively,
ensuring robust predictions against dynamic environments and scale ambiguities.
Comprehensive experiments on the KITTI odometry benchmark demonstrate that our
approach achieves similar or superior accuracy and robustness compared to
state-of-the-art visual and LiDAR odometry methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Robot</span>s for Kiwifruit Harvesting and Pollination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jamie Bell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research was a part of a project that developed mobile robots that
performed targeted pollen spraying and automated harvesting in pergola
structured kiwifruit orchards. Multiple kiwifruit detachment mechanisms were
designed and field testing of one of the concepts showed that the mechanism
could reliably pick kiwifruit. Furthermore, this kiwifruit detachment mechanism
was able to reach over 80 percent of fruit in the cluttered kiwifruit canopy,
whereas the previous state of the art mechanism was only able to reach less
than 70 percent of the fruit. Artificial pollination was performed by detecting
flowers and then spraying pollen in solution onto the detected flowers from a
line of sprayers on a boom, while driving at up to 1.4 ms-1. In addition, the
height of the canopy was measured and the spray boom was moved up and down to
keep the boom close enough to the flowers for the spray to reach the flowers,
while minimising collisions with the canopy. Mobile robot navigation was
performed using a 2D lidar in apple orchards and vineyards. Lidar navigation in
kiwifruit orchards was more challenging because the pergola structure only
provides a small amount of data for the direction of rows, compared to the
amount of data from the overhead canopy, the undulating ground and other
objects in the orchards. Multiple methods are presented here for extracting
structure defining features from 3D lidar data in kiwifruit orchards. In
addition, a 3D lidar navigation system -- which performed row following, row
end detection and row end turns -- was tested for over 30 km of autonomous
driving in kiwifruit orchards. Computer vision algorithms for row detection and
row following were also tested. The computer vision algorithm worked as well as
the 3D lidar row following method in testing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Constitutional <span class="highlight-title">Control</span>ler: Doubt-Calibrated Steering of Compliant
  Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Kohaut, Felix Divo, Navid Hamid, Benedict Flade, Julian Eggert, Devendra Singh Dhami, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring reliable and rule-compliant behavior of autonomous agents in
uncertain environments remains a fundamental challenge in modern robotics. Our
work shows how neuro-symbolic systems, which integrate probabilistic, symbolic
white-box reasoning models with deep learning methods, offer a powerful
solution to this challenge. This enables the simultaneous consideration of
explicit rules and neural models trained on noisy data, combining the strength
of structured reasoning with flexible representations. To this end, we
introduce the Constitutional Controller (CoCo), a novel framework designed to
enhance the safety and reliability of agents by reasoning over deep
probabilistic logic programs representing constraints such as those found in
shared traffic spaces. Furthermore, we propose the concept of self-doubt,
implemented as a probability density conditioned on doubt features such as
travel velocity, employed sensors, or health factors. In a real-world aerial
mobility study, we demonstrate CoCo's advantages for intelligent autonomous
systems to learn appropriate doubts and navigate complex and uncertain
environments safely and compliantly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ All-UWB <span class="highlight-title">SLAM</span> Using UWB Radar and UWB AOA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charith Premachandra, Achala Athukorala, U-Xuan Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been a growing interest in autonomous systems designed to operate
in adverse conditions (e.g. smoke, dust), where the visible light spectrum
fails. In this context, Ultra-wideband (UWB) radar is capable of penetrating
through such challenging environmental conditions due to the lower frequency
components within its broad bandwidth. Therefore, UWB radar has emerged as a
potential sensing technology for Simultaneous Localization and Mapping (SLAM)
in vision-denied environments where optical sensors (e.g. LiDAR, Camera) are
prone to failure. Existing approaches involving UWB radar as the primary
exteroceptive sensor generally extract features in the environment, which are
later initialized as landmarks in a map. However, these methods are constrained
by the number of distinguishable features in the environment. Hence, this paper
proposes a novel method incorporating UWB Angle of Arrival (AOA) measurements
into UWB radar-based SLAM systems to improve the accuracy and scalability of
SLAM in feature-deficient environments. The AOA measurements are obtained using
UWB anchor-tag units which are dynamically deployed by the robot in featureless
areas during mapping of the environment. This paper thoroughly discusses
prevailing constraints associated with UWB AOA measurement units and presents
solutions to overcome them. Our experimental results show that integrating UWB
AOA units with UWB radar enables SLAM in vision-denied feature-deficient
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Emergence of Deep <span class="highlight-title">Reinforcement</span> Learning for Path <span class="highlight-title">Planning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thanh Thi Nguyen, Saeid Nahavandi, Imran Razzak, Dung Nguyen, Nhat Truong Pham, Quoc Viet Hung Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing demand for autonomous systems in complex and dynamic
environments has driven significant research into intelligent path planning
methodologies. For decades, graph-based search algorithms, linear programming
techniques, and evolutionary computation methods have served as foundational
approaches in this domain. Recently, deep reinforcement learning (DRL) has
emerged as a powerful method for enabling autonomous agents to learn optimal
navigation strategies through interaction with their environments. This survey
provides a comprehensive overview of traditional approaches as well as the
recent advancements in DRL applied to path planning tasks, focusing on
autonomous vehicles, drones, and robotic platforms. Key algorithms across both
conventional and learning-based paradigms are categorized, with their
innovations and practical implementations highlighted. This is followed by a
thorough discussion of their respective strengths and limitations in terms of
computational efficiency, scalability, adaptability, and robustness. The survey
concludes by identifying key open challenges and outlining promising avenues
for future research. Special attention is given to hybrid approaches that
integrate DRL with classical planning techniques to leverage the benefits of
both learning-based adaptability and deterministic reliability, offering
promising directions for robust and resilient autonomous navigation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the Proceedings of the 2025 IEEE
  International Conference on Systems, Man, and Cybernetics (SMC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Low-Latency Event-Based Velocimetry for Quadrotor <span class="highlight-title">Control</span> in a Narrow
  Pipe 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonard Bauersfeld, <span class="highlight-author">Davide Scaramuzza</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous quadrotor flight in confined spaces such as pipes and tunnels
presents significant challenges due to unsteady, self-induced aerodynamic
disturbances. Very recent advances have enabled flight in such conditions, but
they either rely on constant motion through the pipe to mitigate airflow
recirculation effects or suffer from limited stability during hovering. In this
work, we present the first closed-loop control system for quadrotors for
hovering in narrow pipes that leverages real-time flow field measurements. We
develop a low-latency, event-based smoke velocimetry method that estimates
local airflow at high temporal resolution. This flow information is used by a
disturbance estimator based on a recurrent convolutional neural network, which
infers force and torque disturbances in real time. The estimated disturbances
are integrated into a learning-based controller trained via reinforcement
learning. The flow-feedback control proves particularly effective during
lateral translation maneuvers in the pipe cross-section. There, the real-time
disturbance information enables the controller to effectively counteract
transient aerodynamic effects, thereby preventing collisions with the pipe
wall. To the best of our knowledge, this work represents the first
demonstration of an aerial robot with closed-loop control informed by real-time
flow field measurements. This opens new directions for research on flight in
aerodynamically complex environments. In addition, our work also sheds light on
the characteristic flow structures that emerge during flight in narrow,
circular pipes, providing new insights at the intersection of robotics and
fluid dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MobileUse: A GUI Agent with Hierarchical Reflection for Autonomous
  Mobile Operation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ning Li, Xiangmou Qu, Jiamu Zhou, Jun Wang, Muning Wen, Kounianhua Du, Xingyu Lou, Qiuying Peng, Jun Wang, Weinan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Multimodal Large Language Models (MLLMs) have enabled the
development of mobile agents that can understand visual inputs and follow user
instructions, unlocking new possibilities for automating complex tasks on
mobile devices. However, applying these models to real-world mobile scenarios
remains a significant challenge due to the long-horizon task execution,
difficulty in error recovery, and the cold-start problem in unfamiliar
environments. To address these challenges, we propose MobileUse, a GUI agent
designed for robust and adaptive mobile task execution. To improve resilience
in long-horizon tasks and dynamic environments, we introduce a hierarchical
reflection architecture that enables the agent to self-monitor, detect, and
recover from errors across multiple temporal scales-ranging from individual
actions to overall task completion-while maintaining efficiency through a
reflection-on-demand strategy. To tackle cold-start issues, we further
introduce a proactive exploration module, which enriches the agent's
understanding of the environment through self-planned exploration. Evaluations
on AndroidWorld and AndroidLab benchmarks demonstrate that MobileUse
establishes new state-of-the-art performance, achieving success rates of 62.9%
and 44.2%, respectively. To facilitate real-world applications, we release an
out-of-the-box toolkit for automated task execution on physical mobile devices,
which is available at https://github.com/MadeAgents/mobile-use.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A technical report on a GUI agent based on multi-agent systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> RepILN: Reparameterized Inertial <span class="highlight-title">Localization</span> Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanshan Zhang, Tianshui Wen, Si<span class="highlight-author">yue Wang</span>, Qi Zhang, Ziheng Zhou, Lingxiang Zheng, Yu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inertial localization is regarded as a promising positioning solution for
consumer-grade IoT devices due to its cost-effectiveness and independence from
external infrastructure. However, data-driven inertial localization methods
often rely on increasingly complex network architectures to improve accuracy,
which challenges the limited computational resources of IoT devices. Moreover,
these methods frequently overlook the importance of modeling long-term
dependencies in inertial measurements - a critical factor for accurate
trajectory reconstruction - thereby limiting localization performance. To
address these challenges, we propose a reparameterized inertial localization
network that uses a multi-branch structure during training to enhance feature
extraction. At inference time, this structure is transformed into an equivalent
single-path architecture to improve parameter efficiency. To further capture
long-term dependencies in motion trajectories, we introduce a temporal-scale
sparse attention mechanism that selectively emphasizes key trajectory segments
while suppressing noise. Additionally, a gated convolutional unit is
incorporated to effectively integrate long-range dependencies with local
fine-grained features. Extensive experiments on public benchmarks demonstrate
that our method achieves a favorable trade-off between accuracy and model
compactness. For example, on the RoNIN dataset, our approach reduces the
Absolute Trajectory Error (ATE) by 2.59% compared to RoNIN-ResNet while
reducing the number of parameters by 3.86%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VLM-UDMC: VLM-Enhanced Unified <span class="highlight-title">Decision</span>-Making and Motion <span class="highlight-title">Control</span> for
  Urban Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haichao Liu, Haoren Guo, Pei Liu, Benshan Ma, Yuxiang Zhang, Jun Ma, Tong Heng Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene understanding and risk-aware attentions are crucial for human drivers
to make safe and effective driving decisions. To imitate this cognitive ability
in urban autonomous driving while ensuring the transparency and
interpretability, we propose a vision-language model (VLM)-enhanced unified
decision-making and motion control framework, named VLM-UDMC. This framework
incorporates scene reasoning and risk-aware insights into an upper-level slow
system, which dynamically reconfigures the optimal motion planning for the
downstream fast system. The reconfiguration is based on real-time environmental
changes, which are encoded through context-aware potential functions. More
specifically, the upper-level slow system employs a two-step reasoning policy
with Retrieval-Augmented Generation (RAG), leveraging foundation models to
process multimodal inputs and retrieve contextual knowledge, thereby generating
risk-aware insights. Meanwhile, a lightweight multi-kernel decomposed LSTM
provides real-time trajectory predictions for heterogeneous traffic
participants by extracting smoother trend representations for short-horizon
trajectory prediction. The effectiveness of the proposed VLM-UDMC framework is
verified via both simulations and real-world experiments with a full-size
autonomous vehicle. It is demonstrated that the presented VLM-UDMC effectively
leverages scene understanding and attention decomposition for rational driving
decisions, thus improving the overall urban driving performance. Our
open-source project is available at https://github.com/henryhcliu/vlmudmc.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CHADET: Cross-Hierarchical-Attention for Depth-Completion Using
  <span class="highlight-title">Unsupervised</span> Lightweight Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15189v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15189v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Christiansen Marsim, Jinwoo Jeon, Yeeun Kim, Myeongwoo Jeong, Hyun Myung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depth information which specifies the distance between objects and current
position of the robot is essential for many robot tasks such as navigation.
Recently, researchers have proposed depth completion frameworks to provide
dense depth maps that offer comprehensive information about the surrounding
environment. However, existing methods show significant trade-offs between
computational efficiency and accuracy during inference. The substantial memory
and computational requirements make them unsuitable for real-time applications,
highlighting the need to improve the completeness and accuracy of depth
information while improving processing speed to enhance robot performance in
various tasks. To address these challenges, in this paper, we propose
CHADET(cross-hierarchical-attention depth-completion transformer), a
lightweight depth-completion network that can generate accurate dense depth
maps from RGB images and sparse depth points. For each pair, its feature is
extracted from the depthwise blocks and passed to the equally lightweight
transformer-based decoder. In the decoder, we utilize the novel
cross-hierarchical-attention module that refines the image features from the
depth information. Our approach improves the quality and reduces memory usage
of the depth map prediction, as validated in both KITTI, NYUv2, and VOID
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bundle Adjustment in the Eager Mode 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12190v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12190v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zitong Zhan, Huan Xu, Zihang Fang, Xinpeng Wei, Yaoyu Hu, Chen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bundle adjustment (BA) is a critical technique in various robotic
applications such as simultaneous localization and mapping (SLAM), augmented
reality (AR), and photogrammetry. BA optimizes parameters such as camera poses
and 3D landmarks to align them with observations. With the growing importance
of deep learning in perception systems, there is an increasing need to
integrate BA with deep learning frameworks for enhanced reliability and
performance. However, widely-used C++-based BA libraries, such as GTSAM,
g$^2$o, and Ceres, lack native integration with modern deep learning libraries
like PyTorch. This limitation affects their flexibility, adaptability, ease of
debugging, and overall implementation efficiency. To address this gap, we
introduce an eager-mode BA library seamlessly integrated with PyTorch with high
efficiency. Our approach includes GPU-accelerated, differentiable, and sparse
operations designed for \nth{2}-order optimization, Lie group and Lie algebra
operations, and linear solvers. Our eager-mode BA on GPU demonstrates
substantial runtime efficiency, achieving an average speedup of 18.5$\times$,
22$\times$, and 23$\times$ compared to GTSAM, g$^2$o, and Ceres, respectively.
The source code will be available at https://github.com/sair-lab/bae.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI Space Cortex: An Experimental System for Future Era Space <span class="highlight-title">Exploration</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06574v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06574v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Touma, Ersin Daş, Erica Tevere, Martin Feather, Ksenia Kolcio, Maurice Prather, Alberto Candela, Ashish Goel, Erik Kramer, Hari Nayar, Lorraine Fesq, Joel W. Burdick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our Robust, Explainable Autonomy for Scientific Icy Moon Operations (REASIMO)
effort contributes to NASA's Concepts for Ocean worlds Life Detection
Technology (COLDTech) program, which explores science platform technologies for
ocean worlds such as Europa and Enceladus. Ocean world missions pose
significant operational challenges. These include long communication lags,
limited power, and lifetime limitations caused by radiation damage and hostile
conditions. Given these operational limitations, onboard autonomy will be vital
for future Ocean world missions. Besides the management of nominal lander
operations, onboard autonomy must react appropriately in the event of
anomalies. Traditional spacecraft rely on a transition into 'safe-mode' in
which non-essential components and subsystems are powered off to preserve
safety and maintain communication with Earth. For a severely time-limited Ocean
world mission, resolutions to these anomalies that can be executed without
Earth-in-the-loop communication and associated delays are paramount for
completion of the mission objectives and science goals. To address these
challenges, the REASIMO effort aims to demonstrate a robust level of
AI-assisted autonomy for such missions, including the ability to detect and
recover from anomalies, and to perform missions based on pre-trained behaviors
rather than hard-coded, predetermined logic like all prior space missions. We
developed an AI-assisted, personality-driven, intelligent framework for control
of an Ocean world mission by combining a mix of advanced technologies. To
demonstrate the capabilities of the framework, we perform tests of autonomous
sampling operations on a lander-manipulator testbed at the NASA Jet Propulsion
Laboratory, approximating possible surface conditions such a mission might
encounter.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Multi-Camera Tokenization with Triplanes for End-to-End
  Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boris Ivanovic, Cristiano Saltori, Yurong You, Yan Wang, Wenjie Luo, Marco Pavone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive Transformers are increasingly being deployed as end-to-end
robot and autonomous vehicle (AV) policy architectures, owing to their
scalability and potential to leverage internet-scale pretraining for
generalization. Accordingly, tokenizing sensor data efficiently is paramount to
ensuring the real-time feasibility of such architectures on embedded hardware.
To this end, we present an efficient triplane-based multi-camera tokenization
strategy that leverages recent advances in 3D neural reconstruction and
rendering to produce sensor tokens that are agnostic to the number of input
cameras and their resolution, while explicitly accounting for their geometry
around an AV. Experiments on a large-scale AV dataset and state-of-the-art
neural simulator demonstrate that our approach yields significant savings over
current image patch-based tokenization strategies, producing up to 72% fewer
tokens, resulting in up to 50% faster policy inference while achieving the same
open-loop motion planning accuracy and improved offroad rates in closed-loop
driving simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predictive Planner for Autonomous Driving with Consistency Models <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08033v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08033v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anjian Li, Sangjae Bae, David Isele, Ryne Beeson, Faizan M. Tariq
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory prediction and planning are essential for autonomous vehicles to
navigate safely and efficiently in dynamic environments. Traditional approaches
often treat them separately, limiting the ability for interactive planning.
While recent diffusion-based generative models have shown promise in
multi-agent trajectory generation, their slow sampling is less suitable for
high-frequency planning tasks. In this paper, we leverage the consistency model
to build a predictive planner that samples from a joint distribution of ego and
surrounding agents, conditioned on the ego vehicle's navigational goal. Trained
on real-world human driving datasets, our consistency model generates
higher-quality trajectories with fewer sampling steps than standard diffusion
models, making it more suitable for real-time deployment. To enforce multiple
planning constraints simultaneously on the ego trajectory, a novel online
guided sampling approach inspired by the Alternating Direction Method of
Multipliers (ADMM) is introduced. Evaluated on the Waymo Open Motion Dataset
(WOMD), our method enables proactive behavior such as nudging and yielding, and
also demonstrates smoother, safer, and more efficient trajectories and
satisfaction of multiple constraints under a limited computational budget.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 28th IEEE International Conference on Intelligent
  Transportation Systems (ITSC) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model-Free and Real-Time Bioinspired Unicycle-Based Source Seeking:
  Differential Wheeled <span class="highlight-title">Robot</span>ic Experiments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02184v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02184v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed A. Elgohary, Sameh A. Eisa, Shivam Bajpai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many autonomous robots aimed at source-seeking are studied, and their
controls designed, using unicycle modeling and formulation. This is true not
only for model-based controllers, but also for model-free, real-time control
methods such as extremum seeking control (ESC). In this paper, we propose a
unicycle-based ESC design applicable to differential wheeled robots that: (1)
is very simple design, based on one simple control-affine law, and without
state integrators; (2) attenuates oscillations known to persist in ESC designs
(i.e., fully stop at the source); and (3) operates in a model-free, real-time
setting, tolerating environmental/sensor noise. We provide simulation and
real-world robotic experimental results for fixed and moving light source
seeking by a differential wheeled robot using our proposed design. Results
indicate clear advantages of our proposed design when compared to the
literature, including attenuation of undesired oscillations, improved
convergence speed, and better handling of noise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D Active Metric-Semantic <span class="highlight-title">SLAM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.06950v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.06950v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuezhan Tao, Xu Liu, Igor Spasojevic, Saurav Agarwal, Vijay Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this letter, we address the problem of exploration and metric-semantic
mapping of multi-floor GPS-denied indoor environments using Size Weight and
Power (SWaP) constrained aerial robots. Most previous work in exploration
assumes that robot localization is solved. However, neglecting the state
uncertainty of the agent can ultimately lead to cascading errors both in the
resulting map and in the state of the agent itself. Furthermore, actions that
reduce localization errors may be at direct odds with the exploration task. We
propose a framework that balances the efficiency of exploration with actions
that reduce the state uncertainty of the agent. In particular, our algorithmic
approach for active metric-semantic SLAM is built upon sparse information
abstracted from raw problem data, to make it suitable for SWaP-constrained
robots. Furthermore, we integrate this framework within a fully autonomous
aerial robotic system that achieves autonomous exploration in cluttered, 3D
environments. From extensive real-world experiments, we showed that by
including Semantic Loop Closure (SLC), we can reduce the robot pose estimation
errors by over 90% in translation and approximately 75% in yaw, and the
uncertainties in pose estimates and semantic maps by over 70% and 65%,
respectively. Although discussed in the context of indoor multi-floor
exploration, our system can be used for various other applications, such as
infrastructure inspection and precision agriculture where reliable GPS data may
not be available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interactive <span class="highlight-title">Navigation</span> for Legged Manipulators with Learned Arm-Pushing
  <span class="highlight-title">Control</span>ler 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01474v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01474v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihai Bi, Kai Chen, Chunxin Zheng, Yulin Li, Haoang Li, Jun Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactive navigation is crucial in scenarios where proactively interacting
with objects can yield shorter paths, thus significantly improving traversal
efficiency. Existing methods primarily focus on using the robot body to
relocate large obstacles (which could be comparable to the size of a robot).
However, they prove ineffective in narrow or constrained spaces where the
robot's dimensions restrict its manipulation capabilities. This paper
introduces a novel interactive navigation framework for legged manipulators,
featuring an active arm-pushing mechanism that enables the robot to reposition
movable obstacles in space-constrained environments. To this end, we develop a
reinforcement learning-based arm-pushing controller with a two-stage reward
strategy for large-object manipulation. Specifically, this strategy first
directs the manipulator to a designated pushing zone to achieve a kinematically
feasible contact configuration. Then, the end effector is guided to maintain
its position at appropriate contact points for stable object displacement while
preventing toppling. The simulations validate the robustness of the arm-pushing
controller, showing that the two-stage reward strategy improves policy
convergence and long-term performance. Real-world experiments further
demonstrate the effectiveness of the proposed navigation framework, which
achieves shorter paths and reduced traversal time. The open-source project can
be found at
https://github.com/Zhihaibi/Interactive-Navigation-for-legged-manipulator.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Visuo-Tactile <span class="highlight-title">Fusion</span> with Predictive Force Attention for
  Dexterous Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13982v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13982v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinzhou Li, Tianhao Wu, Jiyao Zhang, Zeyuan Chen, Haotian Jin, Mingdong Wu, Yujun Shen, Yaodong Yang, Hao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effectively utilizing multi-sensory data is important for robots to
generalize across diverse tasks. However, the heterogeneous nature of these
modalities makes fusion challenging. Existing methods propose strategies to
obtain comprehensively fused features but often ignore the fact that each
modality requires different levels of attention at different manipulation
stages. To address this, we propose a force-guided attention fusion module that
adaptively adjusts the weights of visual and tactile features without human
labeling. We also introduce a self-supervised future force prediction auxiliary
task to reinforce the tactile modality, improve data imbalance, and encourage
proper adjustment. Our method achieves an average success rate of 93% across
three fine-grained, contactrich tasks in real-world experiments. Further
analysis shows that our policy appropriately adjusts attention to each modality
at different manipulation stages. The videos can be viewed at
https://adaptac-dex.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Coord<span class="highlight-title">Field</span>: Coordination <span class="highlight-title">Field</span> for Agentic UAV Task Allocation In
  Low-altitude Urban Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.00091v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.00091v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tengchao Zhang, Yonglin Tian, Fei Lin, Jun Huang, Patrik P. Süli, Qinghua Ni, Rui Qin, Xiao Wang, Fei-<span class="highlight-author">Yue Wang</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing demand for heterogeneous Unmanned Aerial Vehicle (UAV)
swarms to perform complex tasks in urban environments, system design now faces
major challenges, including efficient semantic understanding, flexible task
planning, and the ability to dynamically adjust coordination strategies in
response to evolving environmental conditions and continuously changing task
requirements. To address the limitations of existing methods, this paper
proposes CoordField, a coordination field agent system for coordinating
heterogeneous drone swarms in complex urban scenarios. In this system, large
language models (LLMs) is responsible for interpreting high-level human
instructions and converting them into executable commands for the UAV swarms,
such as patrol and target tracking. Subsequently, a Coordination field
mechanism is proposed to guide UAV motion and task selection, enabling
decentralized and adaptive allocation of emergent tasks. A total of 50 rounds
of comparative testing were conducted across different models in a 2D
simulation space to evaluate their performance. Experimental results
demonstrate that the proposed system achieves superior performance in terms of
task coverage, response time, and adaptability to dynamic changes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving AEBS Validation Through Objective Intervention Classification
  Leveraging the <span class="highlight-title">Prediction</span> Divergence Principle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.07872v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.07872v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Betschinske, Steven Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The safety validation of automatic emergency braking system (AEBS) requires
accurately distinguishing between false positive (FP) and true positive (TP)
system activations. While simulations allow straightforward differentiation by
comparing scenarios with and without interventions, analyzing activations from
open-loop resimulations - such as those from field operational testing (FOT) -
is more complex. This complexity arises from scenario parameter uncertainty and
the influence of driver interventions in the recorded data. Human labeling is
frequently used to address these challenges, relying on subjective assessments
of intervention necessity or situational criticality, potentially introducing
biases and limitations. This work proposes a rule-based classification approach
leveraging the Prediction Divergence Principle (PDP) to address those issues.
Applied to a simplified AEBS, the proposed method reveals key strengths,
limitations, and system requirements for effective implementation. The findings
suggest that combining this approach with human labeling may enhance the
transparency and consistency of classification, thereby improving the overall
validation process. While the rule set for classification derived in this work
adopts a conservative approach, the paper outlines future directions for
refinement and broader applicability. Finally, this work highlights the
potential of such methods to complement existing practices, paving the way for
more reliable and reproducible AEBS validation frameworks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted for publication at the 2025 IEEE
  International Automated Vehicle Validation Conference (IAVVC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gesture-<span class="highlight-title">Control</span>led Aerial <span class="highlight-title">Robot</span> Formation for Human-<span class="highlight-title">Swarm</span> Interaction in
  Safety Monitoring Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15333v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15333v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vít Krátký, Giuseppe Silano, Matouš Vrba, Christos Papaioannidis, Ioannis Mademlis, Robert Pěnička, Ioannis Pitas, Martin Saska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a formation control approach for contactless
gesture-based Human-Swarm Interaction (HSI) between a team of multi-rotor
Unmanned Aerial Vehicles (UAVs) and a human worker. The approach is designed to
monitor the safety of human workers, particularly those operating at heights.
In the proposed dynamic formation scheme, one UAV acts as the formation leader,
equipped with sensors for detecting human workers and recognizing gestures. The
follower UAVs maintain a predetermined formation relative to the worker's
position, providing additional perspectives of the monitored scene. Hand
gestures enable the human worker to specify movement and action commands for
the UAV team and to initiate other mission-related tasks without requiring
additional communication channels or specific markers. Combined with a novel
unified human detection and tracking algorithm, a human position estimation
method, and a gesture detection pipeline, the proposed approach represents the
first instance of an HSI system incorporating all these modules onboard
real-world UAVs. Simulations and field experiments involving three UAVs and a
human worker in a mock-up scenario demonstrate the effectiveness and
responsiveness of the proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2025 IEEE. Personal use of this material is permitted. Permission
  from IEEE must be obtained for all other uses, in any current or future
  media, including reprinting/republishing this material for advertising or
  promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Constitutional Filter: Bayesian <span class="highlight-title">Estimation</span> of Compliant Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18347v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18347v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Kohaut, Felix Divo, Benedict Flade, Devendra Singh Dhami, Julian Eggert, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting agents impacted by legal policies, physical limitations, and
operational preferences is inherently difficult. In recent years,
neuro-symbolic methods have emerged, integrating machine learning and symbolic
reasoning models into end-to-end learnable systems. Hereby, a promising avenue
for expressing high-level constraints over multi-modal input data in robotics
has opened up. This work introduces an approach for Bayesian estimation of
agents expected to comply with a human-interpretable neuro-symbolic model we
call its Constitution. Hence, we present the Constitutional Filter (CoFi),
leading to improved tracking of agents by leveraging expert knowledge,
incorporating deep learning architectures, and accounting for environmental
uncertainties. CoFi extends the general, recursive Bayesian estimation setting,
ensuring compatibility with a vast landscape of established techniques such as
Particle Filters. To underpin the advantages of CoFi, we evaluate its
performance on real-world marine traffic data. Beyond improved performance, we
show how CoFi can learn to trust and adapt to the level of compliance of an
agent, recovering baseline performance even if the assumed Constitution clashes
with reality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stimulating Imagination: Towards General-purpose "Something Something
  Placement" <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01655v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01655v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianyang Wu, Jie Gu, Xiaokang Ma, Fangzhou Qiu, Chu Tang, Jingmin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  General-purpose object placement is a fundamental capability of an
intelligent generalist robot: being capable of rearranging objects following
precise human instructions even in novel environments. This work is dedicated
to achieving general-purpose object placement with ``something something''
instructions. Specifically, we break the entire process down into three parts,
including object localization, goal imagination and robot control, and propose
a method named SPORT. SPORT leverages a pre-trained large vision model for
broad semantic reasoning about objects, and learns a diffusion-based pose
estimator to ensure physically-realistic results in 3D space. Only object types
(movable or reference) are communicated between these two parts, which brings
two benefits. One is that we can fully leverage the powerful ability of
open-set object recognition and localization since no specific fine-tuning is
needed for the robotic scenario. Moreover, the diffusion-based estimator only
need to ``imagine" the object poses after the placement, while no necessity for
their semantic information. Thus the training burden is greatly reduced and no
massive training is required. The training data for the goal pose estimation is
collected in simulation and annotated by using GPT-4. Experimental results
demonstrate the effectiveness of our approach. SPORT can not only generate
promising 3D goal poses for unseen simulated objects, but also be seamlessly
applied to real-world settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, accepted to the 2025 IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint Travel Route <span class="highlight-title">Optimization</span> Framework for Platooning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.07623v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.07623v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akif Adas, Stefano Arrigoni, Mattia Brambilla, Monica Barbara Nicoli, Edoardo Sabbioni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Platooning represents an advanced driving technology designed to assist
drivers in traffic convoys of varying lengths, enhancing road safety, reducing
driver fatigue, and improving fuel efficiency. Sophisticated automated driving
assistance systems have facilitated this innovation. Recent advancements in
platooning emphasize cooperative mechanisms within both centralized and
decentralized architectures enabled by vehicular communication technologies.
This study introduces a cooperative route planning optimization framework aimed
at promoting the adoption of platooning through a centralized platoon formation
strategy at the system level. This approach is envisioned as a transitional
phase from individual (ego) driving to fully collaborative driving.
Additionally, this research formulates and incorporates travel cost metrics
related to fuel consumption, driver fatigue, and travel time, considering
regulatory constraints on consecutive driving durations. The performance of
these cost metrics has been evaluated using Dijkstra's and A* shortest path
algorithms within a network graph framework. The results indicate that the
proposed architecture achieves an average cost improvement of 14 % compared to
individual route planning for long road trips.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Six-DoF Hand-Based Teleoperation for Omnidirectional Aerial <span class="highlight-title">Robot</span>s <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15009v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15009v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinjie Li, Jiaxuan Li, Kotaro Kaneko, Haokun Liu, Liming Shu, Moju Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Omnidirectional aerial robots offer full 6-DoF independent control over
position and orientation, making them popular for aerial manipulation. Although
advancements in robotic autonomy, human operation remains essential in complex
aerial environments. Existing teleoperation approaches for multirotors fail to
fully leverage the additional DoFs provided by omnidirectional rotation.
Additionally, the dexterity of human fingers should be exploited for more
engaged interaction. In this work, we propose an aerial teleoperation system
that brings the rotational flexibility of human hands into the unbounded aerial
workspace. Our system includes two motion-tracking marker sets--one on the
shoulder and one on the hand--along with a data glove to capture hand gestures.
Using these inputs, we design four interaction modes for different tasks,
including Spherical Mode and Cartesian Mode for long-range moving, Operation
Mode for precise manipulation, as well as Locking Mode for temporary pauses,
where the hand gestures are utilized for seamless mode switching. We evaluate
our system on a vertically mounted valve-turning task in the real world,
demonstrating how each mode contributes to effective aerial manipulation. This
interaction framework bridges human dexterity with aerial robotics, paving the
way for enhanced aerial teleoperation in unstructured environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 10 figures. This work has been accepted to IROS 2025. The
  video is released in https://youtu.be/n0IQEnjPzrw?si=Zp3kb3ss-D_AySOE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Leveraging Semantic <span class="highlight-title">Graph</span>s for Efficient and <span class="highlight-title">Robust</span> <span class="highlight-title">LiDAR</span> <span class="highlight-title">SLAM</span> <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.11145v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.11145v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neng Wang, Huimin Lu, Zhiqiang Zheng, <span class="highlight-author">Hesheng Wang</span>, Yun-Hui Liu, <span class="highlight-author">Xieyuanli Chen</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and robust simultaneous localization and mapping (SLAM) is crucial
for autonomous mobile systems, typically achieved by leveraging the geometric
features of the environment. Incorporating semantics provides a richer scene
representation that not only enhances localization accuracy in SLAM but also
enables advanced cognitive functionalities for downstream navigation and
planning tasks. Existing point-wise semantic LiDAR SLAM methods often suffer
from poor efficiency and generalization, making them less robust in diverse
real-world scenarios. In this paper, we propose a semantic graph-enhanced SLAM
framework, named SG-SLAM, which effectively leverages the geometric, semantic,
and topological characteristics inherent in environmental structures. The
semantic graph serves as a fundamental component that facilitates critical
functionalities of SLAM, including robust relocalization during odometry
failures, accurate loop closing, and semantic graph map construction. Our
method employs a dual-threaded architecture, with one thread dedicated to
online odometry and relocalization, while the other handles loop closure, pose
graph optimization, and map update. This design enables our method to operate
in real time and generate globally consistent semantic graph maps and point
cloud maps. We extensively evaluate our method across the KITTI, MulRAN, and
Apollo datasets, and the results demonstrate its superiority compared to
state-of-the-art methods. Our method has been released at
https://github.com/nubot-nudt/SG-SLAM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures,Accpted for IROS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interaction-Merged Motion <span class="highlight-title">Planning</span>: Effectively Leveraging Diverse
  Motion <span class="highlight-title">Dataset</span>s for <span class="highlight-title">Robust</span> <span class="highlight-title">Planning</span> <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.04790v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.04790v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giwon Lee, Wooseong Jeong, Daehee Park, Jaewoo Jeong, Kuk-Jin Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion planning is a crucial component of autonomous robot driving. While
various trajectory datasets exist, effectively utilizing them for a target
domain remains challenging due to differences in agent interactions and
environmental characteristics. Conventional approaches, such as domain
adaptation or ensemble learning, leverage multiple source datasets but suffer
from domain imbalance, catastrophic forgetting, and high computational costs.
To address these challenges, we propose Interaction-Merged Motion Planning
(IMMP), a novel approach that leverages parameter checkpoints trained on
different domains during adaptation to the target domain. IMMP follows a
two-step process: pre-merging to capture agent behaviors and interactions,
sufficiently extracting diverse information from the source domain, followed by
merging to construct an adaptable model that efficiently transfers diverse
interactions to the target domain. Our method is evaluated on various planning
benchmarks and models, demonstrating superior performance compared to
conventional approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LaViPlan : Language-Guided Visual Path <span class="highlight-title">Planning</span> with RLVR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.12911v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.12911v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hayeon Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) scenarios in autonomous driving refer to situations
that deviate from the training domain, often leading to unexpected and
potentially hazardous behavior from planners that lack prior exposure to such
cases. Recently, Vision-Language Models (VLMs) have been introduced into
autonomous driving research for their promising generalization capabilities in
OOD settings. Early studies demonstrated that VLMs could recognize OOD
scenarios and generate user-level decisions such as "go straight" or "turn
right." However, a new challenge has emerged due to the misalignment between
the VLM's high-level decisions or visual reasoning expressed in language, and
the low-level predicted trajectories interpreted as actions. In this paper, we
propose LaViPlan, a framework that leverages Reinforcement Learning with
Verifiable Rewards (RLVR) to optimize VLMs using planning-oriented metrics.
This approach addresses the vision-language-action misalignment observed in
existing VLMs fine-tuned via supervised learning, which can recognize driving
scenarios but often produce context-unaware decisions. Experimental results
demonstrate that our method improves situational awareness and decision-making
under OOD conditions, highlighting its potential to mitigate the misalignment
issue. This work introduces a promising post-training paradigm for VLM agents
in the context of autonomous driving.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-07-20T00:00:00Z">2025-07-20</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">28</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning-Based Modeling of a Magnetically Steerable Soft Suction Device
  for Endoscopic Endonasal Interventions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Majid Roshanfar, Alex Zhang, Changyan He, Amir Hooshiar, Dale J. Podolsky, Thomas Looi, Eric Diller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This letter introduces a novel learning-based modeling framework for a
magnetically steerable soft suction device designed for endoscopic endonasal
brain tumor resection. The device is miniaturized (4 mm outer diameter, 2 mm
inner diameter, 40 mm length), 3D printed using biocompatible SIL 30 material,
and integrates embedded Fiber Bragg Grating (FBG) sensors for real-time shape
feedback. Shape reconstruction is represented using four Bezier control points,
enabling a compact and smooth model of the device's deformation. A data-driven
model was trained on 5,097 experimental samples covering a range of magnetic
field magnitudes (0-14 mT), actuation frequencies (0.2-1.0 Hz), and vertical
tip distances (90-100 mm), using both Neural Network (NN) and Random Forest
(RF) architectures. The RF model outperformed the NN across all metrics,
achieving a mean root mean square error of 0.087 mm in control point prediction
and a mean shape reconstruction error of 0.064 mm. Feature importance analysis
further revealed that magnetic field components predominantly influence distal
control points, while frequency and distance affect the base configuration.
This learning-based approach effectively models the complex nonlinear behavior
of hyperelastic soft robots under magnetic actuation without relying on
simplified physical assumptions. By enabling sub-millimeter shape prediction
accuracy and real-time inference, this work represents an advancement toward
the intelligent control of magnetically actuated soft robotic tools in
minimally invasive neurosurgery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Kicking to Causality: Simulating Infant Agency <span class="highlight-title">Detection</span> with a
  <span class="highlight-title">Robust</span> Intrinsic Reward 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xia Xu, Jochen Triesch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While human infants robustly discover their own causal efficacy, standard
reinforcement learning agents remain brittle, as their reliance on
correlation-based rewards fails in noisy, ecologically valid scenarios. To
address this, we introduce the Causal Action Influence Score (CAIS), a novel
intrinsic reward rooted in causal inference. CAIS quantifies an action's
influence by measuring the 1-Wasserstein distance between the learned
distribution of sensory outcomes conditional on that action, $p(h|a)$, and the
baseline outcome distribution, $p(h)$. This divergence provides a robust reward
that isolates the agent's causal impact from confounding environmental noise.
We test our approach in a simulated infant-mobile environment where
correlation-based perceptual rewards fail completely when the mobile is
subjected to external forces. In stark contrast, CAIS enables the agent to
filter this noise, identify its influence, and learn the correct policy.
Furthermore, the high-quality predictive model learned for CAIS allows our
agent, when augmented with a surprise signal, to successfully reproduce the
"extinction burst" phenomenon. We conclude that explicitly inferring causality
is a crucial mechanism for developing a robust sense of agency, offering a
psychologically plausible framework for more adaptive autonomous systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analytical Formulation of Autonomous Vehicle Freeway Merging <span class="highlight-title">Control</span>
  with State-Dependent Discharge Rates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qing Tang, Xianbiao Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The core of the freeway merging control problem lies in dynamic queue
propagation and dissipation linked to merging vehicle behavior. Traditionally,
queuing is modeled through demand-supply interactions with time varying demand
and fixed capacity. However, field observations show flow rates decrease during
congestion at freeway merges due to the impact of intersecting traffic, a
factor overlooked in fundamental diagrams. This manuscript introduces an
analytical approach to characterize and control the dynamic multi-stage merging
of autonomous vehicles, prioritizing traffic efficiency and safety. For the
first time, the effective discharge rate at the merging point, reduced by the
multi-stage dynamic merging process, is analytically derived using a closed
form formulation. Leveraging this expression, performance metrics such as queue
length and traffic delay are derived as the first objective. Additionally, a
crash risk function is established to quantitatively assess potential
collisions during the merging process, serving as the second objective.
Finally, the problem is formulated as a dynamic programming model to jointly
minimize delay and crash risk, with the merging location and speed as decision
variables. Given the terminal state, the ramp vehicle merging task is
formulated as a recursive optimization problem, employing backward induction to
find the minimum cost solution. Numerical experiments using the NGSIM dataset
validate the derived effective discharge rate. The results indicate that the
proposed model outperforms two benchmark algorithms, leading to a more
efficient and safer merging process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Transactions on Intelligent
  Transportation Systems (2025) as a regular paper (minor revision approved)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Place Recognition for Large-Scale UAV Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15089v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15089v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ioannis Tsampikos Papapetros, Ioannis Kansizoglou, Antonios Gasteratos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Place Recognition (vPR) plays a crucial role in Unmanned Aerial
Vehicle (UAV) navigation, enabling robust localization across diverse
environments. Despite significant advancements, aerial vPR faces unique
challenges due to the limited availability of large-scale, high-altitude
datasets, which limits model generalization, along with the inherent rotational
ambiguity in UAV imagery. To address these challenges, we introduce LASED, a
large-scale aerial dataset with approximately one million images,
systematically sampled from 170,000 unique locations throughout Estonia over a
decade, offering extensive geographic and temporal diversity. Its structured
design ensures clear place separation significantly enhancing model training
for aerial scenarios. Furthermore, we propose the integration of steerable
Convolutional Neural Networks (CNNs) to explicitly handle rotational variance,
leveraging their inherent rotational equivariance to produce robust,
orientation-invariant feature representations. Our extensive benchmarking
demonstrates that models trained on LASED achieve significantly higher recall
compared to those trained on smaller, less diverse datasets, highlighting the
benefits of extensive geographic coverage and temporal diversity. Moreover,
steerable CNNs effectively address rotational ambiguity inherent in aerial
imagery, consistently outperforming conventional convolutional architectures,
achieving on average 12\% recall improvement over the best-performing
non-steerable network. By combining structured, large-scale datasets with
rotation-equivariant neural networks, our approach significantly enhances model
robustness and generalization for aerial vPR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Search-Based Autonomous Vehicle Motion <span class="highlight-title">Planning</span> Using Game Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pouya Panahandeh, Mohammad Pirani, Baris Fidan, Amir Khajepour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a search-based interactive motion planning scheme
for autonomous vehicles (AVs), using a game-theoretic approach. In contrast to
traditional search-based approaches, the newly developed approach considers
other road users (e.g. drivers and pedestrians) as intelligent agents rather
than static obstacles. This leads to the generation of a more realistic path
for the AV. Due to the low computational time, the proposed motion planning
scheme is implementable in real-time applications. The performance of the
developed motion planning scheme is compared with existing motion planning
techniques and validated through experiments using WATonoBus, an electrical
all-weather autonomous shuttle bus.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Touch in the Wild: Learning Fine-Grained Manipulation with a Portable
  Visuo-Tactile Gripper 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15062v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15062v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyue Zhu, Binghao Huang, Yunzhu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Handheld grippers are increasingly used to collect human demonstrations due
to their ease of deployment and versatility. However, most existing designs
lack tactile sensing, despite the critical role of tactile feedback in precise
manipulation. We present a portable, lightweight gripper with integrated
tactile sensors that enables synchronized collection of visual and tactile data
in diverse, real-world, and in-the-wild settings. Building on this hardware, we
propose a cross-modal representation learning framework that integrates visual
and tactile signals while preserving their distinct characteristics. The
learning procedure allows the emergence of interpretable representations that
consistently focus on contacting regions relevant for physical interactions.
When used for downstream manipulation tasks, these representations enable more
efficient and effective policy learning, supporting precise robotic
manipulation based on multimodal feedback. We validate our approach on
fine-grained tasks such as test tube insertion and pipette-based fluid
transfer, demonstrating improved accuracy and robustness under external
disturbances. Our project page is available at
https://binghao-huang.github.io/touch_in_the_wild/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>More videos can be found on our
  website:https://binghao-huang.github.io/touch_in_the_wild/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EBA-AI: Ethics-Guided Bias-Aware AI for Efficient Underwater Image
  Enhancement and Coral Reef Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15036v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15036v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lyes Saad Saoud, Irfan Hussain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Underwater image enhancement is vital for marine conservation, particularly
coral reef monitoring. However, AI-based enhancement models often face dataset
bias, high computational costs, and lack of transparency, leading to potential
misinterpretations. This paper introduces EBA-AI, an ethics-guided bias-aware
AI framework to address these challenges. EBA-AI leverages CLIP embeddings to
detect and mitigate dataset bias, ensuring balanced representation across
varied underwater environments. It also integrates adaptive processing to
optimize energy efficiency, significantly reducing GPU usage while maintaining
competitive enhancement quality. Experiments on LSUI400, Oceanex, and UIEB100
show that while PSNR drops by a controlled 1.0 dB, computational savings enable
real-time feasibility for large-scale marine monitoring. Additionally,
uncertainty estimation and explainability techniques enhance trust in AI-driven
environmental decisions. Comparisons with CycleGAN, FunIEGAN, RAUNENet,
WaterNet, UGAN, PUGAN, and UTUIE validate EBA-AI's effectiveness in balancing
efficiency, fairness, and interpretability in underwater image processing. By
addressing key limitations of AI-driven enhancement, this work contributes to
sustainable, bias-aware, and computationally efficient marine conservation
efforts. For interactive visualizations, animations, source code, and access to
the preprint, visit: https://lyessaadsaoud.github.io/EBA-AI/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CPED-NCBFs: A Conformal <span class="highlight-title">Prediction</span> for Expert Demonstration-based Neural
  <span class="highlight-title">Control</span> Barrier Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15022v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15022v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sumeadh MS, Kevin Dsouza, Ravi Prakash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Among the promising approaches to enforce safety in control systems, learning
Control Barrier Functions (CBFs) from expert demonstrations has emerged as an
effective strategy. However, a critical challenge remains: verifying that the
learned CBFs truly enforce safety across the entire state space. This is
especially difficult when CBF is represented using neural networks (NCBFs).
Several existing verification techniques attempt to address this problem
including SMT-based solvers, mixed-integer programming (MIP), and interval or
bound-propagation methods but these approaches often introduce loose,
conservative bounds. To overcome these limitations, in this work we use
CPED-NCBFs a split-conformal prediction based verification strategy to verify
the learned NCBF from the expert demonstrations. We further validate our method
on point mass systems and unicycle models to demonstrate the effectiveness of
the proposed theory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6pages, 4figures, Submitted to the prestigious Indian Control
  Conference (ICC), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FCRF: Flexible Constructivism Reflection for Long-Horizon <span class="highlight-title">Robot</span>ic Task
  <span class="highlight-title">Planning</span> with Large Language Models <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufan Song, Jiatao Zhang, Zeng Gu, Qingmiao Liang, Tuocheng Hu, Wei Song, Shiqiang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous error correction is critical for domestic robots to achieve
reliable execution of complex long-horizon tasks. Prior work has explored
self-reflection in Large Language Models (LLMs) for task planning error
correction; however, existing methods are constrained by inflexible
self-reflection mechanisms that limit their effectiveness. Motivated by these
limitations and inspired by human cognitive adaptation, we propose the Flexible
Constructivism Reflection Framework (FCRF), a novel Mentor-Actor architecture
that enables LLMs to perform flexible self-reflection based on task difficulty,
while constructively integrating historical valuable experience with failure
lessons. We evaluated FCRF on diverse domestic tasks through simulation in
AlfWorld and physical deployment in the real-world environment. Experimental
results demonstrate that FCRF significantly improves overall performance and
self-reflection flexibility in complex long-horizon robotic tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, IROS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Heterogeneous object manipulation on nonlinear soft surface through
  linear <span class="highlight-title">control</span>ler 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pratik Ingle, Kasper Støy, Andres Faiña
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manipulation surfaces indirectly control and reposition objects by actively
modifying their shape or properties rather than directly gripping objects.
These surfaces, equipped with dense actuator arrays, generate dynamic
deformations. However, a high-density actuator array introduces considerable
complexity due to increased degrees of freedom (DOF), complicating control
tasks. High DOF restrict the implementation and utilization of manipulation
surfaces in real-world applications as the maintenance and control of such
systems exponentially increase with array/surface size. Learning-based control
approaches may ease the control complexity, but they require extensive training
samples and struggle to generalize for heterogeneous objects. In this study, we
introduce a simple, precise and robust PID-based linear close-loop feedback
control strategy for heterogeneous object manipulation on MANTA-RAY
(Manipulation with Adaptive Non-rigid Textile Actuation with Reduced Actuation
density). Our approach employs a geometric transformation-driven PID
controller, directly mapping tilt angle control outputs(1D/2D) to actuator
commands to eliminate the need for extensive black-box training. We validate
the proposed method through simulations and experiments on a physical system,
successfully manipulating objects with diverse geometries, weights and
textures, including fragile objects like eggs and apples. The outcomes
demonstrate that our approach is highly generalized and offers a practical and
reliable solution for object manipulation on soft robotic manipulation,
facilitating real-world implementation without prohibitive training demands.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Designing <span class="highlight-title">Robot</span>s with, not for: A Co-Design Framework for Empowering
  Interactions in Forensic Psychiatry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiaoqiao Ren, Remko Proesmans, Arend Pissens, Lara Dehandschutter, William Denecker, Lotte Rouckhout, Joke Carrette, Peter Vanhopplinus, Tony Belpaeme, Francis wyffels
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forensic mental health care involves the treatment of individuals with severe
mental disorders who have committed violent offences. These settings are often
characterized by high levels of bureaucracy, risk avoidance, and restricted
autonomy. Patients frequently experience a profound loss of control over their
lives, leading to heightened psychological stress-sometimes resulting in
isolation as a safety measure. In this study, we explore how co-design can be
used to collaboratively develop a companion robot that helps monitor and
regulate stress while maintaining tracking of the patients' interaction
behaviours for long-term intervention. We conducted four co-design workshops in
a forensic psychiatric clinic with patients, caregivers, and therapists. Our
process began with the presentation of an initial speculative prototype to
therapists, enabling reflection on shared concerns, ethical risks, and
desirable features. This was followed by a creative ideation session with
patients, a third workshop focused on defining desired functions and emotional
responses, and we are planning a final prototype demo to gather direct patient
feedback. Our findings emphasize the importance of empowering patients in the
design process and adapting proposals based on their current emotional state.
The goal was to empower the patient in the design process and ensure each
patient's voice was heard.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Digital twin and extended reality for teleoperation of the electric
  vehicle battery disassembly 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tero Kaarlela, Sami Salo, Jose Outeiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Disassembling and sorting Electric Vehicle Batteries (EVBs) supports a
sustainable transition to electric vehicles by enabling a closed-loop supply
chain. Currently, the manual disassembly process exposes workers to hazards,
including electrocution and toxic chemicals. We propose a teleoperated system
for the safe disassembly and sorting of EVBs. A human-in-the-loop can create
and save disassembly sequences for unknown EVB types, enabling future
automation. An RGB camera aligns the physical and digital twins of the EVB, and
the digital twin of the robot is based on the Robot Operating System (ROS)
middleware. This hybrid approach combines teleoperation and automation to
improve safety, adaptability, and efficiency in EVB disassembly and sorting.
The economic contribution is realized by reducing labor dependency and
increasing throughput in battery recycling. An online pilot study was set up to
evaluate the usability of the presented approach, and the results demonstrate
the potential as a user-friendly solution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One Step Beyond: Feedthrough & Placement-Aware Rectilinear Floorplanner 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14914v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14914v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhexuan Xu, Jie Wang, Siyuan Xu, Zijie Geng, Mingxuan Yuan, Feng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Floorplanning determines the shapes and locations of modules on a chip canvas
and plays a critical role in optimizing the chip's Power, Performance, and Area
(PPA) metrics. However, existing floorplanning approaches often fail to
integrate with subsequent physical design stages, leading to suboptimal
in-module component placement and excessive inter-module feedthrough. To tackle
this challenge, we propose Flora, a three-stage feedthrough and placement aware
rectilinear floorplanner. In the first stage, Flora employs wiremask and
position mask techniques to achieve coarse-grained optimization of HPWL and
feedthrough. In the second stage, under the constraint of a fixed outline,
Flora achieves a zero-whitespace layout by locally resizing module shapes,
thereby performing fine-grained optimization of feedthrough and improving
component placement. In the third stage, Flora utilizes a fast tree
search-based method to efficiently place components-including macros and
standard cells-within each module, subsequently adjusting module boundaries
based on the placement results to enable cross-stage optimization. Experimental
results show that Flora outperforms recent state-of-the-art floorplanning
approaches, achieving an average reduction of 6% in HPWL, 5.16% in FTpin,
29.15% in FTmod, and a 14% improvement in component placement performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoMoCAVs: Cohesive <span class="highlight-title">Decision</span>-Guided Motion <span class="highlight-title">Planning</span> for Connected and
  Autonomous Vehicles with Multi-Policy <span class="highlight-title">Reinforcement</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pan Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving demands reliable and efficient solutions to closely
related problems such as decision-making and motion planning. In this work,
decision-making refers specifically to highway lane selection, while motion
planning involves generating control commands (such as speed and steering) to
reach the chosen lane. In the context of Connected Autonomous Vehicles (CAVs),
achieving both flexible and safe lane selection alongside precise trajectory
execution remains a significant challenge. This paper proposes a framework
called Cohesive Decision-Guided Motion Planning (CDGMP), which tightly
integrates decision-making and motion planning using a Mixture of Experts (MoE)
inspired architecture combined with multi-policy reinforcement learning. By
coordinating multiple specialized sub-networks through a gating mechanism, the
method decomposes the complex driving task into modular components. Each
sub-network focuses on a specific aspect of driving, improving efficiency by
activating only the most relevant modules during inference. This design also
enhances safety through modular specialization. CDGMP improves the adaptability
and robustness of CAVs across diverse traffic scenarios, offering a scalable
solution to real-world autonomy challenges. The architectural principles behind
CDGMP, especially the use of MoE, also provide a strong foundation for other
high-dimensional decision and control tasks. Simulation results (available at
https://youtu.be/_-4OXNHV0UY) demonstrate reliable performance in both lane
selection and motion planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Multi-Agent <span class="highlight-title">Reinforcement</span> Learning with <span class="highlight-title">Control</span> Barrier
  Functions for Safety-Critical Autonomous Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14850v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14850v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        H. M. Sabbir Ahmad, Ehsan Sabouni, Alexander Wasilkoff, Param Budhraja, Zijian Guo, Songyuan Zhang, Chuchu Fan, Christos Cassandras, Wenchao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of safe policy learning in multi-agent safety-critical
autonomous systems. In such systems, it is necessary for each agent to meet the
safety requirements at all times while also cooperating with other agents to
accomplish the task. Toward this end, we propose a safe Hierarchical
Multi-Agent Reinforcement Learning (HMARL) approach based on Control Barrier
Functions (CBFs). Our proposed hierarchical approach decomposes the overall
reinforcement learning problem into two levels learning joint cooperative
behavior at the higher level and learning safe individual behavior at the lower
or agent level conditioned on the high-level policy. Specifically, we propose a
skill-based HMARL-CBF algorithm in which the higher level problem involves
learning a joint policy over the skills for all the agents and the lower-level
problem involves learning policies to execute the skills safely with CBFs. We
validate our approach on challenging environment scenarios whereby a large
number of agents have to safely navigate through conflicting road networks.
Compared with existing state of the art methods, our approach significantly
improves the safety achieving near perfect (within 5%) success/safety rate
while also improving performance across all the environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KGN-Pro: Keypoint-Based <span class="highlight-title">Grasp</span> <span class="highlight-title">Prediction</span> through Probabilistic 2D-3D
  Correspondence Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingran Chen, Baorun Li, Jian Yang, Yong Liu, Guangyao Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-level robotic manipulation tasks demand flexible 6-DoF grasp estimation
to serve as a basic function. Previous approaches either directly generate
grasps from point-cloud data, suffering from challenges with small objects and
sensor noise, or infer 3D information from RGB images, which introduces
expensive annotation requirements and discretization issues. Recent methods
mitigate some challenges by retaining a 2D representation to estimate grasp
keypoints and applying Perspective-n-Point (PnP) algorithms to compute 6-DoF
poses. However, these methods are limited by their non-differentiable nature
and reliance solely on 2D supervision, which hinders the full exploitation of
rich 3D information. In this work, we present KGN-Pro, a novel grasping network
that preserves the efficiency and fine-grained object grasping of previous KGNs
while integrating direct 3D optimization through probabilistic PnP layers.
KGN-Pro encodes paired RGB-D images to generate Keypoint Map, and further
outputs a 2D confidence map to weight keypoint contributions during
re-projection error minimization. By modeling the weighted sum of squared
re-projection errors probabilistically, the network effectively transmits 3D
supervision to its 2D keypoint predictions, enabling end-to-end learning.
Experiments on both simulated and real-world platforms demonstrate that KGN-Pro
outperforms existing methods in terms of grasp cover rate and success rate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Light Future: Multimodal Action Frame <span class="highlight-title">Prediction</span> via InstructPix2Pix <span class="chip">WACV 2026</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14809v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14809v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zesen Zhong, Duomin Zhang, Yijia Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting future motion trajectories is a critical capability across domains
such as robotics, autonomous systems, and human activity forecasting, enabling
safer and more intelligent decision-making. This paper proposes a novel,
efficient, and lightweight approach for robot action prediction, offering
significantly reduced computational cost and inference latency compared to
conventional video prediction models. Importantly, it pioneers the adaptation
of the InstructPix2Pix model for forecasting future visual frames in robotic
tasks, extending its utility beyond static image editing. We implement a deep
learning-based visual prediction framework that forecasts what a robot will
observe 100 frames (10 seconds) into the future, given a current image and a
textual instruction. We repurpose and fine-tune the InstructPix2Pix model to
accept both visual and textual inputs, enabling multimodal future frame
prediction. Experiments on the RoboTWin dataset (generated based on real-world
scenarios) demonstrate that our method achieves superior SSIM and PSNR compared
to state-of-the-art baselines in robot action prediction tasks. Unlike
conventional video prediction models that require multiple input frames, heavy
computation, and slow inference latency, our approach only needs a single image
and a text prompt as input. This lightweight design enables faster inference,
reduced GPU demands, and flexible multimodal control, particularly valuable for
applications like robotics and sports motion trajectory analytics, where motion
trajectory precision is prioritized over visual fidelity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages including appendix, 5 tables, 8 figures, to be submitted to
  WACV 2026</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CBAGAN-RRT: Convolutional Block Attention Generative Adversarial Network
  for Sampling-Based Path <span class="highlight-title">Planning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.10442v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.10442v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Sagar, Sai Teja Gilukara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sampling-based path planning algorithms play an important role in autonomous
robotics. However, a common problem among these algorithms is that the initial
path generated is not optimal, and the convergence is too slow for real-world
applications. In this paper, we propose a novel image-based learning algorithm
using a Convolutional Block Attention Generative Adversarial Network
(CBAGAN-RRT) with a combination of spatial and channel attention and a novel
loss function to design the heuristics, find a better optimal path, and improve
the convergence of the algorithm, both concerning time and speed. The
probability distribution of the paths generated from our GAN model is used to
guide the sampling process for the RRT algorithm. We demonstrate that our
algorithm outperforms the previous state-of-the-art algorithms using both the
image quality generation metrics, like IOU Score, Dice Score, FID score, and
path planning metrics like time cost and the number of nodes. Ablation studies
show the effectiveness of various components in our network architecture. The
advantage of our approach is that we can avoid the complicated preprocessing in
the state space, our model can be generalized to complex environments like
those containing turns and narrow passages without loss of accuracy, and our
model can be easily integrated with other sampling-based path planning
algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CROSS-GAiT: Cross-Attention-Based Multimodal Representation <span class="highlight-title">Fusion</span> for
  Parametric Gait Adaptation in Complex Terrains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17262v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17262v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gershom Seneviratne, Kasun Weerakoon, Mohamed Elnoor, Vignesh Rajgopal, Harshavarthan Varatharajan, Mohamed Khalid M Jaffar, Jason Pusey, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present CROSS-GAiT, a novel algorithm for quadruped robots that uses Cross
Attention to fuse terrain representations derived from visual and time-series
inputs; including linear accelerations, angular velocities, and joint efforts.
These fused representations are used to continuously adjust two critical gait
parameters (step height and hip splay), enabling adaptive gaits that respond
dynamically to varying terrain conditions. To generate terrain representations,
we process visual inputs through a masked Vision Transformer (ViT) encoder and
time-series data through a dilated causal convolutional encoder. The Cross
Attention mechanism then selects and integrates the most relevant features from
each modality, combining terrain characteristics with robot dynamics for
informed gait adaptation. This fused representation allows CROSS-GAiT to
continuously adjust gait parameters in response to unpredictable terrain
conditions in real-time. We train CROSS-GAiT on a diverse set of terrains
including asphalt, concrete, brick pavements, grass, dense vegetation, pebbles,
gravel, and sand and validate its generalization ability on unseen
environments. Our hardware implementation on the Ghost Robotics Vision 60
demonstrates superior performance in challenging terrains, such as high-density
vegetation, unstable surfaces, sandbanks, and deformable substrates. We observe
at least a 7.04% reduction in IMU energy density and a 27.3% reduction in total
joint effort, which directly correlates with increased stability and reduced
energy usage when compared to state-of-the-art methods. Furthermore, CROSS-GAiT
demonstrates at least a 64.5% increase in success rate and a 4.91% reduction in
time to reach the goal in four complex scenarios. Additionally, the learned
representations perform 4.48% better than the state-of-the-art on a terrain
classification task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Design and Characterization of a Micro-Vibration Adhesion System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.05351v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.05351v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siqian Li, Xi Wang, Jung-Che Chang, Xin Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, miniature wall-climbing robots have attracted widespread
attention due to their significant potential in equipment inspection and
in-situ repair applications. Traditional wall-climbing systems typically rely
on electromagnetic, electrostatic, vacuum suction, or van der Waals forces for
controllable adhesion. However, these conventional methods impose limitations
when striving for both a compact design and high-speed mobility. This paper
proposes a novel Vibration-Based Adhesion (VBA) technique, which utilizes a
flexible disk vibrating near a surface to generate a strong and controllable
attractive force without direct contact. By employing an electric motor as the
vibration source, the constructed VBA system was experimentally evaluated,
achieving an adhesion-to-weight ratio exceeding 51 times. The experimental
results demonstrate that this adhesion mechanism not only provides a high
normal force but also maintains minimal shear force, making it particularly
suitable for high-speed movement and heavy load applications in miniature
wall-climbing robots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ G-Dex<span class="highlight-title">Grasp</span>: Generalizable Dexterous <span class="highlight-title">Grasp</span>ing Synthesis Via Part-Aware
  Prior Retrieval and Prior-Assisted Generation <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.19457v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.19457v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juntao Jian, Xiuping Liu, Zixuan Chen, Manyi Li, Jian Liu, Ruizhen Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in dexterous grasping synthesis have demonstrated significant
progress in producing reasonable and plausible grasps for many task purposes.
But it remains challenging to generalize to unseen object categories and
diverse task instructions. In this paper, we propose G-DexGrasp, a
retrieval-augmented generation approach that can produce high-quality dexterous
hand configurations for unseen object categories and language-based task
instructions. The key is to retrieve generalizable grasping priors, including
the fine-grained contact part and the affordance-related distribution of
relevant grasping instances, for the following synthesis pipeline.
Specifically, the fine-grained contact part and affordance act as generalizable
guidance to infer reasonable grasping configurations for unseen objects with a
generative model, while the relevant grasping distribution plays as
regularization to guarantee the plausibility of synthesized grasps during the
subsequent refinement optimization. Our comparison experiments validate the
effectiveness of our key designs for generalization and demonstrate the
remarkable performance against the existing approaches. Project page:
https://g-dexgrasp.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning-based 3D Reconstruction in Autonomous Driving: A Comprehensive
  <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.14537v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.14537v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liewen Liao, Weihao Yan, Ming Yang, Songan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning-based 3D reconstruction has emerged as a transformative technique in
autonomous driving, enabling precise modeling of both dynamic and static
environments through advanced neural representations. Despite data
augmentation, 3D reconstruction inspires pioneering solution for vital tasks in
the field of autonomous driving, such as scene understanding and closed-loop
simulation. We investigates the details of 3D reconstruction and conducts a
multi-perspective, in-depth analysis of recent advancements. Specifically, we
first provide a systematic introduction of preliminaries, including data
modalities, benchmarks and technical preliminaries of learning-based 3D
reconstruction, facilitating instant identification of suitable methods
according to sensor suites. Then, we systematically review learning-based 3D
reconstruction methods in autonomous driving, categorizing approaches by
subtasks and conducting multi-dimensional analysis and summary to establish a
comprehensive technical reference. The development trends and existing
challenges are summarized in the context of learning-based 3D reconstruction in
autonomous driving. We hope that our review will inspire future researches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ To Lead or to Follow? Adaptive <span class="highlight-title">Robot</span> Task <span class="highlight-title">Planning</span> in Human-<span class="highlight-title">Robot</span>
  Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01483v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01483v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Noormohammadi-Asl, Stephen L. Smith, Kerstin Dautenhahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adaptive task planning is fundamental to ensuring effective and seamless
human-robot collaboration. This paper introduces a robot task planning
framework that takes into account both human leading/following preferences and
performance, specifically focusing on task allocation and scheduling in
collaborative settings. We present a proactive task allocation approach with
three primary objectives: enhancing team performance, incorporating human
preferences, and upholding a positive human perception of the robot and the
collaborative experience. Through a user study, involving an autonomous mobile
manipulator robot working alongside participants in a collaborative scenario,
we confirm that the task planning framework successfully attains all three
intended goals, thereby contributing to the advancement of adaptive task
planning in human-robot collaboration. This paper mainly focuses on the first
two objectives, and we discuss the third objective, participants' perception of
the robot, tasks, and collaboration in a companion paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Maple<span class="highlight-title">Grasp</span>: Mask-guided Feature Pooling for Language-driven Efficient
  <span class="highlight-title">Robot</span>ic <span class="highlight-title">Grasp</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06535v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06535v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vineet Bhat, Naman Patel, Prashanth Krishnamurthy, Ramesh Karri, Farshad Khorrami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic manipulation of unseen objects via natural language commands remains
challenging. Language driven robotic grasping (LDRG) predicts stable grasp
poses from natural language queries and RGB-D images. We propose MapleGrasp, a
novel framework that leverages mask-guided feature pooling for efficient
vision-language driven grasping. Our two-stage training first predicts
segmentation masks from CLIP-based vision-language features. The second stage
pools features within these masks to generate pixel-level grasp predictions,
improving efficiency, and reducing computation. Incorporating mask pooling
results in a 7% improvement over prior approaches on the OCID-VLG benchmark.
Furthermore, we introduce RefGraspNet, an open-source dataset eight times
larger than existing alternatives, significantly enhancing model generalization
for open-vocabulary grasping. MapleGrasp scores a strong grasping accuracy of
89\% when compared with competing methods in the RefGraspNet benchmark. Our
method achieves comparable performance to larger Vision-Language-Action models
on the LIBERO benchmark, and shows significantly better generalization to
unseen tasks. Real-world experiments on a Franka arm demonstrate 73% success
rate with unseen objects, surpassing competitive baselines by 11%. Code will be
released after publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Cars Move: Analyzing Driving <span class="highlight-title">Dynamic</span>s for Safer Urban Traffic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04020v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04020v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangan Qian, Jinyu Miao, Xinyu Jiao, Ziang Luo, Zheng Fu, Yining Shi, Yunlong Wang, Kun Jiang, Diange Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the spatial dynamics of cars within urban systems is essential
for optimizing infrastructure management and resource allocation. Recent
empirical approaches for analyzing traffic patterns have gained traction due to
their applicability to city-scale policy development. However, conventional
methodologies often rely on fragmented grid-based techniques, which may
overlook critical interdependencies among spatial elements and temporal
continuity. These limitations can compromise analytical effectiveness in
complex urban environments. To address these challenges, we propose
PriorMotion, a data integration framework designed to systematically uncover
movement patterns through driving dynamics analysis. Our approach combines
multi-scale empirical observations with customized analytical tools to capture
evolving spatial-temporal trends in urban traffic. Comprehensive evaluations
demonstrate that PriorMotion significantly enhances analytical outcomes,
including increased accuracy in traffic pattern analysis, improved adaptability
to heterogeneous data environments, and reduced long-term projection errors.
Validation confirms its effectiveness for urban infrastructure management
applications requiring precise characterization of complex spatial-temporal
interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Making VLMs More <span class="highlight-title">Robot</span>-Friendly: Self-Critical Distillation of Low-Level
  Procedural Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.08224v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.08224v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chan Young Park, Jillian Fisher, Marius Memmel, Dipika Khullar, Seoho Yun, Abhishek Gupta, Yejin Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown promise in robotic procedural
planning, yet their human-centric reasoning often omits the low-level, grounded
details needed for robotic execution. Vision-language models (VLMs) offer a
path toward more perceptually grounded plans, but current methods either rely
on expensive, large-scale models or are constrained to narrow simulation
settings. We introduce SelfReVision, a lightweight and scalable
self-improvement framework for vision-language procedural planning.
SelfReVision enables small VLMs to iteratively critique, revise, and verify
their own plans-without external supervision or teacher models-drawing
inspiration from chain-of-thought prompting and self-instruct paradigms.
Through this self-distillation loop, models generate higher-quality,
execution-ready plans that can be used both at inference and for continued
fine-tuning. Using models varying from 3B to 72B, our results show that
SelfReVision not only boosts performance over weak base VLMs but also
outperforms models 100X the size, yielding improved control in downstream
embodied tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code Available: https://github.com/chan0park/SelfReVision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing Object Goal <span class="highlight-title">Navigation</span> Through LLM-enhanced Object Affinities
  Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09971v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09971v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengying Lin, Shugao Liu, Dingxi Zhang, Yaran Chen, Zhaoran Wang, Haoran Li, Dongbin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object-goal navigation requires mobile robots to efficiently locate targets
with visual and spatial information, yet existing methods struggle with
generalization in unseen environments. Heuristic approaches with naive metrics
fail in complex layouts, while graph-based and learning-based methods suffer
from environmental biases and limited generalization. Although Large Language
Models (LLMs) as planners or agents offer a rich knowledge base, they are
cost-inefficient and lack targeted historical experience. To address these
challenges, we propose the LLM-enhanced Object Affinities Transfer (LOAT)
framework, integrating LLM-derived semantics with learning-based approaches to
leverage experiential object affinities for better generalization in unseen
settings. LOAT employs a dual-module strategy: one module accesses LLMs' vast
knowledge, and the other applies learned object semantic relationships,
dynamically fusing these sources based on context. Evaluations in AI2-THOR and
Habitat simulators show significant improvements in navigation success and
efficiency, and real-world deployment demonstrates the zero-shot ability of
LOAT to enhance object-goal navigation systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safe <span class="highlight-title">Navigation</span> in Uncertain Crowded Environments Using Risk Adaptive
  CVaR Barrier Functions <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.06513v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.06513v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Wang, Taekyung Kim, Bardh Hoxha, Georgios Fainekos, Dimitra Panagou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot navigation in dynamic, crowded environments poses a significant
challenge due to the inherent uncertainties in the obstacle model. In this
work, we propose a risk-adaptive approach based on the Conditional
Value-at-Risk Barrier Function (CVaR-BF), where the risk level is automatically
adjusted to accept the minimum necessary risk, achieving a good performance in
terms of safety and optimization feasibility under uncertainty. Additionally,
we introduce a dynamic zone-based barrier function which characterizes the
collision likelihood by evaluating the relative state between the robot and the
obstacle. By integrating risk adaptation with this new function, our approach
adaptively expands the safety margin, enabling the robot to proactively avoid
obstacles in highly dynamic environments. Comparisons and ablation studies
demonstrate that our method outperforms existing social navigation approaches,
and validate the effectiveness of our proposed framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2025 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS). Project page: {https://lawliet9666.github.io/cvarbf/}</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-07-19T00:00:00Z">2025-07-19</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">22</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ X-Nav: Learning End-to-End Cross-Embodiment <span class="highlight-title">Navigation</span> for Mobile <span class="highlight-title">Robot</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haitong Wang, Aaron Hao Tan, Angus Fung, Goldie Nejat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing navigation methods are primarily designed for specific robot
embodiments, limiting their generalizability across diverse robot platforms. In
this paper, we introduce X-Nav, a novel framework for end-to-end
cross-embodiment navigation where a single unified policy can be deployed
across various embodiments for both wheeled and quadrupedal robots. X-Nav
consists of two learning stages: 1) multiple expert policies are trained using
deep reinforcement learning with privileged observations on a wide range of
randomly generated robot embodiments; and 2) a single general policy is
distilled from the expert policies via navigation action chunking with
transformer (Nav-ACT). The general policy directly maps visual and
proprioceptive observations to low-level control commands, enabling
generalization to novel robot embodiments. Simulated experiments demonstrated
that X-Nav achieved zero-shot transfer to both unseen embodiments and
photorealistic environments. A scalability study showed that the performance of
X-Nav improves when trained with an increasing number of randomly generated
embodiments. An ablation study confirmed the design choices of X-Nav.
Furthermore, real-world experiments were conducted to validate the
generalizability of X-Nav in real-world environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Extrinsic Dexterity for Occluded <span class="highlight-title">Grasp</span>ing on <span class="highlight-title">Grasp</span>
  Constraining Walls 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keita Kobashi, Masayoshi Tomizuka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses the problem of occluded grasping, where primary grasp
configurations of an object are not available due to occlusion with
environment. Simple parallel grippers often struggle with such tasks due to
limited dexterity and actuation constraints. Prior works have explored object
pose reorientation such as pivoting by utilizing extrinsic contacts between an
object and an environment feature like a wall, to make the object graspable.
However, such works often assume the presence of a short wall, and this
assumption may not always hold in real-world scenarios. If the wall available
for interaction is too large or too tall, the robot may still fail to grasp the
object even after pivoting, and the robot must combine different types of
actions to grasp. To address this, we propose a hierarchical reinforcement
learning (RL) framework. We use Q-learning to train a high-level policy that
selects the type of action expected to yield the highest reward. The selected
low-level skill then samples a specific robot action in continuous space. To
guide the robot to an appropriate location for executing the selected action,
we adopt a Conditional Variational Autoencoder (CVAE). We condition the CVAE on
the object point cloud and the skill ID, enabling it to infer a suitable
location based on the object geometry and the selected skill. To promote
generalization, we apply domain randomization during the training of low-level
skills. The RL policy is trained entirely in simulation with a box-like object
and deployed to six objects in real world. We conduct experiments to evaluate
our method and demonstrate both its generalizability and robust sim-to-real
transfer performance with promising success rates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sensor-Space Based <span class="highlight-title">Robust</span> Kinematic <span class="highlight-title">Control</span> of Redundant Soft
  Manipulator by Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16842v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16842v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinan Meng, Kun Qian, Jiong Yang, Renbo Su, Zhenhong Li, Charlie C. L. Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The intrinsic compliance and high degree of freedom (DoF) of redundant soft
manipulators facilitate safe interaction and flexible task execution. However,
effective kinematic control remains highly challenging, as it must handle
deformations caused by unknown external loads and avoid actuator saturation due
to improper null-space regulation - particularly in confined environments. In
this paper, we propose a Sensor-Space Imitation Learning Kinematic Control
(SS-ILKC) framework to enable robust kinematic control under actuator
saturation and restrictive environmental constraints. We employ a dual-learning
strategy: a multi-goal sensor-space control framework based on reinforcement
learning principle is trained in simulation to develop robust control policies
for open spaces, while a generative adversarial imitation learning approach
enables effective policy learning from sparse expert demonstrations for
confined spaces. To enable zero-shot real-world deployment, a pre-processed
sim-to-real transfer mechanism is proposed to mitigate the
simulation-to-reality gap and accurately characterize actuator saturation
limits. Experimental results demonstrate that our method can effectively
control a pneumatically actuated soft manipulator, achieving precise
path-following and object manipulation in confined environments under unknown
loading conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AquaChat: An LLM-Guided ROV Framework for Adaptive Inspection of
  Aquaculture Net Pens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Waseem Akram, Muhayy Ud Din, Abdelhaleem Saad, Irfan Hussain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspection of aquaculture net pens is essential for maintaining the
structural integrity, biosecurity, and operational efficiency of fish farming
systems. Traditional inspection approaches rely on pre-programmed missions or
manual control, offering limited adaptability to dynamic underwater conditions
and user-specific demands. In this study, we propose AquaChat, a novel Remotely
Operated Vehicle (ROV) framework that integrates Large Language Models (LLMs)
for intelligent and adaptive net pen inspection. The system features a
multi-layered architecture: (1) a high-level planning layer that interprets
natural language user commands using an LLM to generate symbolic task plans;
(2) a mid-level task manager that translates plans into ROV control sequences;
and (3) a low-level motion control layer that executes navigation and
inspection tasks with precision. Real-time feedback and event-triggered
replanning enhance robustness in challenging aquaculture environments. The
framework is validated through experiments in both simulated and controlled
aquatic environments representative of aquaculture net pens. Results
demonstrate improved task flexibility, inspection accuracy, and operational
efficiency. AquaChat illustrates the potential of integrating language-based AI
with marine robotics to enable intelligent, user-interactive inspection systems
for sustainable aquaculture operations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Corridor-based Adaptive <span class="highlight-title">Control</span> Barrier and Lyapunov Functions for Safe
  Mobile <span class="highlight-title">Robot</span> <span class="highlight-title">Navigation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Mohammad, Nicola Bezzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe navigation in unknown and cluttered environments remains a challenging
problem in robotics. Model Predictive Contour Control (MPCC) has shown promise
for performant obstacle avoidance by enabling precise and agile trajectory
tracking, however, existing methods lack formal safety assurances. To address
this issue, we propose a general Control Lyapunov Function (CLF) and Control
Barrier Function (CBF) enabled MPCC framework that enforces safety constraints
derived from a free-space corridor around the planned trajectory. To enhance
feasibility, we dynamically adapt the CBF parameters at runtime using a Soft
Actor-Critic (SAC) policy. The approach is validated with extensive simulations
and an experiment on mobile robot navigation in unknown cluttered environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be presented in the 64th IEEE Conference on Decision and Control
  (CDC 25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-aware Probabilistic 3D Human Motion Forecasting via
  Invertible Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Ma, Kanglei Zhou, Fuyang Yu, Frederick W. B. Li, Xiaohui Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D human motion forecasting aims to enable autonomous applications.
Estimating uncertainty for each prediction (i.e., confidence based on
probability density or quantile) is essential for safety-critical contexts like
human-robot collaboration to minimize risks. However, existing diverse motion
forecasting approaches struggle with uncertainty quantification due to implicit
probabilistic representations hindering uncertainty modeling. We propose
ProbHMI, which introduces invertible networks to parameterize poses in a
disentangled latent space, enabling probabilistic dynamics modeling. A
forecasting module then explicitly predicts future latent distributions,
allowing effective uncertainty quantification. Evaluated on benchmarks, ProbHMI
achieves strong performance for both deterministic and diverse prediction while
validating uncertainty calibration, critical for risk-aware decision making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Koopman Operator Based Linear Model Predictive <span class="highlight-title">Control</span> for 2D Quadruped
  Trotting, Bounding, and Gait Transition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chun-Ming Yang, Pranav A. Bhounsule
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online optimal control of quadrupedal robots would enable them to plan their
movement in novel scenarios. Linear Model Predictive Control (LMPC) has emerged
as a practical approach for real-time control. In LMPC, an optimization problem
with a quadratic cost and linear constraints is formulated over a finite
horizon and solved on the fly. However, LMPC relies on linearizing the
equations of motion (EOM), which may lead to poor solution quality. In this
paper, we use Koopman operator theory and the Extended Dynamic Mode
Decomposition (EDMD) to create a linear model of the system in high dimensional
space, thus retaining the nonlinearity of the EOM. We model the aerial phase
and ground contact phases using different linear models. Then, using LMPC, we
demonstrate bounding, trotting, and bound-to-trot and trot-to-bound gait
transitions in level and rough terrains. The main novelty is the use of Koopman
operator theory to create hybrid models of a quadrupedal system and demonstrate
the online generation of multiple gaits and gaits transitions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BT-TL-DMPs: A Novel <span class="highlight-title">Robot</span> TAMP Framework Combining Behavior Tree,
  Temporal Logic and <span class="highlight-title">Dynamic</span>al Movement Primitives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zezhi Liu, Shizhen Wu, Hanqian Luo, Deyun Qin, Yongchun Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of Learning from Demonstration (LfD), enabling robots to
generalize learned manipulation skills to novel scenarios for long-horizon
tasks remains challenging. Specifically, it is still difficult for robots to
adapt the learned skills to new environments with different task and motion
requirements, especially in long-horizon, multi-stage scenarios with intricate
constraints. This paper proposes a novel hierarchical framework, called
BT-TL-DMPs, that integrates Behavior Tree (BT), Temporal Logic (TL), and
Dynamical Movement Primitives (DMPs) to address this problem. Within this
framework, Signal Temporal Logic (STL) is employed to formally specify complex,
long-horizon task requirements and constraints. These STL specifications are
systematically transformed to generate reactive and modular BTs for high-level
decision-making task structure. An STL-constrained DMP optimization method is
proposed to optimize the DMP forcing term, allowing the learned motion
primitives to adapt flexibly while satisfying intricate spatiotemporal
requirements and, crucially, preserving the essential dynamics learned from
demonstrations. The framework is validated through simulations demonstrating
generalization capabilities under various STL constraints and real-world
experiments on several long-horizon robotic manipulation tasks. The results
demonstrate that the proposed framework effectively bridges the symbolic-motion
gap, enabling more reliable and generalizable autonomous manipulation for
complex robotic tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A 21-DOF Humanoid Dexterous Hand with Hybrid SMA-Motor Actuation: CYJ
  Hand-0 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Chai, Xiang Yao, Mengfan Hou, Yanghong Li, Erbao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CYJ Hand-0 is a 21-DOF humanoid dexterous hand featuring a hybrid
tendon-driven actuation system that combines shape memory alloys (SMAs) and DC
motors. The hand employs high-strength fishing line as artificial tendons and
uses a fully 3D-printed AlSi10Mg metal frame designed to replicate the skeletal
and tendon-muscle structure of the human hand. A linear motor-driven module
controls finger flexion, while an SMA-based module enables finger extension and
lateral abduction. These modules are integrated into a compact hybrid actuation
unit mounted on a custom rear support structure. Mechanical and kinematic
experiments, conducted under an Arduino Mega 2560-based control system,
validate the effectiveness of the design and demonstrate its biomimetic
dexterity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Motion <span class="highlight-title">Segmentation</span> and Egomotion <span class="highlight-title">Estimation</span> from Event-Based Normal
  Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Hua, Dehao Yuan, Cornelia Fermüller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a robust framework for motion segmentation and
egomotion estimation using event-based normal flow, tailored specifically for
neuromorphic vision sensors. In contrast to traditional methods that rely
heavily on optical flow or explicit depth estimation, our approach exploits the
sparse, high-temporal-resolution event data and incorporates geometric
constraints between normal flow, scene structure, and inertial measurements.
The proposed optimization-based pipeline iteratively performs event
over-segmentation, isolates independently moving objects via residual analysis,
and refines segmentations using hierarchical clustering informed by motion
similarity and temporal consistency. Experimental results on the EVIMO2v2
dataset validate that our method achieves accurate segmentation and
translational motion estimation without requiring full optical flow
computation. This approach demonstrates significant advantages at object
boundaries and offers considerable potential for scalable, real-time robotic
and navigation applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Koopman Operator Based Time-Delay Embeddings and State History Augmented
  LQR for Periodic Hybrid Systems: Bouncing Pendulum and Bipedal Walking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chun-Ming Yang, Pranav A. Bhounsule
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-delay embedding is a technique that uses snapshots of state history over
time to build a linear state space model of a nonlinear smooth system. We
demonstrate that periodic non-smooth or hybrid system can also be modeled as a
linear state space system using this approach as long as its behavior is
consistent in modes and timings. We extended time-delay embeddings to generate
a linear model of two periodic hybrid systems: the bouncing pendulum and the
simplest walker with control inputs. This leads to a novel state history
augmented linear quadratic regulator (LQR) which uses current and past state
history for feedback control.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ASMA: An Adaptive Safety Margin Algorithm for Vision-Language Drone
  <span class="highlight-title">Navigation</span> via Scene-Aware <span class="highlight-title">Control</span> Barrier Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10283v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10283v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sourav Sanyal, Kaushik Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly evolving field of vision-language navigation (VLN), ensuring
safety for physical agents remains an open challenge. For a human-in-the-loop
language-operated drone to navigate safely, it must understand natural language
commands, perceive the environment, and simultaneously avoid hazards in real
time. Control Barrier Functions (CBFs) are formal methods that enforce safe
operating conditions. Model Predictive Control (MPC) is an optimization
framework that plans a sequence of future actions over a prediction horizon,
ensuring smooth trajectory tracking while obeying constraints. In this work, we
consider a VLN-operated drone platform and enhance its safety by formulating a
novel scene-aware CBF that leverages ego-centric observations from a camera
which has both Red-Green-Blue as well as Depth (RGB-D) channels. A CBF-less
baseline system uses a Vision-Language Encoder with cross-modal attention to
convert commands into an ordered sequence of landmarks. An object detection
model identifies and verifies these landmarks in the captured images to
generate a planned path. To further enhance safety, an Adaptive Safety Margin
Algorithm (ASMA) is proposed. ASMA tracks moving objects and performs
scene-aware CBF evaluation on-the-fly, which serves as an additional constraint
within the MPC framework. By continuously identifying potentially risky
observations, the system performs prediction in real time about unsafe
conditions and proactively adjusts its control actions to maintain safe
navigation throughout the trajectory. Deployed on a Parrot Bebop2 quadrotor in
the Gazebo environment using the Robot Operating System (ROS), ASMA achieves
64%-67% increase in success rates with only a slight increase (1.4%-5.8%) in
trajectory lengths compared to the baseline CBF-less VLN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Robotics and Automation Letters
  (RA-L)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Grasp</span>MAS: Zero-Shot Language-driven <span class="highlight-title">Grasp</span> <span class="highlight-title">Detection</span> with Multi-Agent
  System <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.18448v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.18448v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quang Nguyen, Tri Le, Huy Nguyen, Thieu Vo, Tung D. Ta, Baoru Huang, Minh N. Vu, Anh Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language-driven grasp detection has the potential to revolutionize
human-robot interaction by allowing robots to understand and execute grasping
tasks based on natural language commands. However, existing approaches face two
key challenges. First, they often struggle to interpret complex text
instructions or operate ineffectively in densely cluttered environments.
Second, most methods require a training or finetuning step to adapt to new
domains, limiting their generation in real-world applications. In this paper,
we introduce GraspMAS, a new multi-agent system framework for language-driven
grasp detection. GraspMAS is designed to reason through ambiguities and improve
decision-making in real-world scenarios. Our framework consists of three
specialized agents: Planner, responsible for strategizing complex queries;
Coder, which generates and executes source code; and Observer, which evaluates
the outcomes and provides feedback. Intensive experiments on two large-scale
datasets demonstrate that our GraspMAS significantly outperforms existing
baselines. Additionally, robot experiments conducted in both simulation and
real-world settings further validate the effectiveness of our approach. Our
project page is available at https://zquang2202.github.io/GraspMAS
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IROS 2025. Webpage:
  https://zquang2202.github.io/GraspMAS/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ALLO: A Photorealistic <span class="highlight-title">Dataset</span> and Data Generation Pipeline for Anomaly
  <span class="highlight-title">Detection</span> During <span class="highlight-title">Robot</span>ic Proximity Operations in Lunar Orbit <span class="chip">ICRA'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20435v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20435v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Selina Leveugle, Chang Won Lee, Svetlana Stolpner, Chris Langley, Paul Grouchy, Steven Waslander, Jonathan Kelly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  NASA's forthcoming Lunar Gateway space station, which will be uncrewed most
of the time, will need to operate with an unprecedented level of autonomy.
Enhancing autonomy on the Gateway presents several unique challenges, one of
which is to equip the Canadarm3, the Gateway's external robotic system, with
the capability to perform worksite monitoring. Monitoring will involve using
the arm's inspection cameras to detect any anomalies within the operating
environment, a task complicated by the widely-varying lighting conditions in
space. In this paper, we introduce the visual anomaly detection and
localization task for space applications and establish a benchmark with our
novel synthetic dataset called ALLO (for Anomaly Localization in Lunar Orbit).
We develop a complete data generation pipeline to create ALLO, which we use to
evaluate the performance of state-of-the-art visual anomaly detection
algorithms. Given the low tolerance for risk during space operations and the
lack of relevant data, we emphasize the need for novel, robust, and accurate
anomaly detection methods to handle the challenging visual conditions found in
lunar orbit and beyond.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to International Conference on Robotics and Automation
  (ICRA'25), Atlanta, USA, May 19-23, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Informed Hybrid Zonotope-based Motion <span class="highlight-title">Planning</span> Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.09309v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.09309v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Xie, Johannes Betz, Amr Alanwar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimal path planning in nonconvex free spaces is notoriously challenging, as
formulating such problems as mixed-integer linear programs (MILPs) is NP-hard.
We propose HZ-MP, an informed Hybrid Zonotope-based Motion Planner, as an
alternative approach that decomposes the obstacle-free space and performs
low-dimensional face sampling guided by an ellipsotope heuristic, enabling
focused exploration along promising transit regions. This structured
exploration eliminates the excessive, unreachable sampling that degrades
existing informed planners such as AIT* and EIT* in narrow gaps or boxed-goal
scenarios. We prove that HZ-MP is probabilistically complete and asymptotically
optimal. It converges to near-optimal trajectories in finite time and scales to
high-dimensional cluttered scenes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Slide<span class="highlight-title">SLAM</span>: Sparse, Lightweight, Decentralized Metric-Semantic <span class="highlight-title">SLAM</span> for
  Multi-<span class="highlight-title">Robot</span> <span class="highlight-title">Navigation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17249v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17249v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Liu, Jiuzhou Lei, Ankit Prabhu, Yuezhan Tao, Igor Spasojevic, Pratik Chaudhari, Nikolay Atanasov, Vijay Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper develops a real-time decentralized metric-semantic SLAM algorithm
that enables a heterogeneous robot team to collaboratively construct
object-based metric-semantic maps. The proposed framework integrates a
data-driven front-end for instance segmentation from either RGBD cameras or
LiDARs and a custom back-end for optimizing robot trajectories and object
landmarks in the map. To allow multiple robots to merge their information, we
design semantics-driven place recognition algorithms that leverage the
informativeness and viewpoint invariance of the object-level metric-semantic
map for inter-robot loop closure detection. A communication module is designed
to track each robot's observations and those of other robots whenever
communication links are available. The framework supports real-time,
decentralized operation onboard the robots and has been integrated with three
types of aerial and ground platforms. We validate its effectiveness through
experiments in both indoor and outdoor environments, as well as benchmarks on
public datasets and comparisons with existing methods. The framework is
open-sourced and suitable for both single-agent and multi-robot real-time
metric-semantic SLAM applications. The code is available at:
https://github.com/KumarRobotics/SLIDE_SLAM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Xu Liu, Jiuzhou Lei, and Ankit Prabhu contributed equally to this
  work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Resource-Efficient Affordance Grounding with Complementary Depth and
  Semantic Prompts <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02600v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02600v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhou Huang, Fan Yang, Guoliang Zhu, Gen Li, Hao Shi, Yukun Zuo, Wenrui Chen, Zhiyong Li, Kailun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Affordance refers to the functional properties that an agent perceives and
utilizes from its environment, and is key perceptual information required for
robots to perform actions. This information is rich and multimodal in nature.
Existing multimodal affordance methods face limitations in extracting useful
information, mainly due to simple structural designs, basic fusion methods, and
large model parameters, making it difficult to meet the performance
requirements for practical deployment. To address these issues, this paper
proposes the BiT-Align image-depth-text affordance mapping framework. The
framework includes a Bypass Prompt Module (BPM) and a Text Feature Guidance
(TFG) attention selection mechanism. BPM integrates the auxiliary modality
depth image directly as a prompt to the primary modality RGB image, embedding
it into the primary modality encoder without introducing additional encoders.
This reduces the model's parameter count and effectively improves functional
region localization accuracy. The TFG mechanism guides the selection and
enhancement of attention heads in the image encoder using textual features,
improving the understanding of affordance characteristics. Experimental results
demonstrate that the proposed method achieves significant performance
improvements on public AGD20K and HICO-IIF datasets. On the AGD20K dataset,
compared with the current state-of-the-art method, we achieve a 6.0%
improvement in the KLD metric, while reducing model parameters by 88.8%,
demonstrating practical application values. The source code will be made
publicly available at https://github.com/DAWDSE/BiT-Align.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IROS 2025. The source code will be made publicly
  available at https://github.com/DAWDSE/BiT-Align</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Granularity-Aware Affordances from Human-Object Interaction for
  Tool-Based Functional Dexterous <span class="highlight-title">Grasp</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00614v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00614v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Yang, Wenrui Chen, Kailun Yang, Haoran Lin, Dongsheng Luo, Conghui Tang, Zhiyong Li, Yaonan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To enable robots to use tools, the initial step is teaching robots to employ
dexterous gestures for touching specific areas precisely where tasks are
performed. Affordance features of objects serve as a bridge in the functional
interaction between agents and objects. However, leveraging these affordance
cues to help robots achieve functional tool grasping remains unresolved. To
address this, we propose a granularity-aware affordance feature extraction
method for locating functional affordance areas and predicting dexterous coarse
gestures. We study the intrinsic mechanisms of human tool use. On one hand, we
use fine-grained affordance features of object-functional finger contact areas
to locate functional affordance regions. On the other hand, we use highly
activated coarse-grained affordance features in hand-object interaction regions
to predict grasp gestures. Additionally, we introduce a model-based
post-processing module that transforms affordance localization and gesture
prediction into executable robotic actions. This forms GAAF-Dex, a complete
framework that learns Granularity-Aware Affordances from human-object
interaction to enable tool-based functional grasping with dexterous hands.
Unlike fully-supervised methods that require extensive data annotation, we
employ a weakly supervised approach to extract relevant cues from exocentric
(Exo) images of hand-object interactions to supervise feature extraction in
egocentric (Ego) images. To support this approach, we have constructed a
small-scale dataset, Functional Affordance Hand-object Interaction Dataset
(FAH), which includes nearly 6K images of functional hand-object interaction
Exo images and Ego images. Extensive experiments on the dataset demonstrate
that our method outperforms state-of-the-art methods. The source code and the
established dataset are available at https://github.com/yangfan293/GAAF-DEX.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Neural Networks and Learning Systems
  (TNNLS). The source code and the established dataset are available at
  https://github.com/yangfan293/GAAF-DEX</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EgoEvGesture: Gesture Recognition Based on Egocentric Event Camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12419v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12419v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luming Wang, Hao Shi, Xiaoting Yin, Kailun Yang, Kaiwei Wang, Jian Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Egocentric gesture recognition is a pivotal technology for enhancing natural
human-computer interaction, yet traditional RGB-based solutions suffer from
motion blur and illumination variations in dynamic scenarios. While event
cameras show distinct advantages in handling high dynamic range with ultra-low
power consumption, existing RGB-based architectures face inherent limitations
in processing asynchronous event streams due to their synchronous frame-based
nature. Moreover, from an egocentric perspective, event cameras record data
that includes events generated by both head movements and hand gestures,
thereby increasing the complexity of gesture recognition. To address this, we
propose a novel network architecture specifically designed for event data
processing, incorporating (1) a lightweight CNN with asymmetric depthwise
convolutions to reduce parameters while preserving spatiotemporal features, (2)
a plug-and-play state-space model as context block that decouples head movement
noise from gesture dynamics, and (3) a parameter-free Bins-Temporal Shift
Module (BTSM) that shifts features along bins and temporal dimensions to fuse
sparse events efficiently. We further establish the EgoEvGesture dataset, the
first large-scale dataset for egocentric gesture recognition using event
cameras. Experimental results demonstrate that our method achieves 62.7%
accuracy tested on unseen subjects with only 7M parameters, 3.1% higher than
state-of-the-art approaches. Notable misclassifications in freestyle motions
stem from high inter-personal variability and unseen test patterns differing
from training data. Moreover, our approach achieved a remarkable accuracy of
97.0% on the DVS128 Gesture, demonstrating the effectiveness and generalization
capability of our method on public datasets. The dataset and models are made
available at https://github.com/3190105222/EgoEv_Gesture.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to SMC 2025. The dataset and models are made available at
  https://github.com/3190105222/EgoEv_Gesture</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WMNav: Integrating Vision-Language Models into World Models for Object
  Goal <span class="highlight-title">Navigation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02247v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02247v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dujun Nie, Xianda Guo, Yiqun Duan, Ruijun Zhang, Long Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object Goal Navigation-requiring an agent to locate a specific object in an
unseen environment-remains a core challenge in embodied AI. Although recent
progress in Vision-Language Model (VLM)-based agents has demonstrated promising
perception and decision-making abilities through prompting, none has yet
established a fully modular world model design that reduces risky and costly
interactions with the environment by predicting the future state of the world.
We introduce WMNav, a novel World Model-based Navigation framework powered by
Vision-Language Models (VLMs). It predicts possible outcomes of decisions and
builds memories to provide feedback to the policy module. To retain the
predicted state of the environment, WMNav proposes the online maintained
Curiosity Value Map as part of the world model memory to provide dynamic
configuration for navigation policy. By decomposing according to a human-like
thinking process, WMNav effectively alleviates the impact of model
hallucination by making decisions based on the feedback difference between the
world model plan and observation. To further boost efficiency, we implement a
two-stage action proposer strategy: broad exploration followed by precise
localization. Extensive evaluation on HM3D and MP3D validates WMNav surpasses
existing zero-shot benchmarks in both success rate and exploration efficiency
(absolute improvement: +3.2% SR and +3.2% SPL on HM3D, +13.5% SR and +1.1% SPL
on MP3D). Project page: https://b0b8k1ng.github.io/WMNav/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intelligent <span class="highlight-title">LiDAR</span> <span class="highlight-title">Navigation</span>: Leveraging External Information and
  Semantic Maps with LLM as Copilot <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08493v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08493v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fujing Xie, Jiajie Zhang, Sören Schwertfeger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional robot navigation systems primarily utilize occupancy grid maps
and laser-based sensing technologies, as demonstrated by the popular move_base
package in ROS. Unlike robots, humans navigate not only through spatial
awareness and physical distances but also by integrating external information,
such as elevator maintenance updates from public notification boards and
experiential knowledge, like the need for special access through certain doors.
With the development of Large Language Models (LLMs), which possesses text
understanding and intelligence close to human performance, there is now an
opportunity to infuse robot navigation systems with a level of understanding
akin to human cognition. In this study, we propose using osmAG (Area Graph in
OpensStreetMap textual format), an innovative semantic topometric hierarchical
map representation, to bridge the gap between the capabilities of ROS move_base
and the contextual understanding offered by LLMs. Our methodology employs LLMs
as an actual copilot in robot navigation, enabling the integration of a broader
range of informational inputs while maintaining the robustness of traditional
robotic navigation systems. Our code, demo, map, experiment results can be
accessed at
https://github.com/xiexiexiaoxiexie/Intelligent-LiDAR-Navigation-LLM-as-Copilot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IROS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Grasp</span>VLA: a <span class="highlight-title">Grasp</span>ing Foundation Model Pre-trained on Billion-scale
  Synthetic Action Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.03233v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.03233v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengliang Deng, Mi Yan, Songlin Wei, Haixin Ma, Yuxin Yang, Jiayi Chen, Zhiqi Zhang, Taoyu Yang, Xuheng Zhang, Heming Cui, Zhizheng Zhang, He Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embodied foundation models are gaining increasing attention for their
zero-shot generalization, scalability, and adaptability to new tasks through
few-shot post-training. However, existing models rely heavily on real-world
data, which is costly and labor-intensive to collect. Synthetic data offers a
cost-effective alternative, yet its potential remains largely underexplored. To
bridge this gap, we explore the feasibility of training Vision-Language-Action
models entirely with large-scale synthetic action data. We curate SynGrasp-1B,
a billion-frame robotic grasping dataset generated in simulation with
photorealistic rendering and extensive domain randomization. Building on this,
we present GraspVLA, a VLA model pretrained on large-scale synthetic action
data as a foundational model for grasping tasks. GraspVLA integrates
autoregressive perception tasks and flow-matching-based action generation into
a unified Chain-of-Thought process, enabling joint training on synthetic action
data and Internet semantics data. This design helps mitigate sim-to-real gaps
and facilitates the transfer of learned actions to a broader range of
Internet-covered objects, achieving open-vocabulary generalization in grasping.
Extensive evaluations across real-world and simulation benchmarks demonstrate
GraspVLA's advanced zero-shot generalizability and few-shot adaptability to
specific human preferences. We will release SynGrasp-1B dataset and pre-trained
weights to benefit the community.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-07-18T00:00:00Z">2025-07-18</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">45</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalized Socially Assistive <span class="highlight-title">Robot</span>s With End-to-End Speech-Language
  Models For Well-Being Support 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengxue Fu, Zhonghao Shi, Minyu Huang, Siqi Liu, Mina Kian, Yirui Song, Maja J. Matarić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Socially assistive robots (SARs) have shown great potential for supplementing
well-being support. However, prior studies have found that existing dialogue
pipelines for SARs remain limited in real-time latency, back-channeling, and
personalized speech dialogue. Toward addressing these limitations, we propose
using integrated end-to-end speech-language models (SLMs) with SARs. This work
1) evaluated the usability of an SLM-enabled SAR dialogue system through a
small user study, and 2) identified remaining limitations through study user
feedback to inform future improvements. We conducted a small within-participant
user study with university students (N = 11) whose results showed that
participants perceived an SLM-enabled SAR system as capable of providing
empathetic feedback, natural turn-taking, back-channeling, and adaptive
responses. We also found that participants reported the robot's nonverbal
behaviors as lacking variability and synchronization with conversation, and the
SLM's verbal feedback as generic and repetitive. These findings highlighted the
need for real-time robot movement synchronized with conversation, improved
prompting or fine-tuning to generate outputs better aligned with mental health
practices, and more expressive, adaptive vocal generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Summarizing Normative Driving Behavior From Large-Scale NDS <span class="highlight-title">Dataset</span>s for
  Vehicle System Development <span class="chip">SC 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gregory Beale, Gibran Ali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a methodology to process large-scale naturalistic driving
studies (NDS) to describe the driving behavior for five vehicle metrics,
including speed, speeding, lane keeping, following distance, and headway,
contextualized by roadway characteristics, vehicle classes, and driver
demographics. Such descriptions of normative driving behaviors can aid in the
development of vehicle safety and intelligent transportation systems. The
methodology is demonstrated using data from the Second Strategic Highway
Research Program (SHRP 2) NDS, which includes over 34 million miles of driving
across more than 3,400 drivers. Summaries of each driving metric were generated
using vehicle, GPS, and forward radar data. Additionally, interactive online
analytics tools were developed to visualize and compare driving behavior across
groups through dynamic data selection and grouping. For example, among drivers
on 65-mph roads for the SHRP 2 NDS, females aged 16-19 exceeded the speed limit
by 7.5 to 15 mph slightly more often than their male counterparts, and younger
drivers maintained headways under 1.5 seconds more frequently than older
drivers. This work supports better vehicle systems and safer infrastructure by
quantifying normative driving behaviors and offers a methodology for analyzing
NDS datasets for cross group comparisons.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 2025 IEEE International Conference on Intelligent
  Transportation Systems (ITSC 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Recursive Lie-Group Formulation for the Second-Order Time Derivatives
  of the Inverse <span class="highlight-title">Dynamic</span>s of parallel Kinematic Manipulators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Mueller, Shivesh Kumar, Thomas Kordik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Series elastic actuators (SEA) were introduced for serial robotic arms. Their
model-based trajectory tracking control requires the second time derivatives of
the inverse dynamics solution, for which algorithms were proposed. Trajectory
control of parallel kinematics manipulators (PKM) equipped with SEAs has not
yet been pursued. Key element for this is the computationally efficient
evaluation of the second time derivative of the inverse dynamics solution. This
has not been presented in the literature, and is addressed in the present paper
for the first time. The special topology of PKM is exploited reusing the
recursive algorithms for evaluating the inverse dynamics of serial robots. A
Lie group formulation is used and all relations are derived within this
framework. Numerical results are presented for a 6-DOF Gough-Stewart platform
(as part of an exoskeleton), and for a planar PKM when a flatness-based control
scheme is applied.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-Aware Behavior Learning with Heuristic Motion Memory for
  Underwater Manipulation <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus Buchholz, Ignacio Carlucho, Michele Grimaldi, Maria Koskinopoulou, Yvan R. Petillot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous motion planning is critical for efficient and safe underwater
manipulation in dynamic marine environments. Current motion planning methods
often fail to effectively utilize prior motion experiences and adapt to
real-time uncertainties inherent in underwater settings. In this paper, we
introduce an Adaptive Heuristic Motion Planner framework that integrates a
Heuristic Motion Space (HMS) with Bayesian Networks to enhance motion planning
for autonomous underwater manipulation. Our approach employs the Probabilistic
Roadmap (PRM) algorithm within HMS to optimize paths by minimizing a composite
cost function that accounts for distance, uncertainty, energy consumption, and
execution time. By leveraging HMS, our framework significantly reduces the
search space, thereby boosting computational performance and enabling real-time
planning capabilities. Bayesian Networks are utilized to dynamically update
uncertainty estimates based on real-time sensor data and environmental
conditions, thereby refining the joint probability of path success. Through
extensive simulations and real-world test scenarios, we showcase the advantages
of our method in terms of enhanced performance and robustness. This
probabilistic approach significantly advances the capability of autonomous
underwater robots, ensuring optimized motion planning in the face of dynamic
marine challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2025 IEEE International Conference on Intelligent Robots
  and Systems (IROS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MorphIt: Flexible Spherical Approximation of <span class="highlight-title">Robot</span> Morphology for
  Representation-driven Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14061v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14061v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nataliya Nechyporenko, Yutong Zhang, Sean Campbell, Alessandro Roncone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What if a robot could rethink its own morphological representation to better
meet the demands of diverse tasks? Most robotic systems today treat their
physical form as a fixed constraint rather than an adaptive resource, forcing
the same rigid geometric representation to serve applications with vastly
different computational and precision requirements. We introduce MorphIt, a
novel algorithm for approximating robot morphology using spherical primitives
that balances geometric accuracy with computational efficiency. Unlike existing
approaches that rely on either labor-intensive manual specification or
inflexible computational methods, MorphIt implements an automatic
gradient-based optimization framework with tunable parameters that provides
explicit control over the physical fidelity versus computational cost tradeoff.
Quantitative evaluations demonstrate that MorphIt outperforms baseline
approaches (Variational Sphere Set Approximation and Adaptive Medial-Axis
Approximation) across multiple metrics, achieving better mesh approximation
with fewer spheres and reduced computational overhead. Our experiments show
enhanced robot capabilities in collision detection accuracy, contact-rich
interaction simulation, and navigation through confined spaces. By dynamically
adapting geometric representations to task requirements, robots can now exploit
their physical embodiment as an active resource rather than an inflexible
parameter, opening new frontiers for manipulation in environments where
physical form must continuously balance precision with computational
tractability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Design of a Modular Mobile Inspection and Maintenance <span class="highlight-title">Robot</span> for an
  Orbital Servicing Hub 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyuan Wang, Mark A Post, Mathieu Deremetz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of autonomous robots in space is an essential part of the "New Space"
commercial ecosystem of assembly and re-use of space hardware components in
Earth orbit and beyond. The STARFAB project aims to create a ground
demonstration of an orbital automated warehouse as a hub for sustainable
commercial operations and servicing. A critical part of this fully-autonomous
robotic facility will be the capability to monitor, inspect, and assess the
condition of both the components stored in the warehouse, and the STARFAB
facility itself. This paper introduces ongoing work on the STARFAB Mobile
Inspection Module (MIM). The MIM uses Standard Interconnects (SI) so that it
can be carried by Walking Manipulators (WM) as an independently-mobile robot,
and multiple MIMs can be stored and retrieved as needed for operations on
STARFAB. The MIM carries high-resolution cameras, a 3D profilometer, and a
thermal imaging sensor, with the capability to add other modular sensors. A
grasping tool and torque wrench are stored within the modular body for use by
an attached WM for maintenance operations. Implementation and testing is still
ongoing at the time of writing. This paper details the concept of operations
for the MIM as an on-orbit autonomous inspection and maintenance system, the
mechanical and electronic design of the MIM, and the sensors package used for
non-destructive testing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In proceedings of the Towards Autonomous Robotic Systems 2025
  conference (TAROS 2025), York, UK 6 pages, one page of references, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EdgeVLA: Efficient Vision-Language-Action Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paweł Budzianowski, Wesley Maa, Matthew Freed, Jingxiang Mo, Winston Hsiao, Aaron Xie, Tomasz Młoduchowski, Viraj Tipnis, Benjamin Bolte
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) have emerged as a promising approach to address
the data scarcity challenge in robotics, enabling the development of
generalizable visuomotor control policies. While models like OpenVLA showcase
the potential of this paradigm, deploying large-scale VLMs on
resource-constrained mobile manipulation systems remains a significant hurdle.
This paper introduces Edge VLA (EVLA), a novel approach designed to
significantly enhance the inference speed of Vision-Language-Action (VLA)
models. EVLA maintains the representational power of these models while
enabling real-time performance on edge devices. We achieve this through two key
innovations: 1) Eliminating the autoregressive requirement for end-effector
position prediction, leading to a 7x speedup in inference, and 2) Leveraging
the efficiency of Small Language Models (SLMs), demonstrating comparable
training performance to larger models with significantly reduced computational
demands. Our early results demonstrate that EVLA achieves comparable training
characteristics to OpenVLA while offering substantial gains in inference speed
and memory efficiency. We release our model checkpoints and training
\href{https://github.com/kscalelabs/evla }{codebase} to foster further
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A multi-strategy improved snake optimizer for three-dimensional UAV path
  <span class="highlight-title">planning</span> and engineering problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Genliang Li, Yaxin Cui, Jinyu Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metaheuristic algorithms have gained widespread application across various
fields owing to their ability to generate diverse solutions. One such algorithm
is the Snake Optimizer (SO), a progressive optimization approach. However, SO
suffers from the issues of slow convergence speed and susceptibility to local
optima. In light of these shortcomings, we propose a novel Multi-strategy
Improved Snake Optimizer (MISO). Firstly, we propose a new adaptive random
disturbance strategy based on sine function to alleviate the risk of getting
trapped in a local optimum. Secondly, we introduce adaptive Levy flight
strategy based on scale factor and leader and endow the male snake leader with
flight capability, which makes it easier for the algorithm to leap out of the
local optimum and find the global optimum. More importantly, we put forward a
position update strategy combining elite leadership and Brownian motion,
effectively accelerating the convergence speed while ensuring precision.
Finally, to demonstrate the performance of MISO, we utilize 30 CEC2017 test
functions and the CEC2022 test suite, comparing it with 11 popular algorithms
across different dimensions to validate its effectiveness. Moreover, Unmanned
Aerial Vehicle (UAV) has been widely used in various fields due to its
advantages of low cost, high mobility and easy operation. However, the UAV path
planning problem is crucial for flight safety and efficiency, and there are
still challenges in establishing and optimizing the path model. Therefore, we
apply MISO to the UAV 3D path planning problem as well as 6 engineering design
problems to assess its feasibility in practical applications. The experimental
results demonstrate that MISO exceeds other competitive algorithms in terms of
solution quality and stability, establishing its strong potential for
application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>59 pages, 22 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conceptual and Design Principles for a Self-Referential Algorithm
  Mimicking Neuronal Assembly Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14011v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14011v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paolo Totaro, Alberto Mangiante
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article proposes a method to formalise models of cognitive processes
grounded in experience, considering experience from the perspective of a living
system and not from that of an observer of the living system. The perspective
of a living system is defined by the need of the system to preserve the vital
equilibria. The method is based on an algorithmic schema that we call
Environment Generative Operator (EGO) and uses a self-referential language
developed for this purpose which we call E-language. EGO simulates cognitive
processes as operations on neuron assemblies as understood by Hebb. In this
article we present an EGO prototype (EGO-P) which has already been implemented
and tested.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A segmented <span class="highlight-title">robot</span> <span class="highlight-title">grasp</span>ing perception neural network for edge AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13970v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13970v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Casper Bröcheler, Thomas Vroom, Derrick Timmermans, Alan van den Akker, Guangzhi Tang, Charalampos S. Kouzinopoulos, Rico Möckel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic grasping, the ability of robots to reliably secure and manipulate
objects of varying shapes, sizes and orientations, is a complex task that
requires precise perception and control. Deep neural networks have shown
remarkable success in grasp synthesis by learning rich and abstract
representations of objects. When deployed at the edge, these models can enable
low-latency, low-power inference, making real-time grasping feasible in
resource-constrained environments. This work implements Heatmap-Guided Grasp
Detection, an end-to-end framework for the detection of 6-Dof grasp poses, on
the GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware
techniques, including input dimensionality reduction, model partitioning, and
quantisation. Experimental evaluation on the GraspNet-1Billion benchmark
validates the feasibility of fully on-chip inference, highlighting the
potential of low-power MCUs for real-time, autonomous manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SMC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Minimalist <span class="highlight-title">Control</span>ler for Autonomously Self-Aggregating <span class="highlight-title">Robot</span>ic
  <span class="highlight-title">Swarm</span>s: Enabling Compact Formations in Multitasking Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Eduarda Silva de Macedo, Ana Paula Chiarelli de Souza, Roberto Silvio Ubertino Rosso Jr., Yuri Kaszubowski Lopes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of simple emergent behaviors in swarm robotics has been
well-rehearsed in the literature. A recent study has shown how self-aggregation
is possible in a multitask approach -- where multiple self-aggregation task
instances occur concurrently in the same environment. The multitask approach
poses new challenges, in special, how the dynamic of each group impacts the
performance of others. So far, the multitask self-aggregation of groups of
robots suffers from generating a circular formation -- that is not fully
compact -- or is not fully autonomous. In this paper, we present a multitask
self-aggregation where groups of homogeneous robots sort themselves into
different compact clusters, relying solely on a line-of-sight sensor. Our
multitask self-aggregation behavior was able to scale well and achieve a
compact formation. We report scalability results from a series of simulation
trials with different configurations in the number of groups and the number of
robots per group. We were able to improve the multitask self-aggregation
behavior performance in terms of the compactness of the clusters, keeping the
proportion of clustered robots found in other studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages total (6 pages of content + 1 page of references). Short
  paper manuscript submitted to TAROS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeHMO: Neural Hamilton-Jacobi Reachability Learning for Decentralized
  Safe Multi-Agent Motion <span class="highlight-title">Planning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyi Chen, Ahmed H. Qureshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe Multi-Agent Motion Planning (MAMP) is a significant challenge in
robotics. Despite substantial advancements, existing methods often face a
dilemma. Decentralized algorithms typically rely on predicting the behavior of
other agents, sharing contracts, or maintaining communication for safety, while
centralized approaches struggle with scalability and real-time decision-making.
To address these challenges, we introduce Neural Hamilton-Jacobi Reachability
Learning (HJR) for Decentralized Multi-Agent Motion Planning. Our method
provides scalable neural HJR modeling to tackle high-dimensional configuration
spaces and capture worst-case collision and safety constraints between agents.
We further propose a decentralized trajectory optimization framework that
incorporates the learned HJR solutions to solve MAMP tasks in real-time. We
demonstrate that our method is both scalable and data-efficient, enabling the
solution of MAMP problems in higher-dimensional scenarios with complex
collision constraints. Our approach generalizes across various dynamical
systems, including a 12-dimensional dual-arm setup, and outperforms a range of
state-of-the-art techniques in successfully addressing challenging MAMP tasks.
Video demonstrations are available at https://youtu.be/IZiePX0p1Mc.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AeroThrow: An Autonomous Aerial Throwing System for Precise Payload
  Delivery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziliang Li, Hongming Chen, Yiyang Lin, Biyu Ye, Ximin Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous aerial systems play an increasingly vital role in a wide range of
applications, particularly for transport and delivery tasks in complex
environments. In airdrop missions, these platforms face the dual challenges of
abrupt control mode switching and inherent system delays along with control
errors. To address these issues, this paper presents an autonomous airdrop
system based on an aerial manipulator (AM). The introduction of additional
actuated degrees of freedom enables active compensation for UAV tracking
errors. By imposing smooth and continuous constraints on the parabolic landing
point, the proposed approach generates aerial throwing trajectories that are
less sensitive to the timing of payload release. A hierarchical disturbance
compensation strategy is incorporated into the Nonlinear Model Predictive
Control (NMPC) framework to mitigate the effects of sudden changes in system
parameters, while the predictive capabilities of NMPC are further exploited to
improve the precision of aerial throwing. Both simulation and real-world
experimental results demonstrate that the proposed system achieves greater
agility and precision in airdrop missions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fixed time convergence guarantees for Higher Order <span class="highlight-title">Control</span> Barrier
  Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Janani S K, Shishir Kolathaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel method for designing higher-order Control Barrier
Functions (CBFs) that guarantee convergence to a safe set within a
user-specified finite. Traditional Higher Order CBFs (HOCBFs) ensure asymptotic
safety but lack mechanisms for fixed-time convergence, which is critical in
time-sensitive and safety-critical applications such as autonomous navigation.
In contrast, our approach imposes a structured differential constraint using
repeated roots in the characteristic polynomial, enabling closed-form
polynomial solutions with exact convergence at a prescribed time. We derive
conditions on the barrier function and its derivatives that ensure forward
invariance and fixed-time reachability, and we provide an explicit formulation
for second-order systems. Our method is evaluated on three robotic systems - a
point-mass model, a unicycle, and a bicycle model and benchmarked against
existing HOCBF approaches. Results demonstrate that our formulation reliably
enforces convergence within the desired time, even when traditional methods
fail. This work provides a tractable and robust framework for real-time control
with provable finite-time safety guarantees.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 PAGES, 2 FIGURES</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe and Performant <span class="highlight-title">Control</span>ler Synthesis using Gradient-based Model
  Predictive <span class="highlight-title">Control</span> and <span class="highlight-title">Control</span> Barrier Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Singh, Aastha Mishra, Manan Tayal, Shishir Kolathaya, Pushpak Jagtap
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring both performance and safety is critical for autonomous systems
operating in real-world environments. While safety filters such as Control
Barrier Functions (CBFs) enforce constraints by modifying nominal controllers
in real time, they can become overly conservative when the nominal policy lacks
safety awareness. Conversely, solving State-Constrained Optimal Control
Problems (SC-OCPs) via dynamic programming offers formal guarantees but is
intractable in high-dimensional systems. In this work, we propose a novel
two-stage framework that combines gradient-based Model Predictive Control (MPC)
with CBF-based safety filtering for co-optimizing safety and performance. In
the first stage, we relax safety constraints as penalties in the cost function,
enabling fast optimization via gradient-based methods. This step improves
scalability and avoids feasibility issues associated with hard constraints. In
the second stage, we modify the resulting controller using a CBF-based
Quadratic Program (CBF-QP), which enforces hard safety constraints with minimal
deviation from the reference. Our approach yields controllers that are both
performant and provably safe. We validate the proposed framework on two case
studies, showcasing its ability to synthesize scalable, safe, and
high-performance controllers for complex, high-dimensional autonomous systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 Pages, 2 Figures. The first two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safety Certification in the Latent space using <span class="highlight-title">Control</span> Barrier Functions
  and World Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13871v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13871v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehul Anand, Shishir Kolathaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthesising safe controllers from visual data typically requires extensive
supervised labelling of safety-critical data, which is often impractical in
real-world settings. Recent advances in world models enable reliable prediction
in latent spaces, opening new avenues for scalable and data-efficient safe
control. In this work, we introduce a semi-supervised framework that leverages
control barrier certificates (CBCs) learned in the latent space of a world
model to synthesise safe visuomotor policies. Our approach jointly learns a
neural barrier function and a safe controller using limited labelled data,
while exploiting the predictive power of modern vision transformers for latent
dynamics modelling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures. arXiv admin note: text overlap with
  arXiv:2409.12616</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Depth3DLane: Fusing Monocular 3D Lane <span class="highlight-title">Detection</span> with <span class="highlight-title">Self-Supervised</span>
  Monocular Depth <span class="highlight-title">Estimation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13857v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13857v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max van den Hoven, Kishaan Jeeveswaran, Pieter Piscaer, Thijs Wensveen, Elahe Arani, Bahram Zonooz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular 3D lane detection is essential for autonomous driving, but
challenging due to the inherent lack of explicit spatial information.
Multi-modal approaches rely on expensive depth sensors, while methods
incorporating fully-supervised depth networks rely on ground-truth depth data
that is impractical to collect at scale. Additionally, existing methods assume
that camera parameters are available, limiting their applicability in scenarios
like crowdsourced high-definition (HD) lane mapping. To address these
limitations, we propose Depth3DLane, a novel dual-pathway framework that
integrates self-supervised monocular depth estimation to provide explicit
structural information, without the need for expensive sensors or additional
ground-truth depth data. Leveraging a self-supervised depth network to obtain a
point cloud representation of the scene, our bird's-eye view pathway extracts
explicit spatial information, while our front view pathway simultaneously
extracts rich semantic information. Depth3DLane then uses 3D lane anchors to
sample features from both pathways and infer accurate 3D lane geometry.
Furthermore, we extend the framework to predict camera parameters on a
per-frame basis and introduce a theoretically motivated fitting procedure to
enhance stability on a per-segment basis. Extensive experiments demonstrate
that Depth3DLane achieves competitive performance on the OpenLane benchmark
dataset. Furthermore, experimental results show that using learned parameters
instead of ground-truth parameters allows Depth3DLane to be applied in
scenarios where camera calibration is infeasible, unlike previous methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Design Analysis of an Innovative Parallel <span class="highlight-title">Robot</span> for Minimally Invasive
  Pancreatic Surgery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13787v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13787v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Doina Pisla, Alexandru Pusca, Andrei Caprariu, Adrian Pisla, Bogdan Gherman, Calin Vaida, Damien Chablat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on the design of a parallel robot designed for robotic
assisted minimally invasive pancreatic surgery. Two alternative architectures,
called ATHENA-1 and ATHENA-2, each with 4 degrees of freedom (DOF) are
proposed. Their kinematic schemes are presented, and the conceptual 3D CAD
models are illustrated. Based on these, two Finite Element Method (FEM)
simulations were performed to determine which architecture has the higher
stiffness. A workspace quantitative analysis is performed to further assess the
usability of the two proposed parallel architectures related to the medical
tasks. The obtained results are used to select the architecture which fit the
required design criteria and will be used to develop the experimental model of
the surgical robot.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AGENTS-LLM: Augmentative GENeration of Challenging Traffic Scenarios
  with an Agentic LLM Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Yao, Salil Bhatnagar, Markus Mazzola, Vasileios Belagiannis, Igor Gilitschenski, Luigi Palmieri, Simon Razniewski, Marcel Hallgarten
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rare, yet critical, scenarios pose a significant challenge in testing and
evaluating autonomous driving planners. Relying solely on real-world driving
scenes requires collecting massive datasets to capture these scenarios. While
automatic generation of traffic scenarios appears promising, data-driven models
require extensive training data and often lack fine-grained control over the
output. Moreover, generating novel scenarios from scratch can introduce a
distributional shift from the original training scenes which undermines the
validity of evaluations especially for learning-based planners. To sidestep
this, recent work proposes to generate challenging scenarios by augmenting
original scenarios from the test set. However, this involves the manual
augmentation of scenarios by domain experts. An approach that is unable to meet
the demands for scale in the evaluation of self-driving systems. Therefore,
this paper introduces a novel LLM-agent based framework for augmenting
real-world traffic scenarios using natural language descriptions, addressing
the limitations of existing methods. A key innovation is the use of an agentic
design, enabling fine-grained control over the output and maintaining high
performance even with smaller, cost-effective LLMs. Extensive human expert
evaluation demonstrates our framework's ability to accurately adhere to user
intent, generating high quality augmented scenarios comparable to those created
manually.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SaWa-ML: Structure-Aware Pose Correction and Weight Adaptation-Based
  <span class="highlight-title">Robust</span> Multi-<span class="highlight-title">Robot</span> <span class="highlight-title">Localization</span> <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junho Choi, Kihwan Ryoo, Jeewon Kim, Taeyun Kim, Eungchang Lee, Myeongwoo Jeong, Kevin Christiansen Marsim, Hyungtae Lim, Hyun Myung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-robot localization is a crucial task for implementing multi-robot
systems. Numerous researchers have proposed optimization-based multi-robot
localization methods that use camera, IMU, and UWB sensors. Nevertheless,
characteristics of individual robot odometry estimates and distance
measurements between robots used in the optimization are not sufficiently
considered. In addition, previous researches were heavily influenced by the
odometry accuracy that is estimated from individual robots. Consequently,
long-term drift error caused by error accumulation is potentially inevitable.
In this paper, we propose a novel visual-inertial-range-based multi-robot
localization method, named SaWa-ML, which enables geometric structure-aware
pose correction and weight adaptation-based robust multi-robot localization.
Our contributions are twofold: (i) we leverage UWB sensor data, whose range
error does not accumulate over time, to first estimate the relative positions
between robots and then correct the positions of each robot, thus reducing
long-term drift errors, (ii) we design adaptive weights for robot pose
correction by considering the characteristics of the sensor data and
visual-inertial odometry estimates. The proposed method has been validated in
real-world experiments, showing a substantial performance increase compared
with state-of-the-art algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to the 2025 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-Time Communication-Aware Ride-Sharing Route <span class="highlight-title">Planning</span> for Urban Air
  Mobility: A Multi-Source Hybrid Attention <span class="highlight-title">Reinforcement</span> Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuejiao Xie, Maonan Wang, Di Zhou, Man-On Pun, Zhu Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Urban Air Mobility (UAM) systems are rapidly emerging as promising solutions
to alleviate urban congestion, with path planning becoming a key focus area.
Unlike ground transportation, UAM trajectory planning has to prioritize
communication quality for accurate location tracking in constantly changing
environments to ensure safety. Meanwhile, a UAM system, serving as an air taxi,
requires adaptive planning to respond to real-time passenger requests,
especially in ride-sharing scenarios where passenger demands are unpredictable
and dynamic. However, conventional trajectory planning strategies based on
predefined routes lack the flexibility to meet varied passenger ride demands.
To address these challenges, this work first proposes constructing a radio map
to evaluate the communication quality of urban airspace. Building on this, we
introduce a novel Multi-Source Hybrid Attention Reinforcement Learning
(MSHA-RL) framework for the challenge of effectively focusing on passengers and
UAM locations, which arises from the significant dimensional disparity between
the representations. This model first generates the alignment among diverse
data sources with large gap dimensions before employing hybrid attention to
balance global and local insights, thereby facilitating responsive, real-time
path planning. Extensive experimental results demonstrate that the approach
enables communication-compliant trajectory planning, reducing travel time and
enhancing operational efficiency while prioritizing passenger safety.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Iteratively Learning Muscle Memory for Legged <span class="highlight-title">Robot</span>s to Master Adaptive
  and High Precision Locomotion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Cheng, Yasser G. Alqaham, Zhenyu Gan, Amit K. Sanyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a scalable and adaptive control framework for legged
robots that integrates Iterative Learning Control (ILC) with a biologically
inspired torque library (TL), analogous to muscle memory. The proposed method
addresses key challenges in robotic locomotion, including accurate trajectory
tracking under unmodeled dynamics and external disturbances. By leveraging the
repetitive nature of periodic gaits and extending ILC to nonperiodic tasks, the
framework enhances accuracy and generalization across diverse locomotion
scenarios. The control architecture is data-enabled, combining a physics-based
model derived from hybrid-system trajectory optimization with real-time
learning to compensate for model uncertainties and external disturbances. A
central contribution is the development of a generalized TL that stores learned
control profiles and enables rapid adaptation to changes in speed, terrain, and
gravitational conditions-eliminating the need for repeated learning and
significantly reducing online computation. The approach is validated on the
bipedal robot Cassie and the quadrupedal robot A1 through extensive simulations
and hardware experiments. Results demonstrate that the proposed framework
reduces joint tracking errors by up to 85% within a few seconds and enables
reliable execution of both periodic and nonperiodic gaits, including slope
traversal and terrain adaptation. Compared to state-of-the-art whole-body
controllers, the learned skills eliminate the need for online computation
during execution and achieve control update rates exceeding 30x those of
existing methods. These findings highlight the effectiveness of integrating ILC
with torque memory as a highly data-efficient and practical solution for legged
locomotion in unstructured and dynamic environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Study of Teleoperation Methods in a Simulated Virtual Eye Surgery
  Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Wang, Yasamin Foroutani, Matthew Nepo, Mercedes Rodriguez, Ji Ma, Jean-Pierre Hubschman, Tsu-Chin Tsao, Jacob Rosen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper examines the performance of Inside and Outside Control modes at
various scaling factors in a simulated vitreoretinal surgical setting. The
IRISS teleoperated surgical system's console (cockpit) was adapted to project a
simulated microscope view of an intraocular setup to a virtual reality (VR)
headset. Five experienced vitreoretinal surgeons and five engineers with no
surgical experience used the system to perform tasks common to vitreoretinal
surgery. Experimental results indicate that Inside Control methods at higher
scaling factors (20 or 30) achieved the best performance overall, though the
optimal scaling factor may vary by task and complexity. Optimizing control
methods and scaling factors could lead to improvements in surgical efficiency
and accuracy, as well as minimize risks in future robotic-assisted intraocular
procedures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe <span class="highlight-title">Robot</span>ic Capsule Cleaning with Integrated Transpupillary and
  Intraocular Optical Coherence Tomo<span class="highlight-title">graph</span>y 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Ting Lai, Yasamin Foroutani, Aya Barzelay, Tsu-Chin Tsao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Secondary cataract is one of the most common complications of vision loss due
to the proliferation of residual lens materials that naturally grow on the lens
capsule after cataract surgery. A potential treatment is capsule cleaning, a
surgical procedure that requires enhanced visualization of the entire capsule
and tool manipulation on the thin membrane. This article presents a robotic
system capable of performing the capsule cleaning procedure by integrating a
standard transpupillary and an intraocular optical coherence tomography probe
on a surgical instrument for equatorial capsule visualization and real-time
tool-to-tissue distance feedback. Using robot precision, the developed system
enables complete capsule mapping in the pupillary and equatorial regions with
in-situ calibration of refractive index and fiber offset, which are still
current challenges in obtaining an accurate capsule model. To demonstrate
effectiveness, the capsule mapping strategy was validated through five
experimental trials on an eye phantom that showed reduced root-mean-square
errors in the constructed capsule model, while the cleaning strategy was
performed in three ex-vivo pig eyes without tissue damage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 27 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved particle <span class="highlight-title">swarm</span> <span class="highlight-title">optimization</span> algorithm: multi-target <span class="highlight-title">trajectory</span>
  <span class="highlight-title">optimization</span> for <span class="highlight-title">swarm</span> drones 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minze Li, Wei Zhao, Ran Chen, Mingqiang Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time trajectory planning for unmanned aerial vehicles (UAVs) in dynamic
environments remains a key challenge due to high computational demands and the
need for fast, adaptive responses. Traditional Particle Swarm Optimization
(PSO) methods, while effective for offline planning, often struggle with
premature convergence and latency in real-time scenarios. To overcome these
limitations, we propose PE-PSO, an enhanced PSO-based online trajectory
planner. The method introduces a persistent exploration mechanism to preserve
swarm diversity and an entropy-based parameter adjustment strategy to
dynamically adapt optimization behavior. UAV trajectories are modeled using
B-spline curves, which ensure path smoothness while reducing optimization
complexity. To extend this capability to UAV swarms, we develop a multi-agent
framework that combines genetic algorithm (GA)-based task allocation with
distributed PE-PSO, supporting scalable and coordinated trajectory generation.
The distributed architecture allows for parallel computation and decentralized
control, enabling effective cooperation among agents while maintaining
real-time performance. Comprehensive simulations demonstrate that the proposed
framework outperforms conventional PSO and other swarm-based planners across
several metrics, including trajectory quality, energy efficiency, obstacle
avoidance, and computation time. These results confirm the effectiveness and
applicability of PE-PSO in real-time multi-UAV operations under complex
environmental conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 papers,7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conformal Contraction for <span class="highlight-title">Robust</span> Nonlinear <span class="highlight-title">Control</span> with
  Distribution-Free Uncertainty Quantification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sihang Wei, Melkior Ornik, Hiroyasu Tsukamoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel robust control framework for continuous-time, perturbed
nonlinear dynamical systems with uncertainty that depends nonlinearly on both
the state and control inputs. Unlike conventional approaches that impose
structural assumptions on the uncertainty, our framework enhances
contraction-based robust control with data-driven uncertainty prediction,
remaining agnostic to the models of the uncertainty and predictor. We
statistically quantify how reliably the contraction conditions are satisfied
under dynamics with uncertainty via conformal prediction, thereby obtaining a
distribution-free and finite-time probabilistic guarantee for exponential
boundedness of the trajectory tracking error. We further propose the
probabilistically robust control invariant (PRCI) tube for distributionally
robust motion planning, within which the perturbed system trajectories are
guaranteed to stay with a finite probability, without explicit knowledge of the
uncertainty model. Numerical simulations validate the effectiveness of the
proposed robust control framework and the performance of the PRCI tube.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE CDC 2025 submission (accepted)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Low-Cost Teleoperation: Augmenting GELLO with Force 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivakanth Sujit, Luca Nunziante, Dan Ogawa Lillrank, Rousslan Fernand Julien Dossa, Kai Arulkumaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we extend the low-cost GELLO teleoperation system, initially
designed for joint position control, with additional force information. Our
first extension is to implement force feedback, allowing users to feel
resistance when interacting with the environment. Our second extension is to
add force information into the data collection process and training of
imitation learning models. We validate our additions by implementing these on a
GELLO system with a Franka Panda arm as the follower robot, performing a user
study, and comparing the performance of policies trained with and without force
information on a range of simulated and real dexterous manipulation tasks.
Qualitatively, users with robotics experience preferred our controller, and the
addition of force inputs improved task success on the majority of tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 2025 IEEE/SICE International Symposium on System
  Integration</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ J-PARSE: Jacobian-based Projection Algorithm for Resolving Singularities
  Effectively in Inverse Kinematic <span class="highlight-title">Control</span> of Serial Manipulators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.00306v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.00306v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivani Guptasarma, Matthew Strong, Honghao Zhen, Monroe Kennedy III
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  J-PARSE is a method for smooth first-order inverse kinematic control of a
serial manipulator near kinematic singularities. The commanded end-effector
velocity is interpreted component-wise, according to the available mobility in
each dimension of the task space. First, a substitute "Safety" Jacobian matrix
is created, keeping the aspect ratio of the manipulability ellipsoid above a
threshold value. The desired motion is then projected onto non-singular and
singular directions, and the latter projection scaled down by a factor informed
by the threshold value. A right-inverse of the non-singular Safety Jacobian is
applied to the modified command. In the absence of joint limits and collisions,
this ensures smooth transition into and out of low-rank poses, guaranteeing
asymptotic stability for target poses within the workspace, and stability for
those outside. Velocity control with J-PARSE is benchmarked against the
Least-Squares and Damped Least-Squares inversions of the Jacobian, and shows
high accuracy in reaching and leaving singular target poses. By expanding the
available workspace of manipulators, the method finds applications in servoing,
teleoperation, and learning. Videos and code are available at
https://jparse-manip.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 25 figures. v1: Fig. 1 replaced with faster-loading
  version. v2: Website at https://jparse-manip.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LSTP-Nav: Lightweight Spatiotemporal Policy for Map-free Multi-agent
  <span class="highlight-title">Navigation</span> with <span class="highlight-title">LiDAR</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16370v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16370v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingrong Diao, Zhirui Sun, Jianwei Peng, Jiankun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe and efficient multi-agent navigation in dynamic environments remains
inherently challenging, particularly when real-time decision-making is required
on resource-constrained platforms. Ensuring collision-free trajectories while
adapting to uncertainties without relying on pre-built maps further complicates
real-world deployment. To address these challenges, we propose LSTP-Nav, a
lightweight end-to-end policy for multi-agent navigation that enables map-free
collision avoidance in complex environments by directly mapping raw LiDAR point
clouds to motion commands. At the core of this framework lies LSTP-Net, an
efficient network that processes raw LiDAR data using a GRU architecture,
enhanced with attention mechanisms to dynamically focus on critical
environmental features while minimizing computational overhead. Additionally, a
novel HS reward optimizes collision avoidance by incorporating angular
velocity, prioritizing obstacles along the predicted heading, and enhancing
training stability. To narrow the sim-to-real gap, we develop
PhysReplay-Simlab, a physics-realistic multi-agent simulator, employs localized
replay to mine near-failure experiences. Relying solely on LiDA, LSTP-Nav
achieves efficient zero-shot sim-to-real transfer on a CPU-only robotic
platform, enabling robust navigation in dynamic environments while maintaining
computation frequencies above 40 Hz. Extensive experiments demonstrate that
LSTP-Nav outperforms baselines with a 9.58% higher success rate and a 12.30%
lower collision rate, underscoring its practicality and robustness for
real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Models and Connected and Automated Vehicles: A <span class="highlight-title">Survey</span> in
  Exploring the Intersection of Transportation and AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10559v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10559v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Shu, Yiting Zhang, Dong Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report investigates the history and impact of Generative Models and
Connected and Automated Vehicles (CAVs), two groundbreaking forces pushing
progress in technology and transportation. By focusing on the application of
generative models within the context of CAVs, the study aims to unravel how
this integration could enhance predictive modeling, simulation accuracy, and
decision-making processes in autonomous vehicles. This thesis discusses the
benefits and challenges of integrating generative models and CAV technology in
transportation. It aims to highlight the progress made, the remaining
obstacles, and the potential for advancements in safety and innovation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Critiques of World Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.05169v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.05169v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Xing, Mingkai Deng, Jinyu Hou, Zhiting Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  World Model, the supposed algorithmic surrogate of the real-world environment
which biological agents experience with and act upon, has been an emerging
topic in recent years because of the rising needs to develop virtual agents
with artificial (general) intelligence. There has been much debate on what a
world model really is, how to build it, how to use it, and how to evaluate it.
In this essay, starting from the imagination in the famed Sci-Fi classic Dune,
and drawing inspiration from the concept of "hypothetical thinking" in
psychology literature, we offer critiques of several schools of thoughts on
world modeling, and argue the primary goal of a world model to be simulating
all actionable possibilities of the real world for purposeful reasoning and
acting. Building on the critiques, we propose a new architecture for a
general-purpose world model, based on hierarchical, multi-level, and mixed
continuous/discrete representations, and a generative and self-supervision
learning framework, with an outlook of a Physical, Agentic, and Nested (PAN)
AGI system enabled by such a model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Duality of Generative AI and <span class="highlight-title">Reinforcement</span> Learning in <span class="highlight-title">Robot</span>ics: A
  <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16411v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16411v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angelo Moroncelli, Vishal Soni, Marco Forgione, Dario Piga, Blerina Spahiu, Loris Roveda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, generative AI and reinforcement learning (RL) have been redefining
what is possible for AI agents that take information flows as input and produce
intelligent behavior. As a result, we are seeing similar advancements in
embodied AI and robotics for control policy generation. Our review paper
examines the integration of generative AI models with RL to advance robotics.
Our primary focus is on the duality between generative AI and RL for robotics
downstream tasks. Specifically, we investigate: (1) The role of prominent
generative AI tools as modular priors for multi-modal input fusion in RL tasks.
(2) How RL can train, fine-tune and distill generative models for policy
generation, such as VLA models, similarly to RL applications in large language
models. We then propose a new taxonomy based on a considerable amount of
selected papers.
  Lastly, we identify open challenges accounting for model scalability,
adaptation and grounding, giving recommendations and insights on future
research directions. We reflect on which generative AI models best fit the RL
tasks and why. On the other side, we reflect on important issues inherent to
RL-enhanced generative policies, such as safety concerns and failure modes, and
what are the limitations of current methods. A curated collection of relevant
research papers is maintained on our GitHub repository, serving as a resource
for ongoing research and development in this field:
https://github.com/clmoro/Robotics-RL-FMs-Integration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for publication to Information Fusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chance-constrained Linear Quadratic Gaussian Games for Multi-<span class="highlight-title">robot</span>
  Interaction under Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.06776v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.06776v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Ren, Giulio Salizzoni, Mustafa Emre Gürsoy, Maryam Kamgarpour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address safe multi-robot interaction under uncertainty. In particular, we
formulate a chance-constrained linear quadratic Gaussian game with coupling
constraints and system uncertainties. We find a tractable reformulation of the
game and propose a dual ascent algorithm. We prove that the algorithm converges
to a feedback generalized Nash equilibrium of the reformulated game, ensuring
the satisfaction of the chance constraints. We test our method in driving
simulations and real-world robot experiments. Our method ensures safety under
uncertainty and generates less conservative trajectories than single-agent
model predictive control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in IEEE Control Systems Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Objective <span class="highlight-title">Reinforcement</span> Learning for Adaptable Personalized
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05223v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05223v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hendrik Surmann, Jorge de Heuvel, Maren Bennewitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human drivers exhibit individual preferences regarding driving style.
Adapting autonomous vehicles to these preferences is essential for user trust
and satisfaction. However, existing end-to-end driving approaches often rely on
predefined driving styles or require continuous user feedback for adaptation,
limiting their ability to support dynamic, context-dependent preferences. We
propose a novel approach using multi-objective reinforcement learning (MORL)
with preference-driven optimization for end-to-end autonomous driving that
enables runtime adaptation to driving style preferences. Preferences are
encoded as continuous weight vectors to modulate behavior along interpretable
style objectives$\unicode{x2013}$including efficiency, comfort, speed, and
aggressiveness$\unicode{x2013}$without requiring policy retraining. Our
single-policy agent integrates vision-based perception in complex mixed-traffic
scenarios and is evaluated in diverse urban environments using the CARLA
simulator. Experimental results demonstrate that the agent dynamically adapts
its driving behavior according to changing preferences while maintaining
performance in terms of collision avoidance and route completion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stonefish: Supporting Machine Learning Research in Marine <span class="highlight-title">Robot</span>ics <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11887v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11887v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michele Grimaldi, Patryk Cieslak, Eduardo Ochoa, Vibhav Bharti, Hayat Rajani, Ignacio Carlucho, Maria Koskinopoulou, Yvan R. Petillot, Nuno Gracias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulations are highly valuable in marine robotics, offering a cost-effective
and controlled environment for testing in the challenging conditions of
underwater and surface operations. Given the high costs and logistical
difficulties of real-world trials, simulators capable of capturing the
operational conditions of subsea environments have become key in developing and
refining algorithms for remotely-operated and autonomous underwater vehicles.
This paper highlights recent enhancements to the Stonefish simulator, an
advanced open-source platform supporting development and testing of marine
robotics solutions. Key updates include a suite of additional sensors, such as
an event-based camera, a thermal camera, and an optical flow camera, as well
as, visual light communication, support for tethered operations, improved
thruster modelling, more flexible hydrodynamics, and enhanced sonar accuracy.
These developments and an automated annotation tool significantly bolster
Stonefish's role in marine robotics research, especially in the field of
machine learning, where training data with a known ground truth is hard or
impossible to collect.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2025 IEEE/RSJ International Conference on Robotics and Automation
  (ICRA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffAD: A Unified Dif<span class="highlight-title">fusion</span> Modeling Approach for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12170v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12170v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Wang, Cong Zhang, Xingguang Qu, Kun Li, Weiwei Liu, Chang Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end autonomous driving (E2E-AD) has rapidly emerged as a promising
approach toward achieving full autonomy. However, existing E2E-AD systems
typically adopt a traditional multi-task framework, addressing perception,
prediction, and planning tasks through separate task-specific heads. Despite
being trained in a fully differentiable manner, they still encounter issues
with task coordination, and the system complexity remains high. In this work,
we introduce DiffAD, a novel diffusion probabilistic model that redefines
autonomous driving as a conditional image generation task. By rasterizing
heterogeneous targets onto a unified bird's-eye view (BEV) and modeling their
latent distribution, DiffAD unifies various driving objectives and jointly
optimizes all driving tasks in a single framework, significantly reducing
system complexity and harmonizing task coordination. The reverse process
iteratively refines the generated BEV image, resulting in more robust and
realistic driving behaviors. Closed-loop evaluations in Carla demonstrate the
superiority of the proposed method, achieving a new state-of-the-art Success
Rate and Driving Score.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures; Code released</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Horticultural Temporal Fruit Monitoring via 3D Instance <span class="highlight-title">Segmentation</span> and
  Re-Identification using Colored <span class="highlight-title">Point Cloud</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07799v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07799v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Fusaro, Federico Magistri, Jens Behley, Alberto Pretto, <span class="highlight-author">Cyrill Stachniss</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and consistent fruit monitoring over time is a key step toward
automated agricultural production systems. However, this task is inherently
difficult due to variations in fruit size, shape, occlusion, orientation, and
the dynamic nature of orchards where fruits may appear or disappear between
observations. In this article, we propose a novel method for fruit instance
segmentation and re-identification on 3D terrestrial point clouds collected
over time. Our approach directly operates on dense colored point clouds,
capturing fine-grained 3D spatial detail. We segment individual fruits using a
learning-based instance segmentation method applied directly to the point
cloud. For each segmented fruit, we extract a compact and discriminative
descriptor using a 3D sparse convolutional neural network. To track fruits
across different times, we introduce an attention-based matching network that
associates fruits with their counterparts from previous sessions. Matching is
performed using a probabilistic assignment scheme, selecting the most likely
associations across time. We evaluate our approach on real-world datasets of
strawberries and apples, demonstrating that it outperforms existing methods in
both instance segmentation and temporal re-identification, enabling robust and
precise fruit monitoring across complex and dynamic orchard environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Computers and Electronics in Agriculture</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GeoPF: Infusing Geometry into Potential <span class="highlight-title">Field</span>s for Reactive <span class="highlight-title">Planning</span> in
  Non-trivial Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.19688v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.19688v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhe Gong, Riddhiman Laha, Luis Figueredo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reactive intelligence remains one of the cornerstones of versatile robotics
operating in cluttered, dynamic, and human-centred environments. Among reactive
approaches, potential fields (PF) continue to be widely adopted due to their
simplicity and real-time applicability. However, existing PF methods typically
oversimplify environmental representations by relying on isotropic, point- or
sphere-based obstacle approximations. In human-centred settings, this
simplification results in overly conservative paths, cumbersome tuning, and
computational overhead -- even breaking real-time requirements. In response, we
propose the Geometric Potential Field (GeoPF), a reactive motion-planning
framework that explicitly infuses geometric primitives -- points, lines,
planes, cubes, and cylinders -- their structure and spatial relationship in
modulating the real-time repulsive response. Extensive quantitative analyses
consistently show GeoPF's higher success rates, reduced tuning complexity (a
single parameter set across experiments), and substantially lower computational
costs (up to 2 orders of magnitude) compared to traditional PF methods.
Real-world experiments further validate GeoPF reliability, robustness, and
practical ease of deployment, as well as its scalability to whole-body
avoidance. GeoPF provides a fresh perspective on reactive planning problems
driving geometric-aware temporal motion generation, enabling flexible and
low-latency motion planning suitable for modern robotic applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of Behavior Foundation Model: Next-Generation Whole-Body
  <span class="highlight-title">Control</span> System of Humanoid <span class="highlight-title">Robot</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20487v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20487v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingqi Yuan, Tao Yu, Wenqi Ge, Xiuyong Yao, Huijiang Wang, Jiayu Chen, Xin Jin, Bo Li, Hua Chen, Wei Zhang, Wenjun Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humanoid robots are drawing significant attention as versatile platforms for
complex motor control, human-robot interaction, and general-purpose physical
intelligence. However, achieving efficient whole-body control (WBC) in
humanoids remains a fundamental challenge due to sophisticated dynamics,
underactuation, and diverse task requirements. While learning-based controllers
have shown promise for complex tasks, their reliance on labor-intensive and
costly retraining for new scenarios limits real-world applicability. To address
these limitations, behavior(al) foundation models (BFMs) have emerged as a new
paradigm that leverages large-scale pre-training to learn reusable primitive
skills and broad behavioral priors, enabling zero-shot or rapid adaptation to a
wide range of downstream tasks. In this paper, we present a comprehensive
overview of BFMs for humanoid WBC, tracing their development across diverse
pre-training pipelines. Furthermore, we discuss real-world applications,
current limitations, urgent challenges, and future opportunities, positioning
BFMs as a key approach toward scalable and general-purpose humanoid
intelligence. Finally, we provide a curated and long-term list of BFM papers
and projects to facilitate more subsequent research, which is available at
https://github.com/yuanmingqi/awesome-bfm-papers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Robust</span>ness <span class="highlight-title">Evaluation</span> of Offline <span class="highlight-title">Reinforcement</span> Learning for <span class="highlight-title">Robot</span>
  <span class="highlight-title">Control</span> Against Action Perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18781v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18781v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shingo Ayabe, Takuto Otomo, Hiroshi Kera, Kazuhiko Kawamoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline reinforcement learning, which learns solely from datasets without
environmental interaction, has gained attention. This approach, similar to
traditional online deep reinforcement learning, is particularly promising for
robot control applications. Nevertheless, its robustness against real-world
challenges, such as joint actuator faults in robots, remains a critical
concern. This study evaluates the robustness of existing offline reinforcement
learning methods using legged robots from OpenAI Gym based on average episodic
rewards. For robustness evaluation, we simulate failures by incorporating both
random and adversarial perturbations, representing worst-case scenarios, into
the joint torque signals. Our experiments show that existing offline
reinforcement learning methods exhibit significant vulnerabilities to these
action perturbations and are more vulnerable than online reinforcement learning
methods, highlighting the need for more robust approaches in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Words to Collisions: LLM-Guided <span class="highlight-title">Evaluation</span> and Adversarial
  Generation of Safety-Critical Driving Scenarios <span class="chip">SC 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02145v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02145v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Gao, Mattia Piccinini, Korbinian Moller, Amr Alanwar, Johannes Betz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring the safety of autonomous vehicles requires virtual scenario-based
testing, which depends on the robust evaluation and generation of
safety-critical scenarios. So far, researchers have used scenario-based testing
frameworks that rely heavily on handcrafted scenarios as safety metrics. To
reduce the effort of human interpretation and overcome the limited scalability
of these approaches, we combine Large Language Models (LLMs) with structured
scenario parsing and prompt engineering to automatically evaluate and generate
safety-critical driving scenarios. We introduce Cartesian and Ego-centric
prompt strategies for scenario evaluation, and an adversarial generation module
that modifies trajectories of risk-inducing vehicles (ego-attackers) to create
critical scenarios. We validate our approach using a 2D simulation framework
and multiple pre-trained LLMs. The results show that the evaluation module
effectively detects collision scenarios and infers scenario safety. Meanwhile,
the new generation module identifies high-risk agents and synthesizes
realistic, safety-critical scenarios. We conclude that an LLM equipped with
domain-informed prompting techniques can effectively evaluate and generate
safety-critical driving scenarios, reducing dependence on handcrafted metrics.
We release our open-source code and scenarios at:
https://github.com/TUM-AVS/From-Words-to-Collisions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Final Version and Paper Accepted at IEEE ITSC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-Overlap-Aware Egocentric Pose <span class="highlight-title">Estimation</span> for Collaborative
  Perception in Connected Autonomy <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14180v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14180v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Huang, Dongkuan Xu, Hao Zhang, Peng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Egocentric pose estimation is a fundamental capability for multi-robot
collaborative perception in connected autonomy, such as connected autonomous
vehicles. During multi-robot operations, a robot needs to know the relative
pose between itself and its teammates with respect to its own coordinates.
However, different robots usually observe completely different views that
contains similar objects, which leads to wrong pose estimation. In addition, it
is unrealistic to allow robots to share their raw observations to detect
overlap due to the limited communication bandwidth constraint. In this paper,
we introduce a novel method for Non-Overlap-Aware Egocentric Pose Estimation
(NOPE), which performs egocentric pose estimation in a multi-robot team while
identifying the non-overlap views and satifying the communication bandwidth
constraint. NOPE is built upon an unified hierarchical learning framework that
integrates two levels of robot learning: (1) high-level deep graph matching for
correspondence identification, which allows to identify if two views are
overlapping or not, (2) low-level position-aware cross-attention graph learning
for egocentric pose estimation. To evaluate NOPE, we conduct extensive
experiments in both high-fidelity simulation and real-world scenarios.
Experimental results have demonstrated that NOPE enables the novel capability
for non-overlapping-aware egocentric pose estimation and achieves state-of-art
performance compared with the existing methods. Our project page at
https://hongh0.github.io/NOPE/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IROS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VMTS: Vision-Assisted Teacher-Student <span class="highlight-title">Reinforcement</span> Learning for
  Multi-Terrain Locomotion in Bipedal <span class="highlight-title">Robot</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.07049v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.07049v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fu Chen, Rui Wan, Peidong Liu, Nanxing Zheng, Bo Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bipedal robots, due to their anthropomorphic design, offer substantial
potential across various applications, yet their control is hindered by the
complexity of their structure. Currently, most research focuses on
proprioception-based methods, which lack the capability to overcome complex
terrain. While visual perception is vital for operation in human-centric
environments, its integration complicates control further. Recent reinforcement
learning (RL) approaches have shown promise in enhancing legged robot
locomotion, particularly with proprioception-based methods. However, terrain
adaptability, especially for bipedal robots, remains a significant challenge,
with most research focusing on flat-terrain scenarios. In this paper, we
introduce a novel mixture of experts teacher-student network RL strategy, which
enhances the performance of teacher-student policies based on visual inputs
through a simple yet effective approach. Our method combines terrain selection
strategies with the teacher policy, resulting in superior performance compared
to traditional models. Additionally, we introduce an alignment loss between the
teacher and student networks, rather than enforcing strict similarity, to
improve the student's ability to navigate diverse terrains. We validate our
approach experimentally on the Limx Dynamic P1 bipedal robot, demonstrating its
feasibility and robustness across multiple terrain types.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EgoVLA: Learning Vision-Language-Action Models from Egocentric Human
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.12440v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.12440v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruihan Yang, Qinxi Yu, Yecheng Wu, Rui Yan, Borui Li, An-Chieh Cheng, Xueyan Zou, Yunhao Fang, Xuxin Cheng, Ri-Zhao Qiu, Hongxu Yin, Sifei Liu, Song Han, Yao Lu, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real robot data collection for imitation learning has led to significant
advancements in robotic manipulation. However, the requirement for robot
hardware in the process fundamentally constrains the scale of the data. In this
paper, we explore training Vision-Language-Action (VLA) models using egocentric
human videos. The benefit of using human videos is not only for their scale but
more importantly for the richness of scenes and tasks. With a VLA trained on
human video that predicts human wrist and hand actions, we can perform Inverse
Kinematics and retargeting to convert the human actions to robot actions. We
fine-tune the model using a few robot manipulation demonstrations to obtain the
robot policy, namely EgoVLA. We propose a simulation benchmark called Ego
Humanoid Manipulation Benchmark, where we design diverse bimanual manipulation
tasks with demonstrations. We fine-tune and evaluate EgoVLA with Ego Humanoid
Manipulation Benchmark and show significant improvements over baselines and
ablate the importance of human data. Videos can be found on our website:
https://rchalyang.github.io/EgoVLA
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>More videos can be found on our website:
  https://rchalyang.github.io/EgoVLA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Robot</span>ic Monitoring of Colorimetric Leaf Sensors for Precision
  Agriculture <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malakhi Hopkins, Alice Kate Li, Shobhita Kramadhati, Jackson Arnold, Akhila Mallavarapu, Chavez Lawrence, Varun Murali, Sanjeev J. Koppal, Cherie R. Kagan, Vijay Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Common remote sensing modalities (RGB, multispectral, hyperspectral imaging
or LiDAR) are often used to indirectly measure crop health and do not directly
capture plant stress indicators. Commercially available direct leaf sensors are
bulky, powered electronics that are expensive and interfere with crop growth.
In contrast, low-cost, passive and bio-degradable leaf sensors offer an
opportunity to advance real-time monitoring as they directly interface with the
crop surface while not interfering with crop growth. To this end, we co-design
a sensor-detector system, where the sensor is a passive colorimetric leaf
sensor that directly measures crop health in a precision agriculture setting,
and the detector autonomously obtains optical signals from these leaf sensors.
The detector comprises a low size weight and power (SWaP) mobile ground robot
with an onboard monocular RGB camera and object detector to localize each leaf
sensor, as well as a hyperspectral camera with a motorized mirror and halogen
light to acquire hyperspectral images. The sensor's crop health-dependent
optical signals can be extracted from the hyperspectral images. The
proof-of-concept system is demonstrated in row-crop environments both indoors
and outdoors where it is able to autonomously navigate, locate and obtain a
hyperspectral image of all leaf sensors present, and acquire interpretable
spectral resonance with 80 $\%$ accuracy within a required retrieval distance
from the sensor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Revised version. Initial version was accepted to the Novel Approaches
  for Precision Agriculture and Forestry with Autonomous Robots IEEE ICRA
  Workshop - 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-07-17T00:00:00Z">2025-07-17</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">16</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCOPE for Hexapod Gait Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jim O'Connor, Jay B. Nash, Derin Gezgin, Gary B. Parker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evolutionary methods have previously been shown to be an effective learning
method for walking gaits on hexapod robots. However, the ability of these
algorithms to evolve an effective policy rapidly degrades as the input space
becomes more complex. This degradation is due to the exponential growth of the
solution space, resulting from an increasing parameter count to handle a more
complex input. In order to address this challenge, we introduce Sparse Cosine
Optimized Policy Evolution (SCOPE). SCOPE utilizes the Discrete Cosine
Transform (DCT) to learn directly from the feature coefficients of an input
matrix. By truncating the coefficient matrix returned by the DCT, we can reduce
the dimensionality of an input while retaining the highest energy features of
the original input. We demonstrate the effectiveness of this method by using
SCOPE to learn the gait of a hexapod robot. The hexapod controller is given a
matrix input containing time-series information of previous poses, which are
then transformed to gait parameters by an evolved policy. In this task, the
addition of SCOPE to a reference algorithm achieves a 20% increase in efficacy.
SCOPE achieves this result by reducing the total input size of the time-series
pose data from 2700 to 54, a 98% decrease. Additionally, SCOPE is capable of
compressing an input to any output shape, provided that each output dimension
is no greater than the corresponding input dimension. This paper demonstrates
that SCOPE is capable of significantly compressing the size of an input to an
evolved controller, resulting in a statistically significant gain in efficacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IJCCI Conference on Evolutionary Computation and Theory and
  Applications, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ERR@HRI 2.0 Challenge: Multimodal <span class="highlight-title">Detection</span> of Errors and Failures in
  Human-<span class="highlight-title">Robot</span> Conversations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13468v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13468v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiye Cao, Maia Stiber, Amama Mahmood, Maria Teresa Parreira, Wendy Ju, Micol Spitale, Hatice Gunes, Chien-Ming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of large language models (LLMs) into conversational robots
has made human-robot conversations more dynamic. Yet, LLM-powered
conversational robots remain prone to errors, e.g., misunderstanding user
intent, prematurely interrupting users, or failing to respond altogether.
Detecting and addressing these failures is critical for preventing
conversational breakdowns, avoiding task disruptions, and sustaining user
trust. To tackle this problem, the ERR@HRI 2.0 Challenge provides a multimodal
dataset of LLM-powered conversational robot failures during human-robot
conversations and encourages researchers to benchmark machine learning models
designed to detect robot failures. The dataset includes 16 hours of dyadic
human-robot interactions, incorporating facial, speech, and head movement
features. Each interaction is annotated with the presence or absence of robot
errors from the system perspective, and perceived user intention to correct for
a mismatch between robot behavior and user expectation. Participants are
invited to form teams and develop machine learning models that detect these
failures using multimodal data. Submissions will be evaluated using various
performance metrics, including detection accuracy and false positive rate. This
challenge represents another key step toward improving failure detection in
human-robot interaction through social signal analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hard-Stop Synthesis for Multi-DOF Compliant Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dean Chen, Armin Pomeroy, Brandon T. Peterson, Will Flanagan, He Kai Lim, Alexandra Stavrakis, Nelson F. SooHoo, Jonathan B. Hopkins, Tyler R. Clites
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compliant mechanisms have significant potential in precision applications due
to their ability to guide motion without contact. However, an inherent
vulnerability to fatigue and mechanical failure has hindered the translation of
compliant mechanisms to real-world applications. This is particularly
challenging in service environments where loading is complex and uncertain, and
the cost of failure is high. In such cases, mechanical hard stops are critical
to prevent yielding and buckling. Conventional hard-stop designs, which rely on
stacking single-DOF limits, must be overly restrictive in multi-DOF space to
guarantee safety in the presence of unknown loads. In this study, we present a
systematic design synthesis method to guarantee overload protection in
compliant mechanisms by integrating coupled multi-DOF motion limits within a
single pair of compact hard-stop surfaces. Specifically, we introduce a
theoretical and practical framework for optimizing the contact surface geometry
to maximize the mechanisms multi-DOF working space while still ensuring that
the mechanism remains within its elastic regime. We apply this synthesis method
to a case study of a caged-hinge mechanism for orthopaedic implants, and
provide numerical and experimental validation that the derived design offers
reliable protection against fatigue, yielding, and buckling. This work
establishes a foundation for precision hard-stop design in compliant systems
operating under uncertain loads, which is a crucial step toward enabling the
application of compliant mechanisms in real-world systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 17 figures. Under review at ASME Journal of Mechanical
  Design</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latent Policy Steering with Embodiment-Agnostic Pretrained World Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqi Wang, Mrinal Verghese, Jeff Schneider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning visuomotor policies via imitation has proven effective across a wide
range of robotic domains. However, the performance of these policies is heavily
dependent on the number of training demonstrations, which requires expensive
data collection in the real world. In this work, we aim to reduce data
collection efforts when learning visuomotor robot policies by leveraging
existing or cost-effective data from a wide range of embodiments, such as
public robot datasets and the datasets of humans playing with objects (human
data from play). Our approach leverages two key insights. First, we use optic
flow as an embodiment-agnostic action representation to train a World Model
(WM) across multi-embodiment datasets, and finetune it on a small amount of
robot data from the target embodiment. Second, we develop a method, Latent
Policy Steering (LPS), to improve the output of a behavior-cloned policy by
searching in the latent space of the WM for better action sequences. In real
world experiments, we observe significant improvements in the performance of
policies trained with a small amount of data (over 50% relative improvement
with 30 demonstrations and over 20% relative improvement with 50
demonstrations) by combining the policy with a WM pretrained on two thousand
episodes sampled from the existing Open X-embodiment dataset across different
robots or a cost-effective human dataset from play.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating <span class="highlight-title">Reinforcement</span> Learning Algorithms for <span class="highlight-title">Navigation</span> in Simulated
  <span class="highlight-title">Robot</span>ic Quadrupeds: A Comparative Study Inspired by Guide Dog Behaviour 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emma M. A. Harrison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robots are increasingly integrated across industries, particularly in
healthcare. However, many valuable applications for quadrupedal robots remain
overlooked. This research explores the effectiveness of three reinforcement
learning algorithms in training a simulated quadruped robot for autonomous
navigation and obstacle avoidance. The goal is to develop a robotic guide dog
simulation capable of path following and obstacle avoidance, with long-term
potential for real-world assistance to guide dogs and visually impaired
individuals. It also seeks to expand research into medical 'pets', including
robotic guide and alert dogs.
  A comparative analysis of thirteen related research papers shaped key
evaluation criteria, including collision detection, pathfinding algorithms,
sensor usage, robot type, and simulation platforms. The study focuses on sensor
inputs, collision frequency, reward signals, and learning progression to
determine which algorithm best supports robotic navigation in complex
environments.
  Custom-made environments were used to ensure fair evaluation of all three
algorithms under controlled conditions, allowing consistent data collection.
Results show that Proximal Policy Optimization (PPO) outperformed Deep
Q-Network (DQN) and Q-learning across all metrics, particularly in average and
median steps to goal per episode.
  By analysing these results, this study contributes to robotic navigation, AI
and medical robotics, offering insights into the feasibility of AI-driven
quadruped mobility and its role in assistive robotics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VITA: Vision-to-Action Flow Matching Policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dechen Gao, Boqi Zhao, Andrew Lee, Ian Chuang, Hanchu Zhou, Hang Wang, Zhe Zhao, Junshan Zhang, Iman Soltani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present VITA, a Vision-To-Action flow matching policy that evolves latent
visual representations into latent actions for visuomotor control. Traditional
flow matching and diffusion policies sample from standard source distributions
(e.g., Gaussian noise) and require additional conditioning mechanisms like
cross-attention to condition action generation on visual information, creating
time and space overheads. VITA proposes a novel paradigm that treats latent
images as the flow source, learning an inherent mapping from vision to action
while eliminating separate conditioning modules and preserving generative
modeling capabilities. Learning flows between fundamentally different
modalities like vision and action is challenging due to sparse action data
lacking semantic structures and dimensional mismatches between high-dimensional
visual representations and raw actions. We address this by creating a
structured action latent space via an autoencoder as the flow matching target,
up-sampling raw actions to match visual representation shapes. Crucially, we
supervise flow matching with both encoder targets and final action outputs
through flow latent decoding, which backpropagates action reconstruction loss
through sequential flow matching ODE solving steps for effective end-to-end
learning. Implemented as simple MLP layers, VITA is evaluated on challenging
bi-manual manipulation tasks on the ALOHA platform, including 5 simulation and
2 real-world tasks. Despite its simplicity, MLP-only VITA outperforms or
matches state-of-the-art generative policies while reducing inference latency
by 50-130% compared to conventional flow matching policies requiring different
conditioning mechanisms or complex architectures. To our knowledge, VITA is the
first MLP-only flow matching policy capable of solving complex bi-manual
manipulation tasks like those in ALOHA benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://ucd-dare.github.io/VITA/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $S^2M^2$: Scalable Stereo Matching Model for Reliable Depth <span class="highlight-title">Estimation</span> <span class="chip">ICCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13229v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13229v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhong Min, Youngpil Jeon, Jimin Kim, Minyong Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pursuit of a generalizable stereo matching model, capable of performing
across varying resolutions and disparity ranges without dataset-specific
fine-tuning, has revealed a fundamental trade-off. Iterative local search
methods achieve high scores on constrained benchmarks, but their core mechanism
inherently limits the global consistency required for true generalization. On
the other hand, global matching architectures, while theoretically more robust,
have been historically rendered infeasible by prohibitive computational and
memory costs. We resolve this dilemma with $S^2M^2$: a global matching
architecture that achieves both state-of-the-art accuracy and high efficiency
without relying on cost volume filtering or deep refinement stacks. Our design
integrates a multi-resolution transformer for robust long-range correspondence,
trained with a novel loss function that concentrates probability on feasible
matches. This approach enables a more robust joint estimation of disparity,
occlusion, and confidence. $S^2M^2$ establishes a new state of the art on the
Middlebury v3 and ETH3D benchmarks, significantly outperforming prior methods
across most metrics while reconstructing high-quality details with competitive
efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, ICCV accepted paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Signal Temporal Logic Compliant Co-design of <span class="highlight-title">Planning</span> and <span class="highlight-title">Control</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manas Sashank Juvvi, Tushar Dilip Kurne, Vaishnavi J, Shishir Kolathaya, Pushpak Jagtap
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents a novel co-design strategy that integrates trajectory
planning and control to handle STL-based tasks in autonomous robots. The method
consists of two phases: $(i)$ learning spatio-temporal motion primitives to
encapsulate the inherent robot-specific constraints and $(ii)$ constructing an
STL-compliant motion plan from these primitives. Initially, we employ
reinforcement learning to construct a library of control policies that perform
trajectories described by the motion primitives. Then, we map motion primitives
to spatio-temporal characteristics. Subsequently, we present a sampling-based
STL-compliant motion planning strategy tailored to meet the STL specification.
The proposed model-free approach, which generates feasible STL-compliant motion
plans across various environments, is validated on differential-drive and
quadruped robots across various STL specifications. Demonstration videos are
available at https://tinyurl.com/m6zp7rsm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-shot transfer of tool-use skills using human demonstrations with
  proximity and tactile sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marina Y. Aoyama, Sethu Vijayakumar, Tetsuya Narita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tools extend the manipulation abilities of robots, much like they do for
humans. Despite human expertise in tool manipulation, teaching robots these
skills faces challenges. The complexity arises from the interplay of two
simultaneous points of contact: one between the robot and the tool, and another
between the tool and the environment. Tactile and proximity sensors play a
crucial role in identifying these complex contacts. However, learning tool
manipulation using these sensors remains challenging due to limited real-world
data and the large sim-to-real gap. To address this, we propose a few-shot
tool-use skill transfer framework using multimodal sensing. The framework
involves pre-training the base policy to capture contact states common in
tool-use skills in simulation and fine-tuning it with human demonstrations
collected in the real-world target domain to bridge the domain gap. We validate
that this framework enables teaching surface-following tasks using tools with
diverse physical and geometric properties with a small number of demonstrations
on the Franka Emika robot arm. Our analysis suggests that the robot acquires
new tool-use skills by transferring the ability to recognise tool-environment
contact relationships from pre-trained to fine-tuned policies. Additionally,
combining proximity and tactile sensors enhances the identification of contact
states and environmental geometry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures, IEEE Robotics and Automation Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lost in Tracking Translation: A Comprehensive Analysis of Visual <span class="highlight-title">SLAM</span> in
  Human-Centered XR and IoT Ecosystems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07146v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07146v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasra Chandio, Khotso Selialia, Joseph DeGol, Luis Garcia, Fatima M. Anwar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in tracking algorithms have empowered nascent applications
across various domains, from steering autonomous vehicles to guiding robots to
enhancing augmented reality experiences for users. However, these algorithms
are application-specific and do not work across applications with different
types of motion; even a tracking algorithm designed for a given application
does not work in scenarios deviating from highly standard conditions. For
example, a tracking algorithm designed for robot navigation inside a building
will not work for tracking the same robot in an outdoor environment. To
demonstrate this problem, we evaluate the performance of the state-of-the-art
tracking methods across various applications and scenarios. To inform our
analysis, we first categorize algorithmic, environmental, and
locomotion-related challenges faced by tracking algorithms. We quantitatively
evaluate the performance using multiple tracking algorithms and representative
datasets for a wide range of Internet of Things (IoT) and Extended Reality (XR)
applications, including autonomous vehicles, drones, and humans. Our analysis
shows that no tracking algorithm works across different applications and
scenarios within applications. Ultimately, using the insights generated from
our analysis, we discuss multiple approaches to improving the tracking
performance using input data characterization, leveraging intermediate
information, and output evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> A Roadmap for Climate-Relevant <span class="highlight-title">Robot</span>ics Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.11623v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.11623v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alan Papalia, Charles Dawson, Laurentiu L. Anton, Norhan Magdy Bayomi, Bianca Champenois, Jung-Hoon Cho, Levi Cai, Joseph DelPreto, Kristen Edwards, Bilha-Catherine Githinji, Cameron Hickert, Vindula Jayawardana, Matthew Kramer, Shreyaa Raghavan, David Russell, Shide Salimi, Jingnan Shi, Soumya Sudhakar, Yanwei Wang, Shouyi Wang, <span class="highlight-author">Luca Carlone</span>, Vijay Kumar, Daniela Rus, John E. Fernandez, Cathy Wu, George Kantor, Derek Young, Hanumant Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Climate change is one of the defining challenges of the 21st century, and
many in the robotics community are looking for ways to contribute. This paper
presents a roadmap for climate-relevant robotics research, identifying
high-impact opportunities for collaboration between roboticists and experts
across climate domains such as energy, the built environment, transportation,
industry, land use, and Earth sciences. These applications include problems
such as energy systems optimization, construction, precision agriculture,
building envelope retrofits, autonomous trucking, and large-scale environmental
monitoring. Critically, we include opportunities to apply not only physical
robots but also the broader robotics toolkit - including planning, perception,
control, and estimation algorithms - to climate-relevant problems. A central
goal of this roadmap is to inspire new research directions and collaboration by
highlighting specific, actionable problems at the intersection of robotics and
climate. This work represents a collaboration between robotics researchers and
domain experts in various climate disciplines, and it serves as an invitation
to the robotics community to bring their expertise to bear on urgent climate
priorities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sampling-Based Motion <span class="highlight-title">Planning</span> with Discrete Configuration-Space
  Symmetries <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.00614v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.00614v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Cohn, Russ Tedrake
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When planning motions in a configuration space that has underlying symmetries
(e.g. when manipulating one or multiple symmetric objects), the ideal planning
algorithm should take advantage of those symmetries to produce shorter
trajectories. However, finite symmetries lead to complicated changes to the
underlying topology of configuration space, preventing the use of standard
algorithms. We demonstrate how the key primitives used for sampling-based
planning can be efficiently implemented in spaces with finite symmetries. A
rigorous theoretical analysis, building upon a study of the geometry of the
configuration space, shows improvements in the sample complexity of several
standard algorithms. Furthermore, a comprehensive slate of experiments
demonstrates the practical improvements in both path length and runtime.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IROS 2025. 8 pages, 2 figures, 4 tables. Interactive
  results available at https://cohnt.github.io/projects/symmetries.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VertiSelector: Automatic Curriculum Learning for Wheeled Mobility on
  Vertically Challenging Terrain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17469v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17469v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Xu, Chenhui Pan, Xuesu Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) has the potential to enable extreme off-road
mobility by circumventing complex kinodynamic modeling, planning, and control
by simulated end-to-end trial-and-error learning experiences. However, most RL
methods are sample-inefficient when training in a large amount of manually
designed simulation environments and struggle at generalizing to the real
world. To address these issues, we introduce VertiSelector (VS), an automatic
curriculum learning framework designed to enhance learning efficiency and
generalization by selectively sampling training terrain. VS prioritizes
vertically challenging terrain with higher Temporal Difference (TD) errors when
revisited, thereby allowing robots to learn at the edge of their evolving
capabilities. By dynamically adjusting the sampling focus, VS significantly
boosts sample efficiency and generalization within the VW-Chrono simulator
built on the Chrono multi-physics engine. Furthermore, we provide simulation
and physical results using VS on a Verti-4-Wheeler platform. These results
demonstrate that VS can achieve 23.08% improvement in terms of success rate by
efficiently sampling during training and robustly generalizing to the real
world.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ V-Max: A <span class="highlight-title">Reinforcement</span> Learning Framework for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.08388v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.08388v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentin Charraut, Waël Doulazmi, Thomas Tournaire, Thibault Buhet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning-based decision-making has the potential to enable generalizable
Autonomous Driving (AD) policies, reducing the engineering overhead of
rule-based approaches. Imitation Learning (IL) remains the dominant paradigm,
benefiting from large-scale human demonstration datasets, but it suffers from
inherent limitations such as distribution shift and imitation gaps.
Reinforcement Learning (RL) presents a promising alternative, yet its adoption
in AD remains limited due to the lack of standardized and efficient research
frameworks. To this end, we introduce V-Max, an open research framework
providing all the necessary tools to make RL practical for AD. V-Max is built
on Waymax, a hardware-accelerated AD simulator designed for large-scale
experimentation. We extend it using ScenarioNet's approach, enabling the fast
simulation of diverse AD datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>RLC 25 - Camera-ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Next-Gen Museum Guides: Autonomous <span class="highlight-title">Navigation</span> and Visitor Interaction
  with an Agentic <span class="highlight-title">Robot</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.12273v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.12273v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Garello, Francesca Cocchella, Alessandra Sciutti, Manuel Catalano, Francesco Rea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous robots are increasingly being tested into public spaces to enhance
user experiences, particularly in cultural and educational settings. This paper
presents the design, implementation, and evaluation of the autonomous museum
guide robot Alter-Ego equipped with advanced navigation and interactive
capabilities. The robot leverages state-of-the-art Large Language Models (LLMs)
to provide real-time, context aware question-and-answer (Q&A) interactions,
allowing visitors to engage in conversations about exhibits. It also employs
robust simultaneous localization and mapping (SLAM) techniques, enabling
seamless navigation through museum spaces and route adaptation based on user
requests. The system was tested in a real museum environment with 34
participants, combining qualitative analysis of visitor-robot conversations and
quantitative analysis of pre and post interaction surveys. Results showed that
the robot was generally well-received and contributed to an engaging museum
experience, despite some limitations in comprehension and responsiveness. This
study sheds light on HRI in cultural spaces, highlighting not only the
potential of AI-driven robotics to support accessibility and knowledge
acquisition, but also the current limitations and challenges of deploying such
technologies in complex, real-world environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Equivariant IMU Preintegration with Biases: a Galilean Group Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05548v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05548v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giulio Delama, Alessandro Fornasier, Robert Mahony, Stephan Weiss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This letter proposes a new approach for Inertial Measurement Unit (IMU)
preintegration, a fundamental building block that can be leveraged in different
optimization-based Inertial Navigation System (INS) localization solutions.
Inspired by recent advances in equivariant theory applied to biased INSs, we
derive a discrete-time formulation of the IMU preintegration on
${\mathbf{Gal}(3) \ltimes \mathfrak{gal}(3)}$, the left-trivialization of the
tangent group of the Galilean group $\mathbf{Gal}(3)$. We define a novel
preintegration error that geometrically couples the navigation states and the
bias leading to lower linearization error. Our method improves in consistency
compared to existing preintegration approaches which treat IMU biases as a
separate state-space. Extensive validation against state-of-the-art methods,
both in simulation and with real-world IMU data, implementation in the Lie++
library, and open-source code are provided.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2025-07-25T20:21:09.790967850Z">
            2025-07-25 20:21:09 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
