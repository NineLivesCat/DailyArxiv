{"2025-07-24T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2507.15857v2","updated":"2025-07-24T17:55:24Z","published":"2025-07-21T17:59:57Z","title":"Diffusion Beats Autoregressive in Data-Constrained Settings","summary":"  Autoregressive (AR) models have long dominated the landscape of large\nlanguage models, driving progress across a wide range of tasks. Recently,\ndiffusion-based language models have emerged as a promising alternative, though\ntheir advantages over AR models remain underexplored. In this paper, we\nsystematically study masked diffusion models in data-constrained settings-where\ntraining involves repeated passes over limited data-and find that they\nsignificantly outperform AR models when compute is abundant but data is scarce.\nDiffusion models make better use of repeated data, achieving lower validation\nloss and superior downstream performance. We interpret this advantage as\nimplicit data augmentation: masked diffusion exposes the model to a diverse\ndistribution of token orderings and prediction tasks, unlike AR's fixed\nleft-to-right factorization. We find new scaling laws for diffusion models and\nderive a closed-form expression for the critical compute threshold at which\ndiffusion begins to outperform AR. These results suggest that when data, not\ncompute, is the bottleneck, diffusion models offer a compelling alternative to\nthe standard AR paradigm. Our code is available at:\nhttps://diffusion-scaling.github.io.\n","authors":["Mihir Prabhudesai","Menging Wu","Amir Zadeh","Katerina Fragkiadaki","Deepak Pathak"],"pdf_url":"https://arxiv.org/pdf/2507.15857v2.pdf","comment":"Project Webpage: https://diffusion-scaling.github.io"},{"id":"http://arxiv.org/abs/2408.10450v2","updated":"2025-07-24T16:51:36Z","published":"2024-08-19T23:16:18Z","title":"RUMI: Rummaging Using Mutual Information","summary":"  This paper presents Rummaging Using Mutual Information (RUMI), a method for\nonline generation of robot action sequences to gather information about the\npose of a known movable object in visually-occluded environments. Focusing on\ncontact-rich rummaging, our approach leverages mutual information between the\nobject pose distribution and robot trajectory for action planning. From an\nobserved partial point cloud, RUMI deduces the compatible object pose\ndistribution and approximates the mutual information of it with workspace\noccupancy in real time. Based on this, we develop an information gain cost\nfunction and a reachability cost function to keep the object within the robot's\nreach. These are integrated into a model predictive control (MPC) framework\nwith a stochastic dynamics model, updating the pose distribution in a closed\nloop. Key contributions include a new belief framework for object pose\nestimation, an efficient information gain computation strategy, and a robust\nMPC-based control scheme. RUMI demonstrates superior performance in both\nsimulated and real tasks compared to baseline methods.\n","authors":["Sheng Zhong","Nima Fazeli","Dmitry Berenson"],"pdf_url":"https://arxiv.org/pdf/2408.10450v2.pdf","comment":"20 pages, 20 figures, accepted by IEEE Transactions on Robotics\n  (T-RO), preprint"},{"id":"http://arxiv.org/abs/2503.07926v2","updated":"2025-07-24T16:08:12Z","published":"2025-03-11T00:12:25Z","title":"Learning Gentle Grasping Using Vision, Sound, and Touch","summary":"  In our daily life, we often encounter objects that are fragile and can be\ndamaged by excessive grasping force, such as fruits. For these objects, it is\nparamount to grasp gently -- not using the maximum amount of force possible,\nbut rather the minimum amount of force necessary. This paper proposes using\nvisual, tactile, and auditory signals to learn to grasp and regrasp objects\nstably and gently. Specifically, we use audio signals as an indicator of\ngentleness during the grasping, and then train an end-to-end action-conditional\nmodel from raw visuo-tactile inputs that predicts both the stability and the\ngentleness of future grasping candidates, thus allowing the selection and\nexecution of the most promising action. Experimental results on a\nmulti-fingered hand over 1,500 grasping trials demonstrated that our model is\nuseful for gentle grasping by validating the predictive performance (3.27%\nhigher accuracy than the vision-only variant) and providing interpretations of\ntheir behavior. Finally, real-world experiments confirmed that the grasping\nperformance with the trained multi-modal model outperformed other baselines\n(17% higher rate for stable and gentle grasps than vision-only). Our approach\nrequires neither tactile sensor calibration nor analytical force modeling,\ndrastically reducing the engineering effort to grasp fragile objects. Dataset\nand videos are available at https://lasr.org/research/gentle-grasping.\n","authors":["Ken Nakahara","Roberto Calandra"],"pdf_url":"https://arxiv.org/pdf/2503.07926v2.pdf","comment":"8 pages. Accepted by 2025 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS)"},{"id":"http://arxiv.org/abs/2405.03572v2","updated":"2025-07-24T15:30:54Z","published":"2024-05-06T15:48:14Z","title":"RoboCar: A Rapidly Deployable Open-Source Platform for Autonomous\n  Driving Research","summary":"  This paper introduces RoboCar, an open-source research platform for\nautonomous driving developed at the University of Luxembourg. RoboCar provides\na modular, cost-effective framework for the development of experimental\nAutonomous Driving Systems (ADS), utilizing the 2018 KIA Soul EV. The platform\nintegrates a robust hardware and software architecture that aligns with the\nvehicle's existing systems, minimizing the need for extensive modifications. It\nsupports various autonomous driving functions and has undergone real-world\ntesting on public roads in Luxembourg City. This paper outlines the platform's\narchitecture, integration challenges, and initial test results, offering\ninsights into its application in advancing autonomous driving research. RoboCar\nis available to anyone at https://github.com/sntubix/robocar and is released\nunder an open-source MIT license.\n","authors":["Mehdi Testouri","Gamal Elghazaly","Raphael Frank"],"pdf_url":"https://arxiv.org/pdf/2405.03572v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18502v1","updated":"2025-07-24T15:18:31Z","published":"2025-07-24T15:18:31Z","title":"Experimental Comparison of Whole-Body Control Formulations for Humanoid\n  Robots in Task Acceleration and Task Force Spaces","summary":"  This paper studies the experimental comparison of two different whole-body\ncontrol formulations for humanoid robots: inverse dynamics whole-body control\n(ID-WBC) and passivity-based whole-body control (PB-WBC). The two controllers\nfundamentally differ from each other as the first is formulated in task\nacceleration space and the latter is in task force space with passivity\nconsiderations. Even though both control methods predict stability under ideal\nconditions in closed-loop dynamics, their robustness against joint friction,\nsensor noise, unmodeled external disturbances, and non-perfect contact\nconditions is not evident. Therefore, we analyze and experimentally compare the\ntwo controllers on a humanoid robot platform through swing foot position and\norientation control, squatting with and without unmodeled additional weights,\nand jumping. We also relate the observed performance and characteristic\ndifferences with the controller formulations and highlight each controller's\nadvantages and disadvantages.\n","authors":["Sait Sovukluk","Grazia Zambella","Tobias Egle","Christian Ott"],"pdf_url":"https://arxiv.org/pdf/2507.18502v1.pdf","comment":"This paper has been accepted for publication in 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2025). -\n  Link to video: https://youtu.be/Nfm50ycz-FU"},{"id":"http://arxiv.org/abs/2404.11557v3","updated":"2025-07-24T14:58:42Z","published":"2024-04-17T17:00:26Z","title":"Spatio-Temporal Motion Retargeting for Quadruped Robots","summary":"  This work presents a motion retargeting approach for legged robots, aimed at\ntransferring the dynamic and agile movements to robots from source motions. In\nparticular, we guide the imitation learning procedures by transferring motions\nfrom source to target, effectively bridging the morphological disparities while\nensuring the physical feasibility of the target system. In the first stage, we\nfocus on motion retargeting at the kinematic level by generating kinematically\nfeasible whole-body motions from keypoint trajectories. Following this, we\nrefine the motion at the dynamic level by adjusting it in the temporal domain\nwhile adhering to physical constraints. This process facilitates policy\ntraining via reinforcement learning, enabling precise and robust motion\ntracking. We demonstrate that our approach successfully transforms noisy motion\nsources, such as hand-held camera videos, into robot-specific motions that\nalign with the morphology and physical properties of the target robots.\nMoreover, we demonstrate terrain-aware motion retargeting to perform BackFlip\non top of a box. We successfully deployed these skills to four robots with\ndifferent dimensions and physical properties in the real world through hardware\nexperiments.\n","authors":["Taerim Yoon","Dongho Kang","Seungmin Kim","Jin Cheng","Minsung Ahn","Stelian Coros","Sungjoon Choi"],"pdf_url":"https://arxiv.org/pdf/2404.11557v3.pdf","comment":"20 pages, 12 figures, videos available at\n  https://taerimyoon.me/Spatio-Temporal-Motion-Retargeting-for-Quadruped-Robots/"},{"id":"http://arxiv.org/abs/2507.16859v2","updated":"2025-07-24T14:41:42Z","published":"2025-07-21T17:22:18Z","title":"Leveraging multi-source and heterogeneous signals for fatigue detection","summary":"  Fatigue detection plays a critical role in safety-critical applications such\nas aviation, mining, and long-haul transport. However, most existing methods\nrely on high-end sensors and controlled environments, limiting their\napplicability in real world settings. This paper formally defines a practical\nyet underexplored problem setting for real world fatigue detection, where\nsystems operating with context-appropriate sensors aim to leverage knowledge\nfrom differently instrumented sources including those using impractical sensors\ndeployed in controlled environments. To tackle this challenge, we propose a\nheterogeneous and multi-source fatigue detection framework that adaptively\nutilizes the available modalities in the target domain while benefiting from\nthe diverse configurations present in source domains. Our experiments,\nconducted using a realistic field-deployed sensor setup and two publicly\navailable datasets, demonstrate the practicality, robustness, and improved\ngeneralization of our approach, paving the practical way for effective fatigue\nmonitoring in sensor-constrained scenarios.\n","authors":["Luobin Cui","Yanlai Wu","Tang Ying","Weikai Li"],"pdf_url":"https://arxiv.org/pdf/2507.16859v2.pdf","comment":"1figures,32pages"},{"id":"http://arxiv.org/abs/2507.18462v1","updated":"2025-07-24T14:39:01Z","published":"2025-07-24T14:39:01Z","title":"A Novel Monte-Carlo Compressed Sensing and Dictionary Learning Method\n  for the Efficient Path Planning of Remote Sensing Robots","summary":"  In recent years, Compressed Sensing (CS) has gained significant interest as a\ntechnique for acquiring high-resolution sensory data using fewer measurements\nthan traditional Nyquist sampling requires. At the same time, autonomous\nrobotic platforms such as drones and rovers have become increasingly popular\ntools for remote sensing and environmental monitoring tasks, including\nmeasurements of temperature, humidity, and air quality. Within this context,\nthis paper presents, to the best of our knowledge, the first investigation into\nhow the structure of CS measurement matrices can be exploited to design\noptimized sampling trajectories for robotic environmental data collection. We\npropose a novel Monte Carlo optimization framework that generates measurement\nmatrices designed to minimize both the robot's traversal path length and the\nsignal reconstruction error within the CS framework. Central to our approach is\nthe application of Dictionary Learning (DL) to obtain a data-driven sparsifying\ntransform, which enhances reconstruction accuracy while further reducing the\nnumber of samples that the robot needs to collect. We demonstrate the\neffectiveness of our method through experiments reconstructing $NO_2$ pollution\nmaps over the Gulf region. The results indicate that our approach can reduce\nrobot travel distance to less than $10\\%$ of a full-coverage path, while\nimproving reconstruction accuracy by over a factor of five compared to\ntraditional CS methods based on DCT and polynomial dictionaries, as well as by\na factor of two compared to previously-proposed Informative Path Planning (IPP)\nmethods.\n","authors":["Alghalya Al-Hajri","Ejmen Al-Ubejdij","Aiman Erbad","Ali Safa"],"pdf_url":"https://arxiv.org/pdf/2507.18462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17351v3","updated":"2025-07-24T14:38:20Z","published":"2025-01-29T00:06:01Z","title":"Realtime Limb Trajectory Optimization for Humanoid Running Through\n  Centroidal Angular Momentum Dynamics","summary":"  One of the essential aspects of humanoid robot running is determining the\nlimb-swinging trajectories. During the flight phases, where the ground reaction\nforces are not available for regulation, the limb swinging trajectories are\nsignificant for the stability of the next stance phase. Due to the conservation\nof angular momentum, improper leg and arm swinging results in highly tilted and\nunsustainable body configurations at the next stance phase landing. In such\ncases, the robotic system fails to maintain locomotion independent of the\nstability of the center of mass trajectories. This problem is more apparent for\nfast and high flight time trajectories. This paper proposes a real-time\nnonlinear limb trajectory optimization problem for humanoid running. The\noptimization problem is tested on two different humanoid robot models, and the\ngenerated trajectories are verified using a running algorithm for both robots\nin a simulation environment.\n","authors":["Sait Sovukluk","Robert Schuller","Johannes Englsberger","Christian Ott"],"pdf_url":"https://arxiv.org/pdf/2501.17351v3.pdf","comment":"This paper has been accepted for publication at the IEEE\n  International Conference on Robotics and Automation (ICRA), Atlanta 2025.\n  Link to video: https://www.youtube.com/watch?v=czfHjwh_A0Y"},{"id":"http://arxiv.org/abs/2507.18444v1","updated":"2025-07-24T14:29:30Z","published":"2025-07-24T14:29:30Z","title":"DSFormer: A Dual-Scale Cross-Learning Transformer for Visual Place\n  Recognition","summary":"  Visual Place Recognition (VPR) is crucial for robust mobile robot\nlocalization, yet it faces significant challenges in maintaining reliable\nperformance under varying environmental conditions and viewpoints. To address\nthis, we propose a novel framework that integrates Dual-Scale-Former\n(DSFormer), a Transformer-based cross-learning module, with an innovative block\nclustering strategy. DSFormer enhances feature representation by enabling\nbidirectional information transfer between dual-scale features extracted from\nthe final two CNN layers, capturing both semantic richness and spatial details\nthrough self-attention for long-range dependencies within each scale and shared\ncross-attention for cross-scale learning. Complementing this, our block\nclustering strategy repartitions the widely used San Francisco eXtra Large\n(SF-XL) training dataset from multiple distinct perspectives, optimizing data\norganization to further bolster robustness against viewpoint variations.\nTogether, these innovations not only yield a robust global embedding adaptable\nto environmental changes but also reduce the required training data volume by\napproximately 30\\% compared to previous partitioning methods. Comprehensive\nexperiments demonstrate that our approach achieves state-of-the-art performance\nacross most benchmark datasets, surpassing advanced reranking methods like\nDELG, Patch-NetVLAD, TransVPR, and R2Former as a global retrieval solution\nusing 512-dim global descriptors, while significantly improving computational\nefficiency.\n","authors":["Haiyang Jiang","Songhao Piao","Chao Gao","Lei Yu","Liguo Chen"],"pdf_url":"https://arxiv.org/pdf/2507.18444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17349v3","updated":"2025-07-24T14:22:31Z","published":"2025-01-28T23:51:44Z","title":"An Efficient Numerical Function Optimization Framework for Constrained\n  Nonlinear Robotic Problems","summary":"  This paper presents a numerical function optimization framework designed for\nconstrained optimization problems in robotics. The tool is designed with\nreal-time considerations and is suitable for online trajectory and control\ninput optimization problems. The proposed framework does not require any\nanalytical representation of the problem and works with constrained block-box\noptimization functions. The method combines first-order gradient-based line\nsearch algorithms with constraint prioritization through nullspace projections\nonto constraint Jacobian space. The tool is implemented in C++ and provided\nonline for community use, along with some numerical and robotic example\nimplementations presented in this paper.\n","authors":["Sait Sovukluk","Christian Ott"],"pdf_url":"https://arxiv.org/pdf/2501.17349v3.pdf","comment":"\\c{opyright} 2025 the authors. This work has been accepted to IFAC\n  for publication under a Creative Commons Licence CC-BY-NC-ND. -\n  Implementation: https://github.com/ssovukluk/ENFORCpp"},{"id":"http://arxiv.org/abs/2507.18436v1","updated":"2025-07-24T14:15:56Z","published":"2025-07-24T14:15:56Z","title":"Evaluating the Pre-Dressing Step: Unfolding Medical Garments Via\n  Imitation Learning","summary":"  Robotic-assisted dressing has the potential to significantly aid both\npatients as well as healthcare personnel, reducing the workload and improving\nthe efficiency in clinical settings. While substantial progress has been made\nin robotic dressing assistance, prior works typically assume that garments are\nalready unfolded and ready for use. However, in medical applications gowns and\naprons are often stored in a folded configuration, requiring an additional\nunfolding step. In this paper, we introduce the pre-dressing step, the process\nof unfolding garments prior to assisted dressing. We leverage imitation\nlearning for learning three manipulation primitives, including both high and\nlow acceleration motions. In addition, we employ a visual classifier to\ncategorise the garment state as closed, partly opened, and fully opened. We\nconduct an empirical evaluation of the learned manipulation primitives as well\nas their combinations. Our results show that highly dynamic motions are not\neffective for unfolding freshly unpacked garments, where the combination of\nmotions can efficiently enhance the opening configuration.\n","authors":["David Blanco-Mulero","Júlia Borràs","Carme Torras"],"pdf_url":"https://arxiv.org/pdf/2507.18436v1.pdf","comment":"6 pages, 4 figures, 2 tables. Accepted to IEEE/RSJ IROS 2025. Project\n  website: https://sites.google.com/view/pre-dressing"},{"id":"http://arxiv.org/abs/2507.17727v2","updated":"2025-07-24T13:55:49Z","published":"2025-07-23T17:41:55Z","title":"CA-Cut: Crop-Aligned Cutout for Data Augmentation to Learn More Robust\n  Under-Canopy Navigation","summary":"  State-of-the-art visual under-canopy navigation methods are designed with\ndeep learning-based perception models to distinguish traversable space from\ncrop rows. While these models have demonstrated successful performance, they\nrequire large amounts of training data to ensure reliability in real-world\nfield deployment. However, data collection is costly, demanding significant\nhuman resources for in-field sampling and annotation. To address this\nchallenge, various data augmentation techniques are commonly employed during\nmodel training, such as color jittering, Gaussian blur, and horizontal flip, to\ndiversify training data and enhance model robustness. In this paper, we\nhypothesize that utilizing only these augmentation techniques may lead to\nsuboptimal performance, particularly in complex under-canopy environments with\nfrequent occlusions, debris, and non-uniform spacing of crops. Instead, we\npropose a novel augmentation method, so-called Crop-Aligned Cutout (CA-Cut)\nwhich masks random regions out in input images that are spatially distributed\naround crop rows on the sides to encourage trained models to capture high-level\ncontextual features even when fine-grained information is obstructed. Our\nextensive experiments with a public cornfield dataset demonstrate that\nmasking-based augmentations are effective for simulating occlusions and\nsignificantly improving robustness in semantic keypoint predictions for visual\nnavigation. In particular, we show that biasing the mask distribution toward\ncrop rows in CA-Cut is critical for enhancing both prediction accuracy and\ngeneralizability across diverse environments achieving up to a 36.9% reduction\nin prediction error. In addition, we conduct ablation studies to determine the\nnumber of masks, the size of each mask, and the spatial distribution of masks\nto maximize overall performance.\n","authors":["Robel Mamo","Taeyeong Choi"],"pdf_url":"https://arxiv.org/pdf/2507.17727v2.pdf","comment":"Accepted for publication at the 12th European Conference on Mobile\n  Robots (ECMR 2025)"},{"id":"http://arxiv.org/abs/2504.04598v2","updated":"2025-07-24T13:48:42Z","published":"2025-04-06T20:02:17Z","title":"B4P: Simultaneous Grasp and Motion Planning for Object Placement via\n  Parallelized Bidirectional Forests and Path Repair","summary":"  Robot pick and place systems have traditionally decoupled grasp, placement,\nand motion planning to build sequential optimization pipelines with the\nassumption that the individual components will be able to work together.\nHowever, this separation introduces sub-optimality, as grasp choices may limit\nor even prohibit feasible motions for a robot to reach the target placement\npose, particularly in cluttered environments with narrow passages. To this end,\nwe propose a forest-based planning framework to simultaneously find grasp\nconfigurations and feasible robot motions that explicitly satisfy downstream\nplacement configurations paired with the selected grasps. Our proposed\nframework leverages a bidirectional sampling-based approach to build a start\nforest, rooted at the feasible grasp regions, and a goal forest, rooted at the\nfeasible placement regions, to facilitate the search through randomly explored\nmotions that connect valid pairs of grasp and placement trees. We demonstrate\nthat the framework's inherent parallelism enables superlinear speedup, making\nit scalable for applications for redundant robot arms (e.g., 7 Degrees of\nFreedom) to work efficiently in highly cluttered environments. Extensive\nexperiments in simulation demonstrate the robustness and efficiency of the\nproposed framework in comparison with multiple baselines under diverse\nscenarios.\n","authors":["Benjamin H. Leebron","Kejia Ren","Yiting Chen","Kaiyu Hang"],"pdf_url":"https://arxiv.org/pdf/2504.04598v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18396v1","updated":"2025-07-24T13:28:05Z","published":"2025-07-24T13:28:05Z","title":"Residual Koopman Model Predictive Control for Enhanced Vehicle Dynamics\n  with Small On-Track Data Input","summary":"  In vehicle trajectory tracking tasks, the simplest approach is the Pure\nPursuit (PP) Control. However, this single-point preview tracking strategy\nfails to consider vehicle model constraints, compromising driving safety. Model\nPredictive Control (MPC) as a widely adopted control method, optimizes control\nactions by incorporating mechanistic models and physical constraints. While its\ncontrol performance critically depends on the accuracy of vehicle modeling.\nTraditional vehicle modeling approaches face inherent trade-offs between\ncapturing nonlinear dynamics and maintaining computational efficiency, often\nresulting in reduced control performance. To address these challenges, this\npaper proposes Residual Koopman Model Predictive Control (RKMPC) framework.\nThis method uses two linear MPC architecture to calculate control inputs: a\nLinear Model Predictive Control (LMPC) computes the baseline control input\nbased on the vehicle kinematic model, and a neural network-based RKMPC\ncalculates the compensation input. The final control command is obtained by\nadding these two components. This design preserves the reliability and\ninterpretability of traditional mechanistic model while achieving performance\noptimization through residual modeling. This method has been validated on the\nCarsim-Matlab joint simulation platform and a physical 1:10 scale F1TENTH\nracing car. Experimental results show that RKMPC requires only 20% of the\ntraining data needed by traditional Koopman Model Predictive Control (KMPC)\nwhile delivering superior tracking performance. Compared to traditional LMPC,\nRKMPC reduces lateral error by 11.7%-22.1%, decreases heading error by\n8.9%-15.8%, and improves front-wheel steering stability by up to 27.6%. The\nimplementation code is available at: https://github.com/ZJU-DDRX/Residual\nKoopman.\n","authors":["Yonghao Fu","Cheng Hu","Haokun Xiong","Zhangpeng Bao","Wenyuan Du","Edoardo Ghignone","Michele Magno","Lei Xie","Hongye Su"],"pdf_url":"https://arxiv.org/pdf/2507.18396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18344v1","updated":"2025-07-24T12:17:37Z","published":"2025-07-24T12:17:37Z","title":"G2S-ICP SLAM: Geometry-aware Gaussian Splatting ICP SLAM","summary":"  In this paper, we present a novel geometry-aware RGB-D Gaussian Splatting\nSLAM system, named G2S-ICP SLAM. The proposed method performs high-fidelity 3D\nreconstruction and robust camera pose tracking in real-time by representing\neach scene element using a Gaussian distribution constrained to the local\ntangent plane. This effectively models the local surface as a 2D Gaussian disk\naligned with the underlying geometry, leading to more consistent depth\ninterpretation across multiple viewpoints compared to conventional 3D\nellipsoid-based representations with isotropic uncertainty. To integrate this\nrepresentation into the SLAM pipeline, we embed the surface-aligned Gaussian\ndisks into a Generalized ICP framework by introducing anisotropic covariance\nprior without altering the underlying registration formulation. Furthermore we\npropose a geometry-aware loss that supervises photometric, depth, and normal\nconsistency. Our system achieves real-time operation while preserving both\nvisual and geometric fidelity. Extensive experiments on the Replica and\nTUM-RGBD datasets demonstrate that G2S-ICP SLAM outperforms prior SLAM systems\nin terms of localization accuracy, reconstruction completeness, while\nmaintaining the rendering quality.\n","authors":["Gyuhyeon Pak","Hae Min Cho","Euntai Kim"],"pdf_url":"https://arxiv.org/pdf/2507.18344v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2507.18317v1","updated":"2025-07-24T11:37:44Z","published":"2025-07-24T11:37:44Z","title":"AF-RLIO: Adaptive Fusion of Radar-LiDAR-Inertial Information for Robust\n  Odometry in Challenging Environments","summary":"  In robotic navigation, maintaining precise pose estimation and navigation in\ncomplex and dynamic environments is crucial. However, environmental challenges\nsuch as smoke, tunnels, and adverse weather can significantly degrade the\nperformance of single-sensor systems like LiDAR or GPS, compromising the\noverall stability and safety of autonomous robots. To address these challenges,\nwe propose AF-RLIO: an adaptive fusion approach that integrates 4D\nmillimeter-wave radar, LiDAR, inertial measurement unit (IMU), and GPS to\nleverage the complementary strengths of these sensors for robust odometry\nestimation in complex environments. Our method consists of three key modules.\nFirstly, the pre-processing module utilizes radar data to assist LiDAR in\nremoving dynamic points and determining when environmental conditions are\ndegraded for LiDAR. Secondly, the dynamic-aware multimodal odometry selects\nappropriate point cloud data for scan-to-map matching and tightly couples it\nwith the IMU using the Iterative Error State Kalman Filter. Lastly, the factor\ngraph optimization module balances weights between odometry and GPS data,\nconstructing a pose graph for optimization. The proposed approach has been\nevaluated on datasets and tested in real-world robotic environments,\ndemonstrating its effectiveness and advantages over existing methods in\nchallenging conditions such as smoke and tunnels.\n","authors":["Chenglong Qian","Yang Xu","Xiufang Shi","Jiming Chen","Liang Li"],"pdf_url":"https://arxiv.org/pdf/2507.18317v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12193v2","updated":"2025-07-24T11:36:31Z","published":"2024-10-16T03:29:33Z","title":"Differentiable Motion Manifold Primitives for Reactive Motion Generation\n  under Kinodynamic Constraints","summary":"  Real-time motion generation -- which is essential for achieving reactive and\nadaptive behavior -- under kinodynamic constraints for high-dimensional systems\nis a crucial yet challenging problem. We address this with a two-step approach:\noffline learning of a lower-dimensional trajectory manifold of task-relevant,\nconstraint-satisfying trajectories, followed by rapid online search within this\nmanifold. Extending the discrete-time Motion Manifold Primitives (MMP)\nframework, we propose Differentiable Motion Manifold Primitives (DMMP), a novel\nneural network architecture that encodes and generates continuous-time,\ndifferentiable trajectories, trained using data collected offline through\ntrajectory optimizations, with a strategy that ensures constraint satisfaction\n-- absent in existing methods. Experiments on dynamic throwing with a 7-DoF\nrobot arm demonstrate that DMMP outperforms prior methods in planning speed,\ntask success, and constraint satisfaction.\n","authors":["Yonghyeon Lee"],"pdf_url":"https://arxiv.org/pdf/2410.12193v2.pdf","comment":"6 pages and 9 figures"},{"id":"http://arxiv.org/abs/2507.17596v2","updated":"2025-07-24T11:04:42Z","published":"2025-07-23T15:28:23Z","title":"PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving","summary":"  While end-to-end autonomous driving models show promising results, their\npractical deployment is often hindered by large model sizes, a reliance on\nexpensive LiDAR sensors and computationally intensive BEV feature\nrepresentations. This limits their scalability, especially for mass-market\nvehicles equipped only with cameras. To address these challenges, we propose\nPRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving\narchitecture operates using only camera data, without explicit BEV\nrepresentation and forgoing the need for LiDAR. PRIX leverages a visual feature\nextractor coupled with a generative planning head to predict safe trajectories\nfrom raw pixel inputs directly. A core component of our architecture is the\nContext-aware Recalibration Transformer (CaRT), a novel module designed to\neffectively enhance multi-level visual features for more robust planning. We\ndemonstrate through comprehensive experiments that PRIX achieves\nstate-of-the-art performance on the NavSim and nuScenes benchmarks, matching\nthe capabilities of larger, multimodal diffusion planners while being\nsignificantly more efficient in terms of inference speed and model size, making\nit a practical solution for real-world deployment. Our work is open-source and\nthe code will be at https://maxiuw.github.io/prix.\n","authors":["Maciej K. Wozniak","Lianhang Liu","Yixi Cai","Patric Jensfelt"],"pdf_url":"https://arxiv.org/pdf/2507.17596v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2507.18276v1","updated":"2025-07-24T10:25:58Z","published":"2025-07-24T10:25:58Z","title":"Adaptive Articulated Object Manipulation On The Fly with Foundation\n  Model Reasoning and Part Grounding","summary":"  Articulated objects pose diverse manipulation challenges for robots. Since\ntheir internal structures are not directly observable, robots must adaptively\nexplore and refine actions to generate successful manipulation trajectories.\nWhile existing works have attempted cross-category generalization in adaptive\narticulated object manipulation, two major challenges persist: (1) the\ngeometric diversity of real-world articulated objects complicates visual\nperception and understanding, and (2) variations in object functions and\nmechanisms hinder the development of a unified adaptive manipulation strategy.\nTo address these challenges, we propose AdaRPG, a novel framework that\nleverages foundation models to extract object parts, which exhibit greater\nlocal geometric similarity than entire objects, thereby enhancing visual\naffordance generalization for functional primitive skills. To support this, we\nconstruct a part-level affordance annotation dataset to train the affordance\nmodel. Additionally, AdaRPG utilizes the common knowledge embedded in\nfoundation models to reason about complex mechanisms and generate high-level\ncontrol codes that invoke primitive skill functions based on part affordance\ninference. Simulation and real-world experiments demonstrate AdaRPG's strong\ngeneralization ability across novel articulated object categories.\n","authors":["Xiaojie Zhang","Yuanfei Wang","Ruihai Wu","Kunqi Xu","Yu Li","Liuyu Xiang","Hao Dong","Zhaofeng He"],"pdf_url":"https://arxiv.org/pdf/2507.18276v1.pdf","comment":"ICCV 2025"},{"id":"http://arxiv.org/abs/2505.20043v2","updated":"2025-07-24T10:23:00Z","published":"2025-05-26T14:28:13Z","title":"Target Tracking via LiDAR-RADAR Sensor Fusion for Autonomous Racing","summary":"  High Speed multi-vehicle Autonomous Racing will increase the safety and\nperformance of road-going Autonomous Vehicles. Precise vehicle detection and\ndynamics estimation from a moving platform is a key requirement for planning\nand executing complex autonomous overtaking maneuvers. To address this\nrequirement, we have developed a Latency-Aware EKF-based Multi Target Tracking\nalgorithm fusing LiDAR and RADAR measurements. The algorithm explots the\ndifferent sensor characteristics by explicitly integrating the Range Rate in\nthe EKF Measurement Function, as well as a-priori knowledge of the racetrack\nduring state prediction. It can handle Out-Of-Sequence Measurements via\nReprocessing using a double State and Measurement Buffer, ensuring sensor delay\ncompensation with no information loss. This algorithm has been implemented on\nTeam PoliMOVE's autonomous racecar, and was proved experimentally by completing\na number of fully autonomous overtaking maneuvers at speeds up to 275 km/h.\n","authors":["Marcello Cellina","Matteo Corno","Sergio Matteo Savaresi"],"pdf_url":"https://arxiv.org/pdf/2505.20043v2.pdf","comment":"IEEE Conference, 6 pages"},{"id":"http://arxiv.org/abs/2507.18262v1","updated":"2025-07-24T10:07:31Z","published":"2025-07-24T10:07:31Z","title":"ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic\n  Grounding for Generalizable Robotic Manipulation","summary":"  Semantics-driven 3D spatial constraints align highlevel semantic\nrepresentations with low-level action spaces, facilitating the unification of\ntask understanding and execution in robotic manipulation. The synergistic\nreasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation\nModels (VFMs) enables cross-modal 3D spatial constraint construction.\nNevertheless, existing methods have three key limitations: (1) coarse semantic\ngranularity in constraint modeling, (2) lack of real-time closed-loop planning,\n(3) compromised robustness in semantically diverse environments. To address\nthese challenges, we propose ReSem3D, a unified manipulation framework for\nsemantically diverse environments, leveraging the synergy between VFMs and\nMLLMs to achieve fine-grained visual grounding and dynamically constructs\nhierarchical 3D spatial constraints for real-time manipulation. Specifically,\nthe framework is driven by hierarchical recursive reasoning in MLLMs, which\ninteract with VFMs to automatically construct 3D spatial constraints from\nnatural language instructions and RGB-D observations in two stages: part-level\nextraction and region-level refinement. Subsequently, these constraints are\nencoded as real-time optimization objectives in joint space, enabling reactive\nbehavior to dynamic disturbances. Extensive simulation and real-world\nexperiments are conducted in semantically rich household and sparse chemical\nlab environments. The results demonstrate that ReSem3D performs diverse\nmanipulation tasks under zero-shot conditions, exhibiting strong adaptability\nand generalization. Code and videos at https://resem3d.github.io.\n","authors":["Chenyu Su","Weiwei Shang","Chen Qian","Fei Zhang","Shuang Cong"],"pdf_url":"https://arxiv.org/pdf/2507.18262v1.pdf","comment":"12 pages,9 figures"},{"id":"http://arxiv.org/abs/2507.18248v1","updated":"2025-07-24T09:40:47Z","published":"2025-07-24T09:40:47Z","title":"Evaluation of facial landmark localization performance in a surgical\n  setting","summary":"  The use of robotics, computer vision, and their applications is becoming\nincreasingly widespread in various fields, including medicine. Many face\ndetection algorithms have found applications in neurosurgery, ophthalmology,\nand plastic surgery. A common challenge in using these algorithms is variable\nlighting conditions and the flexibility of detection positions to identify and\nprecisely localize patients. The proposed experiment tests the MediaPipe\nalgorithm for detecting facial landmarks in a controlled setting, using a\nrobotic arm that automatically adjusts positions while the surgical light and\nthe phantom remain in a fixed position. The results of this study demonstrate\nthat the improved accuracy of facial landmark detection under surgical lighting\nsignificantly enhances the detection performance at larger yaw and pitch\nangles. The increase in standard deviation/dispersion occurs due to imprecise\ndetection of selected facial landmarks. This analysis allows for a discussion\non the potential integration of the MediaPipe algorithm into medical\nprocedures.\n","authors":["Ines Frajtag","Marko Švaco","Filip Šuligoj"],"pdf_url":"https://arxiv.org/pdf/2507.18248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.16068v2","updated":"2025-07-24T09:25:12Z","published":"2025-07-21T21:09:15Z","title":"Compositional Coordination for Multi-Robot Teams with Large Language\n  Models","summary":"  Multi-robot coordination has traditionally relied on a mission-specific and\nexpert-driven pipeline, where natural language mission descriptions are\nmanually translated by domain experts into mathematical formulation, algorithm\ndesign, and executable code. This conventional process is labor-intensive,\ninaccessible to non-experts, and inflexible to changes in mission requirements.\nHere, we propose LAN2CB (Language to Collective Behavior), a novel framework\nthat leverages large language models (LLMs) to streamline and generalize the\nmulti-robot coordination pipeline. LAN2CB transforms natural language (NL)\nmission descriptions into executable Python code for multi-robot systems\nthrough two core modules: (1) Mission Analysis, which parses mission\ndescriptions into behavior trees, and (2) Code Generation, which leverages the\nbehavior tree and a structured knowledge base to generate robot control code.\nWe further introduce a dataset of natural language mission descriptions to\nsupport development and benchmarking. Experiments in both simulation and\nreal-world environments demonstrate that LAN2CB enables robust and flexible\nmulti-robot coordination from natural language, significantly reducing manual\nengineering effort and supporting broad generalization across diverse mission\ntypes. Website: https://sites.google.com/view/lan-cb\n","authors":["Zhehui Huang","Guangyao Shi","Yuwei Wu","Vijay Kumar","Gaurav S. Sukhatme"],"pdf_url":"https://arxiv.org/pdf/2507.16068v2.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2507.18206v1","updated":"2025-07-24T09:02:13Z","published":"2025-07-24T09:02:13Z","title":"MoRPI-PINN: A Physics-Informed Framework for Mobile Robot Pure Inertial\n  Navigation","summary":"  A fundamental requirement for full autonomy in mobile robots is accurate\nnavigation even in situations where satellite navigation or cameras are\nunavailable. In such practical situations, relying only on inertial sensors\nwill result in navigation solution drift due to the sensors' inherent noise and\nerror terms. One of the emerging solutions to mitigate drift is to maneuver the\nrobot in a snake-like slithering motion to increase the inertial\nsignal-to-noise ratio, allowing the regression of the mobile robot position. In\nthis work, we propose MoRPI-PINN as a physics-informed neural network framework\nfor accurate inertial-based mobile robot navigation. By embedding physical laws\nand constraints into the training process, MoRPI-PINN is capable of providing\nan accurate and robust navigation solution. Using real-world experiments, we\nshow accuracy improvements of over 85% compared to other approaches. MoRPI-PINN\nis a lightweight approach that can be implemented even on edge devices and used\nin any typical mobile robot application.\n","authors":["Arup Kumar Sahoo","Itzik Klein"],"pdf_url":"https://arxiv.org/pdf/2507.18206v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2507.18160v1","updated":"2025-07-24T07:54:45Z","published":"2025-07-24T07:54:45Z","title":"Autonomous UAV Navigation for Search and Rescue Missions Using Computer\n  Vision and Convolutional Neural Networks","summary":"  In this paper, we present a subsystem, using Unmanned Aerial Vehicles (UAV),\nfor search and rescue missions, focusing on people detection, face recognition\nand tracking of identified individuals. The proposed solution integrates a UAV\nwith ROS2 framework, that utilizes multiple convolutional neural networks (CNN)\nfor search missions. System identification and PD controller deployment are\nperformed for autonomous UAV navigation. The ROS2 environment utilizes the\nYOLOv11 and YOLOv11-pose CNNs for tracking purposes, and the dlib library CNN\nfor face recognition. The system detects a specific individual, performs face\nrecognition and starts tracking. If the individual is not yet known, the UAV\noperator can manually locate the person, save their facial image and\nimmediately initiate the tracking process. The tracking process relies on\nspecific keypoints identified on the human body using the YOLOv11-pose CNN\nmodel. These keypoints are used to track a specific individual and maintain a\nsafe distance. To enhance accurate tracking, system identification is\nperformed, based on measurement data from the UAVs IMU. The identified system\nparameters are used to design PD controllers that utilize YOLOv11-pose to\nestimate the distance between the UAVs camera and the identified individual.\nThe initial experiments, conducted on 14 known individuals, demonstrated that\nthe proposed subsystem can be successfully used in real time. The next step\ninvolves implementing the system on a large experimental UAV for field use and\nintegrating autonomous navigation with GPS-guided control for rescue operations\nplanning.\n","authors":["Luka Šiktar","Branimir Ćaran","Bojan Šekoranja","Marko Švaco"],"pdf_url":"https://arxiv.org/pdf/2507.18160v1.pdf","comment":"The paper is accepted and presented on the 34th International\n  Conference on Robotics in Alpe-Adria-Danube Region, RAAD 2025, Belgrade\n  Serbia"},{"id":"http://arxiv.org/abs/2507.17519v2","updated":"2025-07-24T07:45:43Z","published":"2025-07-23T13:55:37Z","title":"Terrain-Aware Adaptation for Two-Dimensional UAV Path Planners","summary":"  Multi-UAV Coverage Path Planning (mCPP) algorithms in popular commercial\nsoftware typically treat a Region of Interest (RoI) only as a 2D plane,\nignoring important3D structure characteristics. This leads to incomplete\n3Dreconstructions, especially around occluded or vertical surfaces. In this\npaper, we propose a modular algorithm that can extend commercial\ntwo-dimensional path planners to facilitate terrain-aware planning by adjusting\naltitude and camera orientations. To demonstrate it, we extend the well-known\nDARP (Divide Areas for Optimal Multi-Robot Coverage Path Planning) algorithm\nand produce DARP-3D. We present simulation results in multiple 3D environments\nand a real-world flight test using DJI hardware. Compared to baseline, our\napproach consistently captures improved 3D reconstructions, particularly in\nareas with significant vertical features. An open-source implementation of the\nalgorithm is available here:https://github.com/konskara/TerraPlan\n","authors":["Kostas Karakontis","Thanos Petsanis","Athanasios Ch. Kapoutsis","Pavlos Ch. Kapoutsis","Elias B. Kosmatopoulos"],"pdf_url":"https://arxiv.org/pdf/2507.17519v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18138v1","updated":"2025-07-24T07:00:20Z","published":"2025-07-24T07:00:20Z","title":"A Modular Residual Learning Framework to Enhance Model-Based Approach\n  for Robust Locomotion","summary":"  This paper presents a novel approach that combines the advantages of both\nmodel-based and learning-based frameworks to achieve robust locomotion. The\nresidual modules are integrated with each corresponding part of the model-based\nframework, a footstep planner and dynamic model designed using heuristics, to\ncomplement performance degradation caused by a model mismatch. By utilizing a\nmodular structure and selecting the appropriate learning-based method for each\nresidual module, our framework demonstrates improved control performance in\nenvironments with high uncertainty, while also achieving higher learning\nefficiency compared to baseline methods. Moreover, we observed that our\nproposed methodology not only enhances control performance but also provides\nadditional benefits, such as making nominal controllers more robust to\nparameter tuning. To investigate the feasibility of our framework, we\ndemonstrated residual modules combined with model predictive control in a real\nquadrupedal robot. Despite uncertainties beyond the simulation, the robot\nsuccessfully maintains balance and tracks the commanded velocity.\n","authors":["Min-Gyu Kim","Dongyun Kang","Hajun Kim","Hae-Won Park"],"pdf_url":"https://arxiv.org/pdf/2507.18138v1.pdf","comment":"8 pages, IEEE RA-L accepted (July 2025)"},{"id":"http://arxiv.org/abs/2502.00352v2","updated":"2025-07-24T06:12:24Z","published":"2025-02-01T07:16:15Z","title":"A Differentiated Reward Method for Reinforcement Learning based\n  Multi-Vehicle Cooperative Decision-Making Algorithms","summary":"  Reinforcement learning (RL) shows great potential for optimizing\nmulti-vehicle cooperative driving strategies through the state-action-reward\nfeedback loop, but it still faces challenges such as low sample efficiency.\nThis paper proposes a differentiated reward method based on steady-state\ntransition systems, which incorporates state transition gradient information\ninto the reward design by analyzing traffic flow characteristics, aiming to\noptimize action selection and policy learning in multi-vehicle cooperative\ndecision-making. The performance of the proposed method is validated in RL\nalgorithms such as MAPPO, MADQN, and QMIX under varying autonomous vehicle\npenetration. The results show that the differentiated reward method\nsignificantly accelerates training convergence and outperforms centering reward\nand others in terms of traffic efficiency, safety, and action rationality.\nAdditionally, the method demonstrates strong scalability and environmental\nadaptability, providing a novel approach for multi-agent cooperative\ndecision-making in complex traffic scenarios.\n","authors":["Ye Han","Lijun Zhang","Dejian Meng","Zhuang Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.00352v2.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2503.06736v2","updated":"2025-07-24T06:00:33Z","published":"2025-03-09T19:29:15Z","title":"Safe, Task-Consistent Manipulation with Operational Space Control\n  Barrier Functions","summary":"  Safe real-time control of robotic manipulators in unstructured environments\nrequires handling numerous safety constraints without compromising task\nperformance. Traditional approaches, such as artificial potential fields\n(APFs), suffer from local minima, oscillations, and limited scalability, while\nmodel predictive control (MPC) can be computationally expensive. Control\nbarrier functions (CBFs) offer a promising alternative due to their high level\nof robustness and low computational cost, but these safety filters must be\ncarefully designed to avoid significant reductions in the overall performance\nof the manipulator. In this work, we introduce an Operational Space Control\nBarrier Function (OSCBF) framework that integrates safety constraints while\npreserving task-consistent behavior. Our approach scales to hundreds of\nsimultaneous constraints while retaining real-time control rates, ensuring\ncollision avoidance, singularity prevention, and workspace containment even in\nhighly cluttered settings or during dynamic motions. By explicitly accounting\nfor the task hierarchy in the CBF objective, we prevent degraded performance\nacross both joint-space and operational-space tasks, when at the limit of\nsafety. We validate performance in both simulation and hardware, and release\nour open-source high-performance code and media on our project webpage,\nhttps://stanfordasl.github.io/oscbf/\n","authors":["Daniel Morton","Marco Pavone"],"pdf_url":"https://arxiv.org/pdf/2503.06736v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.16685v2","updated":"2025-07-24T04:54:15Z","published":"2025-06-20T01:57:47Z","title":"Compliant Residual DAgger: Improving Real-World Contact-Rich\n  Manipulation with Human Corrections","summary":"  We address key challenges in Dataset Aggregation (DAgger) for real-world\ncontact-rich manipulation: how to collect informative human correction data and\nhow to effectively update policies with this new data. We introduce Compliant\nResidual DAgger (CR-DAgger), which contains two novel components: 1) a\nCompliant Intervention Interface that leverages compliance control, allowing\nhumans to provide gentle, accurate delta action corrections without\ninterrupting the ongoing robot policy execution; and 2) a Compliant Residual\nPolicy formulation that learns from human corrections while incorporating force\nfeedback and force control. Our system significantly enhances performance on\nprecise contact-rich manipulation tasks using minimal correction data,\nimproving base policy success rates by over 50\\% on two challenging tasks (book\nflipping and belt assembly) while outperforming both retraining-from-scratch\nand finetuning approaches. Through extensive real-world experiments, we provide\npractical guidance for implementing effective DAgger in real-world robot\nlearning tasks. Result videos are available at:\nhttps://compliant-residual-dagger.github.io/\n","authors":["Xiaomeng Xu","Yifan Hou","Zeyi Liu","Shuran Song"],"pdf_url":"https://arxiv.org/pdf/2506.16685v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10055v2","updated":"2025-07-24T04:25:41Z","published":"2025-07-14T08:40:24Z","title":"Hand Gesture Recognition for Collaborative Robots Using Lightweight Deep\n  Learning in Real-Time Robotic Systems","summary":"  Direct and natural interaction is essential for intuitive human-robot\ncollaboration, eliminating the need for additional devices such as joysticks,\ntablets, or wearable sensors. In this paper, we present a lightweight deep\nlearning-based hand gesture recognition system that enables humans to control\ncollaborative robots naturally and efficiently. This model recognizes eight\ndistinct hand gestures with only 1,103 parameters and a compact size of 22 KB,\nachieving an accuracy of 93.5%. To further optimize the model for real-world\ndeployment on edge devices, we applied quantization and pruning using\nTensorFlow Lite, reducing the final model size to just 7 KB. The system was\nsuccessfully implemented and tested on a Universal Robot UR5 collaborative\nrobot within a real-time robotic framework based on ROS2. The results\ndemonstrate that even extremely lightweight models can deliver accurate and\nresponsive hand gesture-based control for collaborative robots, opening new\npossibilities for natural human-robot interaction in constrained environments.\n","authors":[" Muhtadin","I Wayan Agus Darmawan","Muhammad Hilmi Rusydiansyah","I Ketut Eddy Purnama","Chastine Fatichah","Mauridhi Hery Purnomo"],"pdf_url":"https://arxiv.org/pdf/2507.10055v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.16214v2","updated":"2025-07-24T04:02:42Z","published":"2025-07-22T04:13:03Z","title":"Adaptive Relative Pose Estimation Framework with Dual Noise Tuning for\n  Safe Approaching Maneuvers","summary":"  Accurate and robust relative pose estimation is crucial for enabling\nchallenging Active Debris Removal (ADR) missions targeting tumbling derelict\nsatellites such as ESA's ENVISAT. This work presents a complete pipeline\nintegrating advanced computer vision techniques with adaptive nonlinear\nfiltering to address this challenge. A Convolutional Neural Network (CNN),\nenhanced with image preprocessing, detects structural markers (corners) from\nchaser imagery, whose 2D coordinates are converted to 3D measurements using\ncamera modeling. These measurements are fused within an Unscented Kalman Filter\n(UKF) framework, selected for its ability to handle nonlinear relative\ndynamics, to estimate the full relative pose. Key contributions include the\nintegrated system architecture and a dual adaptive strategy within the UKF:\ndynamic tuning of the measurement noise covariance compensates for varying CNN\nmeasurement uncertainty, while adaptive tuning of the process noise covariance,\nutilizing measurement residual analysis, accounts for unmodeled dynamics or\nmaneuvers online. This dual adaptation enhances robustness against both\nmeasurement imperfections and dynamic model uncertainties. The performance of\nthe proposed adaptive integrated system is evaluated through high-fidelity\nsimulations using a realistic ENVISAT model, comparing estimates against ground\ntruth under various conditions, including measurement outages. This\ncomprehensive approach offers an enhanced solution for robust onboard relative\nnavigation, significantly advancing the capabilities required for safe\nproximity operations during ADR missions.\n","authors":["Batu Candan","Simone Servadio"],"pdf_url":"https://arxiv.org/pdf/2507.16214v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.02250v2","updated":"2025-07-24T04:00:35Z","published":"2025-04-03T03:38:13Z","title":"Designing Effective Human-Swarm Interaction Interfaces: Insights from a\n  User Study on Task Performance","summary":"  In this paper, we present a systematic method of design for human-swarm\ninteraction interfaces, combining theoretical insights with empirical\nevaluation. We first derived ten design principles from existing literature,\napplying them to key information dimensions identified through goal-directed\ntask analysis and developed a tablet-based interface for a target search task.\nWe then conducted a user study with 31 participants where humans were required\nto guide a robotic swarm to a target in the presence of three types of hazards\nthat pose a risk to the robots: Distributed, Moving, and Spreading. Performance\nwas measured based on the proximity of the robots to the target and the number\nof deactivated robots at the end of the task. Results indicate that at least\none robot was brought closer to the target in 98% of tasks, demonstrating the\ninterface's success in fulfilling the primary objective of the task.\nAdditionally, in nearly 67% of tasks, more than 50% of the robots reached the\ntarget. Moreover, particularly better performance was noted in moving hazards.\nAdditionally, the interface appeared to help minimise robot deactivation, as\nevidenced by nearly 94% of tasks where participants managed to keep more than\n50% of the robots active, ensuring that most of the swarm remained operational.\nHowever, its effectiveness varied across hazards, with robot deactivation being\nlowest in distributed hazard scenarios, suggesting that the interface provided\nthe most support in these conditions.\n","authors":["Wasura D. Wattearachchi","Erandi Lakshika","Kathryn Kasmarik","Michael Barlow"],"pdf_url":"https://arxiv.org/pdf/2504.02250v2.pdf","comment":"8 pages, 4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2507.18070v1","updated":"2025-07-24T03:49:43Z","published":"2025-07-24T03:49:43Z","title":"Modular Robot and Landmark Localisation Using Relative Bearing\n  Measurements","summary":"  In this paper we propose a modular nonlinear least squares filtering approach\nfor systems composed of independent subsystems. The state and error covariance\nestimate of each subsystem is updated independently, even when a relative\nmeasurement simultaneously depends on the states of multiple subsystems. We\nintegrate the Covariance Intersection (CI) algorithm as part of our solution in\norder to prevent double counting of information when subsystems share estimates\nwith each other. An alternative derivation of the CI algorithm based on least\nsquares estimation makes this integration possible. We particularise the\nproposed approach to the robot-landmark localization problem. In this problem,\nnoisy measurements of the bearing angle to a stationary landmark position\nmeasured relative to the SE(2) pose of a moving robot couple the estimation\nproblems for the robot pose and the landmark position. In a randomized\nsimulation study, we benchmark the proposed modular method against a monolithic\njoint state filter to elucidate their respective trade-offs. In this study we\nalso include variants of the proposed method that achieve a graceful\ndegradation of performance with reduced communication and bandwidth\nrequirements.\n","authors":["Behzad Zamani","Jochen Trumpf","Chris Manzie"],"pdf_url":"https://arxiv.org/pdf/2507.18070v1.pdf","comment":"Submitted to RA-L"},{"id":"http://arxiv.org/abs/2507.18033v1","updated":"2025-07-24T02:05:28Z","published":"2025-07-24T02:05:28Z","title":"OpenNav: Open-World Navigation with Multimodal Large Language Models","summary":"  Pre-trained large language models (LLMs) have demonstrated strong\ncommon-sense reasoning abilities, making them promising for robotic navigation\nand planning tasks. However, despite recent progress, bridging the gap between\nlanguage descriptions and actual robot actions in the open-world, beyond merely\ninvoking limited predefined motion primitives, remains an open challenge. In\nthis work, we aim to enable robots to interpret and decompose complex language\ninstructions, ultimately synthesizing a sequence of trajectory points to\ncomplete diverse navigation tasks given open-set instructions and open-set\nobjects. We observe that multi-modal large language models (MLLMs) exhibit\nstrong cross-modal understanding when processing free-form language\ninstructions, demonstrating robust scene comprehension. More importantly,\nleveraging their code-generation capability, MLLMs can interact with\nvision-language perception models to generate compositional 2D bird-eye-view\nvalue maps, effectively integrating semantic knowledge from MLLMs with spatial\ninformation from maps to reinforce the robot's spatial understanding. To\nfurther validate our approach, we effectively leverage large-scale autonomous\nvehicle datasets (AVDs) to validate our proposed zero-shot vision-language\nnavigation framework in outdoor navigation tasks, demonstrating its capability\nto execute a diverse range of free-form natural language navigation\ninstructions while maintaining robustness against object detection errors and\nlinguistic ambiguities. Furthermore, we validate our system on a Husky robot in\nboth indoor and outdoor scenes, demonstrating its real-world robustness and\napplicability. Supplementary videos are available at\nhttps://trailab.github.io/OpenNav-website/\n","authors":["Mingfeng Yuan","Letian Wang","Steven L. Waslander"],"pdf_url":"https://arxiv.org/pdf/2507.18033v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06174v5","updated":"2025-07-24T00:40:26Z","published":"2025-07-08T16:54:34Z","title":"Fast Bilateral Teleoperation and Imitation Learning Using Sensorless\n  Force Control via Accurate Dynamics Model","summary":"  In recent years, the advancement of imitation learning has led to increased\ninterest in teleoperating low-cost manipulators to collect demonstration data.\nHowever, most existing systems rely on unilateral control, which only transmits\ntarget position values. While this approach is easy to implement and suitable\nfor slow, non-contact tasks, it struggles with fast or contact-rich operations\ndue to the absence of force feedback. This work demonstrates that fast\nteleoperation with force feedback is feasible even with force-sensorless,\nlow-cost manipulators by leveraging 4-channel bilateral control. Based on\naccurately identified manipulator dynamics, our method integrates nonlinear\nterms compensation, velocity and external force estimation, and variable gain\ncorresponding to inertial variation. Furthermore, using data collected by\n4-channel bilateral control, we show that incorporating force information into\nboth the input and output of learned policies improves performance in imitation\nlearning. These results highlight the practical effectiveness of our system for\nhigh-fidelity teleoperation and data collection on affordable hardware.\n","authors":["Koki Yamane","Yunhan Li","Masashi Konosu","Koki Inami","Junji Oaki","Sho Sakaino","Toshiaki Tsuji"],"pdf_url":"https://arxiv.org/pdf/2507.06174v5.pdf","comment":"20 pages, 9 figures, Submitted to CoRL 2025"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2507.18634v1","updated":"2025-07-24T17:59:56Z","published":"2025-07-24T17:59:56Z","title":"Captain Cinema: Towards Short Movie Generation","summary":"  We present Captain Cinema, a generation framework for short movie generation.\nGiven a detailed textual description of a movie storyline, our approach firstly\ngenerates a sequence of keyframes that outline the entire narrative, which\nensures long-range coherence in both the storyline and visual appearance (e.g.,\nscenes and characters). We refer to this step as top-down keyframe planning.\nThese keyframes then serve as conditioning signals for a video synthesis model,\nwhich supports long context learning, to produce the spatio-temporal dynamics\nbetween them. This step is referred to as bottom-up video synthesis. To support\nstable and efficient generation of multi-scene long narrative cinematic works,\nwe introduce an interleaved training strategy for Multimodal Diffusion\nTransformers (MM-DiT), specifically adapted for long-context video data. Our\nmodel is trained on a specially curated cinematic dataset consisting of\ninterleaved data pairs. Our experiments demonstrate that Captain Cinema\nperforms favorably in the automated creation of visually coherent and narrative\nconsistent short movies in high quality and efficiency. Project page:\nhttps://thecinema.ai\n","authors":["Junfei Xiao","Ceyuan Yang","Lvmin Zhang","Shengqu Cai","Yang Zhao","Yuwei Guo","Gordon Wetzstein","Maneesh Agrawala","Alan Yuille","Lu Jiang"],"pdf_url":"https://arxiv.org/pdf/2507.18634v1.pdf","comment":"Under review. Project page: https://thecinema.ai"},{"id":"http://arxiv.org/abs/2507.18633v1","updated":"2025-07-24T17:59:44Z","published":"2025-07-24T17:59:44Z","title":"Identifying Prompted Artist Names from Generated Images","summary":"  A common and controversial use of text-to-image models is to generate\npictures by explicitly naming artists, such as \"in the style of Greg\nRutkowski\". We introduce a benchmark for prompted-artist recognition:\npredicting which artist names were invoked in the prompt from the image alone.\nThe dataset contains 1.95M images covering 110 artists and spans four\ngeneralization settings: held-out artists, increasing prompt complexity,\nmultiple-artist prompts, and different text-to-image models. We evaluate\nfeature similarity baselines, contrastive style descriptors, data attribution\nmethods, supervised classifiers, and few-shot prototypical networks.\nGeneralization patterns vary: supervised and few-shot models excel on seen\nartists and complex prompts, whereas style descriptors transfer better when the\nartist's style is pronounced; multi-artist prompts remain the most challenging.\nOur benchmark reveals substantial headroom and provides a public testbed to\nadvance the responsible moderation of text-to-image models. We release the\ndataset and benchmark to foster further research:\nhttps://graceduansu.github.io/IdentifyingPromptedArtists/\n","authors":["Grace Su","Sheng-Yu Wang","Aaron Hertzmann","Eli Shechtman","Jun-Yan Zhu","Richard Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.18633v1.pdf","comment":"Project page:\n  https://graceduansu.github.io/IdentifyingPromptedArtists"},{"id":"http://arxiv.org/abs/2507.18632v1","updated":"2025-07-24T17:59:36Z","published":"2025-07-24T17:59:36Z","title":"SIDA: Synthetic Image Driven Zero-shot Domain Adaptation","summary":"  Zero-shot domain adaptation is a method for adapting a model to a target\ndomain without utilizing target domain image data. To enable adaptation without\ntarget images, existing studies utilize CLIP's embedding space and text\ndescription to simulate target-like style features. Despite the previous\nachievements in zero-shot domain adaptation, we observe that these text-driven\nmethods struggle to capture complex real-world variations and significantly\nincrease adaptation time due to their alignment process. Instead of relying on\ntext descriptions, we explore solutions leveraging image data, which provides\ndiverse and more fine-grained style cues. In this work, we propose SIDA, a\nnovel and efficient zero-shot domain adaptation method leveraging synthetic\nimages. To generate synthetic images, we first create detailed, source-like\nimages and apply image translation to reflect the style of the target domain.\nWe then utilize the style features of these synthetic images as a proxy for the\ntarget domain. Based on these features, we introduce Domain Mix and Patch Style\nTransfer modules, which enable effective modeling of real-world variations. In\nparticular, Domain Mix blends multiple styles to expand the intra-domain\nrepresentations, and Patch Style Transfer assigns different styles to\nindividual patches. We demonstrate the effectiveness of our method by showing\nstate-of-the-art performance in diverse zero-shot adaptation scenarios,\nparticularly in challenging domains. Moreover, our approach achieves high\nefficiency by significantly reducing the overall adaptation time.\n","authors":["Ye-Chan Kim","SeungJu Cha","Si-Woo Kim","Taewhan Kim","Dong-Jin Kim"],"pdf_url":"https://arxiv.org/pdf/2507.18632v1.pdf","comment":"Accepted to ACM MM 2025"},{"id":"http://arxiv.org/abs/2507.18625v1","updated":"2025-07-24T17:58:03Z","published":"2025-07-24T17:58:03Z","title":"3D Software Synthesis Guided by Constraint-Expressive Intermediate\n  Representation","summary":"  Graphical user interface (UI) software has undergone a fundamental\ntransformation from traditional two-dimensional (2D) desktop/web/mobile\ninterfaces to spatial three-dimensional (3D) environments. While existing work\nhas made remarkable success in automated 2D software generation, such as\nHTML/CSS and mobile app interface code synthesis, the generation of 3D software\nstill remains under-explored. Current methods for 3D software generation\nusually generate the 3D environments as a whole and cannot modify or control\nspecific elements in the software. Furthermore, these methods struggle to\nhandle the complex spatial and semantic constraints inherent in the real world.\nTo address the challenges, we present Scenethesis, a novel\nrequirement-sensitive 3D software synthesis approach that maintains formal\ntraceability between user specifications and generated 3D software. Scenethesis\nis built upon ScenethesisLang, a domain-specific language that serves as a\ngranular constraint-aware intermediate representation (IR) to bridge natural\nlanguage requirements and executable 3D software. It serves both as a\ncomprehensive scene description language enabling fine-grained modification of\n3D software elements and as a formal constraint-expressive specification\nlanguage capable of expressing complex spatial constraints. By decomposing 3D\nsoftware synthesis into stages operating on ScenethesisLang, Scenethesis\nenables independent verification, targeted modification, and systematic\nconstraint satisfaction. Our evaluation demonstrates that Scenethesis\naccurately captures over 80% of user requirements and satisfies more than 90%\nof hard constraints while handling over 100 constraints simultaneously.\nFurthermore, Scenethesis achieves a 42.8% improvement in BLIP-2 visual\nevaluation scores compared to the state-of-the-art method.\n","authors":["Shuqing Li","Anson Y. Lam","Yun Peng","Wenxuan Wang","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2507.18625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.15857v2","updated":"2025-07-24T17:55:24Z","published":"2025-07-21T17:59:57Z","title":"Diffusion Beats Autoregressive in Data-Constrained Settings","summary":"  Autoregressive (AR) models have long dominated the landscape of large\nlanguage models, driving progress across a wide range of tasks. Recently,\ndiffusion-based language models have emerged as a promising alternative, though\ntheir advantages over AR models remain underexplored. In this paper, we\nsystematically study masked diffusion models in data-constrained settings-where\ntraining involves repeated passes over limited data-and find that they\nsignificantly outperform AR models when compute is abundant but data is scarce.\nDiffusion models make better use of repeated data, achieving lower validation\nloss and superior downstream performance. We interpret this advantage as\nimplicit data augmentation: masked diffusion exposes the model to a diverse\ndistribution of token orderings and prediction tasks, unlike AR's fixed\nleft-to-right factorization. We find new scaling laws for diffusion models and\nderive a closed-form expression for the critical compute threshold at which\ndiffusion begins to outperform AR. These results suggest that when data, not\ncompute, is the bottleneck, diffusion models offer a compelling alternative to\nthe standard AR paradigm. Our code is available at:\nhttps://diffusion-scaling.github.io.\n","authors":["Mihir Prabhudesai","Menging Wu","Amir Zadeh","Katerina Fragkiadaki","Deepak Pathak"],"pdf_url":"https://arxiv.org/pdf/2507.15857v2.pdf","comment":"Project Webpage: https://diffusion-scaling.github.io"},{"id":"http://arxiv.org/abs/2507.18616v1","updated":"2025-07-24T17:53:26Z","published":"2025-07-24T17:53:26Z","title":"SynC: Synthetic Image Caption Dataset Refinement with One-to-many\n  Mapping for Zero-shot Image Captioning","summary":"  Zero-shot Image Captioning (ZIC) increasingly utilizes synthetic datasets\ngenerated by text-to-image (T2I) models to mitigate the need for costly manual\nannotation. However, these T2I models often produce images that exhibit\nsemantic misalignments with their corresponding input captions (e.g., missing\nobjects, incorrect attributes), resulting in noisy synthetic image-caption\npairs that can hinder model training. Existing dataset pruning techniques are\nlargely designed for removing noisy text in web-crawled data. However, these\nmethods are ill-suited for the distinct challenges of synthetic data, where\ncaptions are typically well-formed, but images may be inaccurate\nrepresentations. To address this gap, we introduce SynC, a novel framework\nspecifically designed to refine synthetic image-caption datasets for ZIC.\nInstead of conventional filtering or regeneration, SynC focuses on reassigning\ncaptions to the most semantically aligned images already present within the\nsynthetic image pool. Our approach employs a one-to-many mapping strategy by\ninitially retrieving multiple relevant candidate images for each caption. We\nthen apply a cycle-consistency-inspired alignment scorer that selects the best\nimage by verifying its ability to retrieve the original caption via\nimage-to-text retrieval. Extensive evaluations demonstrate that SynC\nconsistently and significantly improves performance across various ZIC models\non standard benchmarks (MS-COCO, Flickr30k, NoCaps), achieving state-of-the-art\nresults in several scenarios. SynC offers an effective strategy for curating\nrefined synthetic data to enhance ZIC.\n","authors":["Si-Woo Kim","MinJu Jeon","Ye-Chan Kim","Soeun Lee","Taewhan Kim","Dong-Jin Kim"],"pdf_url":"https://arxiv.org/pdf/2507.18616v1.pdf","comment":"Accepted to ACM Multimedia 2025"},{"id":"http://arxiv.org/abs/2506.03654v3","updated":"2025-07-24T17:28:09Z","published":"2025-06-04T07:46:24Z","title":"MambaNeXt-YOLO: A Hybrid State Space Model for Real-time Object\n  Detection","summary":"  Real-time object detection is a fundamental but challenging task in computer\nvision, particularly when computational resources are limited. Although\nYOLO-series models have set strong benchmarks by balancing speed and accuracy,\nthe increasing need for richer global context modeling has led to the use of\nTransformer-based architectures. Nevertheless, Transformers have high\ncomputational complexity because of their self-attention mechanism, which\nlimits their practicality for real-time and edge deployments. To overcome these\nchallenges, recent developments in linear state space models, such as Mamba,\nprovide a promising alternative by enabling efficient sequence modeling with\nlinear complexity. Building on this insight, we propose MambaNeXt-YOLO, a novel\nobject detection framework that balances accuracy and efficiency through three\nkey contributions: (1) MambaNeXt Block: a hybrid design that integrates CNNs\nwith Mamba to effectively capture both local features and long-range\ndependencies; (2) Multi-branch Asymmetric Fusion Pyramid Network (MAFPN): an\nenhanced feature pyramid architecture that improves multi-scale object\ndetection across various object sizes; and (3) Edge-focused Efficiency: our\nmethod achieved 66.6% mAP at 31.9 FPS on the PASCAL VOC dataset without any\npre-training and supports deployment on edge devices such as the NVIDIA Jetson\nXavier NX and Orin NX.\n","authors":["Xiaochun Lei","Siqi Wu","Weilin Wu","Zetao Jiang"],"pdf_url":"https://arxiv.org/pdf/2506.03654v3.pdf","comment":"This paper is under consideration at Image and Vision Computing"},{"id":"http://arxiv.org/abs/2507.18594v1","updated":"2025-07-24T17:24:59Z","published":"2025-07-24T17:24:59Z","title":"DRWKV: Focusing on Object Edges for Low-Light Image Enhancement","summary":"  Low-light image enhancement remains a challenging task, particularly in\npreserving object edge continuity and fine structural details under extreme\nillumination degradation. In this paper, we propose a novel model, DRWKV\n(Detailed Receptance Weighted Key Value), which integrates our proposed Global\nEdge Retinex (GER) theory, enabling effective decoupling of illumination and\nedge structures for enhanced edge fidelity. Secondly, we introduce Evolving WKV\nAttention, a spiral-scanning mechanism that captures spatial edge continuity\nand models irregular structures more effectively. Thirdly, we design the\nBilateral Spectrum Aligner (Bi-SAB) and a tailored MS2-Loss to jointly align\nluminance and chrominance features, improving visual naturalness and mitigating\nartifacts. Extensive experiments on five LLIE benchmarks demonstrate that DRWKV\nachieves leading performance in PSNR, SSIM, and NIQE while maintaining low\ncomputational complexity. Furthermore, DRWKV enhances downstream performance in\nlow-light multi-object tracking tasks, validating its generalization\ncapabilities.\n","authors":["Xuecheng Bai","Yuxiang Wang","Boyu Hu","Qinyuan Jie","Chuanzhi Xu","Hongru Xiao","Kechen Li","Vera Chung"],"pdf_url":"https://arxiv.org/pdf/2507.18594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07966v2","updated":"2025-07-24T17:20:41Z","published":"2025-07-10T17:47:40Z","title":"Scaling RL to Long Videos","summary":"  We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.0% and 70.7% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-R1 across multiple benchmarks. Moreover, LongVILA-R1\nshows steady performance improvements as the number of input video frames\nincreases. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames / around 256k tokens).\n","authors":["Yukang Chen","Wei Huang","Baifeng Shi","Qinghao Hu","Hanrong Ye","Ligeng Zhu","Zhijian Liu","Pavlo Molchanov","Jan Kautz","Xiaojuan Qi","Sifei Liu","Hongxu Yin","Yao Lu","Song Han"],"pdf_url":"https://arxiv.org/pdf/2507.07966v2.pdf","comment":"Code at https://github.com/NVlabs/Long-RL and model at\n  https://huggingface.co/Efficient-Large-Model/LongVILA-R1-7B"},{"id":"http://arxiv.org/abs/2505.09193v4","updated":"2025-07-24T16:57:30Z","published":"2025-05-14T06:55:37Z","title":"BiECVC: Gated Diversification of Bidirectional Contexts for Learned\n  Video Compression","summary":"  Recent forward prediction-based learned video compression (LVC) methods have\nachieved impressive results, even surpassing VVC reference software VTM under\nthe Low Delay B (LDB) configuration. In contrast, learned bidirectional video\ncompression (BVC) remains underexplored and still lags behind its forward-only\ncounterparts. This performance gap is mainly due to the limited ability to\nextract diverse and accurate contexts: most existing BVCs primarily exploit\ntemporal motion while neglecting non-local correlations across frames.\nMoreover, they lack the adaptability to dynamically suppress harmful contexts\narising from fast motion or occlusion. To tackle these challenges, we propose\nBiECVC, a BVC framework that incorporates diversified local and non-local\ncontext modeling along with adaptive context gating. For local context\nenhancement, BiECVC reuses high-quality features from lower layers and aligns\nthem using decoded motion vectors without introducing extra motion overhead. To\nmodel non-local dependencies efficiently, we adopt a linear attention mechanism\nthat balances performance and complexity. To further mitigate the impact of\ninaccurate context prediction, we introduce Bidirectional Context Gating,\ninspired by data-dependent decay in recent autoregressive language models, to\ndynamically filter contextual information based on conditional coding results.\nExtensive experiments demonstrate that BiECVC achieves state-of-the-art\nperformance, reducing the bit-rate by 13.4% and 15.7% compared to VTM 13.2\nunder the Random Access (RA) configuration with intra periods of 32 and 64,\nrespectively. To our knowledge, BiECVC is the first learned video codec to\nsurpass VTM 13.2 RA across all standard test datasets.\n","authors":["Wei Jiang","Junru Li","Kai Zhang","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.09193v4.pdf","comment":"Accepted to ACMMM 2025"},{"id":"http://arxiv.org/abs/2507.18576v1","updated":"2025-07-24T16:49:19Z","published":"2025-07-24T16:49:19Z","title":"SafeWork-R1: Coevolving Safety and Intelligence under the\n  AI-45$^{\\circ}$ Law","summary":"  We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that\ndemonstrates the coevolution of capabilities and safety. It is developed by our\nproposed SafeLadder framework, which incorporates large-scale, progressive,\nsafety-oriented reinforcement learning post-training, supported by a suite of\nmulti-principled verifiers. Unlike previous alignment methods such as RLHF that\nsimply learn human preferences, SafeLadder enables SafeWork-R1 to develop\nintrinsic safety reasoning and self-reflection abilities, giving rise to safety\n`aha' moments. Notably, SafeWork-R1 achieves an average improvement of\n$46.54\\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks\nwithout compromising general capabilities, and delivers state-of-the-art safety\nperformance compared to leading proprietary models such as GPT-4.1 and Claude\nOpus 4. To further bolster its reliability, we implement two distinct\ninference-time intervention methods and a deliberative search mechanism,\nenforcing step-level verification. Finally, we further develop\nSafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and\nSafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and\ncapability can co-evolve synergistically, highlighting the generalizability of\nour framework in building robust, reliable, and trustworthy general-purpose AI.\n","authors":["Shanghai AI Lab"," :","Yicheng Bao","Guanxu Chen","Mingkang Chen","Yunhao Chen","Chiyu Chen","Lingjie Chen","Sirui Chen","Xinquan Chen","Jie Cheng","Yu Cheng","Dengke Deng","Yizhuo Ding","Dan Ding","Xiaoshan Ding","Yi Ding","Zhichen Dong","Lingxiao Du","Yuyu Fan","Xinshun Feng","Yanwei Fu","Yuxuan Gao","Ruijun Ge","Tianle Gu","Lujun Gui","Jiaxuan Guo","Qianxi He","Yuenan Hou","Xuhao Hu","Hong Huang","Kaichen Huang","Shiyang Huang","Yuxian Jiang","Shanzhe Lei","Jie Li","Lijun Li","Hao Li","Juncheng Li","Xiangtian Li","Yafu Li","Lingyu Li","Xueyan Li","Haotian Liang","Dongrui Liu","Qihua Liu","Zhixuan Liu","Bangwei Liu","Huacan Liu","Yuexiao Liu","Zongkai Liu","Chaochao Lu","Yudong Lu","Xiaoya Lu","Zhenghao Lu","Qitan Lv","Caoyuan Ma","Jiachen Ma","Xiaoya Ma","Zhongtian Ma","Lingyu Meng","Ziqi Miao","Yazhe Niu","Yuezhang Peng","Yuan Pu","Han Qi","Chen Qian","Xingge Qiao","Jingjing Qu","Jiashu Qu","Wanying Qu","Wenwen Qu","Xiaoye Qu","Qihan Ren","Qingnan Ren","Qingyu Ren","Jing Shao","Wenqi Shao","Shuai Shao","Dongxing Shi","Xin Song","Xinhao Song","Yan Teng","Xuan Tong","Yingchun Wang","Xuhong Wang","Shujie Wang","Xin Wang","Yige Wang","Yixu Wang","Yuanfu Wang","Futing Wang","Ruofan Wang","Wenjie Wang","Yajie Wang","Muhao Wei","Xiaoyu Wen","Fenghua Weng","Yuqi Wu","Yingtong Xiong","Xingcheng Xu","Chao Yang","Yue Yang","Yang Yao","Yulei Ye","Zhenyun Yin","Yi Yu","Bo Zhang","Qiaosheng Zhang","Jinxuan Zhang","Yexin Zhang","Yinqiang Zheng","Hefeng Zhou","Zhanhui Zhou","Pengyu Zhu","Qingzi Zhu","Yubo Zhu","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2507.18576v1.pdf","comment":"47 pages, 18 figures, authors are listed in alphabetical order by\n  their last names"},{"id":"http://arxiv.org/abs/2507.18575v1","updated":"2025-07-24T16:48:50Z","published":"2025-07-24T16:48:50Z","title":"HybridTM: Combining Transformer and Mamba for 3D Semantic Segmentation","summary":"  Transformer-based methods have demonstrated remarkable capabilities in 3D\nsemantic segmentation through their powerful attention mechanisms, but the\nquadratic complexity limits their modeling of long-range dependencies in\nlarge-scale point clouds. While recent Mamba-based approaches offer efficient\nprocessing with linear complexity, they struggle with feature representation\nwhen extracting 3D features. However, effectively combining these complementary\nstrengths remains an open challenge in this field. In this paper, we propose\nHybridTM, the first hybrid architecture that integrates Transformer and Mamba\nfor 3D semantic segmentation. In addition, we propose the Inner Layer Hybrid\nStrategy, which combines attention and Mamba at a finer granularity, enabling\nsimultaneous capture of long-range dependencies and fine-grained local\nfeatures. Extensive experiments demonstrate the effectiveness and\ngeneralization of our HybridTM on diverse indoor and outdoor datasets.\nFurthermore, our HybridTM achieves state-of-the-art performance on ScanNet,\nScanNet200, and nuScenes benchmarks. The code will be made available at\nhttps://github.com/deepinact/HybridTM.\n","authors":["Xinyu Wang","Jinghua Hou","Zhe Liu","Yingying Zhu"],"pdf_url":"https://arxiv.org/pdf/2507.18575v1.pdf","comment":"7 pages, 5 figures"},{"id":"http://arxiv.org/abs/2507.18569v1","updated":"2025-07-24T16:45:05Z","published":"2025-07-24T16:45:05Z","title":"Adversarial Distribution Matching for Diffusion Distillation Towards\n  Efficient Image and Video Synthesis","summary":"  Distribution Matching Distillation (DMD) is a promising score distillation\ntechnique that compresses pre-trained teacher diffusion models into efficient\none-step or multi-step student generators. Nevertheless, its reliance on the\nreverse Kullback-Leibler (KL) divergence minimization potentially induces mode\ncollapse (or mode-seeking) in certain applications. To circumvent this inherent\ndrawback, we propose Adversarial Distribution Matching (ADM), a novel framework\nthat leverages diffusion-based discriminators to align the latent predictions\nbetween real and fake score estimators for score distillation in an adversarial\nmanner. In the context of extremely challenging one-step distillation, we\nfurther improve the pre-trained generator by adversarial distillation with\nhybrid discriminators in both latent and pixel spaces. Different from the mean\nsquared error used in DMD2 pre-training, our method incorporates the\ndistributional loss on ODE pairs collected from the teacher model, and thus\nproviding a better initialization for score distillation fine-tuning in the\nnext stage. By combining the adversarial distillation pre-training with ADM\nfine-tuning into a unified pipeline termed DMDX, our proposed method achieves\nsuperior one-step performance on SDXL compared to DMD2 while consuming less GPU\ntime. Additional experiments that apply multi-step ADM distillation on\nSD3-Medium, SD3.5-Large, and CogVideoX set a new benchmark towards efficient\nimage and video synthesis.\n","authors":["Yanzuo Lu","Yuxi Ren","Xin Xia","Shanchuan Lin","Xing Wang","Xuefeng Xiao","Andy J. Ma","Xiaohua Xie","Jian-Huang Lai"],"pdf_url":"https://arxiv.org/pdf/2507.18569v1.pdf","comment":"Accepted by ICCV 2025 (Highlight)"},{"id":"http://arxiv.org/abs/2507.18566v1","updated":"2025-07-24T16:41:47Z","published":"2025-07-24T16:41:47Z","title":"Facial Demorphing from a Single Morph Using a Latent Conditional GAN","summary":"  A morph is created by combining two (or more) face images from two (or more)\nidentities to create a composite image that is highly similar to both\nconstituent identities, allowing the forged morph to be biometrically\nassociated with more than one individual. Morph Attack Detection (MAD) can be\nused to detect a morph, but does not reveal the constituent images. Demorphing\n- the process of deducing the constituent images - is thus vital to provide\nadditional evidence about a morph. Existing demorphing methods suffer from the\nmorph replication problem, where the outputs tend to look very similar to the\nmorph itself, or assume that train and test morphs are generated using the same\nmorph technique. The proposed method overcomes these issues. The method\ndecomposes a morph in latent space allowing it to demorph images created from\nunseen morph techniques and face styles. We train our method on morphs created\nfrom synthetic faces and test on morphs created from real faces using arbitrary\nmorph techniques. Our method outperforms existing methods by a considerable\nmargin and produces high fidelity demorphed face images.\n","authors":["Nitish Shukla","Arun Ross"],"pdf_url":"https://arxiv.org/pdf/2507.18566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18565v1","updated":"2025-07-24T16:41:26Z","published":"2025-07-24T16:41:26Z","title":"Deep Learning-Based Age Estimation and Gender Deep Learning-Based Age\n  Estimation and Gender Classification for Targeted Advertisement","summary":"  This paper presents a novel deep learning-based approach for simultaneous age\nand gender classification from facial images, designed to enhance the\neffectiveness of targeted advertising campaigns. We propose a custom\nConvolutional Neural Network (CNN) architecture, optimized for both tasks,\nwhich leverages the inherent correlation between age and gender information\npresent in facial features. Unlike existing methods that often treat these\ntasks independently, our model learns shared representations, leading to\nimproved performance. The network is trained on a large, diverse dataset of\nfacial images, carefully pre-processed to ensure robustness against variations\nin lighting, pose, and image quality. Our experimental results demonstrate a\nsignificant improvement in gender classification accuracy, achieving 95%, and a\ncompetitive mean absolute error of 5.77 years for age estimation. Critically,\nwe analyze the performance across different age groups, identifying specific\nchallenges in accurately estimating the age of younger individuals. This\nanalysis reveals the need for targeted data augmentation and model refinement\nto address these biases. Furthermore, we explore the impact of different CNN\narchitectures and hyperparameter settings on the overall performance, providing\nvaluable insights for future research.\n","authors":["Muhammad Imran Zaman","Nisar Ahmed"],"pdf_url":"https://arxiv.org/pdf/2507.18565v1.pdf","comment":"6"},{"id":"http://arxiv.org/abs/2410.01804v6","updated":"2025-07-24T16:36:50Z","published":"2024-10-02T17:59:09Z","title":"EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis","summary":"  We present Exact Volumetric Ellipsoid Rendering (EVER), a method for\nreal-time differentiable emission-only volume rendering. Unlike recent\nrasterization based approach by 3D Gaussian Splatting (3DGS), our primitive\nbased representation allows for exact volume rendering, rather than alpha\ncompositing 3D Gaussian billboards. As such, unlike 3DGS our formulation does\nnot suffer from popping artifacts and view dependent density, but still\nachieves frame rates of $\\sim\\!30$ FPS at 720p on an NVIDIA RTX4090. Since our\napproach is built upon ray tracing it enables effects such as defocus blur and\ncamera distortion (e.g. such as from fisheye cameras), which are difficult to\nachieve by rasterization. We show that our method is more accurate with fewer\nblending issues than 3DGS and follow-up work on view-consistent rendering,\nespecially on the challenging large-scale scenes from the Zip-NeRF dataset\nwhere it achieves sharpest results among real-time techniques.\n","authors":["Alexander Mai","Peter Hedman","George Kopanas","Dor Verbin","David Futschik","Qiangeng Xu","Falko Kuester","Jonathan T. Barron","Yinda Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01804v6.pdf","comment":"Project page: https://half-potato.gitlab.io/posts/ever"},{"id":"http://arxiv.org/abs/2507.18558v1","updated":"2025-07-24T16:33:04Z","published":"2025-07-24T16:33:04Z","title":"Synthetic Data Augmentation for Enhanced Chicken Carcass Instance\n  Segmentation","summary":"  The poultry industry has been driven by broiler chicken production and has\ngrown into the world's largest animal protein sector. Automated detection of\nchicken carcasses on processing lines is vital for quality control, food\nsafety, and operational efficiency in slaughterhouses and poultry processing\nplants. However, developing robust deep learning models for tasks like instance\nsegmentation in these fast-paced industrial environments is often hampered by\nthe need for laborious acquisition and annotation of large-scale real-world\nimage datasets. We present the first pipeline generating photo-realistic,\nautomatically labeled synthetic images of chicken carcasses. We also introduce\na new benchmark dataset containing 300 annotated real-world images, curated\nspecifically for poultry segmentation research. Using these datasets, this\nstudy investigates the efficacy of synthetic data and automatic data annotation\nto enhance the instance segmentation of chicken carcasses, particularly when\nreal annotated data from the processing line is scarce. A small real dataset\nwith varying proportions of synthetic images was evaluated in prominent\ninstance segmentation models. Results show that synthetic data significantly\nboosts segmentation performance for chicken carcasses across all models. This\nresearch underscores the value of synthetic data augmentation as a viable and\neffective strategy to mitigate data scarcity, reduce manual annotation efforts,\nand advance the development of robust AI-driven automated detection systems for\nchicken carcasses in the poultry processing industry.\n","authors":["Yihong Feng","Chaitanya Pallerla","Xiaomin Lin","Pouya Sohrabipour Sr","Philip Crandall","Wan Shou","Yu She","Dongyi Wang"],"pdf_url":"https://arxiv.org/pdf/2507.18558v1.pdf","comment":"Submitted for journal reviewing"},{"id":"http://arxiv.org/abs/2504.04704v2","updated":"2025-07-24T16:25:51Z","published":"2025-04-07T03:22:15Z","title":"LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important","summary":"  The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV.\n","authors":["Manlai Liang","JiaMing Zhang","Xiong Li","Jinlong Li"],"pdf_url":"https://arxiv.org/pdf/2504.04704v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.03737v2","updated":"2025-07-24T16:21:11Z","published":"2025-07-04T17:56:43Z","title":"Outdoor Monocular SLAM with Global Scale-Consistent 3D Gaussian\n  Pointmaps","summary":"  3D Gaussian Splatting (3DGS) has become a popular solution in SLAM due to its\nhigh-fidelity and real-time novel view synthesis performance. However, some\nprevious 3DGS SLAM methods employ a differentiable rendering pipeline for\ntracking, lack geometric priors in outdoor scenes. Other approaches introduce\nseparate tracking modules, but they accumulate errors with significant camera\nmovement, leading to scale drift. To address these challenges, we propose a\nrobust RGB-only outdoor 3DGS SLAM method: S3PO-GS. Technically, we establish a\nself-consistent tracking module anchored in the 3DGS pointmap, which avoids\ncumulative scale drift and achieves more precise and robust tracking with fewer\niterations. Additionally, we design a patch-based pointmap dynamic mapping\nmodule, which introduces geometric priors while avoiding scale ambiguity. This\nsignificantly enhances tracking accuracy and the quality of scene\nreconstruction, making it particularly suitable for complex outdoor\nenvironments. Our experiments on the Waymo, KITTI, and DL3DV datasets\ndemonstrate that S3PO-GS achieves state-of-the-art results in novel view\nsynthesis and outperforms other 3DGS SLAM methods in tracking accuracy. Project\npage: https://3dagentworld.github.io/S3PO-GS/.\n","authors":["Chong Cheng","Sicheng Yu","Zijian Wang","Yifan Zhou","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2507.03737v2.pdf","comment":"Accepted by ICCV2025"},{"id":"http://arxiv.org/abs/2507.18552v1","updated":"2025-07-24T16:19:43Z","published":"2025-07-24T16:19:43Z","title":"VideoMind: An Omni-Modal Video Dataset with Intent Grounding for\n  Deep-Cognitive Video Understanding","summary":"  This paper introduces VideoMind, a video-centric omni-modal dataset designed\nfor deep video content cognition and enhanced multi-modal feature\nrepresentation. The dataset comprises 103K video samples (3K reserved for\ntesting), each paired with audio and systematically detailed textual\ndescriptions. Specifically, every video and its audio is described across three\nhierarchical layers (factual, abstract, and intent), progressing from surface\nto depth. It contains over 22 million words, averaging ~225 words per sample.\nVideoMind's key distinction from existing datasets is its provision of intent\nexpressions, which require contextual integration across the entire video and\nare not directly observable. These deep-cognitive expressions are generated\nusing a Chain-of-Thought (COT) approach, prompting the mLLM through\nstep-by-step reasoning. Each description includes annotations for subject,\nplace, time, event, action, and intent, supporting downstream recognition\ntasks. Crucially, we establish a gold-standard benchmark with 3,000 manually\nvalidated samples for evaluating deep-cognitive video understanding. We design\nhybrid-cognitive retrieval experiments, scored by multi-level retrieval\nmetrics, to appropriately assess deep video comprehension. Evaluation results\nfor models (e.g., InternVideo, VAST, UMT-L) are released. VideoMind serves as a\npowerful benchmark for fine-grained cross-modal alignment and advances fields\nrequiring in-depth video understanding, such as emotion and intent recognition.\nThe data is publicly available on GitHub, HuggingFace, and OpenDataLab,\nhttps://github.com/cdx-cindy/VideoMind.\n","authors":["Baoyao Yang","Wanyun Li","Dixin Chen","Junxiang Chen","Wenbin Yao","Haifeng Lin"],"pdf_url":"https://arxiv.org/pdf/2507.18552v1.pdf","comment":"7 pages; 14 figures"},{"id":"http://arxiv.org/abs/2507.18551v1","updated":"2025-07-24T16:19:08Z","published":"2025-07-24T16:19:08Z","title":"A 3D Cross-modal Keypoint Descriptor for MR-US Matching and Registration","summary":"  Intraoperative registration of real-time ultrasound (iUS) to preoperative\nMagnetic Resonance Imaging (MRI) remains an unsolved problem due to severe\nmodality-specific differences in appearance, resolution, and field-of-view. To\naddress this, we propose a novel 3D cross-modal keypoint descriptor for MRI-iUS\nmatching and registration. Our approach employs a patient-specific\nmatching-by-synthesis approach, generating synthetic iUS volumes from\npreoperative MRI. This enables supervised contrastive training to learn a\nshared descriptor space.\n  A probabilistic keypoint detection strategy is then employed to identify\nanatomically salient and modality-consistent locations. During training, a\ncurriculum-based triplet loss with dynamic hard negative mining is used to\nlearn descriptors that are i) robust to iUS artifacts such as speckle noise and\nlimited coverage, and ii) rotation-invariant . At inference, the method detects\nkeypoints in MR and real iUS images and identifies sparse matches, which are\nthen used to perform rigid registration. Our approach is evaluated using 3D\nMRI-iUS pairs from the ReMIND dataset. Experiments show that our approach\noutperforms state-of-the-art keypoint matching methods across 11 patients, with\nan average precision of $69.8\\%$. For image registration, our method achieves a\ncompetitive mean Target Registration Error of 2.39 mm on the ReMIND2Reg\nbenchmark.\n  Compared to existing iUS-MR registration approach, our framework is\ninterpretable, requires no manual initialization, and shows robustness to iUS\nfield-of-view variation. Code is available at\nhttps://github.com/morozovdd/CrossKEY.\n","authors":["Daniil Morozov","Reuben Dorent","Nazim Haouchine"],"pdf_url":"https://arxiv.org/pdf/2507.18551v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2507.18550v1","updated":"2025-07-24T16:18:46Z","published":"2025-07-24T16:18:46Z","title":"On the Performance of Concept Probing: The Influence of the Data\n  (Extended Version)","summary":"  Concept probing has recently garnered increasing interest as a way to help\ninterpret artificial neural networks, dealing both with their typically large\nsize and their subsymbolic nature, which ultimately renders them unfeasible for\ndirect human interpretation. Concept probing works by training additional\nclassifiers to map the internal representations of a model into human-defined\nconcepts of interest, thus allowing humans to peek inside artificial neural\nnetworks. Research on concept probing has mainly focused on the model being\nprobed or the probing model itself, paying limited attention to the data\nrequired to train such probing models. In this paper, we address this gap.\nFocusing on concept probing in the context of image classification tasks, we\ninvestigate the effect of the data used to train probing models on their\nperformance. We also make available concept labels for two widely used\ndatasets.\n","authors":["Manuel de Sousa Ribeiro","Afonso Leote","João Leite"],"pdf_url":"https://arxiv.org/pdf/2507.18550v1.pdf","comment":"Extended version of the paper published in Proceedings of the\n  European Conference on Artificial Intelligence (ECAI 2025)"},{"id":"http://arxiv.org/abs/2503.07926v2","updated":"2025-07-24T16:08:12Z","published":"2025-03-11T00:12:25Z","title":"Learning Gentle Grasping Using Vision, Sound, and Touch","summary":"  In our daily life, we often encounter objects that are fragile and can be\ndamaged by excessive grasping force, such as fruits. For these objects, it is\nparamount to grasp gently -- not using the maximum amount of force possible,\nbut rather the minimum amount of force necessary. This paper proposes using\nvisual, tactile, and auditory signals to learn to grasp and regrasp objects\nstably and gently. Specifically, we use audio signals as an indicator of\ngentleness during the grasping, and then train an end-to-end action-conditional\nmodel from raw visuo-tactile inputs that predicts both the stability and the\ngentleness of future grasping candidates, thus allowing the selection and\nexecution of the most promising action. Experimental results on a\nmulti-fingered hand over 1,500 grasping trials demonstrated that our model is\nuseful for gentle grasping by validating the predictive performance (3.27%\nhigher accuracy than the vision-only variant) and providing interpretations of\ntheir behavior. Finally, real-world experiments confirmed that the grasping\nperformance with the trained multi-modal model outperformed other baselines\n(17% higher rate for stable and gentle grasps than vision-only). Our approach\nrequires neither tactile sensor calibration nor analytical force modeling,\ndrastically reducing the engineering effort to grasp fragile objects. Dataset\nand videos are available at https://lasr.org/research/gentle-grasping.\n","authors":["Ken Nakahara","Roberto Calandra"],"pdf_url":"https://arxiv.org/pdf/2503.07926v2.pdf","comment":"8 pages. Accepted by 2025 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS)"},{"id":"http://arxiv.org/abs/2507.18541v1","updated":"2025-07-24T16:08:01Z","published":"2025-07-24T16:08:01Z","title":"Unposed 3DGS Reconstruction with Probabilistic Procrustes Mapping","summary":"  3D Gaussian Splatting (3DGS) has emerged as a core technique for 3D\nrepresentation. Its effectiveness largely depends on precise camera poses and\naccurate point cloud initialization, which are often derived from pretrained\nMulti-View Stereo (MVS) models. However, in unposed reconstruction task from\nhundreds of outdoor images, existing MVS models may struggle with memory limits\nand lose accuracy as the number of input images grows. To address this\nlimitation, we propose a novel unposed 3DGS reconstruction framework that\nintegrates pretrained MVS priors with the probabilistic Procrustes mapping\nstrategy. The method partitions input images into subsets, maps submaps into a\nglobal space, and jointly optimizes geometry and poses with 3DGS. Technically,\nwe formulate the mapping of tens of millions of point clouds as a probabilistic\nProcrustes problem and solve a closed-form alignment. By employing\nprobabilistic coupling along with a soft dustbin mechanism to reject uncertain\ncorrespondences, our method globally aligns point clouds and poses within\nminutes across hundreds of images. Moreover, we propose a joint optimization\nframework for 3DGS and camera poses. It constructs Gaussians from\nconfidence-aware anchor points and integrates 3DGS differentiable rendering\nwith an analytical Jacobian to jointly refine scene and poses, enabling\naccurate reconstruction and pose estimation. Experiments on Waymo and KITTI\ndatasets show that our method achieves accurate reconstruction from unposed\nimage sequences, setting a new state of the art for unposed 3DGS\nreconstruction.\n","authors":["Chong Cheng","Zijian Wang","Sicheng Yu","Yu Hu","Nanjie Yao","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2507.18541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18537v1","updated":"2025-07-24T16:04:55Z","published":"2025-07-24T16:04:55Z","title":"TTS-VAR: A Test-Time Scaling Framework for Visual Auto-Regressive\n  Generation","summary":"  Scaling visual generation models is essential for real-world content\ncreation, yet requires substantial training and computational expenses.\nAlternatively, test-time scaling has garnered growing attention due to resource\nefficiency and promising performance. In this work, we present TTS-VAR, the\nfirst general test-time scaling framework for visual auto-regressive (VAR)\nmodels, modeling the generation process as a path searching problem. To\ndynamically balance computational efficiency with exploration capacity, we\nfirst introduce an adaptive descending batch size schedule throughout the\ncausal generation process. Besides, inspired by VAR's hierarchical\ncoarse-to-fine multi-scale generation, our framework integrates two key\ncomponents: (i) At coarse scales, we observe that generated tokens are hard for\nevaluation, possibly leading to erroneous acceptance of inferior samples or\nrejection of superior samples. Noticing that the coarse scales contain\nsufficient structural information, we propose clustering-based diversity\nsearch. It preserves structural variety through semantic feature clustering,\nenabling later selection on samples with higher potential. (ii) In fine scales,\nresampling-based potential selection prioritizes promising candidates using\npotential scores, which are defined as reward functions incorporating\nmulti-scale generation history. Experiments on the powerful VAR model Infinity\nshow a notable 8.7% GenEval score improvement (from 0.69 to 0.75). Key insights\nreveal that early-stage structural features effectively influence final\nquality, and resampling efficacy varies across generation scales. Code is\navailable at https://github.com/ali-vilab/TTS-VAR.\n","authors":["Zhekai Chen","Ruihang Chu","Yukang Chen","Shiwei Zhang","Yujie Wei","Yingya Zhang","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2507.18537v1.pdf","comment":"10 Tables, 9 Figures"},{"id":"http://arxiv.org/abs/2503.08510v2","updated":"2025-07-24T16:04:36Z","published":"2025-03-11T15:00:22Z","title":"External Knowledge Injection for CLIP-Based Class-Incremental Learning","summary":"  Class-Incremental Learning (CIL) enables learning systems to continuously\nadapt to evolving data streams. With the advancement of pre-training,\nleveraging pre-trained vision-language models (e.g., CLIP) offers a promising\nstarting point for CIL. However, CLIP makes decisions by matching visual\nembeddings to class names, overlooking the rich contextual information conveyed\nthrough language. For instance, the concept of ``cat'' can be decomposed into\nfeatures like tail, fur, and face for recognition. Besides, since the model is\ncontinually updated, these detailed features are overwritten in CIL, requiring\nexternal knowledge for compensation. In this paper, we introduce ExterNal\nknowledGe INjEction (ENGINE) for CLIP-based CIL. To enhance knowledge transfer\nfrom outside the dataset, we propose a dual-branch injection tuning framework\nthat encodes informative knowledge from both visual and textual modalities. The\nvisual branch is enhanced with data augmentation to enrich the visual features,\nwhile the textual branch leverages GPT-4 to rewrite discriminative descriptors.\nIn addition to this on-the-fly knowledge injection, we also implement\npost-tuning knowledge by re-ranking the prediction results during inference.\nWith the injected knowledge, the model can better capture informative features\nfor downstream tasks as data evolves. Extensive experiments demonstrate the\nstate-of-the-art performance of ENGINE. Code is available at:\nhttps://github.com/LAMDA-CL/ICCV25-ENGINE\n","authors":["Da-Wei Zhou","Kai-Wen Li","Jingyi Ning","Han-Jia Ye","Lijun Zhang","De-Chuan Zhan"],"pdf_url":"https://arxiv.org/pdf/2503.08510v2.pdf","comment":"Accepted to ICCV 2025. Code is available at:\n  https://github.com/LAMDA-CL/ICCV25-ENGINE"},{"id":"http://arxiv.org/abs/2505.01969v2","updated":"2025-07-24T16:03:56Z","published":"2025-05-04T02:38:10Z","title":"MC3D-AD: A Unified Geometry-aware Reconstruction Model for\n  Multi-category 3D Anomaly Detection","summary":"  3D Anomaly Detection (AD) is a promising means of controlling the quality of\nmanufactured products. However, existing methods typically require carefully\ntraining a task-specific model for each category independently, leading to high\ncost, low efficiency, and weak generalization. Therefore, this paper presents a\nnovel unified model for Multi-Category 3D Anomaly Detection (MC3D-AD) that aims\nto utilize both local and global geometry-aware information to reconstruct\nnormal representations of all categories. First, to learn robust and\ngeneralized features of different categories, we propose an adaptive\ngeometry-aware masked attention module that extracts geometry variation\ninformation to guide mask attention. Then, we introduce a local geometry-aware\nencoder reinforced by the improved mask attention to encode group-level feature\ntokens. Finally, we design a global query decoder that utilizes point cloud\nposition embeddings to improve the decoding process and reconstruction ability.\nThis leads to local and global geometry-aware reconstructed feature tokens for\nthe AD task. MC3D-AD is evaluated on two publicly available Real3D-AD and\nAnomaly-ShapeNet datasets, and exhibits significant superiority over current\nstate-of-the-art single-category methods, achieving 3.1\\% and 9.3\\% improvement\nin object-level AUROC over Real3D-AD and Anomaly-ShapeNet, respectively. The\ncode is available at https://github.com/iCAN-SZU/MC3D-AD.\n","authors":["Jiayi Cheng","Can Gao","Jie Zhou","Jiajun Wen","Tao Dai","Jinbao Wang"],"pdf_url":"https://arxiv.org/pdf/2505.01969v2.pdf","comment":"7 pages of main text, 3 pages of appendix, accepted to IJCAI 2025"},{"id":"http://arxiv.org/abs/2507.18534v1","updated":"2025-07-24T16:01:34Z","published":"2025-07-24T16:01:34Z","title":"Elucidating the Design Space of Arbitrary-Noise-Based Diffusion Models","summary":"  EDM elucidates the unified design space of diffusion models, yet its fixed\nnoise patterns restricted to pure Gaussian noise, limit advancements in image\nrestoration. Our study indicates that forcibly injecting Gaussian noise\ncorrupts the degraded images, overextends the image transformation distance,\nand increases restoration complexity. To address this problem, our proposed EDA\nElucidates the Design space of Arbitrary-noise-based diffusion models.\nTheoretically, EDA expands the freedom of noise pattern while preserving the\noriginal module flexibility of EDM, with rigorous proof that increased noise\ncomplexity incurs no additional computational overhead during restoration. EDA\nis validated on three typical tasks: MRI bias field correction (global smooth\nnoise), CT metal artifact reduction (global sharp noise), and natural image\nshadow removal (local boundary-aware noise). With only 5 sampling steps, EDA\noutperforms most task-specific methods and achieves state-of-the-art\nperformance in bias field correction and shadow removal.\n","authors":["Xingyu Qiu","Mengying Yang","Xinghua Ma","Dong Liang","Yuzhen Li","Fanding Li","Gongning Luo","Wei Wang","Kuanquan Wang","Shuo Li"],"pdf_url":"https://arxiv.org/pdf/2507.18534v1.pdf","comment":"21 pages, 4 figures"},{"id":"http://arxiv.org/abs/2507.18532v1","updated":"2025-07-24T15:58:39Z","published":"2025-07-24T15:58:39Z","title":"COT-AD: Cotton Analysis Dataset","summary":"  This paper presents COT-AD, a comprehensive Dataset designed to enhance\ncotton crop analysis through computer vision. Comprising over 25,000 images\ncaptured throughout the cotton growth cycle, with 5,000 annotated images,\nCOT-AD includes aerial imagery for field-scale detection and segmentation and\nhigh-resolution DSLR images documenting key diseases. The annotations cover\npest and disease recognition, vegetation, and weed analysis, addressing a\ncritical gap in cotton-specific agricultural datasets. COT-AD supports tasks\nsuch as classification, segmentation, image restoration, enhancement, deep\ngenerative model-based cotton crop synthesis, and early disease management,\nadvancing data-driven crop management\n","authors":["Akbar Ali","Mahek Vyas","Soumyaratna Debnath","Chanda Grover Kamra","Jaidev Sanjay Khalane","Reuben Shibu Devanesan","Indra Deep Mastan","Subramanian Sankaranarayanan","Pankaj Khanna","Shanmuganathan Raman"],"pdf_url":"https://arxiv.org/pdf/2507.18532v1.pdf","comment":"Dataset publicly available at:\n  https://ieee-dataport.org/documents/cot-adcotton-analysis-dataset. Accepted\n  to IEEE International Conference on Image Processing (ICIP) 2025"},{"id":"http://arxiv.org/abs/2507.18531v1","updated":"2025-07-24T15:58:36Z","published":"2025-07-24T15:58:36Z","title":"IntentVCNet: Bridging Spatio-Temporal Gaps for Intention-Oriented\n  Controllable Video Captioning","summary":"  Intent-oriented controlled video captioning aims to generate targeted\ndescriptions for specific targets in a video based on customized user intent.\nCurrent Large Visual Language Models (LVLMs) have gained strong instruction\nfollowing and visual comprehension capabilities. Although the LVLMs\ndemonstrated proficiency in spatial and temporal understanding respectively, it\nwas not able to perform fine-grained spatial control in time sequences in\ndirect response to instructions. This substantial spatio-temporal gap\ncomplicates efforts to achieve fine-grained intention-oriented control in\nvideo. Towards this end, we propose a novel IntentVCNet that unifies the\ntemporal and spatial understanding knowledge inherent in LVLMs to bridge the\nspatio-temporal gap from both prompting and model perspectives. Specifically,\nwe first propose a prompt combination strategy designed to enable LLM to model\nthe implicit relationship between prompts that characterize user intent and\nvideo sequences. We then propose a parameter efficient box adapter that\naugments the object semantic information in the global visual context so that\nthe visual token has a priori information about the user intent. The final\nexperiment proves that the combination of the two strategies can further\nenhance the LVLM's ability to model spatial details in video sequences, and\nfacilitate the LVLMs to accurately generate controlled intent-oriented\ncaptions. Our proposed method achieved state-of-the-art results in several open\nsource LVLMs and was the runner-up in the IntentVC challenge. Our code is\navailable on https://github.com/thqiu0419/IntentVCNet.\n","authors":["Tianheng Qiu","Jingchun Gao","Jingyu Li","Huiyi Leong","Xuan Huang","Xi Wang","Xiaocheng Zhang","Kele Xu","Lan Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.18531v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.09027v2","updated":"2025-07-24T15:55:00Z","published":"2025-06-10T17:53:29Z","title":"Diffuse and Disperse: Image Generation with Representation\n  Regularization","summary":"  The development of diffusion-based generative models over the past decade has\nlargely proceeded independently of progress in representation learning. These\ndiffusion models typically rely on regression-based objectives and generally\nlack explicit regularization. In this work, we propose \\textit{Dispersive\nLoss}, a simple plug-and-play regularizer that effectively improves\ndiffusion-based generative models. Our loss function encourages internal\nrepresentations to disperse in the hidden space, analogous to contrastive\nself-supervised learning, with the key distinction that it requires no positive\nsample pairs and therefore does not interfere with the sampling process used\nfor regression. Compared to the recent method of representation alignment\n(REPA), our approach is self-contained and minimalist, requiring no\npre-training, no additional parameters, and no external data. We evaluate\nDispersive Loss on the ImageNet dataset across a range of models and report\nconsistent improvements over widely used and strong baselines. We hope our work\nwill help bridge the gap between generative modeling and representation\nlearning.\n","authors":["Runqian Wang","Kaiming He"],"pdf_url":"https://arxiv.org/pdf/2506.09027v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18522v1","updated":"2025-07-24T15:46:38Z","published":"2025-07-24T15:46:38Z","title":"GaussianFusionOcc: A Seamless Sensor Fusion Approach for 3D Occupancy\n  Prediction Using 3D Gaussians","summary":"  3D semantic occupancy prediction is one of the crucial tasks of autonomous\ndriving. It enables precise and safe interpretation and navigation in complex\nenvironments. Reliable predictions rely on effective sensor fusion, as\ndifferent modalities can contain complementary information. Unlike conventional\nmethods that depend on dense grid representations, our approach,\nGaussianFusionOcc, uses semantic 3D Gaussians alongside an innovative sensor\nfusion mechanism. Seamless integration of data from camera, LiDAR, and radar\nsensors enables more precise and scalable occupancy prediction, while 3D\nGaussian representation significantly improves memory efficiency and inference\nspeed. GaussianFusionOcc employs modality-agnostic deformable attention to\nextract essential features from each sensor type, which are then used to refine\nGaussian properties, resulting in a more accurate representation of the\nenvironment. Extensive testing with various sensor combinations demonstrates\nthe versatility of our approach. By leveraging the robustness of multi-modal\nfusion and the efficiency of Gaussian representation, GaussianFusionOcc\noutperforms current state-of-the-art models.\n","authors":["Tomislav Pavković","Mohammad-Ali Nikouei Mahani","Johannes Niedermayer","Johannes Betz"],"pdf_url":"https://arxiv.org/pdf/2507.18522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20144v3","updated":"2025-07-24T15:45:59Z","published":"2025-02-27T14:35:47Z","title":"Robust sensitivity control in digital pathology via tile score\n  distribution matching","summary":"  Deploying digital pathology models across medical centers is challenging due\nto distribution shifts. Recent advances in domain generalization improve model\ntransferability in terms of aggregated performance measured by the Area Under\nCurve (AUC). However, clinical regulations often require to control the\ntransferability of other metrics, such as prescribed sensitivity levels. We\nintroduce a novel approach to control the sensitivity of whole slide image\n(WSI) classification models, based on optimal transport and Multiple Instance\nLearning (MIL). Validated across multiple cohorts and tasks, our method enables\nrobust sensitivity control with only a handful of calibration samples,\nproviding a practical solution for reliable deployment of computational\npathology systems.\n","authors":["Arthur Pignet","John Klein","Genevieve Robin","Antoine Olivier"],"pdf_url":"https://arxiv.org/pdf/2502.20144v3.pdf","comment":"Camera ready version. Accepted at MICCAI 2025"},{"id":"http://arxiv.org/abs/2507.18517v1","updated":"2025-07-24T15:40:44Z","published":"2025-07-24T15:40:44Z","title":"Object segmentation in the wild with foundation models: application to\n  vision assisted neuro-prostheses for upper limbs","summary":"  In this work, we address the problem of semantic object segmentation using\nfoundation models. We investigate whether foundation models, trained on a large\nnumber and variety of objects, can perform object segmentation without\nfine-tuning on specific images containing everyday objects, but in highly\ncluttered visual scenes. The ''in the wild'' context is driven by the target\napplication of vision guided upper limb neuroprostheses. We propose a method\nfor generating prompts based on gaze fixations to guide the Segment Anything\nModel (SAM) in our segmentation scenario, and fine-tune it on egocentric visual\ndata. Evaluation results of our approach show an improvement of the IoU\nsegmentation quality metric by up to 0.51 points on real-world challenging data\nof Grasping-in-the-Wild corpus which is made available on the RoboFlow Platform\n(https://universe.roboflow.com/iwrist/grasping-in-the-wild)\n","authors":["Bolutife Atoki","Jenny Benois-Pineau","Renaud Péteri","Fabien Baldacci","Aymar de Rugy"],"pdf_url":"https://arxiv.org/pdf/2507.18517v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20292v6","updated":"2025-07-24T15:38:22Z","published":"2025-02-27T17:17:43Z","title":"Visual Adaptive Prompting for Compositional Zero-Shot Learning","summary":"  Vision-Language Models (VLMs) have demonstrated impressive multimodal\ncapabilities in learning joint representations of visual and textual data,\nmaking them powerful tools for tasks such as Compositional Zero-Shot Learning\n(CZSL). CZSL requires models to generalize to novel combinations of visual\nprimitives--such as attributes and objects--that were not explicitly\nencountered during training. Recent works in prompting for CZSL have focused on\nmodifying inputs for the text encoder, often using static prompts that do not\nchange across varying visual contexts. However, these approaches struggle to\nfully capture varying visual contexts, as they focus on text adaptation rather\nthan leveraging visual features for compositional reasoning. To address this,\nwe propose a Visual Adaptive Prompting System (VAPS) that leverages a learnable\nvisual prompt repository and similarity-based retrieval mechanism within the\nframework of VLMs to bridge the gap between semantic and visual features. Our\nmethod introduces a dynamic visual prompt repository mechanism that selects the\nmost relevant attribute and object prompts based on the visual features of the\nimage. Our proposed system includes a visual prompt adapter that encourages the\nmodel to learn a more generalizable embedding space. Experiments on three CZSL\nbenchmarks, across both closed and open-world scenarios, demonstrate\nstate-of-the-art results.\n","authors":["Kyle Stein","Arash Mahyari","Guillermo Francia","Eman El-Sheikh"],"pdf_url":"https://arxiv.org/pdf/2502.20292v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10084v2","updated":"2025-07-24T15:37:18Z","published":"2025-07-14T09:11:33Z","title":"A Transfer Learning-Based Method for Water Body Segmentation in Remote\n  Sensing Imagery: A Case Study of the Zhada Tulin Area","summary":"  The Tibetan Plateau, known as the Asian Water Tower, faces significant water\nsecurity challenges due to its high sensitivity to climate change. Advancing\nEarth observation for sustainable water monitoring is thus essential for\nbuilding climate resilience in this region. This study proposes a two-stage\ntransfer learning strategy using the SegFormer model to overcome domain shift\nand data scarcit--key barriers in developing robust AI for climate-sensitive\napplications. After pre-training on a diverse source domain, our model was\nfine-tuned for the arid Zhada Tulin area. Experimental results show a\nsubstantial performance boost: the Intersection over Union (IoU) for water body\nsegmentation surged from 25.50% (direct transfer) to 64.84%. This AI-driven\naccuracy is crucial for disaster risk reduction, particularly in monitoring\nflash flood-prone systems. More importantly, the high-precision map reveals a\nhighly concentrated spatial distribution of water, with over 80% of the water\narea confined to less than 20% of the river channel length. This quantitative\nfinding provides crucial evidence for understanding hydrological processes and\ndesigning targeted water management and climate adaptation strategies. Our work\nthus demonstrates an effective technical solution for monitoring arid plateau\nregions and contributes to advancing AI-powered Earth observation for disaster\npreparedness in critical transboundary river headwaters.\n","authors":["Haonan Chen","Xin Tong"],"pdf_url":"https://arxiv.org/pdf/2507.10084v2.pdf","comment":"13 pages, 6 figures, 2 tables"},{"id":"http://arxiv.org/abs/2507.18513v1","updated":"2025-07-24T15:33:55Z","published":"2025-07-24T15:33:55Z","title":"Towards Large Scale Geostatistical Methane Monitoring with Part-based\n  Object Detection","summary":"  Object detection is one of the main applications of computer vision in remote\nsensing imagery. Despite its increasing availability, the sheer volume of\nremote sensing data poses a challenge when detecting rare objects across large\ngeographic areas. Paradoxically, this common challenge is crucial to many\napplications, such as estimating environmental impact of certain human\nactivities at scale. In this paper, we propose to address the problem by\ninvestigating the methane production and emissions of bio-digesters in France.\nWe first introduce a novel dataset containing bio-digesters, with small\ntraining and validation sets, and a large test set with a high imbalance\ntowards observations without objects since such sites are rare. We develop a\npart-based method that considers essential bio-digester sub-elements to boost\ninitial detections. To this end, we apply our method to new, unseen regions to\nbuild an inventory of bio-digesters. We then compute geostatistical estimates\nof the quantity of methane produced that can be attributed to these\ninfrastructures in a given area at a given time.\n","authors":["Adhemar de Senneville","Xavier Bou","Thibaud Ehret","Rafael Grompone","Jean Louis Bonne","Nicolas Dumelie","Thomas Lauvaux","Gabriele Facciolo"],"pdf_url":"https://arxiv.org/pdf/2507.18513v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18512v1","updated":"2025-07-24T15:33:31Z","published":"2025-07-24T15:33:31Z","title":"Explaining How Visual, Textual and Multimodal Encoders Share Concepts","summary":"  Sparse autoencoders (SAEs) have emerged as a powerful technique for\nextracting human-interpretable features from neural networks activations.\nPrevious works compared different models based on SAE-derived features but\nthose comparisons have been restricted to models within the same modality. We\npropose a novel indicator allowing quantitative comparison of models across SAE\nfeatures, and use it to conduct a comparative study of visual, textual and\nmultimodal encoders. We also propose to quantify the Comparative Sharedness of\nindividual features between different classes of models. With these two new\ntools, we conduct several studies on 21 encoders of the three types, with two\nsignificantly different sizes, and considering generalist and domain specific\ndatasets. The results allow to revisit previous studies at the light of\nencoders trained in a multimodal context and to quantify to which extent all\nthese models share some representations or features. They also suggest that\nvisual features that are specific to VLMs among vision encoders are shared with\ntext encoders, highlighting the impact of text pretraining. The code is\navailable at https://github.com/CEA-LIST/SAEshareConcepts\n","authors":["Clément Cornet","Romaric Besançon","Hervé Le Borgne"],"pdf_url":"https://arxiv.org/pdf/2507.18512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18503v1","updated":"2025-07-24T15:19:23Z","published":"2025-07-24T15:19:23Z","title":"Human Scanpath Prediction in Target-Present Visual Search with\n  Semantic-Foveal Bayesian Attention","summary":"  In goal-directed visual tasks, human perception is guided by both top-down\nand bottom-up cues. At the same time, foveal vision plays a crucial role in\ndirecting attention efficiently. Modern research on bio-inspired computational\nattention models has taken advantage of advancements in deep learning by\nutilizing human scanpath data to achieve new state-of-the-art performance. In\nthis work, we assess the performance of SemBA-FAST, i.e. Semantic-based\nBayesian Attention for Foveal Active visual Search Tasks, a top-down framework\ndesigned for predicting human visual attention in target-present visual search.\nSemBA-FAST integrates deep object detection with a probabilistic semantic\nfusion mechanism to generate attention maps dynamically, leveraging pre-trained\ndetectors and artificial foveation to update top-down knowledge and improve\nfixation prediction sequentially. We evaluate SemBA-FAST on the COCO-Search18\nbenchmark dataset, comparing its performance against other scanpath prediction\nmodels. Our methodology achieves fixation sequences that closely match human\nground-truth scanpaths. Notably, it surpasses baseline and other top-down\napproaches and competes, in some cases, with scanpath-informed models. These\nfindings provide valuable insights into the capabilities of semantic-foveal\nprobabilistic frameworks for human-like attention modelling, with implications\nfor real-time cognitive computing and robotics.\n","authors":["João Luzio","Alexandre Bernardino","Plinio Moreno"],"pdf_url":"https://arxiv.org/pdf/2507.18503v1.pdf","comment":"To be published in the 2025 IEEE International Conference on\n  Development and Learning (ICDL)"},{"id":"http://arxiv.org/abs/2403.18915v2","updated":"2025-07-24T15:19:06Z","published":"2024-03-27T18:08:14Z","title":"PLOT-TAL: Prompt Learning with Optimal Transport for Few-Shot Temporal\n  Action Localization","summary":"  Few-shot temporal action localization (TAL) methods that adapt large models\nvia single-prompt tuning often fail to produce precise temporal boundaries.\nThis stems from the model learning a non-discriminative mean representation of\nan action from sparse data, which compromises generalization. We address this\nby proposing a new paradigm based on multi-prompt ensembles, where a set of\ndiverse, learnable prompts for each action is encouraged to specialize on\ncompositional sub-events. To enforce this specialization, we introduce\nPLOT-TAL, a framework that leverages Optimal Transport (OT) to find a globally\noptimal alignment between the prompt ensemble and the video's temporal\nfeatures. Our method establishes a new state-of-the-art on the challenging\nfew-shot benchmarks of THUMOS'14 and EPIC-Kitchens, without requiring complex\nmeta-learning. The significant performance gains, particularly at high IoU\nthresholds, validate our hypothesis and demonstrate the superiority of learning\ndistributed, compositional representations for precise temporal localization.\n","authors":["Edward Fish","Andrew Gilbert"],"pdf_url":"https://arxiv.org/pdf/2403.18915v2.pdf","comment":"Accepted to ICCVWS"},{"id":"http://arxiv.org/abs/2507.18498v1","updated":"2025-07-24T15:13:11Z","published":"2025-07-24T15:13:11Z","title":"Delving into Mapping Uncertainty for Mapless Trajectory Prediction","summary":"  Recent advances in autonomous driving are moving towards mapless approaches,\nwhere High-Definition (HD) maps are generated online directly from sensor data,\nreducing the need for expensive labeling and maintenance. However, the\nreliability of these online-generated maps remains uncertain. While\nincorporating map uncertainty into downstream trajectory prediction tasks has\nshown potential for performance improvements, current strategies provide\nlimited insights into the specific scenarios where this uncertainty is\nbeneficial. In this work, we first analyze the driving scenarios in which\nmapping uncertainty has the greatest positive impact on trajectory prediction\nand identify a critical, previously overlooked factor: the agent's kinematic\nstate. Building on these insights, we propose a novel Proprioceptive Scenario\nGating that adaptively integrates map uncertainty into trajectory prediction\nbased on forecasts of the ego vehicle's future kinematics. This lightweight,\nself-supervised approach enhances the synergy between online mapping and\ntrajectory prediction, providing interpretability around where uncertainty is\nadvantageous and outperforming previous integration methods. Additionally, we\nintroduce a Covariance-based Map Uncertainty approach that better aligns with\nmap geometry, further improving trajectory prediction. Extensive ablation\nstudies confirm the effectiveness of our approach, achieving up to 23.6%\nimprovement in mapless trajectory prediction performance over the\nstate-of-the-art method using the real-world nuScenes driving dataset. Our\ncode, data, and models are publicly available at\nhttps://github.com/Ethan-Zheng136/Map-Uncertainty-for-Trajectory-Prediction.\n","authors":["Zongzheng Zhang","Xuchong Qiu","Boran Zhang","Guantian Zheng","Xunjiang Gu","Guoxuan Chi","Huan-ang Gao","Leichen Wang","Ziming Liu","Xinrun Li","Igor Gilitschenski","Hongyang Li","Hang Zhao","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2507.18498v1.pdf","comment":"Accepted to IROS 2025, Project Page:\n  https://ethan-zheng136.github.io/Dev-Unc/"},{"id":"http://arxiv.org/abs/2506.02574v2","updated":"2025-07-24T15:05:23Z","published":"2025-06-03T07:55:16Z","title":"Dynamic mapping from static labels: remote sensing dynamic sample\n  generation with temporal-spectral embedding","summary":"  Accurate remote sensing geographic mapping requires timely and representative\nsamples. However, rapid land surface changes often render static samples\nobsolete within months, making manual sample updates labor-intensive and\nunsustainable. To address this challenge, we propose TasGen, a two-stage\nTemporal spectral-aware Automatic Sample Generation method for generating\ndynamic training samples from single-date static labels without human\nintervention. Land surface dynamics often manifest as anomalies in\ntemporal-spectral sequences. %These anomalies are multivariate yet unified:\ntemporal, spectral, or joint anomalies stem from different mechanisms and\ncannot be naively coupled, as this may obscure the nature of changes. Yet, any\nland surface state corresponds to a coherent temporal-spectral signature, which\nwould be lost if the two dimensions are modeled separately. To effectively\ncapture these dynamics, TasGen first disentangles temporal and spectral\nfeatures to isolate their individual contributions, and then couples them to\nmodel their synergistic interactions. In the first stage, we introduce a\nhierarchical temporal-spectral variational autoencoder (HTS-VAE) with a\ndual-dimension embedding to learn low-dimensional latent patterns of normal\nsamples by first disentangling and then jointly embedding temporal and spectral\ninformation. This temporal-spectral embedding enables robust anomaly detection\nby identifying deviations from learned joint patterns. In the second stage, a\nclassifier trained on stable samples relabels change points across time to\ngenerate dynamic samples. To not only detect but also explain surface dynamics,\nwe further propose an anomaly interpretation method based on Gibbs sampling,\nwhich attributes changes to specific spectral-temporal dimensions.\n","authors":["Shuai Yuan","Shuang Chen","Tianwu Lin","Jincheng Yuan","Geng Tian","Yang Xu","Jie Wang","Peng Gong"],"pdf_url":"https://arxiv.org/pdf/2506.02574v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.16761v2","updated":"2025-07-24T14:58:44Z","published":"2025-07-22T16:56:02Z","title":"Faithful, Interpretable Chest X-ray Diagnosis with Anti-Aliased B-cos\n  Networks","summary":"  Faithfulness and interpretability are essential for deploying deep neural\nnetworks (DNNs) in safety-critical domains such as medical imaging. B-cos\nnetworks offer a promising solution by replacing standard linear layers with a\nweight-input alignment mechanism, producing inherently interpretable,\nclass-specific explanations without post-hoc methods. While maintaining\ndiagnostic performance competitive with state-of-the-art DNNs, standard B-cos\nmodels suffer from severe aliasing artifacts in their explanation maps, making\nthem unsuitable for clinical use where clarity is essential. In this work, we\naddress these limitations by introducing anti-aliasing strategies using\nFLCPooling (FLC) and BlurPool (BP) to significantly improve explanation\nquality. Our experiments on chest X-ray datasets demonstrate that the modified\n$\\text{B-cos}_\\text{FLC}$ and $\\text{B-cos}_\\text{BP}$ preserve strong\npredictive performance while providing faithful and artifact-free explanations\nsuitable for clinical application in multi-class and multi-label settings. Code\navailable at: GitHub repository (url:\nhttps://github.com/mkleinma/B-cos-medical-paper).\n","authors":["Marcel Kleinmann","Shashank Agnihotri","Margret Keuper"],"pdf_url":"https://arxiv.org/pdf/2507.16761v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18484v1","updated":"2025-07-24T14:56:21Z","published":"2025-07-24T14:56:21Z","title":"Reinforced Embodied Active Defense: Exploiting Adaptive Interaction for\n  Robust Visual Perception in Adversarial 3D Environments","summary":"  Adversarial attacks in 3D environments have emerged as a critical threat to\nthe reliability of visual perception systems, particularly in safety-sensitive\napplications such as identity verification and autonomous driving. These\nattacks employ adversarial patches and 3D objects to manipulate deep neural\nnetwork (DNN) predictions by exploiting vulnerabilities within complex scenes.\nExisting defense mechanisms, such as adversarial training and purification,\nprimarily employ passive strategies to enhance robustness. However, these\napproaches often rely on pre-defined assumptions about adversarial tactics,\nlimiting their adaptability in dynamic 3D settings. To address these\nchallenges, we introduce Reinforced Embodied Active Defense (Rein-EAD), a\nproactive defense framework that leverages adaptive exploration and interaction\nwith the environment to improve perception robustness in 3D adversarial\ncontexts. By implementing a multi-step objective that balances immediate\nprediction accuracy with predictive entropy minimization, Rein-EAD optimizes\ndefense strategies over a multi-step horizon. Additionally, Rein-EAD involves\nan uncertainty-oriented reward-shaping mechanism that facilitates efficient\npolicy updates, thereby reducing computational overhead and supporting\nreal-world applicability without the need for differentiable environments.\nComprehensive experiments validate the effectiveness of Rein-EAD, demonstrating\na substantial reduction in attack success rates while preserving standard\naccuracy across diverse tasks. Notably, Rein-EAD exhibits robust generalization\nto unseen and adaptive attacks, making it suitable for real-world complex\ntasks, including 3D object classification, face recognition and autonomous\ndriving.\n","authors":["Xiao Yang","Lingxuan Wu","Lizhong Wang","Chengyang Ying","Hang Su","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2507.18484v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2404.00540"},{"id":"http://arxiv.org/abs/2507.18483v1","updated":"2025-07-24T14:56:18Z","published":"2025-07-24T14:56:18Z","title":"A COCO-Formatted Instance-Level Dataset for Plasmodium Falciparum\n  Detection in Giemsa-Stained Blood Smears","summary":"  Accurate detection of Plasmodium falciparum in Giemsa-stained blood smears is\nan essential component of reliable malaria diagnosis, especially in developing\ncountries. Deep learning-based object detection methods have demonstrated\nstrong potential for automated Malaria diagnosis, but their adoption is limited\nby the scarcity of datasets with detailed instance-level annotations. In this\nwork, we present an enhanced version of the publicly available NIH malaria\ndataset, with detailed bounding box annotations in COCO format to support\nobject detection training. We validated the revised annotations by training a\nFaster R-CNN model to detect infected and non-infected red blood cells, as well\nas white blood cells. Cross-validation on the original dataset yielded F1\nscores of up to 0.88 for infected cell detection. These results underscore the\nimportance of annotation volume and consistency, and demonstrate that automated\nannotation refinement combined with targeted manual correction can produce\ntraining data of sufficient quality for robust detection performance. The\nupdated annotations set is publicly available via GitHub:\nhttps://github.com/MIRA-Vision-Microscopy/malaria-thin-smear-coco.\n","authors":["Frauke Wilm","Luis Carlos Rivera Monroy","Mathias Öttl","Lukas Mürdter","Leonid Mill","Andreas Maier"],"pdf_url":"https://arxiv.org/pdf/2507.18483v1.pdf","comment":"7 pages, 4 figures, 2 tables, accepted at MICCAI 2025 Open Data"},{"id":"http://arxiv.org/abs/2507.18481v1","updated":"2025-07-24T14:55:33Z","published":"2025-07-24T14:55:33Z","title":"Q-Former Autoencoder: A Modern Framework for Medical Anomaly Detection","summary":"  Anomaly detection in medical images is an important yet challenging task due\nto the diversity of possible anomalies and the practical impossibility of\ncollecting comprehensively annotated data sets. In this work, we tackle\nunsupervised medical anomaly detection proposing a modernized autoencoder-based\nframework, the Q-Former Autoencoder, that leverages state-of-the-art pretrained\nvision foundation models, such as DINO, DINOv2 and Masked Autoencoder. Instead\nof training encoders from scratch, we directly utilize frozen vision foundation\nmodels as feature extractors, enabling rich, multi-stage, high-level\nrepresentations without domain-specific fine-tuning. We propose the usage of\nthe Q-Former architecture as the bottleneck, which enables the control of the\nlength of the reconstruction sequence, while efficiently aggregating multiscale\nfeatures. Additionally, we incorporate a perceptual loss computed using\nfeatures from a pretrained Masked Autoencoder, guiding the reconstruction\ntowards semantically meaningful structures. Our framework is evaluated on four\ndiverse medical anomaly detection benchmarks, achieving state-of-the-art\nresults on BraTS2021, RESC, and RSNA. Our results highlight the potential of\nvision foundation model encoders, pretrained on natural images, to generalize\neffectively to medical image analysis tasks without further fine-tuning. We\nrelease the code and models at https://github.com/emirhanbayar/QFAE.\n","authors":["Francesco Dalmonte","Emirhan Bayar","Emre Akbas","Mariana-Iuliana Georgescu"],"pdf_url":"https://arxiv.org/pdf/2507.18481v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2506.12006v3","updated":"2025-07-24T14:51:39Z","published":"2025-06-13T17:56:39Z","title":"crossMoDA Challenge: Evolution of Cross-Modality Domain Adaptation\n  Techniques for Vestibular Schwannoma and Cochlea Segmentation from 2021 to\n  2023","summary":"  The cross-Modality Domain Adaptation (crossMoDA) challenge series, initiated\nin 2021 in conjunction with the International Conference on Medical Image\nComputing and Computer Assisted Intervention (MICCAI), focuses on unsupervised\ncross-modality segmentation, learning from contrast-enhanced T1 (ceT1) and\ntransferring to T2 MRI. The task is an extreme example of domain shift chosen\nto serve as a meaningful and illustrative benchmark. From a clinical\napplication perspective, it aims to automate Vestibular Schwannoma (VS) and\ncochlea segmentation on T2 scans for more cost-effective VS management. Over\ntime, the challenge objectives have evolved to enhance its clinical relevance.\nThe challenge evolved from using single-institutional data and basic\nsegmentation in 2021 to incorporating multi-institutional data and Koos grading\nin 2022, and by 2023, it included heterogeneous routine data and\nsub-segmentation of intra- and extra-meatal tumour components. In this work, we\nreport the findings of the 2022 and 2023 editions and perform a retrospective\nanalysis of the challenge progression over the years. The observations from the\nsuccessive challenge contributions indicate that the number of outliers\ndecreases with an expanding dataset. This is notable since the diversity of\nscanning protocols of the datasets concurrently increased. The winning approach\nof the 2023 edition reduced the number of outliers on the 2021 and 2022 testing\ndata, demonstrating how increased data heterogeneity can enhance segmentation\nperformance even on homogeneous data. However, the cochlea Dice score declined\nin 2023, likely due to the added complexity from tumour sub-annotations\naffecting overall segmentation performance. While progress is still needed for\nclinically acceptable VS segmentation, the plateauing performance suggests that\na more challenging cross-modal task may better serve future benchmarking.\n","authors":["Navodini Wijethilake","Reuben Dorent","Marina Ivory","Aaron Kujawa","Stefan Cornelissen","Patrick Langenhuizen","Mohamed Okasha","Anna Oviedova","Hexin Dong","Bogyeong Kang","Guillaume Sallé","Luyi Han","Ziyuan Zhao","Han Liu","Yubo Fan","Tao Yang","Shahad Hardan","Hussain Alasmawi","Santosh Sanjeev","Yuzhou Zhuang","Satoshi Kondo","Maria Baldeon Calisto","Shaikh Muhammad Uzair Noman","Cancan Chen","Ipek Oguz","Rongguo Zhang","Mina Rezaei","Susana K. Lai-Yuen","Satoshi Kasai","Yunzhi Huang","Chih-Cheng Hung","Mohammad Yaqub","Lisheng Wang","Benoit M. Dawant","Cuntai Guan","Ritse Mann","Vincent Jaouen","Tae-Eui Kam","Li Zhang","Jonathan Shapey","Tom Vercauteren"],"pdf_url":"https://arxiv.org/pdf/2506.12006v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18473v1","updated":"2025-07-24T14:48:44Z","published":"2025-07-24T14:48:44Z","title":"CRUISE: Cooperative Reconstruction and Editing in V2X Scenarios using\n  Gaussian Splatting","summary":"  Vehicle-to-everything (V2X) communication plays a crucial role in autonomous\ndriving, enabling cooperation between vehicles and infrastructure. While\nsimulation has significantly contributed to various autonomous driving tasks,\nits potential for data generation and augmentation in V2X scenarios remains\nunderexplored. In this paper, we introduce CRUISE, a comprehensive\nreconstruction-and-synthesis framework designed for V2X driving environments.\nCRUISE employs decomposed Gaussian Splatting to accurately reconstruct\nreal-world scenes while supporting flexible editing. By decomposing dynamic\ntraffic participants into editable Gaussian representations, CRUISE allows for\nseamless modification and augmentation of driving scenes. Furthermore, the\nframework renders images from both ego-vehicle and infrastructure views,\nenabling large-scale V2X dataset augmentation for training and evaluation. Our\nexperimental results demonstrate that: 1) CRUISE reconstructs real-world V2X\ndriving scenes with high fidelity; 2) using CRUISE improves 3D detection across\nego-vehicle, infrastructure, and cooperative views, as well as cooperative 3D\ntracking on the V2X-Seq benchmark; and 3) CRUISE effectively generates\nchallenging corner cases.\n","authors":["Haoran Xu","Saining Zhang","Peishuo Li","Baijun Ye","Xiaoxue Chen","Huan-ang Gao","Jv Zheng","Xiaowei Song","Ziqiao Peng","Run Miao","Jinrang Jia","Yifeng Shi","Guangqi Yi","Hang Zhao","Hao Tang","Hongyang Li","Kaicheng Yu","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2507.18473v1.pdf","comment":"IROS 2025, Code: https://github.com/SainingZhang/CRUISE"},{"id":"http://arxiv.org/abs/2507.18457v1","updated":"2025-07-24T14:37:00Z","published":"2025-07-24T14:37:00Z","title":"Revisiting Physically Realizable Adversarial Object Attack against\n  LiDAR-based Detection: Clarifying Problem Formulation and Experimental\n  Protocols","summary":"  Adversarial robustness in LiDAR-based 3D object detection is a critical\nresearch area due to its widespread application in real-world scenarios. While\nmany digital attacks manipulate point clouds or meshes, they often lack\nphysical realizability, limiting their practical impact. Physical adversarial\nobject attacks remain underexplored and suffer from poor reproducibility due to\ninconsistent setups and hardware differences. To address this, we propose a\ndevice-agnostic, standardized framework that abstracts key elements of physical\nadversarial object attacks, supports diverse methods, and provides open-source\ncode with benchmarking protocols in simulation and real-world settings. Our\nframework enables fair comparison, accelerates research, and is validated by\nsuccessfully transferring simulated attacks to a physical LiDAR system. Beyond\nthe framework, we offer insights into factors influencing attack success and\nadvance understanding of adversarial robustness in real-world LiDAR perception.\n","authors":["Luo Cheng","Hanwei Zhang","Lijun Zhang","Holger Hermanns"],"pdf_url":"https://arxiv.org/pdf/2507.18457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18447v1","updated":"2025-07-24T14:33:06Z","published":"2025-07-24T14:33:06Z","title":"PDB-Eval: An Evaluation of Large Multimodal Models for Description and\n  Explanation of Personalized Driving Behavior","summary":"  Understanding a driver's behavior and intentions is important for potential\nrisk assessment and early accident prevention. Safety and driver assistance\nsystems can be tailored to individual drivers' behavior, significantly\nenhancing their effectiveness. However, existing datasets are limited in\ndescribing and explaining general vehicle movements based on external visual\nevidence. This paper introduces a benchmark, PDB-Eval, for a detailed\nunderstanding of Personalized Driver Behavior, and aligning Large Multimodal\nModels (MLLMs) with driving comprehension and reasoning. Our benchmark consists\nof two main components, PDB-X and PDB-QA. PDB-X can evaluate MLLMs'\nunderstanding of temporal driving scenes. Our dataset is designed to find valid\nvisual evidence from the external view to explain the driver's behavior from\nthe internal view. To align MLLMs' reasoning abilities with driving tasks, we\npropose PDB-QA as a visual explanation question-answering task for MLLM\ninstruction fine-tuning. As a generic learning task for generative models like\nMLLMs, PDB-QA can bridge the domain gap without harming MLLMs'\ngeneralizability. Our evaluation indicates that fine-tuning MLLMs on\nfine-grained descriptions and explanations can effectively bridge the gap\nbetween MLLMs and the driving domain, which improves zero-shot performance on\nquestion-answering tasks by up to 73.2%. We further evaluate the MLLMs\nfine-tuned on PDB-X in Brain4Cars' intention prediction and AIDE's recognition\ntasks. We observe up to 12.5% performance improvements on the turn intention\nprediction task in Brain4Cars, and consistent performance improvements up to\n11.0% on all tasks in AIDE.\n","authors":["Junda Wu","Jessica Echterhoff","Kyungtae Han","Amr Abdelraouf","Rohit Gupta","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2507.18447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18444v1","updated":"2025-07-24T14:29:30Z","published":"2025-07-24T14:29:30Z","title":"DSFormer: A Dual-Scale Cross-Learning Transformer for Visual Place\n  Recognition","summary":"  Visual Place Recognition (VPR) is crucial for robust mobile robot\nlocalization, yet it faces significant challenges in maintaining reliable\nperformance under varying environmental conditions and viewpoints. To address\nthis, we propose a novel framework that integrates Dual-Scale-Former\n(DSFormer), a Transformer-based cross-learning module, with an innovative block\nclustering strategy. DSFormer enhances feature representation by enabling\nbidirectional information transfer between dual-scale features extracted from\nthe final two CNN layers, capturing both semantic richness and spatial details\nthrough self-attention for long-range dependencies within each scale and shared\ncross-attention for cross-scale learning. Complementing this, our block\nclustering strategy repartitions the widely used San Francisco eXtra Large\n(SF-XL) training dataset from multiple distinct perspectives, optimizing data\norganization to further bolster robustness against viewpoint variations.\nTogether, these innovations not only yield a robust global embedding adaptable\nto environmental changes but also reduce the required training data volume by\napproximately 30\\% compared to previous partitioning methods. Comprehensive\nexperiments demonstrate that our approach achieves state-of-the-art performance\nacross most benchmark datasets, surpassing advanced reranking methods like\nDELG, Patch-NetVLAD, TransVPR, and R2Former as a global retrieval solution\nusing 512-dim global descriptors, while significantly improving computational\nefficiency.\n","authors":["Haiyang Jiang","Songhao Piao","Chao Gao","Lei Yu","Liguo Chen"],"pdf_url":"https://arxiv.org/pdf/2507.18444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18433v1","updated":"2025-07-24T14:12:20Z","published":"2025-07-24T14:12:20Z","title":"DiagR1: A Vision-Language Model Trained via Reinforcement Learning for\n  Digestive Pathology Diagnosis","summary":"  Multimodal large models have shown great potential in automating pathology\nimage analysis. However, current multimodal models for gastrointestinal\npathology are constrained by both data quality and reasoning transparency:\npervasive noise and incomplete annotations in public datasets predispose vision\nlanguage models to factual hallucinations when generating diagnostic text,\nwhile the absence of explicit intermediate reasoning chains renders the outputs\ndifficult to audit and thus less trustworthy in clinical practice. To address\nthese issues, we construct a large scale gastrointestinal pathology dataset\ncontaining both microscopic descriptions and diagnostic conclusions, and\npropose a prompt argumentation strategy that incorporates lesion classification\nand anatomical site information. This design guides the model to better capture\nimage specific features and maintain semantic consistency in generation.\nFurthermore, we employ a post training pipeline that combines supervised fine\ntuning with Group Relative Policy Optimization (GRPO) to improve reasoning\nquality and output structure. Experimental results on real world pathology\nreport generation tasks demonstrate that our approach significantly outperforms\nstate of the art open source and proprietary baselines in terms of generation\nquality, structural completeness, and clinical relevance. Our solution\noutperforms state of the art models with 18.7% higher clinical relevance, 32.4%\nimproved structural completeness, and 41.2% fewer diagnostic errors,\ndemonstrating superior accuracy and clinical utility compared to existing\nsolutions.\n","authors":["Minxi Ouyang","Lianghui Zhu","Yaqing Bao","Qiang Huang","Jingli Ouyang","Tian Guan","Xitong Ling","Jiawen Li","Song Duan","Wenbin Dai","Li Zheng","Xuemei Zhang","Yonghong He"],"pdf_url":"https://arxiv.org/pdf/2507.18433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18429v1","updated":"2025-07-24T14:08:33Z","published":"2025-07-24T14:08:33Z","title":"NLML-HPE: Head Pose Estimation with Limited Data via Manifold Learning","summary":"  Head pose estimation (HPE) plays a critical role in various computer vision\napplications such as human-computer interaction and facial recognition. In this\npaper, we propose a novel deep learning approach for head pose estimation with\nlimited training data via non-linear manifold learning called NLML-HPE. This\nmethod is based on the combination of tensor decomposition (i.e., Tucker\ndecomposition) and feed forward neural networks. Unlike traditional\nclassification-based approaches, our method formulates head pose estimation as\na regression problem, mapping input landmarks into a continuous representation\nof pose angles. To this end, our method uses tensor decomposition to split each\nEuler angle (yaw, pitch, roll) to separate subspaces and models each dimension\nof the underlying manifold as a cosine curve. We address two key challenges: 1.\nAlmost all HPE datasets suffer from incorrect and inaccurate pose annotations.\nHence, we generated a precise and consistent 2D head pose dataset for our\ntraining set by rotating 3D head models for a fixed set of poses and rendering\nthe corresponding 2D images. 2. We achieved real-time performance with limited\ntraining data as our method accurately captures the nature of rotation of an\nobject from facial landmarks. Once the underlying manifold for rotation around\neach axis is learned, the model is very fast in predicting unseen data. Our\ntraining and testing code is available online along with our trained models:\nhttps: //github.com/MahdiGhafoorian/NLML_HPE.\n","authors":["Mahdi Ghafourian","Federico M. Sukno"],"pdf_url":"https://arxiv.org/pdf/2507.18429v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10510v4","updated":"2025-07-24T14:06:15Z","published":"2024-12-13T19:11:18Z","title":"DEFAME: Dynamic Evidence-based FAct-checking with Multimodal Experts","summary":"  The proliferation of disinformation demands reliable and scalable\nfact-checking solutions. We present Dynamic Evidence-based FAct-checking with\nMultimodal Experts (DEFAME), a modular, zero-shot MLLM pipeline for\nopen-domain, text-image claim verification. DEFAME operates in a six-stage\nprocess, dynamically selecting the tools and search depth to extract and\nevaluate textual and visual evidence. Unlike prior approaches that are\ntext-only, lack explainability, or rely solely on parametric knowledge, DEFAME\nperforms end-to-end verification, accounting for images in claims and evidence\nwhile generating structured, multimodal reports. Evaluation on the popular\nbenchmarks VERITE, AVerITeC, and MOCHEG shows that DEFAME surpasses all\nprevious methods, establishing itself as the new state-of-the-art fact-checking\nsystem for uni- and multimodal fact-checking. Moreover, we introduce a new\nmultimodal benchmark, ClaimReview2024+, featuring claims after the knowledge\ncutoff of GPT-4o, avoiding data leakage. Here, DEFAME drastically outperforms\nthe GPT-4o baselines, showing temporal generalizability and the potential for\nreal-time fact-checking.\n","authors":["Tobias Braun","Mark Rothermel","Marcus Rohrbach","Anna Rohrbach"],"pdf_url":"https://arxiv.org/pdf/2412.10510v4.pdf","comment":"ICML 2025 version. 9 pages main paper, 35 pages with appendix, 18\n  figures and 7 tables. Corrected two inconsistent numbers in Table 2"},{"id":"http://arxiv.org/abs/2507.18424v1","updated":"2025-07-24T14:01:02Z","published":"2025-07-24T14:01:02Z","title":"Self-Supervised Ultrasound-Video Segmentation with Feature Prediction\n  and 3D Localised Loss","summary":"  Acquiring and annotating large datasets in ultrasound imaging is challenging\ndue to low contrast, high noise, and susceptibility to artefacts. This process\nrequires significant time and clinical expertise. Self-supervised learning\n(SSL) offers a promising solution by leveraging unlabelled data to learn useful\nrepresentations, enabling improved segmentation performance when annotated data\nis limited. Recent state-of-the-art developments in SSL for video data include\nV-JEPA, a framework solely based on feature prediction, avoiding pixel level\nreconstruction or negative samples. We hypothesise that V-JEPA is well-suited\nto ultrasound imaging, as it is less sensitive to noisy pixel-level detail\nwhile effectively leveraging temporal information. To the best of our\nknowledge, this is the first study to adopt V-JEPA for ultrasound video data.\nSimilar to other patch-based masking SSL techniques such as VideoMAE, V-JEPA is\nwell-suited to ViT-based models. However, ViTs can underperform on small\nmedical datasets due to lack of inductive biases, limited spatial locality and\nabsence of hierarchical feature learning. To improve locality understanding, we\npropose a novel 3D localisation auxiliary task to improve locality in ViT\nrepresentations during V-JEPA pre-training. Our results show V-JEPA with our\nauxiliary task improves segmentation performance significantly across various\nfrozen encoder configurations, with gains up to 3.4\\% using 100\\% and up to\n8.35\\% using only 10\\% of the training data.\n","authors":["Edward Ellis","Robert Mendel","Andrew Bulpitt","Nasim Parsa","Michael F Byrne","Sharib Ali"],"pdf_url":"https://arxiv.org/pdf/2507.18424v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02452v3","updated":"2025-07-24T13:59:57Z","published":"2025-02-04T16:19:20Z","title":"Personalization Toolkit: Training Free Personalization of Large Vision\n  Language Models","summary":"  Personalization of Large Vision-Language Models (LVLMs) involves customizing\nmodels to recognize specific users and object instances, and to generate\ncontextually tailored responses. Existing approaches typically rely on\ntime-consuming test-time training for each user or object, making them\nimpractical for real-world deployment, a limitation reflected in current\npersonalization benchmarks, which are focused on object-centric, single-concept\nevaluations. In this paper, we present a novel training-free approach to LVLM\npersonalization and introduce a comprehensive real-world benchmark designed to\nrigorously evaluate various aspects of the personalization task. Our method\nleverages pre-trained vision foundation models to extract distinctive features,\napplies retrieval-augmented generation (RAG) techniques to identify instances\nwithin visual inputs, and employs visual prompting strategies to guide model\noutputs. Our model-agnostic vision toolkit enables efficient and flexible\nmulti-concept personalization across both images and videos, without any\nadditional training. We achieve state-of-the-art results, surpassing existing\ntraining-based methods.\n","authors":["Soroush Seifi","Vaggelis Dorovatas","Daniel Olmeda Reino","Rahaf Aljundi"],"pdf_url":"https://arxiv.org/pdf/2502.02452v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06735v2","updated":"2025-07-24T13:57:08Z","published":"2025-07-09T10:48:00Z","title":"Residual Prior-driven Frequency-aware Network for Image Fusion","summary":"  Image fusion aims to integrate complementary information across modalities to\ngenerate high-quality fused images, thereby enhancing the performance of\nhigh-level vision tasks. While global spatial modeling mechanisms show\npromising results, constructing long-range feature dependencies in the spatial\ndomain incurs substantial computational costs. Additionally, the absence of\nground-truth exacerbates the difficulty of capturing complementary features\neffectively. To tackle these challenges, we propose a Residual Prior-driven\nFrequency-aware Network, termed as RPFNet. Specifically, RPFNet employs a\ndual-branch feature extraction framework: the Residual Prior Module (RPM)\nextracts modality-specific difference information from residual maps, thereby\nproviding complementary priors for fusion; the Frequency Domain Fusion Module\n(FDFM) achieves efficient global feature modeling and integration through\nfrequency-domain convolution. Additionally, the Cross Promotion Module (CPM)\nenhances the synergistic perception of local details and global structures\nthrough bidirectional feature interaction. During training, we incorporate an\nauxiliary decoder and saliency structure loss to strengthen the model's\nsensitivity to modality-specific differences. Furthermore, a combination of\nadaptive weight-based frequency contrastive loss and SSIM loss effectively\nconstrains the solution space, facilitating the joint capture of local details\nand global features while ensuring the retention of complementary information.\nExtensive experiments validate the fusion performance of RPFNet, which\neffectively integrates discriminative features, enhances texture details and\nsalient objects, and can effectively facilitate the deployment of the\nhigh-level vision task.\n","authors":["Guan Zheng","Xue Wang","Wenhua Qian","Peng Liu","Runzhuo Ma"],"pdf_url":"https://arxiv.org/pdf/2507.06735v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17727v2","updated":"2025-07-24T13:55:49Z","published":"2025-07-23T17:41:55Z","title":"CA-Cut: Crop-Aligned Cutout for Data Augmentation to Learn More Robust\n  Under-Canopy Navigation","summary":"  State-of-the-art visual under-canopy navigation methods are designed with\ndeep learning-based perception models to distinguish traversable space from\ncrop rows. While these models have demonstrated successful performance, they\nrequire large amounts of training data to ensure reliability in real-world\nfield deployment. However, data collection is costly, demanding significant\nhuman resources for in-field sampling and annotation. To address this\nchallenge, various data augmentation techniques are commonly employed during\nmodel training, such as color jittering, Gaussian blur, and horizontal flip, to\ndiversify training data and enhance model robustness. In this paper, we\nhypothesize that utilizing only these augmentation techniques may lead to\nsuboptimal performance, particularly in complex under-canopy environments with\nfrequent occlusions, debris, and non-uniform spacing of crops. Instead, we\npropose a novel augmentation method, so-called Crop-Aligned Cutout (CA-Cut)\nwhich masks random regions out in input images that are spatially distributed\naround crop rows on the sides to encourage trained models to capture high-level\ncontextual features even when fine-grained information is obstructed. Our\nextensive experiments with a public cornfield dataset demonstrate that\nmasking-based augmentations are effective for simulating occlusions and\nsignificantly improving robustness in semantic keypoint predictions for visual\nnavigation. In particular, we show that biasing the mask distribution toward\ncrop rows in CA-Cut is critical for enhancing both prediction accuracy and\ngeneralizability across diverse environments achieving up to a 36.9% reduction\nin prediction error. In addition, we conduct ablation studies to determine the\nnumber of masks, the size of each mask, and the spatial distribution of masks\nto maximize overall performance.\n","authors":["Robel Mamo","Taeyeong Choi"],"pdf_url":"https://arxiv.org/pdf/2507.17727v2.pdf","comment":"Accepted for publication at the 12th European Conference on Mobile\n  Robots (ECMR 2025)"},{"id":"http://arxiv.org/abs/2507.18407v1","updated":"2025-07-24T13:46:50Z","published":"2025-07-24T13:46:50Z","title":"DCFFSNet: Deep Connectivity Feature Fusion Separation Network for\n  Medical Image Segmentation","summary":"  Medical image segmentation leverages topological connectivity theory to\nenhance edge precision and regional consistency. However, existing deep\nnetworks integrating connectivity often forcibly inject it as an additional\nfeature module, resulting in coupled feature spaces with no standardized\nmechanism to quantify different feature strengths. To address these issues, we\npropose DCFFSNet (Dual-Connectivity Feature Fusion-Separation Network). It\nintroduces an innovative feature space decoupling strategy. This strategy\nquantifies the relative strength between connectivity features and other\nfeatures. It then builds a deep connectivity feature fusion-separation\narchitecture. This architecture dynamically balances multi-scale feature\nexpression. Experiments were conducted on the ISIC2018, DSB2018, and MoNuSeg\ndatasets. On ISIC2018, DCFFSNet outperformed the next best model (CMUNet) by\n1.3% (Dice) and 1.2% (IoU). On DSB2018, it surpassed TransUNet by 0.7% (Dice)\nand 0.9% (IoU). On MoNuSeg, it exceeded CSCAUNet by 0.8% (Dice) and 0.9% (IoU).\nThe results demonstrate that DCFFSNet exceeds existing mainstream methods\nacross all metrics. It effectively resolves segmentation fragmentation and\nachieves smooth edge transitions. This significantly enhances clinical\nusability.\n","authors":["Xun Ye","Ruixiang Tang","Mingda Zhang","Jianglong Qin"],"pdf_url":"https://arxiv.org/pdf/2507.18407v1.pdf","comment":"16 pages , 11 figures"},{"id":"http://arxiv.org/abs/2507.18405v1","updated":"2025-07-24T13:45:48Z","published":"2025-07-24T13:45:48Z","title":"Iwin Transformer: Hierarchical Vision Transformer using Interleaved\n  Windows","summary":"  We introduce Iwin Transformer, a novel position-embedding-free hierarchical\nvision transformer, which can be fine-tuned directly from low to high\nresolution, through the collaboration of innovative interleaved window\nattention and depthwise separable convolution. This approach uses attention to\nconnect distant tokens and applies convolution to link neighboring tokens,\nenabling global information exchange within a single module, overcoming Swin\nTransformer's limitation of requiring two consecutive blocks to approximate\nglobal attention. Extensive experiments on visual benchmarks demonstrate that\nIwin Transformer exhibits strong competitiveness in tasks such as image\nclassification (87.4 top-1 accuracy on ImageNet-1K), semantic segmentation and\nvideo action recognition. We also validate the effectiveness of the core\ncomponent in Iwin as a standalone module that can seamlessly replace the\nself-attention module in class-conditional image generation. The concepts and\nmethods introduced by the Iwin Transformer have the potential to inspire future\nresearch, like Iwin 3D Attention in video generation. The code and models are\navailable at https://github.com/cominder/Iwin-Transformer.\n","authors":["Simin Huo","Ning Li"],"pdf_url":"https://arxiv.org/pdf/2507.18405v1.pdf","comment":"14 pages, 10 figures, Submitted to IEEE Transactions on Pattern\n  Analysis and Machine Intelligence"},{"id":"http://arxiv.org/abs/2507.15292v4","updated":"2025-07-24T13:26:19Z","published":"2025-07-21T06:47:44Z","title":"EndoControlMag: Robust Endoscopic Vascular Motion Magnification with\n  Periodic Reference Resetting and Hierarchical Tissue-aware Dual-Mask Control","summary":"  Visualizing subtle vascular motions in endoscopic surgery is crucial for\nsurgical precision and decision-making, yet remains challenging due to the\ncomplex and dynamic nature of surgical scenes. To address this, we introduce\nEndoControlMag, a training-free, Lagrangian-based framework with\nmask-conditioned vascular motion magnification tailored to endoscopic\nenvironments. Our approach features two key modules: a Periodic Reference\nResetting (PRR) scheme that divides videos into short overlapping clips with\ndynamically updated reference frames to prevent error accumulation while\nmaintaining temporal coherence, and a Hierarchical Tissue-aware Magnification\n(HTM) framework with dual-mode mask dilation. HTM first tracks vessel cores\nusing a pretrained visual tracking model to maintain accurate localization\ndespite occlusions and view changes. It then applies one of two adaptive\nsoftening strategies to surrounding tissues: motion-based softening that\nmodulates magnification strength proportional to observed tissue displacement,\nor distance-based exponential decay that simulates biomechanical force\nattenuation. This dual-mode approach accommodates diverse surgical\nscenarios-motion-based softening excels with complex tissue deformations while\ndistance-based softening provides stability during unreliable optical flow\nconditions. We evaluate EndoControlMag on our EndoVMM24 dataset spanning four\ndifferent surgery types and various challenging scenarios, including\nocclusions, instrument disturbance, view changes, and vessel deformations.\nQuantitative metrics, visual assessments, and expert surgeon evaluations\ndemonstrate that EndoControlMag significantly outperforms existing methods in\nboth magnification accuracy and visual quality while maintaining robustness\nacross challenging surgical conditions. The code, dataset, and video results\nare available at https://szupc.github.io/EndoControlMag/.\n","authors":["An Wang","Rulin Zhou","Mengya Xu","Yiru Ye","Longfei Gou","Yiting Chang","Hao Chen","Chwee Ming Lim","Jiankun Wang","Hongliang Ren"],"pdf_url":"https://arxiv.org/pdf/2507.15292v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11937v4","updated":"2025-07-24T13:24:21Z","published":"2025-03-15T01:06:34Z","title":"Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I\n  Diffusion Adapter via Conditional Variational Autoencoder","summary":"  Text-to-Image (T2I) Diffusion Models have achieved remarkable performance in\ngenerating high quality images. However, enabling precise control of continuous\nattributes, especially multiple attributes simultaneously, in a new domain\n(e.g., numeric values like eye openness or car width) with text-only guidance\nremains a significant challenge. To address this, we introduce the Attribute\n(Att) Adapter, a novel plug-and-play module designed to enable fine-grained,\nmulti-attributes control in pretrained diffusion models. Our approach learns a\nsingle control adapter from a set of sample images that can be unpaired and\ncontain multiple visual attributes. The Att-Adapter leverages the decoupled\ncross attention module to naturally harmonize the multiple domain attributes\nwith text conditioning. We further introduce Conditional Variational\nAutoencoder (CVAE) to the Att-Adapter to mitigate overfitting, matching the\ndiverse nature of the visual world. Evaluations on two public datasets show\nthat Att-Adapter outperforms all LoRA-based baselines in controlling continuous\nattributes. Additionally, our method enables a broader control range and also\nimproves disentanglement across multiple attributes, surpassing StyleGAN-based\ntechniques. Notably, Att-Adapter is flexible, requiring no paired synthetic\ndata for training, and is easily scalable to multiple attributes within a\nsingle model.\n","authors":["Wonwoong Cho","Yan-Ying Chen","Matthew Klenk","David I. Inouye","Yanxia Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.11937v4.pdf","comment":"ICCV'25 (Highlight), The project page is available at\n  https://tri-mac.github.io/att-adapter/"},{"id":"http://arxiv.org/abs/2507.18385v1","updated":"2025-07-24T12:59:42Z","published":"2025-07-24T12:59:42Z","title":"HumanMaterial: Human Material Estimation from a Single Image via\n  Progressive Training","summary":"  Full-body Human inverse rendering based on physically-based rendering aims to\nacquire high-quality materials, which helps achieve photo-realistic rendering\nunder arbitrary illuminations. This task requires estimating multiple material\nmaps and usually relies on the constraint of rendering result. The absence of\nconstraints on the material maps makes inverse rendering an ill-posed task.\nPrevious works alleviated this problem by building material dataset for\ntraining, but their simplified material data and rendering equation lead to\nrendering results with limited realism, especially that of skin. To further\nalleviate this problem, we construct a higher-quality dataset (OpenHumanBRDF)\nbased on scanned real data and statistical material data. In addition to the\nnormal, diffuse albedo, roughness, specular albedo, we produce displacement and\nsubsurface scattering to enhance the realism of rendering results, especially\nfor the skin. With the increase in prediction tasks for more materials, using\nan end-to-end model as in the previous work struggles to balance the importance\namong various material maps, and leads to model underfitting. Therefore, we\ndesign a model (HumanMaterial) with progressive training strategy to make full\nuse of the supervision information of the material maps and improve the\nperformance of material estimation. HumanMaterial first obtain the initial\nmaterial results via three prior models, and then refine the results by a\nfinetuning model. Prior models estimate different material maps, and each map\nhas different significance for rendering results. Thus, we design a Controlled\nPBR Rendering (CPR) loss, which enhances the importance of the materials to be\noptimized during the training of prior models. Extensive experiments on\nOpenHumanBRDF dataset and real data demonstrate that our method achieves\nstate-of-the-art performance.\n","authors":["Yu Jiang","Jiahao Xia","Jiongming Qin","Yusen Wang","Tuo Cao","Chunxia Xiao"],"pdf_url":"https://arxiv.org/pdf/2507.18385v1.pdf","comment":"14"},{"id":"http://arxiv.org/abs/2409.05260v3","updated":"2025-07-24T12:58:54Z","published":"2024-09-09T01:11:47Z","title":"Scalable Frame Sampling for Video Classification: A Semi-Optimal Policy\n  Approach with Reduced Search Space","summary":"  Given a video with $T$ frames, frame sampling is a task to select $N \\ll T$\nframes, so as to maximize the performance of a fixed video classifier. Not just\nbrute-force search, but most existing methods suffer from its vast search space\nof $\\binom{T}{N}$, especially when $N$ gets large. To address this challenge,\nwe introduce a novel perspective of reducing the search space from $O(T^N)$ to\n$O(T)$. Instead of exploring the entire $O(T^N)$ space, our proposed\nsemi-optimal policy selects the top $N$ frames based on the independently\nestimated value of each frame using per-frame confidence, significantly\nreducing the computational complexity. We verify that our semi-optimal policy\ncan efficiently approximate the optimal policy, particularly under practical\nsettings. Additionally, through extensive experiments on various datasets and\nmodel architectures, we demonstrate that learning our semi-optimal policy\nensures stable and high performance regardless of the size of $N$ and $T$.\n","authors":["Junho Lee","Jeongwoo Shin","Seung Woo Ko","Seongsu Ha","Joonseok Lee"],"pdf_url":"https://arxiv.org/pdf/2409.05260v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18382v1","updated":"2025-07-24T12:57:22Z","published":"2025-07-24T12:57:22Z","title":"Towards Consistent Long-Term Pose Generation","summary":"  Current approaches to pose generation rely heavily on intermediate\nrepresentations, either through two-stage pipelines with quantization or\nautoregressive models that accumulate errors during inference. This fundamental\nlimitation leads to degraded performance, particularly in long-term pose\ngeneration where maintaining temporal coherence is crucial. We propose a novel\none-stage architecture that directly generates poses in continuous coordinate\nspace from minimal context - a single RGB image and text description - while\nmaintaining consistent distributions between training and inference. Our key\ninnovation is eliminating the need for intermediate representations or\ntoken-based generation by operating directly on pose coordinates through a\nrelative movement prediction mechanism that preserves spatial relationships,\nand a unified placeholder token approach that enables single-forward generation\nwith identical behavior during training and inference. Through extensive\nexperiments on Penn Action and First-Person Hand Action Benchmark (F-PHAB)\ndatasets, we demonstrate that our approach significantly outperforms existing\nquantization-based and autoregressive methods, especially in long-term\ngeneration scenarios.\n","authors":["Yayuan Li","Filippos Bellos","Jason Corso"],"pdf_url":"https://arxiv.org/pdf/2507.18382v1.pdf","comment":"10 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2504.21706v3","updated":"2025-07-24T12:57:18Z","published":"2025-04-30T14:50:02Z","title":"Vision Transformers in Precision Agriculture: A Comprehensive Survey","summary":"  Detecting plant diseases is a crucial aspect of modern agriculture, as it\nplays a key role in maintaining crop health and increasing overall yield.\nTraditional approaches, though still valuable, often rely on manual inspection\nor conventional machine learning techniques, both of which face limitations in\nscalability and accuracy. Recently, Vision Transformers (ViTs) have emerged as\na promising alternative, offering advantages such as improved handling of\nlong-range dependencies and better scalability for visual tasks. This review\nexplores the application of ViTs in precision agriculture, covering a range of\ntasks. We begin by introducing the foundational architecture of ViTs and\ndiscussing their transition from Natural Language Processing (NLP) to Computer\nVision. The discussion includes the concept of inductive bias in traditional\nmodels like Convolutional Neural Networks (CNNs), and how ViTs mitigate these\nbiases. We provide a comprehensive review of recent literature, focusing on key\nmethodologies, datasets, and performance metrics. This study also includes a\ncomparative analysis of CNNs and ViTs, along with a review of hybrid models and\nperformance enhancements. Technical challenges such as data requirements,\ncomputational demands, and model interpretability are addressed, along with\npotential solutions. Finally, we outline future research directions and\ntechnological advancements that could further support the integration of ViTs\nin real-world agricultural settings. Our goal with this study is to offer\npractitioners and researchers a deeper understanding of how ViTs are poised to\ntransform smart and precision agriculture.\n","authors":["Saber Mehdipour","Seyed Abolghasem Mirroshandel","Seyed Amirhossein Tabatabaei"],"pdf_url":"https://arxiv.org/pdf/2504.21706v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18374v1","updated":"2025-07-24T12:50:46Z","published":"2025-07-24T12:50:46Z","title":"Towards Effective Human-in-the-Loop Assistive AI Agents","summary":"  Effective human-AI collaboration for physical task completion has significant\npotential in both everyday activities and professional domains. AI agents\nequipped with informative guidance can enhance human performance, but\nevaluating such collaboration remains challenging due to the complexity of\nhuman-in-the-loop interactions. In this work, we introduce an evaluation\nframework and a multimodal dataset of human-AI interactions designed to assess\nhow AI guidance affects procedural task performance, error reduction and\nlearning outcomes. Besides, we develop an augmented reality (AR)-equipped AI\nagent that provides interactive guidance in real-world tasks, from cooking to\nbattlefield medicine. Through human studies, we share empirical insights into\nAI-assisted human performance and demonstrate that AI-assisted collaboration\nimproves task completion.\n","authors":["Filippos Bellos","Yayuan Li","Cary Shu","Ruey Day","Jeffrey M. Siskind","Jason J. Corso"],"pdf_url":"https://arxiv.org/pdf/2507.18374v1.pdf","comment":"10 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2507.18371v1","updated":"2025-07-24T12:48:14Z","published":"2025-07-24T12:48:14Z","title":"MVG4D: Image Matrix-Based Multi-View and Motion Generation for 4D\n  Content Creation from a Single Image","summary":"  Advances in generative modeling have significantly enhanced digital content\ncreation, extending from 2D images to complex 3D and 4D scenes. Despite\nsubstantial progress, producing high-fidelity and temporally consistent dynamic\n4D content remains a challenge. In this paper, we propose MVG4D, a novel\nframework that generates dynamic 4D content from a single still image by\ncombining multi-view synthesis with 4D Gaussian Splatting (4D GS). At its core,\nMVG4D employs an image matrix module that synthesizes temporally coherent and\nspatially diverse multi-view images, providing rich supervisory signals for\ndownstream 3D and 4D reconstruction. These multi-view images are used to\noptimize a 3D Gaussian point cloud, which is further extended into the temporal\ndomain via a lightweight deformation network. Our method effectively enhances\ntemporal consistency, geometric fidelity, and visual realism, addressing key\nchallenges in motion discontinuity and background degradation that affect prior\n4D GS-based methods. Extensive experiments on the Objaverse dataset demonstrate\nthat MVG4D outperforms state-of-the-art baselines in CLIP-I, PSNR, FVD, and\ntime efficiency. Notably, it reduces flickering artifacts and sharpens\nstructural details across views and time, enabling more immersive AR/VR\nexperiences. MVG4D sets a new direction for efficient and controllable 4D\ngeneration from minimal inputs.\n","authors":["Xiaotian Chen","DongFu Yin","Fei Richard Yu","Xuanchen Li","Xinhao Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.18371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17347v2","updated":"2025-07-24T12:46:21Z","published":"2025-07-23T09:28:25Z","title":"Swin-TUNA : A Novel PEFT Approach for Accurate Food Image Segmentation","summary":"  In the field of food image processing, efficient semantic segmentation\ntechniques are crucial for industrial applications. However, existing\nlarge-scale Transformer-based models (such as FoodSAM) face challenges in\nmeeting practical deploymentrequirements due to their massive parameter counts\nand high computational resource demands. This paper introduces TUNable Adapter\nmodule (Swin-TUNA), a Parameter Efficient Fine-Tuning (PEFT) method that\nintegrates multiscale trainable adapters into the Swin Transformer\narchitecture, achieving high-performance food image segmentation by updating\nonly 4% of the parameters. The core innovation of Swin-TUNA lies in its\nhierarchical feature adaptation mechanism: it designs separable convolutions in\ndepth and dimensional mappings of varying scales to address the differences in\nfeatures between shallow and deep networks, combined with a dynamic balancing\nstrategy for tasks-agnostic and task-specific features. Experiments demonstrate\nthat this method achieves mIoU of 50.56% and 74.94% on the FoodSeg103 and\nUECFoodPix Complete datasets, respectively, surpassing the fully parameterized\nFoodSAM model while reducing the parameter count by 98.7% (to only 8.13M).\nFurthermore, Swin-TUNA exhibits faster convergence and stronger generalization\ncapabilities in low-data scenarios, providing an efficient solution for\nassembling lightweight food image.\n","authors":["Haotian Chen","Zhiyong Xiao"],"pdf_url":"https://arxiv.org/pdf/2507.17347v2.pdf","comment":"After discussion among the authors, some parts of the paper are\n  deemed inappropriate and will be revised and resubmitted"},{"id":"http://arxiv.org/abs/2507.02987v3","updated":"2025-07-24T12:44:31Z","published":"2025-07-01T11:14:45Z","title":"Leveraging the Structure of Medical Data for Improved Representation\n  Learning","summary":"  Building generalizable medical AI systems requires pretraining strategies\nthat are data-efficient and domain-aware. Unlike internet-scale corpora,\nclinical datasets such as MIMIC-CXR offer limited image counts and scarce\nannotations, but exhibit rich internal structure through multi-view imaging. We\npropose a self-supervised framework that leverages the inherent structure of\nmedical datasets. Specifically, we treat paired chest X-rays (i.e., frontal and\nlateral views) as natural positive pairs, learning to reconstruct each view\nfrom sparse patches while aligning their latent embeddings. Our method requires\nno textual supervision and produces informative representations. Evaluated on\nMIMIC-CXR, we show strong performance compared to supervised objectives and\nbaselines being trained without leveraging structure. This work provides a\nlightweight, modality-agnostic blueprint for domain-specific pretraining where\ndata is structured but scarce\n","authors":["Andrea Agostini","Sonia Laguna","Alain Ryser","Samuel Ruiperez-Campillo","Moritz Vandenhirtz","Nicolas Deperrois","Farhad Nooralahzadeh","Michael Krauthammer","Thomas M. Sutter","Julia E. Vogt"],"pdf_url":"https://arxiv.org/pdf/2507.02987v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.19634v4","updated":"2025-07-24T12:39:27Z","published":"2025-04-28T09:49:35Z","title":"NSegment : Label-specific Deformations for Remote Sensing Image\n  Segmentation","summary":"  Labeling errors in remote sensing (RS) image segmentation datasets often\nremain implicit and subtle due to ambiguous class boundaries, mixed pixels,\nshadows, complex terrain features, and subjective annotator bias. Furthermore,\nthe scarcity of annotated RS data due to high image acquisition and labeling\ncosts complicates training noise-robust models. While sophisticated mechanisms\nsuch as label selection or noise correction might address this issue, they tend\nto increase training time and add implementation complexity. In this letter, we\npropose NSegment-a simple yet effective data augmentation solution to mitigate\nthis issue. Unlike traditional methods, it applies elastic transformations only\nto segmentation labels, varying deformation intensity per sample in each\ntraining epoch to address annotation inconsistencies. Experimental results\ndemonstrate that our approach improves the performance of RS image segmentation\non various state-of-the-art models.\n","authors":["Yechan Kim","DongHo Yoon","SooYeon Kim","Moongu Jeon"],"pdf_url":"https://arxiv.org/pdf/2504.19634v4.pdf","comment":"The paper is being revised substantially and will be resubmitted."},{"id":"http://arxiv.org/abs/2507.18362v1","updated":"2025-07-24T12:33:10Z","published":"2025-07-24T12:33:10Z","title":"UniSegDiff: Boosting Unified Lesion Segmentation via a Staged Diffusion\n  Model","summary":"  The Diffusion Probabilistic Model (DPM) has demonstrated remarkable\nperformance across a variety of generative tasks. The inherent randomness in\ndiffusion models helps address issues such as blurring at the edges of medical\nimages and labels, positioning Diffusion Probabilistic Models (DPMs) as a\npromising approach for lesion segmentation. However, we find that the current\ntraining and inference strategies of diffusion models result in an uneven\ndistribution of attention across different timesteps, leading to longer\ntraining times and suboptimal solutions. To this end, we propose UniSegDiff, a\nnovel diffusion model framework designed to address lesion segmentation in a\nunified manner across multiple modalities and organs. This framework introduces\na staged training and inference approach, dynamically adjusting the prediction\ntargets at different stages, forcing the model to maintain high attention\nacross all timesteps, and achieves unified lesion segmentation through\npre-training the feature extraction network for segmentation. We evaluate\nperformance on six different organs across various imaging modalities.\nComprehensive experimental results demonstrate that UniSegDiff significantly\noutperforms previous state-of-the-art (SOTA) approaches. The code is available\nat https://github.com/HUYILONG-Z/UniSegDiff.\n","authors":["Yilong Hu","Shijie Chang","Lihe Zhang","Feng Tian","Weibing Sun","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2507.18362v1.pdf","comment":"MICCAI2025"},{"id":"http://arxiv.org/abs/2507.18354v1","updated":"2025-07-24T12:25:45Z","published":"2025-07-24T12:25:45Z","title":"Deformable Convolution Module with Globally Learned Relative Offsets for\n  Fundus Vessel Segmentation","summary":"  Deformable convolution can adaptively change the shape of convolution kernel\nby learning offsets to deal with complex shape features. We propose a novel\nplug and play deformable convolutional module that uses attention and\nfeedforward networks to learn offsets, so that the deformable patterns can\ncapture long-distance global features. Compared with previously existing\ndeformable convolutions, the proposed module learns the sub pixel displacement\nfield and adaptively warps the feature maps across all channels rather than\ndirectly deforms the convolution kernel , which is equivalent to a relative\ndeformation of the kernel sampling grids, achieving global feature deformation\nand the decoupling of kernel size and learning network. Considering that the\nfundus blood vessels have globally self similar complex edges, we design a deep\nlearning model for fundus blood vessel segmentation, GDCUnet, based on the\nproposed convolutional module. Empirical evaluations under the same\nconfiguration and unified framework show that GDCUnet has achieved state of the\nart performance on public datasets. Further ablation experiments demonstrated\nthat the proposed deformable convolutional module could more significantly\nlearn the complex features of fundus blood vessels, enhancing the model\nrepresentation and generalization capabilities.The proposed module is similar\nto the interface of conventional convolution, we suggest applying it to more\nmachine vision tasks with complex global self similar features.\n","authors":["Lexuan Zhu","Yuxuan Li","Yuning Ren"],"pdf_url":"https://arxiv.org/pdf/2507.18354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00046v2","updated":"2025-07-24T12:24:50Z","published":"2025-04-30T03:31:40Z","title":"SR-NeRV: Improving Embedding Efficiency of Neural Video Representation\n  via Super-Resolution","summary":"  Implicit Neural Representations (INRs) have garnered significant attention\nfor their ability to model complex signals in various domains. Recently,\nINR-based frameworks have shown promise in neural video compression by\nembedding video content into compact neural networks. However, these methods\noften struggle to reconstruct high-frequency details under stringent\nconstraints on model size, which are critical in practical compression\nscenarios. To address this limitation, we propose an INR-based video\nrepresentation framework that integrates a general-purpose super-resolution\n(SR) network. This design is motivated by the observation that high-frequency\ncomponents tend to exhibit low temporal redundancy across frames. By offloading\nthe reconstruction of fine details to a dedicated SR network pre-trained on\nnatural images, the proposed method improves visual fidelity. Experimental\nresults demonstrate that the proposed method outperforms conventional INR-based\nbaselines in reconstruction quality, while maintaining a comparable model size.\n","authors":["Taiga Hayami","Kakeru Koizumi","Hiroshi Watanabe"],"pdf_url":"https://arxiv.org/pdf/2505.00046v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18348v1","updated":"2025-07-24T12:20:00Z","published":"2025-07-24T12:20:00Z","title":"VB-Mitigator: An Open-source Framework for Evaluating and Advancing\n  Visual Bias Mitigation","summary":"  Bias in computer vision models remains a significant challenge, often\nresulting in unfair, unreliable, and non-generalizable AI systems. Although\nresearch into bias mitigation has intensified, progress continues to be\nhindered by fragmented implementations and inconsistent evaluation practices.\nDisparate datasets and metrics used across studies complicate reproducibility,\nmaking it difficult to fairly assess and compare the effectiveness of various\napproaches. To overcome these limitations, we introduce the Visual Bias\nMitigator (VB-Mitigator), an open-source framework designed to streamline the\ndevelopment, evaluation, and comparative analysis of visual bias mitigation\ntechniques. VB-Mitigator offers a unified research environment encompassing 12\nestablished mitigation methods, 7 diverse benchmark datasets. A key strength of\nVB-Mitigator is its extensibility, allowing for seamless integration of\nadditional methods, datasets, metrics, and models. VB-Mitigator aims to\naccelerate research toward fairness-aware computer vision models by serving as\na foundational codebase for the research community to develop and assess their\napproaches. To this end, we also recommend best evaluation practices and\nprovide a comprehensive performance comparison among state-of-the-art\nmethodologies.\n","authors":["Ioannis Sarridis","Christos Koutlis","Symeon Papadopoulos","Christos Diou"],"pdf_url":"https://arxiv.org/pdf/2507.18348v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18342v1","updated":"2025-07-24T12:14:49Z","published":"2025-07-24T12:14:49Z","title":"EgoExoBench: A Benchmark for First- and Third-person View Video\n  Understanding in MLLMs","summary":"  Transferring and integrating knowledge across first-person (egocentric) and\nthird-person (exocentric) viewpoints is intrinsic to human intelligence,\nenabling humans to learn from others and convey insights from their own\nexperiences. Despite rapid progress in multimodal large language models\n(MLLMs), their ability to perform such cross-view reasoning remains unexplored.\nTo address this, we introduce EgoExoBench, the first benchmark for\negocentric-exocentric video understanding and reasoning. Built from publicly\navailable datasets, EgoExoBench comprises over 7,300 question-answer pairs\nspanning eleven sub-tasks organized into three core challenges: semantic\nalignment, viewpoint association, and temporal reasoning. We evaluate 13\nstate-of-the-art MLLMs and find that while these models excel on single-view\ntasks, they struggle to align semantics across perspectives, accurately\nassociate views, and infer temporal dynamics in the ego-exo context. We hope\nEgoExoBench can serve as a valuable resource for research on embodied agents\nand intelligent assistants seeking human-like cross-view intelligence.\n","authors":["Yuping He","Yifei Huang","Guo Chen","Baoqi Pei","Jilan Xu","Tong Lu","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2507.18342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07464v2","updated":"2025-07-24T12:13:12Z","published":"2025-07-10T06:31:26Z","title":"Degradation-Agnostic Statistical Facial Feature Transformation for Blind\n  Face Restoration in Adverse Weather Conditions","summary":"  With the increasing deployment of intelligent CCTV systems in outdoor\nenvironments, there is a growing demand for face recognition systems optimized\nfor challenging weather conditions. Adverse weather significantly degrades\nimage quality, which in turn reduces recognition accuracy. Although recent face\nimage restoration (FIR) models based on generative adversarial networks (GANs)\nand diffusion models have shown progress, their performance remains limited due\nto the lack of dedicated modules that explicitly address weather-induced\ndegradations. This leads to distorted facial textures and structures. To\naddress these limitations, we propose a novel GAN-based blind FIR framework\nthat integrates two key components: local Statistical Facial Feature\nTransformation (SFFT) and Degradation-Agnostic Feature Embedding (DAFE). The\nlocal SFFT module enhances facial structure and color fidelity by aligning the\nlocal statistical distributions of low-quality (LQ) facial regions with those\nof high-quality (HQ) counterparts. Complementarily, the DAFE module enables\nrobust statistical facial feature extraction under adverse weather conditions\nby aligning LQ and HQ encoder representations, thereby making the restoration\nprocess adaptive to severe weather-induced degradations. Experimental results\ndemonstrate that the proposed degradation-agnostic SFFT model outperforms\nexisting state-of-the-art FIR methods based on GAN and diffusion models,\nparticularly in suppressing texture distortions and accurately reconstructing\nfacial structures. Furthermore, both the SFFT and DAFE modules are empirically\nvalidated in enhancing structural fidelity and perceptual quality in face\nrestoration under challenging weather scenarios.\n","authors":["Chang-Hwan Son"],"pdf_url":"https://arxiv.org/pdf/2507.07464v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18334v1","updated":"2025-07-24T12:05:17Z","published":"2025-07-24T12:05:17Z","title":"Improving Bird Classification with Primary Color Additives","summary":"  We address the problem of classifying bird species using their song\nrecordings, a challenging task due to environmental noise, overlapping\nvocalizations, and missing labels. Existing models struggle with low-SNR or\nmulti-species recordings. We hypothesize that birds can be classified by\nvisualizing their pitch pattern, speed, and repetition, collectively called\nmotifs. Deep learning models applied to spectrogram images help, but similar\nmotifs across species cause confusion. To mitigate this, we embed frequency\ninformation into spectrograms using primary color additives. This enhances\nspecies distinction and improves classification accuracy. Our experiments show\nthat the proposed approach achieves statistically significant gains over models\nwithout colorization and surpasses the BirdCLEF 2024 winner, improving F1 by\n7.3%, ROC-AUC by 6.2%, and CMAP by 6.6%. These results demonstrate the\neffectiveness of incorporating frequency information via colorization.\n","authors":["Ezhini Rasendiran R","Chandresh Kumar Maurya"],"pdf_url":"https://arxiv.org/pdf/2507.18334v1.pdf","comment":"5 pages (Accepted to Interspeech 2025)"},{"id":"http://arxiv.org/abs/2507.18331v1","updated":"2025-07-24T11:58:01Z","published":"2025-07-24T11:58:01Z","title":"Boosting Multi-View Indoor 3D Object Detection via Adaptive 3D Volume\n  Construction","summary":"  This work presents SGCDet, a novel multi-view indoor 3D object detection\nframework based on adaptive 3D volume construction. Unlike previous approaches\nthat restrict the receptive field of voxels to fixed locations on images, we\nintroduce a geometry and context aware aggregation module to integrate\ngeometric and contextual information within adaptive regions in each image and\ndynamically adjust the contributions from different views, enhancing the\nrepresentation capability of voxel features. Furthermore, we propose a sparse\nvolume construction strategy that adaptively identifies and selects voxels with\nhigh occupancy probabilities for feature refinement, minimizing redundant\ncomputation in free space. Benefiting from the above designs, our framework\nachieves effective and efficient volume construction in an adaptive way. Better\nstill, our network can be supervised using only 3D bounding boxes, eliminating\nthe dependence on ground-truth scene geometry. Experimental results demonstrate\nthat SGCDet achieves state-of-the-art performance on the ScanNet, ScanNet200\nand ARKitScenes datasets. The source code is available at\nhttps://github.com/RM-Zhang/SGCDet.\n","authors":["Runmin Zhang","Zhu Yu","Si-Yuan Cao","Lingyu Zhu","Guangyi Zhang","Xiaokai Bai","Hui-Liang Shen"],"pdf_url":"https://arxiv.org/pdf/2507.18331v1.pdf","comment":"Accepted by ICCV2025"},{"id":"http://arxiv.org/abs/2507.18330v1","updated":"2025-07-24T11:57:59Z","published":"2025-07-24T11:57:59Z","title":"GVCCS: A Dataset for Contrail Identification and Tracking on Visible\n  Whole Sky Camera Sequences","summary":"  Aviation's climate impact includes not only CO2 emissions but also\nsignificant non-CO2 effects, especially from contrails. These ice clouds can\nalter Earth's radiative balance, potentially rivaling the warming effect of\naviation CO2. Physics-based models provide useful estimates of contrail\nformation and climate impact, but their accuracy depends heavily on the quality\nof atmospheric input data and on assumptions used to represent complex\nprocesses like ice particle formation and humidity-driven persistence.\nObservational data from remote sensors, such as satellites and ground cameras,\ncould be used to validate and calibrate these models. However, existing\ndatasets don't explore all aspect of contrail dynamics and formation: they\ntypically lack temporal tracking, and do not attribute contrails to their\nsource flights. To address these limitations, we present the Ground Visible\nCamera Contrail Sequences (GVCCS), a new open data set of contrails recorded\nwith a ground-based all-sky camera in the visible range. Each contrail is\nindividually labeled and tracked over time, allowing a detailed analysis of its\nlifecycle. The dataset contains 122 video sequences (24,228 frames) and\nincludes flight identifiers for contrails that form above the camera. As\nreference, we also propose a unified deep learning framework for contrail\nanalysis using a panoptic segmentation model that performs semantic\nsegmentation (contrail pixel identification), instance segmentation (individual\ncontrail separation), and temporal tracking in a single architecture. By\nproviding high-quality, temporally resolved annotations and a benchmark for\nmodel evaluation, our work supports improved contrail monitoring and will\nfacilitate better calibration of physical models. This sets the groundwork for\nmore accurate climate impact understanding and assessments.\n","authors":["Gabriel Jarry","Ramon Dalmau","Philippe Very","Franck Ballerini","Stephania-Denisa Bocu"],"pdf_url":"https://arxiv.org/pdf/2507.18330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18327v1","updated":"2025-07-24T11:53:55Z","published":"2025-07-24T11:53:55Z","title":"Beyond Low-rankness: Guaranteed Matrix Recovery via Modified Nuclear\n  Norm","summary":"  The nuclear norm (NN) has been widely explored in matrix recovery problems,\nsuch as Robust PCA and matrix completion, leveraging the inherent global\nlow-rank structure of the data. In this study, we introduce a new modified\nnuclear norm (MNN) framework, where the MNN family norms are defined by\nadopting suitable transformations and performing the NN on the transformed\nmatrix. The MNN framework offers two main advantages: (1) it jointly captures\nboth local information and global low-rankness without requiring trade-off\nparameter tuning; (2) Under mild assumptions on the transformation, we provided\nexact theoretical recovery guarantees for both Robust PCA and MC tasks-an\nachievement not shared by existing methods that combine local and global\ninformation. Thanks to its general and flexible design, MNN can accommodate\nvarious proven transformations, enabling a unified and effective approach to\nstructured low-rank recovery. Extensive experiments demonstrate the\neffectiveness of our method. Code and supplementary material are available at\nhttps://github.com/andrew-pengjj/modified_nuclear_norm.\n","authors":["Jiangjun Peng","Yisi Luo","Xiangyong Cao","Shuang Xu","Deyu Meng"],"pdf_url":"https://arxiv.org/pdf/2507.18327v1.pdf","comment":"15 pages, 14 figures"},{"id":"http://arxiv.org/abs/2507.18323v1","updated":"2025-07-24T11:49:46Z","published":"2025-07-24T11:49:46Z","title":"A Multi-Dataset Benchmark for Semi-Supervised Semantic Segmentation in\n  ECG Delineation","summary":"  Electrocardiogram (ECG) delineation, the segmentation of meaningful waveform\nfeatures, is critical for clinical diagnosis. Despite recent advances using\ndeep learning, progress has been limited by the scarcity of publicly available\nannotated datasets. Semi-supervised learning presents a promising solution by\nleveraging abundant unlabeled ECG data. In this study, we present the first\nsystematic benchmark for semi-supervised semantic segmentation (SemiSeg) in ECG\ndelineation. We curated and unified multiple public datasets, including\npreviously underused sources, to support robust and diverse evaluation. We\nadopted five representative SemiSeg algorithms from computer vision,\nimplemented them on two different architectures: the convolutional network and\nthe transformer, and evaluated them in two different settings: in-domain and\ncross-domain. Additionally, we propose ECG-specific training configurations and\naugmentation strategies and introduce a standardized evaluation framework. Our\nresults show that the transformer outperforms the convolutional network in\nsemi-supervised ECG delineation. We anticipate that our benchmark will serve as\na foundation for advancing semi-supervised ECG delineation methods and will\nfacilitate further research in this domain.\n","authors":["Minje Park","Jeonghwa Lim","Taehyung Yu","Sunghoon Joo"],"pdf_url":"https://arxiv.org/pdf/2507.18323v1.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2312.12102v3","updated":"2025-07-24T11:44:19Z","published":"2023-12-19T12:26:57Z","title":"I-CEE: Tailoring Explanations of Image Classification Models to User\n  Expertise","summary":"  Effectively explaining decisions of black-box machine learning models is\ncritical to responsible deployment of AI systems that rely on them. Recognizing\ntheir importance, the field of explainable AI (XAI) provides several techniques\nto generate these explanations. Yet, there is relatively little emphasis on the\nuser (the explainee) in this growing body of work and most XAI techniques\ngenerate \"one-size-fits-all\" explanations. To bridge this gap and achieve a\nstep closer towards human-centered XAI, we present I-CEE, a framework that\nprovides Image Classification Explanations tailored to User Expertise. Informed\nby existing work, I-CEE explains the decisions of image classification models\nby providing the user with an informative subset of training data (i.e.,\nexample images), corresponding local explanations, and model decisions.\nHowever, unlike prior work, I-CEE models the informativeness of the example\nimages to depend on user expertise, resulting in different examples for\ndifferent users. We posit that by tailoring the example set to user expertise,\nI-CEE can better facilitate users' understanding and simulatability of the\nmodel. To evaluate our approach, we conduct detailed experiments in both\nsimulation and with human participants (N = 100) on multiple datasets.\nExperiments with simulated users show that I-CEE improves users' ability to\naccurately predict the model's decisions (simulatability) compared to\nbaselines, providing promising preliminary results. Experiments with human\nparticipants demonstrate that our method significantly improves user\nsimulatability accuracy, highlighting the importance of human-centered XAI\n","authors":["Yao Rong","Peizhu Qian","Vaibhav Unhelkar","Enkelejda Kasneci"],"pdf_url":"https://arxiv.org/pdf/2312.12102v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18311v1","updated":"2025-07-24T11:28:53Z","published":"2025-07-24T11:28:53Z","title":"Improving Large Vision-Language Models' Understanding for Field Data","summary":"  Large Vision-Language Models (LVLMs) have shown impressive capabilities\nacross a range of tasks that integrate visual and textual understanding, such\nas image captioning and visual question answering. These models are trained on\nlarge-scale image and video datasets paired with text, enabling them to bridge\nvisual perception and natural language processing. However, their application\nto scientific domains, especially in interpreting complex field data commonly\nused in the natural sciences, remains underexplored. In this work, we introduce\nFieldLVLM, a novel framework designed to improve large vision-language models'\nunderstanding of field data. FieldLVLM consists of two main components: a\nfield-aware language generation strategy and a data-compressed multimodal model\ntuning. The field-aware language generation strategy leverages a\nspecial-purpose machine learning pipeline to extract key physical features from\nfield data, such as flow classification, Reynolds number, and vortex patterns.\nThis information is then converted into structured textual descriptions that\nserve as a dataset. The data-compressed multimodal model tuning focuses on\nLVLMs with these generated datasets, using a data compression strategy to\nreduce the complexity of field inputs and retain only the most informative\nvalues. This ensures compatibility with the models language decoder and guides\nits learning more effectively. Experimental results on newly proposed benchmark\ndatasets demonstrate that FieldLVLM significantly outperforms existing methods\nin tasks involving scientific field data. Our findings suggest that this\napproach opens up new possibilities for applying large vision-language models\nto scientific research, helping bridge the gap between large models and\ndomain-specific discovery.\n","authors":["Xiaomei Zhang","Hanyu Zheng","Xiangyu Zhu","Jinghuan Wei","Junhong Zou","Zhen Lei","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.18311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10072v2","updated":"2025-07-24T11:15:16Z","published":"2025-05-15T08:16:12Z","title":"ToonifyGB: StyleGAN-based Gaussian Blendshapes for 3D Stylized Head\n  Avatars","summary":"  The introduction of 3D Gaussian blendshapes has enabled the real-time\nreconstruction of animatable head avatars from monocular video. Toonify, a\nStyleGAN-based method, has become widely used for facial image stylization. To\nextend Toonify for synthesizing diverse stylized 3D head avatars using Gaussian\nblendshapes, we propose an efficient two-stage framework, ToonifyGB. In Stage 1\n(stylized video generation), we adopt an improved StyleGAN to generate the\nstylized video from the input video frames, which overcomes the limitation of\ncropping aligned faces at a fixed resolution as preprocessing for normal\nStyleGAN. This process provides a more stable stylized video, which enables\nGaussian blendshapes to better capture the high-frequency details of the video\nframes, facilitating the synthesis of high-quality animations in the next\nstage. In Stage 2 (Gaussian blendshapes synthesis), our method learns a\nstylized neutral head model and a set of expression blendshapes from the\ngenerated stylized video. By combining the neutral head model with expression\nblendshapes, ToonifyGB can efficiently render stylized avatars with arbitrary\nexpressions. We validate the effectiveness of ToonifyGB on benchmark datasets\nusing two representative styles: Arcane and Pixar.\n","authors":["Rui-Yang Ju","Sheng-Yen Huang","Yi-Ping Hung"],"pdf_url":"https://arxiv.org/pdf/2505.10072v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12972v2","updated":"2025-07-24T11:14:43Z","published":"2025-03-17T09:31:14Z","title":"Aligning Vision to Language: Annotation-Free Multimodal Knowledge Graph\n  Construction for Enhanced LLMs Reasoning","summary":"  Multimodal reasoning in Large Language Models (LLMs) struggles with\nincomplete knowledge and hallucination artifacts, challenges that textual\nKnowledge Graphs (KGs) only partially mitigate due to their modality isolation.\nWhile Multimodal Knowledge Graphs (MMKGs) promise enhanced cross-modal\nunderstanding, their practical construction is impeded by semantic narrowness\nof manual text annotations and inherent noise in visual-semantic entity\nlinkages. In this paper, we propose Vision-align-to-Language integrated\nKnowledge Graph (VaLiK), a novel approach for constructing MMKGs that enhances\nLLMs reasoning through cross-modal information supplementation. Specifically,\nwe cascade pre-trained Vision-Language Models (VLMs) to align image features\nwith text, transforming them into descriptions that encapsulate image-specific\ninformation. Furthermore, we developed a cross-modal similarity verification\nmechanism to quantify semantic consistency, effectively filtering out noise\nintroduced during feature alignment. Even without manually annotated image\ncaptions, the refined descriptions alone suffice to construct the MMKG.\nCompared to conventional MMKGs construction paradigms, our approach achieves\nsubstantial storage efficiency gains while maintaining direct entity-to-image\nlinkage capability. Experimental results on multimodal reasoning tasks\ndemonstrate that LLMs augmented with VaLiK outperform previous state-of-the-art\nmodels. Our code is published at https://github.com/Wings-Of-Disaster/VaLiK.\n","authors":["Junming Liu","Siyuan Meng","Yanting Gao","Song Mao","Pinlong Cai","Guohang Yan","Yirong Chen","Zilin Bian","Ding Wang","Botian Shi"],"pdf_url":"https://arxiv.org/pdf/2503.12972v2.pdf","comment":"14 pages, 7 figures, 6 tables; Accepted to ICCV 2025"},{"id":"http://arxiv.org/abs/2407.19795v2","updated":"2025-07-24T11:08:59Z","published":"2024-07-29T08:38:46Z","title":"VolDoGer: LLM-assisted Datasets for Domain Generalization in\n  Vision-Language Tasks","summary":"  Domain generalizability is a crucial aspect of a deep learning model since it\ndetermines the capability of the model to perform well on data from unseen\ndomains. However, research on the domain generalizability of deep learning\nmodels for vision-language tasks remains limited, primarily because of the lack\nof required datasets. To address these challenges, we propose VolDoGer:\nVision-Language Dataset for Domain Generalization, a dedicated dataset designed\nfor domain generalization that addresses three vision-language tasks: image\ncaptioning, visual question answering, and visual entailment. We constructed\nVolDoGer by extending LLM-based data annotation techniques to vision-language\ntasks, thereby alleviating the burden of recruiting human annotators. We\nevaluated the domain generalizability of various models, ranging from\nfine-tuned models to a recent multimodal large language model, through\nVolDoGer.\n","authors":["Juhwan Choi","Junehyoung Kwon","JungMin Yun","Seunguk Yu","YoungBin Kim"],"pdf_url":"https://arxiv.org/pdf/2407.19795v2.pdf","comment":"ICCV 2025 Workshop on Curated Data for Efficient Learning (CDEL)"},{"id":"http://arxiv.org/abs/2507.18300v1","updated":"2025-07-24T11:05:24Z","published":"2025-07-24T11:05:24Z","title":"LMM-Det: Make Large Multimodal Models Excel in Object Detection","summary":"  Large multimodal models (LMMs) have garnered wide-spread attention and\ninterest within the artificial intelligence research and industrial\ncommunities, owing to their remarkable capability in multimodal understanding,\nreasoning, and in-context learning, among others. While LMMs have demonstrated\npromising results in tackling multimodal tasks like image captioning, visual\nquestion answering, and visual grounding, the object detection capabilities of\nLMMs exhibit a significant gap compared to specialist detectors. To bridge the\ngap, we depart from the conventional methods of integrating heavy detectors\nwith LMMs and propose LMM-Det, a simple yet effective approach that leverages a\nLarge Multimodal Model for vanilla object Detection without relying on\nspecialized detection modules. Specifically, we conduct a comprehensive\nexploratory analysis when a large multimodal model meets with object detection,\nrevealing that the recall rate degrades significantly compared with specialist\ndetection models. To mitigate this, we propose to increase the recall rate by\nintroducing data distribution adjustment and inference optimization tailored\nfor object detection. We re-organize the instruction conversations to enhance\nthe object detection capabilities of large multimodal models. We claim that a\nlarge multimodal model possesses detection capability without any extra\ndetection modules. Extensive experiments support our claim and show the\neffectiveness of the versatile LMM-Det. The datasets, models, and codes are\navailable at https://github.com/360CVGroup/LMM-Det.\n","authors":["Jincheng Li","Chunyu Xie","Ji Ao","Dawei Leng","Yuhui Yin"],"pdf_url":"https://arxiv.org/pdf/2507.18300v1.pdf","comment":"Accepted at ICCV 2025"},{"id":"http://arxiv.org/abs/2507.17596v2","updated":"2025-07-24T11:04:42Z","published":"2025-07-23T15:28:23Z","title":"PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving","summary":"  While end-to-end autonomous driving models show promising results, their\npractical deployment is often hindered by large model sizes, a reliance on\nexpensive LiDAR sensors and computationally intensive BEV feature\nrepresentations. This limits their scalability, especially for mass-market\nvehicles equipped only with cameras. To address these challenges, we propose\nPRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving\narchitecture operates using only camera data, without explicit BEV\nrepresentation and forgoing the need for LiDAR. PRIX leverages a visual feature\nextractor coupled with a generative planning head to predict safe trajectories\nfrom raw pixel inputs directly. A core component of our architecture is the\nContext-aware Recalibration Transformer (CaRT), a novel module designed to\neffectively enhance multi-level visual features for more robust planning. We\ndemonstrate through comprehensive experiments that PRIX achieves\nstate-of-the-art performance on the NavSim and nuScenes benchmarks, matching\nthe capabilities of larger, multimodal diffusion planners while being\nsignificantly more efficient in terms of inference speed and model size, making\nit a practical solution for real-world deployment. Our work is open-source and\nthe code will be at https://maxiuw.github.io/prix.\n","authors":["Maciej K. Wozniak","Lianhang Liu","Yixi Cai","Patric Jensfelt"],"pdf_url":"https://arxiv.org/pdf/2507.17596v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2507.18288v1","updated":"2025-07-24T10:49:31Z","published":"2025-07-24T10:49:31Z","title":"TCM-Tongue: A Standardized Tongue Image Dataset with Pathological\n  Annotations for AI-Assisted TCM Diagnosis","summary":"  Traditional Chinese medicine (TCM) tongue diagnosis, while clinically\nvaluable, faces standardization challenges due to subjective interpretation and\ninconsistent imaging protocols, compounded by the lack of large-scale,\nannotated datasets for AI development. To address this gap, we present the\nfirst specialized dataset for AI-driven TCM tongue diagnosis, comprising 6,719\nhigh-quality images captured under standardized conditions and annotated with\n20 pathological symptom categories (averaging 2.54 clinically validated labels\nper image, all verified by licensed TCM practitioners). The dataset supports\nmultiple annotation formats (COCO, TXT, XML) for broad usability and has been\nbenchmarked using nine deep learning models (YOLOv5/v7/v8 variants, SSD, and\nMobileNetV2) to demonstrate its utility for AI development. This resource\nprovides a critical foundation for advancing reliable computational tools in\nTCM, bridging the data shortage that has hindered progress in the field, and\nfacilitating the integration of AI into both research and clinical practice\nthrough standardized, high-quality diagnostic data.\n","authors":["Xuebo Jin","Longfei Gao","Anshuo Tong","Zhengyang Chen","Jianlei Kong","Ning Sun","Huijun Ma","Qiang Wang","Yuting Bai","Tingli Su"],"pdf_url":"https://arxiv.org/pdf/2507.18288v1.pdf","comment":"16 pages, 11 figures, 2 Tables"},{"id":"http://arxiv.org/abs/2507.18287v1","updated":"2025-07-24T10:46:43Z","published":"2025-07-24T10:46:43Z","title":"Dissecting the Dental Lung Cancer Axis via Mendelian Randomization and\n  Mediation Analysis","summary":"  Periodontitis and dental caries are common oral diseases affecting billions\nglobally. While observational studies suggest links between these conditions\nand lung cancer, causality remains uncertain. This study used two sample\nMendelian randomization (MR) to explore causal relationships between dental\ntraits (periodontitis, dental caries) and lung cancer subtypes, and to assess\nmediation by pulmonary function. Genetic instruments were derived from the\nlargest available genome wide association studies, including data from 487,823\ndental caries and 506,594 periodontitis cases, as well as lung cancer data from\nthe Transdisciplinary Research of Cancer in Lung consortium. Inverse variance\nweighting was the main analytical method; lung function mediation was assessed\nusing the delta method. The results showed a significant positive causal effect\nof dental caries on overall lung cancer and its subtypes. Specifically, a one\nstandard deviation increase in dental caries incidence was associated with a\n188.0% higher risk of squamous cell lung carcinoma (OR = 2.880, 95% CI =\n1.236--6.713, p = 0.014), partially mediated by declines in forced vital\ncapacity (FVC) and forced expiratory volume in one second (FEV1), accounting\nfor 5.124% and 5.890% of the total effect. No causal effect was found for\nperiodontitis. These findings highlight a causal role of dental caries in lung\ncancer risk and support integrating dental care and pulmonary function\nmonitoring into cancer prevention strategies.\n","authors":["Wenran Zhang","Huihuan Luo","Linda Wei","Ping Nie","Yiqun Wu","Dedong Yu"],"pdf_url":"https://arxiv.org/pdf/2507.18287v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02075v2","updated":"2025-07-24T10:46:04Z","published":"2024-07-02T09:08:06Z","title":"Label Anything: Multi-Class Few-Shot Semantic Segmentation with Visual\n  Prompts","summary":"  We present Label Anything, an innovative neural network architecture designed\nfor few-shot semantic segmentation (FSS) that demonstrates remarkable\ngeneralizability across multiple classes with minimal examples required per\nclass. Diverging from traditional FSS methods that predominantly rely on masks\nfor annotating support images, Label Anything introduces varied visual prompts\n-- points, bounding boxes, and masks -- thereby enhancing the framework's\nversatility and adaptability. Unique to our approach, Label Anything is\nengineered for end-to-end training across multi-class FSS scenarios,\nefficiently learning from diverse support set configurations without\nretraining. This approach enables a \"universal\" application to various FSS\nchallenges, ranging from $1$-way $1$-shot to complex $N$-way $K$-shot\nconfigurations while remaining agnostic to the specific number of class\nexamples. This innovative training strategy reduces computational requirements\nand substantially improves the model's adaptability and generalization across\ndiverse segmentation tasks. Our comprehensive experimental validation,\nparticularly achieving state-of-the-art results on the COCO-$20^i$ benchmark,\nunderscores Label Anything's robust generalization and flexibility. The source\ncode is publicly available at: https://github.com/pasqualedem/LabelAnything.\n","authors":["Pasquale De Marinis","Nicola Fanelli","Raffaele Scaringi","Emanuele Colonna","Giuseppe Fiameni","Gennaro Vessio","Giovanna Castellano"],"pdf_url":"https://arxiv.org/pdf/2407.02075v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11554v3","updated":"2025-07-24T10:37:32Z","published":"2025-07-14T02:59:28Z","title":"Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models","summary":"  Recent advancements in diffusion models (DMs) have been propelled by\nalignment methods that post-train models to better conform to human\npreferences. However, these approaches typically require computation-intensive\ntraining of a base model and a reward model, which not only incurs substantial\ncomputational overhead but may also compromise model accuracy and training\nefficiency. To address these limitations, we propose Inversion-DPO, a novel\nalignment framework that circumvents reward modeling by reformulating Direct\nPreference Optimization (DPO) with DDIM inversion for DMs. Our method conducts\nintractable posterior sampling in Diffusion-DPO with the deterministic\ninversion from winning and losing samples to noise and thus derive a new\npost-training paradigm. This paradigm eliminates the need for auxiliary reward\nmodels or inaccurate appromixation, significantly enhancing both precision and\nefficiency of training. We apply Inversion-DPO to a basic task of text-to-image\ngeneration and a challenging task of compositional image generation. Extensive\nexperiments show substantial performance improvements achieved by Inversion-DPO\ncompared to existing post-training methods and highlight the ability of the\ntrained generative models to generate high-fidelity compositionally coherent\nimages. For the post-training of compostitional image geneation, we curate a\npaired dataset consisting of 11,140 images with complex structural annotations\nand comprehensive scores, designed to enhance the compositional capabilities of\ngenerative models. Inversion-DPO explores a new avenue for efficient,\nhigh-precision alignment in diffusion models, advancing their applicability to\ncomplex realistic generation tasks. Our code is available at\nhttps://github.com/MIGHTYEZ/Inversion-DPO\n","authors":["Zejian Li","Yize Li","Chenye Meng","Zhongni Liu","Yang Ling","Shengyuan Zhang","Guang Yang","Changyuan Yang","Zhiyuan Yang","Lingyun Sun"],"pdf_url":"https://arxiv.org/pdf/2507.11554v3.pdf","comment":"Accepted by ACM MM25"},{"id":"http://arxiv.org/abs/2507.15504v2","updated":"2025-07-24T10:32:09Z","published":"2025-07-21T11:12:39Z","title":"Quantifying and Narrowing the Unknown: Interactive Text-to-Video\n  Retrieval via Uncertainty Minimization","summary":"  Despite recent advances, Text-to-video retrieval (TVR) is still hindered by\nmultiple inherent uncertainties, such as ambiguous textual queries, indistinct\ntext-video mappings, and low-quality video frames. Although interactive systems\nhave emerged to address these challenges by refining user intent through\nclarifying questions, current methods typically rely on heuristic or ad-hoc\nstrategies without explicitly quantifying these uncertainties, limiting their\neffectiveness. Motivated by this gap, we propose UMIVR, an\nUncertainty-Minimizing Interactive Text-to-Video Retrieval framework that\nexplicitly quantifies three critical uncertainties-text ambiguity, mapping\nuncertainty, and frame uncertainty-via principled, training-free metrics:\nsemantic entropy-based Text Ambiguity Score (TAS), Jensen-Shannon\ndivergence-based Mapping Uncertainty Score (MUS), and a Temporal Quality-based\nFrame Sampler (TQFS). By adaptively generating targeted clarifying questions\nguided by these uncertainty measures, UMIVR iteratively refines user queries,\nsignificantly reducing retrieval ambiguity. Extensive experiments on multiple\nbenchmarks validate UMIVR's effectiveness, achieving notable gains in Recall@1\n(69.2\\% after 10 interactive rounds) on the MSR-VTT-1k dataset, thereby\nestablishing an uncertainty-minimizing foundation for interactive TVR.\n","authors":["Bingqing Zhang","Zhuo Cao","Heming Du","Yang Li","Xue Li","Jiajun Liu","Sen Wang"],"pdf_url":"https://arxiv.org/pdf/2507.15504v2.pdf","comment":"Accepted by ICCV 2025"},{"id":"http://arxiv.org/abs/2502.06788v2","updated":"2025-07-24T10:29:52Z","published":"2025-02-10T18:59:58Z","title":"EVEv2: Improved Baselines for Encoder-Free Vision-Language Models","summary":"  Existing encoder-free vision-language models (VLMs) are rapidly narrowing the\nperformance gap with their encoder-based counterparts, highlighting the\npromising potential for unified multimodal systems with structural simplicity\nand efficient deployment. We systematically clarify the performance gap between\nVLMs using pre-trained vision encoders, discrete tokenizers, and minimalist\nvisual layers from scratch, deeply excavating the under-examined\ncharacteristics of encoder-free VLMs. We develop efficient strategies for\nencoder-free VLMs that rival mainstream encoder-based ones. After an in-depth\ninvestigation, we launch EVEv2.0, a new and improved family of encoder-free\nVLMs. We show that: (i) Properly decomposing and hierarchically associating\nvision and language within a unified model reduces interference between\nmodalities. (ii) A well-designed training strategy enables effective\noptimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0\nrepresents a thorough study for developing a decoder-only architecture across\nmodalities, demonstrating superior data efficiency and strong vision-reasoning\ncapability. Code is publicly available at: https://github.com/baaivision/EVE.\n","authors":["Haiwen Diao","Xiaotong Li","Yufeng Cui","Yueze Wang","Haoge Deng","Ting Pan","Wenxuan Wang","Huchuan Lu","Xinlong Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06788v2.pdf","comment":"20 pages, 10 figures, Accepted by ICCV2025 (highlight)"},{"id":"http://arxiv.org/abs/2507.18276v1","updated":"2025-07-24T10:25:58Z","published":"2025-07-24T10:25:58Z","title":"Adaptive Articulated Object Manipulation On The Fly with Foundation\n  Model Reasoning and Part Grounding","summary":"  Articulated objects pose diverse manipulation challenges for robots. Since\ntheir internal structures are not directly observable, robots must adaptively\nexplore and refine actions to generate successful manipulation trajectories.\nWhile existing works have attempted cross-category generalization in adaptive\narticulated object manipulation, two major challenges persist: (1) the\ngeometric diversity of real-world articulated objects complicates visual\nperception and understanding, and (2) variations in object functions and\nmechanisms hinder the development of a unified adaptive manipulation strategy.\nTo address these challenges, we propose AdaRPG, a novel framework that\nleverages foundation models to extract object parts, which exhibit greater\nlocal geometric similarity than entire objects, thereby enhancing visual\naffordance generalization for functional primitive skills. To support this, we\nconstruct a part-level affordance annotation dataset to train the affordance\nmodel. Additionally, AdaRPG utilizes the common knowledge embedded in\nfoundation models to reason about complex mechanisms and generate high-level\ncontrol codes that invoke primitive skill functions based on part affordance\ninference. Simulation and real-world experiments demonstrate AdaRPG's strong\ngeneralization ability across novel articulated object categories.\n","authors":["Xiaojie Zhang","Yuanfei Wang","Ruihai Wu","Kunqi Xu","Yu Li","Liuyu Xiang","Hao Dong","Zhaofeng He"],"pdf_url":"https://arxiv.org/pdf/2507.18276v1.pdf","comment":"ICCV 2025"},{"id":"http://arxiv.org/abs/2507.18262v1","updated":"2025-07-24T10:07:31Z","published":"2025-07-24T10:07:31Z","title":"ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic\n  Grounding for Generalizable Robotic Manipulation","summary":"  Semantics-driven 3D spatial constraints align highlevel semantic\nrepresentations with low-level action spaces, facilitating the unification of\ntask understanding and execution in robotic manipulation. The synergistic\nreasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation\nModels (VFMs) enables cross-modal 3D spatial constraint construction.\nNevertheless, existing methods have three key limitations: (1) coarse semantic\ngranularity in constraint modeling, (2) lack of real-time closed-loop planning,\n(3) compromised robustness in semantically diverse environments. To address\nthese challenges, we propose ReSem3D, a unified manipulation framework for\nsemantically diverse environments, leveraging the synergy between VFMs and\nMLLMs to achieve fine-grained visual grounding and dynamically constructs\nhierarchical 3D spatial constraints for real-time manipulation. Specifically,\nthe framework is driven by hierarchical recursive reasoning in MLLMs, which\ninteract with VFMs to automatically construct 3D spatial constraints from\nnatural language instructions and RGB-D observations in two stages: part-level\nextraction and region-level refinement. Subsequently, these constraints are\nencoded as real-time optimization objectives in joint space, enabling reactive\nbehavior to dynamic disturbances. Extensive simulation and real-world\nexperiments are conducted in semantically rich household and sparse chemical\nlab environments. The results demonstrate that ReSem3D performs diverse\nmanipulation tasks under zero-shot conditions, exhibiting strong adaptability\nand generalization. Code and videos at https://resem3d.github.io.\n","authors":["Chenyu Su","Weiwei Shang","Chen Qian","Fei Zhang","Shuang Cong"],"pdf_url":"https://arxiv.org/pdf/2507.18262v1.pdf","comment":"12 pages,9 figures"},{"id":"http://arxiv.org/abs/2507.18260v1","updated":"2025-07-24T10:03:33Z","published":"2025-07-24T10:03:33Z","title":"Exploiting Gaussian Agnostic Representation Learning with Diffusion\n  Priors for Enhanced Infrared Small Target Detection","summary":"  Infrared small target detection (ISTD) plays a vital role in numerous\npractical applications. In pursuit of determining the performance boundaries,\nresearchers employ large and expensive manual-labeling data for representation\nlearning. Nevertheless, this approach renders the state-of-the-art ISTD methods\nhighly fragile in real-world challenges. In this paper, we first study the\nvariation in detection performance across several mainstream methods under\nvarious scarcity -- namely, the absence of high-quality infrared data -- that\nchallenge the prevailing theories about practical ISTD. To address this\nconcern, we introduce the Gaussian Agnostic Representation Learning.\nSpecifically, we propose the Gaussian Group Squeezer, leveraging Gaussian\nsampling and compression for non-uniform quantization. By exploiting a diverse\narray of training samples, we enhance the resilience of ISTD models against\nvarious challenges. Then, we introduce two-stage diffusion models for\nreal-world reconstruction. By aligning quantized signals closely with\nreal-world distributions, we significantly elevate the quality and fidelity of\nthe synthetic samples. Comparative evaluations against state-of-the-art\ndetection methods in various scarcity scenarios demonstrate the efficacy of the\nproposed approach.\n","authors":["Junyao Li","Yahao Lu","Xingyuan Guo","Xiaoyu Xian","Tiantian Wang","Yukai Shi"],"pdf_url":"https://arxiv.org/pdf/2507.18260v1.pdf","comment":"Submitted to Neural Networks. We propose the Gaussian Group Squeezer,\n  leveraging Gaussian sampling and compression with diffusion models for\n  channel-based data augmentation"},{"id":"http://arxiv.org/abs/2507.12006v3","updated":"2025-07-24T09:57:56Z","published":"2025-07-16T07:59:54Z","title":"Frequency-Dynamic Attention Modulation for Dense Prediction","summary":"  Vision Transformers (ViTs) have significantly advanced computer vision,\ndemonstrating strong performance across various tasks. However, the attention\nmechanism in ViTs makes each layer function as a low-pass filter, and the\nstacked-layer architecture in existing transformers suffers from frequency\nvanishing. This leads to the loss of critical details and textures. We propose\na novel, circuit-theory-inspired strategy called Frequency-Dynamic Attention\nModulation (FDAM), which can be easily plugged into ViTs. FDAM directly\nmodulates the overall frequency response of ViTs and consists of two\ntechniques: Attention Inversion (AttInv) and Frequency Dynamic Scaling\n(FreqScale). Since circuit theory uses low-pass filters as fundamental\nelements, we introduce AttInv, a method that generates complementary high-pass\nfiltering by inverting the low-pass filter in the attention matrix, and\ndynamically combining the two. We further design FreqScale to weight different\nfrequency components for fine-grained adjustments to the target response\nfunction. Through feature similarity analysis and effective rank evaluation, we\ndemonstrate that our approach avoids representation collapse, leading to\nconsistent performance improvements across various models, including SegFormer,\nDeiT, and MaskDINO. These improvements are evident in tasks such as semantic\nsegmentation, object detection, and instance segmentation. Additionally, we\napply our method to remote sensing detection, achieving state-of-the-art\nresults in single-scale settings. The code is available at\nhttps://github.com/Linwei-Chen/FDAM.\n","authors":["Linwei Chen","Lin Gu","Ying Fu"],"pdf_url":"https://arxiv.org/pdf/2507.12006v3.pdf","comment":"Accepted by ICCV 2025"},{"id":"http://arxiv.org/abs/2507.18255v1","updated":"2025-07-24T09:55:20Z","published":"2025-07-24T09:55:20Z","title":"LONG3R: Long Sequence Streaming 3D Reconstruction","summary":"  Recent advancements in multi-view scene reconstruction have been significant,\nyet existing methods face limitations when processing streams of input images.\nThese methods either rely on time-consuming offline optimization or are\nrestricted to shorter sequences, hindering their applicability in real-time\nscenarios. In this work, we propose LONG3R (LOng sequence streaming 3D\nReconstruction), a novel model designed for streaming multi-view 3D scene\nreconstruction over longer sequences. Our model achieves real-time processing\nby operating recurrently, maintaining and updating memory with each new\nobservation. We first employ a memory gating mechanism to filter relevant\nmemory, which, together with a new observation, is fed into a dual-source\nrefined decoder for coarse-to-fine interaction. To effectively capture\nlong-sequence memory, we propose a 3D spatio-temporal memory that dynamically\nprunes redundant spatial information while adaptively adjusting resolution\nalong the scene. To enhance our model's performance on long sequences while\nmaintaining training efficiency, we employ a two-stage curriculum training\nstrategy, each stage targeting specific capabilities. Experiments demonstrate\nthat LONG3R outperforms state-of-the-art streaming methods, particularly for\nlonger sequences, while maintaining real-time inference speed. Project page:\nhttps://zgchen33.github.io/LONG3R/.\n","authors":["Zhuoguang Chen","Minghui Qin","Tianyuan Yuan","Zhe Liu","Hang Zhao"],"pdf_url":"https://arxiv.org/pdf/2507.18255v1.pdf","comment":"Accepted by ICCV 2025. Project page:\n  https://zgchen33.github.io/LONG3R/"},{"id":"http://arxiv.org/abs/2412.03515v2","updated":"2025-07-24T09:55:16Z","published":"2024-12-04T17:57:25Z","title":"Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion","summary":"  Diffusion models have been applied to 3D LiDAR scene completion due to their\nstrong training stability and high completion quality. However, the slow\nsampling speed limits the practical application of diffusion-based scene\ncompletion models since autonomous vehicles require an efficient perception of\nsurrounding environments. This paper proposes a novel distillation method\ntailored for 3D Li- DAR scene completion models, dubbed ScoreLiDAR, which\nachieves efficient yet high-quality scene completion. Score- LiDAR enables the\ndistilled model to sample in significantly fewer steps after distillation. To\nimprove completion quality, we also introduce a novel Structural Loss, which\nencourages the distilled model to capture the geometric structure of the 3D\nLiDAR scene. The loss contains a scene-wise term constraining the holistic\nstructure and a point-wise term constraining the key landmark points and their\nrelative configuration. Extensive experiments demonstrate that ScoreLiDAR\nsignificantly accelerates the completion time from 30.55 to 5.37 seconds per\nframe (>5x) on SemanticKITTI and achieves superior performance compared to\nstate-of-the-art 3D LiDAR scene completion models. Our model and code are\npublicly available on https: //github.com/happyw1nd/ScoreLiDAR.\n","authors":["Shengyuan Zhang","An Zhao","Ling Yang","Zejian Li","Chenye Meng","Haoran Xu","Tianrun Chen","AnYang Wei","Perry Pengyun GU","Lingyun Sun"],"pdf_url":"https://arxiv.org/pdf/2412.03515v2.pdf","comment":"This paper is accept by ICCV'25(Oral), the model and code are\n  publicly available on https: //github.com/happyw1nd/ScoreLiDAR"},{"id":"http://arxiv.org/abs/2506.16297v3","updated":"2025-07-24T09:52:06Z","published":"2025-06-19T13:17:30Z","title":"SyncMapV2: Robust and Adaptive Unsupervised Segmentation","summary":"  Human vision excels at segmenting visual cues without the need for explicit\ntraining, and it remains remarkably robust even as noise severity increases. In\ncontrast, existing AI algorithms struggle to maintain accuracy under similar\nconditions. Here, we present SyncMapV2, the first to solve unsupervised\nsegmentation with state-of-the-art robustness. SyncMapV2 exhibits a minimal\ndrop in mIoU, only 0.01%, under digital corruption, compared to a 23.8% drop\nobserved in SOTA methods. This superior performance extends across various\ntypes of corruption: noise (7.3% vs. 37.7%), weather (7.5% vs. 33.8%), and blur\n(7.0% vs. 29.5%). Notably, SyncMapV2 accomplishes this without any robust\ntraining, supervision, or loss functions. It is based on a learning paradigm\nthat uses self-organizing dynamical equations combined with concepts from\nrandom networks. Moreover, unlike conventional methods that require\nre-initialization for each new input, SyncMapV2 adapts online, mimicking the\ncontinuous adaptability of human vision. Thus, we go beyond the accurate and\nrobust results, and present the first algorithm that can do all the above\nonline, adapting to input rather than re-initializing. In adaptability tests,\nSyncMapV2 demonstrates near-zero performance degradation, which motivates and\nfosters a new generation of robust and adaptive intelligence in the near\nfuture.\n","authors":["Heng Zhang","Zikang Wan","Danilo Vasconcellos Vargas"],"pdf_url":"https://arxiv.org/pdf/2506.16297v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.08418v2","updated":"2025-07-24T09:41:43Z","published":"2025-06-10T03:46:20Z","title":"RadioDUN: A Physics-Inspired Deep Unfolding Network for Radio Map\n  Estimation","summary":"  The radio map represents the spatial distribution of spectrum resources\nwithin a region, supporting efficient resource allocation and interference\nmitigation. However, it is difficult to construct a dense radio map as a\nlimited number of samples can be measured in practical scenarios. While\nexisting works have used deep learning to estimate dense radio maps from sparse\nsamples, they are hard to integrate with the physical characteristics of the\nradio map. To address this challenge, we cast radio map estimation as the\nsparse signal recovery problem. A physical propagation model is further\nincorporated to decompose the problem into multiple factor optimization\nsub-problems, thereby reducing recovery complexity. Inspired by the existing\ncompressive sensing methods, we propose the Radio Deep Unfolding Network\n(RadioDUN) to unfold the optimization process, achieving adaptive parameter\nadjusting and prior fitting in a learnable manner. To account for the radio\npropagation characteristics, we develop a dynamic reweighting module (DRM) to\nadaptively model the importance of each factor for the radio map. Inspired by\nthe shadowing factor in the physical propagation model, we integrate\nobstacle-related factors to express the obstacle-induced signal stochastic\ndecay. The shadowing loss is further designed to constrain the factor\nprediction and act as a supplementary supervised objective, which enhances the\nperformance of RadioDUN. Extensive experiments have been conducted to\ndemonstrate that the proposed method outperforms the state-of-the-art methods.\nOur code will be made publicly available upon publication.\n","authors":["Taiqin Chen","Zikun Zhou","Zheng Fang","Wenzhen Zou","Kangjun Liu","Ke Chen","Yongbing Zhang","Yaowei Wang"],"pdf_url":"https://arxiv.org/pdf/2506.08418v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18248v1","updated":"2025-07-24T09:40:47Z","published":"2025-07-24T09:40:47Z","title":"Evaluation of facial landmark localization performance in a surgical\n  setting","summary":"  The use of robotics, computer vision, and their applications is becoming\nincreasingly widespread in various fields, including medicine. Many face\ndetection algorithms have found applications in neurosurgery, ophthalmology,\nand plastic surgery. A common challenge in using these algorithms is variable\nlighting conditions and the flexibility of detection positions to identify and\nprecisely localize patients. The proposed experiment tests the MediaPipe\nalgorithm for detecting facial landmarks in a controlled setting, using a\nrobotic arm that automatically adjusts positions while the surgical light and\nthe phantom remain in a fixed position. The results of this study demonstrate\nthat the improved accuracy of facial landmark detection under surgical lighting\nsignificantly enhances the detection performance at larger yaw and pitch\nangles. The increase in standard deviation/dispersion occurs due to imprecise\ndetection of selected facial landmarks. This analysis allows for a discussion\non the potential integration of the MediaPipe algorithm into medical\nprocedures.\n","authors":["Ines Frajtag","Marko Švaco","Filip Šuligoj"],"pdf_url":"https://arxiv.org/pdf/2507.18248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18243v1","updated":"2025-07-24T09:32:53Z","published":"2025-07-24T09:32:53Z","title":"DepthDark: Robust Monocular Depth Estimation for Low-Light Environments","summary":"  In recent years, foundation models for monocular depth estimation have\nreceived increasing attention. Current methods mainly address typical daylight\nconditions, but their effectiveness notably decreases in low-light\nenvironments. There is a lack of robust foundational models for monocular depth\nestimation specifically designed for low-light scenarios. This largely stems\nfrom the absence of large-scale, high-quality paired depth datasets for\nlow-light conditions and the effective parameter-efficient fine-tuning (PEFT)\nstrategy. To address these challenges, we propose DepthDark, a robust\nfoundation model for low-light monocular depth estimation. We first introduce a\nflare-simulation module and a noise-simulation module to accurately simulate\nthe imaging process under nighttime conditions, producing high-quality paired\ndepth datasets for low-light conditions. Additionally, we present an effective\nlow-light PEFT strategy that utilizes illumination guidance and multiscale\nfeature fusion to enhance the model's capability in low-light environments. Our\nmethod achieves state-of-the-art depth estimation performance on the\nchallenging nuScenes-Night and RobotCar-Night datasets, validating its\neffectiveness using limited training data and computing resources.\n","authors":["Longjian Zeng","Zunjie Zhu","Rongfeng Lu","Ming Lu","Bolun Zheng","Chenggang Yan","Anke Xue"],"pdf_url":"https://arxiv.org/pdf/2507.18243v1.pdf","comment":"Accepted by ACM MM 2025 conference"},{"id":"http://arxiv.org/abs/2503.04151v2","updated":"2025-07-24T09:25:16Z","published":"2025-03-06T07:01:08Z","title":"Robust Multi-View Learning via Representation Fusion of Sample-Level\n  Attention and Alignment of Simulated Perturbation","summary":"  Recently, multi-view learning (MVL) has garnered significant attention due to\nits ability to fuse discriminative information from multiple views. However,\nreal-world multi-view datasets are often heterogeneous and imperfect, which\nusually causes MVL methods designed for specific combinations of views to lack\napplication potential and limits their effectiveness. To address this issue, we\npropose a novel robust MVL method (namely RML) with simultaneous representation\nfusion and alignment. Specifically, we introduce a simple yet effective\nmulti-view transformer fusion network where we transform heterogeneous\nmulti-view data into homogeneous word embeddings, and then integrate multiple\nviews by the sample-level attention mechanism to obtain a fused representation.\nFurthermore, we propose a simulated perturbation based multi-view contrastive\nlearning framework that dynamically generates the noise and unusable\nperturbations for simulating imperfect data conditions. The simulated noisy and\nunusable data obtain two distinct fused representations, and we utilize\ncontrastive learning to align them for learning discriminative and robust\nrepresentations. Our RML is self-supervised and can also be applied for\ndownstream tasks as a regularization. In experiments, we employ it in\nmulti-view unsupervised clustering, noise-label classification, and as a\nplug-and-play module for cross-modal hashing retrieval. Extensive comparison\nexperiments and ablation studies validate RML's effectiveness. Code is\navailable at https://github.com/SubmissionsIn/RML.\n","authors":["Jie Xu","Na Zhao","Gang Niu","Masashi Sugiyama","Xiaofeng Zhu"],"pdf_url":"https://arxiv.org/pdf/2503.04151v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18237v1","updated":"2025-07-24T09:24:29Z","published":"2025-07-24T09:24:29Z","title":"DATA: Domain-And-Time Alignment for High-Quality Feature Fusion in\n  Collaborative Perception","summary":"  Feature-level fusion shows promise in collaborative perception (CP) through\nbalanced performance and communication bandwidth trade-off. However, its\neffectiveness critically relies on input feature quality. The acquisition of\nhigh-quality features faces domain gaps from hardware diversity and deployment\nconditions, alongside temporal misalignment from transmission delays. These\nchallenges degrade feature quality with cumulative effects throughout the\ncollaborative network. In this paper, we present the Domain-And-Time Alignment\n(DATA) network, designed to systematically align features while maximizing\ntheir semantic representations for fusion. Specifically, we propose a\nConsistency-preserving Domain Alignment Module (CDAM) that reduces domain gaps\nthrough proximal-region hierarchical downsampling and observability-constrained\ndiscriminator. We further propose a Progressive Temporal Alignment Module\n(PTAM) to handle transmission delays via multi-scale motion modeling and\ntwo-stage compensation. Building upon the aligned features, an Instance-focused\nFeature Aggregation Module (IFAM) is developed to enhance semantic\nrepresentations. Extensive experiments demonstrate that DATA achieves\nstate-of-the-art performance on three typical datasets, maintaining robustness\nwith severe communication delays and pose errors. The code will be released at\nhttps://github.com/ChengchangTian/DATA.\n","authors":["Chengchang Tian","Jianwei Ma","Yan Huang","Zhanye Chen","Honghao Wei","Hui Zhang","Wei Hong"],"pdf_url":"https://arxiv.org/pdf/2507.18237v1.pdf","comment":"ICCV 2025, accepted as poster. 22 pages including supplementary\n  materials"},{"id":"http://arxiv.org/abs/2410.22128v2","updated":"2025-07-24T09:23:37Z","published":"2024-10-29T15:28:15Z","title":"PF3plat: Pose-Free Feed-Forward 3D Gaussian Splatting","summary":"  We consider the problem of novel view synthesis from unposed images in a\nsingle feed-forward. Our framework capitalizes on fast speed, scalability, and\nhigh-quality 3D reconstruction and view synthesis capabilities of 3DGS, where\nwe further extend it to offer a practical solution that relaxes common\nassumptions such as dense image views, accurate camera poses, and substantial\nimage overlaps. We achieve this through identifying and addressing unique\nchallenges arising from the use of pixel-aligned 3DGS: misaligned 3D Gaussians\nacross different views induce noisy or sparse gradients that destabilize\ntraining and hinder convergence, especially when above assumptions are not met.\nTo mitigate this, we employ pre-trained monocular depth estimation and visual\ncorrespondence models to achieve coarse alignments of 3D Gaussians. We then\nintroduce lightweight, learnable modules to refine depth and pose estimates\nfrom the coarse alignments, improving the quality of 3D reconstruction and\nnovel view synthesis. Furthermore, the refined estimates are leveraged to\nestimate geometry confidence scores, which assess the reliability of 3D\nGaussian centers and condition the prediction of Gaussian parameters\naccordingly. Extensive evaluations on large-scale real-world datasets\ndemonstrate that PF3plat sets a new state-of-the-art across all benchmarks,\nsupported by comprehensive ablation studies validating our design choices.\nproject page: https://cvlab-kaist.github.io/PF3plat/\n","authors":["Sunghwan Hong","Jaewoo Jung","Heeseong Shin","Jisang Han","Jiaolong Yang","Chong Luo","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2410.22128v2.pdf","comment":"Accepted by ICML'25"},{"id":"http://arxiv.org/abs/2507.18231v1","updated":"2025-07-24T09:22:02Z","published":"2025-07-24T09:22:02Z","title":"PS-GS: Gaussian Splatting for Multi-View Photometric Stereo","summary":"  Integrating inverse rendering with multi-view photometric stereo (MVPS)\nyields more accurate 3D reconstructions than the inverse rendering approaches\nthat rely on fixed environment illumination. However, efficient inverse\nrendering with MVPS remains challenging. To fill this gap, we introduce the\nGaussian Splatting for Multi-view Photometric Stereo (PS-GS), which efficiently\nand jointly estimates the geometry, materials, and lighting of the object that\nis illuminated by diverse directional lights (multi-light). Our method first\nreconstructs a standard 2D Gaussian splatting model as the initial geometry.\nBased on the initialization model, it then proceeds with the deferred inverse\nrendering by the full rendering equation containing a lighting-computing\nmulti-layer perceptron. During the whole optimization, we regularize the\nrendered normal maps by the uncalibrated photometric stereo estimated normals.\nWe also propose the 2D Gaussian ray-tracing for single directional light to\nrefine the incident lighting. The regularizations and the use of multi-view and\nmulti-light images mitigate the ill-posed problem of inverse rendering. After\noptimization, the reconstructed object can be used for novel-view synthesis,\nrelighting, and material and shape editing. Experiments on both synthetic and\nreal datasets demonstrate that our method outperforms prior works in terms of\nreconstruction accuracy and computational efficiency.\n","authors":["Yixiao Chen","Bin Liang","Hanzhi Guo","Yongqing Cheng","Jiayi Zhao","Dongdong Weng"],"pdf_url":"https://arxiv.org/pdf/2507.18231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18225v1","updated":"2025-07-24T09:18:39Z","published":"2025-07-24T09:18:39Z","title":"3D Test-time Adaptation via Graph Spectral Driven Point Shift","summary":"  While test-time adaptation (TTA) methods effectively address domain shifts by\ndynamically adapting pre-trained models to target domain data during online\ninference, their application to 3D point clouds is hindered by their irregular\nand unordered structure. Current 3D TTA methods often rely on computationally\nexpensive spatial-domain optimizations and may require additional training\ndata. In contrast, we propose Graph Spectral Domain Test-Time Adaptation\n(GSDTTA), a novel approach for 3D point cloud classification that shifts\nadaptation to the graph spectral domain, enabling more efficient adaptation by\ncapturing global structural properties with fewer parameters. Point clouds in\ntarget domain are represented as outlier-aware graphs and transformed into\ngraph spectral domain by Graph Fourier Transform (GFT). For efficiency,\nadaptation is performed by optimizing only the lowest 10% of frequency\ncomponents, which capture the majority of the point cloud's energy. An inverse\nGFT (IGFT) is then applied to reconstruct the adapted point cloud with the\ngraph spectral-driven point shift. This process is enhanced by an\neigenmap-guided self-training strategy that iteratively refines both the\nspectral adjustments and the model parameters. Experimental results and\nablation studies on benchmark datasets demonstrate the effectiveness of GSDTTA,\noutperforming existing TTA methods for 3D point cloud classification.\n","authors":["Xin Wei","Qin Yang","Yijie Fang","Mingrui Zhu","Nannan Wang"],"pdf_url":"https://arxiv.org/pdf/2507.18225v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.04599v2","updated":"2025-07-24T09:12:08Z","published":"2025-07-07T01:31:01Z","title":"QR-LoRA: Efficient and Disentangled Fine-tuning via QR Decomposition for\n  Customized Generation","summary":"  Existing text-to-image models often rely on parameter fine-tuning techniques\nsuch as Low-Rank Adaptation (LoRA) to customize visual attributes. However,\nwhen combining multiple LoRA models for content-style fusion tasks,\nunstructured modifications of weight matrices often lead to undesired feature\nentanglement between content and style attributes. We propose QR-LoRA, a novel\nfine-tuning framework leveraging QR decomposition for structured parameter\nupdates that effectively separate visual attributes. Our key insight is that\nthe orthogonal Q matrix naturally minimizes interference between different\nvisual features, while the upper triangular R matrix efficiently encodes\nattribute-specific transformations. Our approach fixes both Q and R matrices\nwhile only training an additional task-specific $\\Delta R$ matrix. This\nstructured design reduces trainable parameters to half of conventional LoRA\nmethods and supports effective merging of multiple adaptations without\ncross-contamination due to the strong disentanglement properties between\n$\\Delta R$ matrices. Experiments demonstrate that QR-LoRA achieves superior\ndisentanglement in content-style fusion tasks, establishing a new paradigm for\nparameter-efficient, disentangled fine-tuning in generative models. The project\npage is available at: https://luna-ai-lab.github.io/QR-LoRA/.\n","authors":["Jiahui Yang","Yongjia Ma","Donglin Di","Hao Li","Wei Chen","Yan Xie","Jianxun Cui","Xun Yang","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2507.04599v2.pdf","comment":"ICCV 2025, 30 pages, 26 figures"},{"id":"http://arxiv.org/abs/2507.18214v1","updated":"2025-07-24T09:08:04Z","published":"2025-07-24T09:08:04Z","title":"LEAF: Latent Diffusion with Efficient Encoder Distillation for Aligned\n  Features in Medical Image Segmentation","summary":"  Leveraging the powerful capabilities of diffusion models has yielded quite\neffective results in medical image segmentation tasks. However, existing\nmethods typically transfer the original training process directly without\nspecific adjustments for segmentation tasks. Furthermore, the commonly used\npre-trained diffusion models still have deficiencies in feature extraction.\nBased on these considerations, we propose LEAF, a medical image segmentation\nmodel grounded in latent diffusion models. During the fine-tuning process, we\nreplace the original noise prediction pattern with a direct prediction of the\nsegmentation map, thereby reducing the variance of segmentation results. We\nalso employ a feature distillation method to align the hidden states of the\nconvolutional layers with the features from a transformer-based vision encoder.\nExperimental results demonstrate that our method enhances the performance of\nthe original diffusion model across multiple segmentation datasets for\ndifferent disease types. Notably, our approach does not alter the model\narchitecture, nor does it increase the number of parameters or computation\nduring the inference phase, making it highly efficient.\n","authors":["Qilin Huang","Tianyu Lin","Zhiguang Chen","Fudan Zheng"],"pdf_url":"https://arxiv.org/pdf/2507.18214v1.pdf","comment":"Accepted at MICCAI 2025"},{"id":"http://arxiv.org/abs/2507.07620v3","updated":"2025-07-24T09:03:07Z","published":"2025-07-10T10:41:13Z","title":"ViLU: Learning Vision-Language Uncertainties for Failure Prediction","summary":"  Reliable Uncertainty Quantification (UQ) and failure prediction remain open\nchallenges for Vision-Language Models (VLMs). We introduce ViLU, a new\nVision-Language Uncertainty quantification framework that contextualizes\nuncertainty estimates by leveraging all task-relevant textual representations.\nViLU constructs an uncertainty-aware multi-modal representation by integrating\nthe visual embedding, the predicted textual embedding, and an image-conditioned\ntextual representation via cross-attention. Unlike traditional UQ methods based\non loss prediction, ViLU trains an uncertainty predictor as a binary classifier\nto distinguish correct from incorrect predictions using a weighted binary\ncross-entropy loss, making it loss-agnostic. In particular, our proposed\napproach is well-suited for post-hoc settings, where only vision and text\nembeddings are available without direct access to the model itself. Extensive\nexperiments on diverse datasets show the significant gains of our method\ncompared to state-of-the-art failure prediction methods. We apply our method to\nstandard classification datasets, such as ImageNet-1k, as well as large-scale\nimage-caption datasets like CC12M and LAION-400M. Ablation studies highlight\nthe critical role of our architecture and training in achieving effective\nuncertainty quantification. Our code is publicly available and can be found\nhere: https://github.com/ykrmm/ViLU.\n","authors":["Marc Lafon","Yannis Karmim","Julio Silva-Rodríguez","Paul Couairon","Clément Rambour","Raphaël Fournier-Sniehotta","Ismail Ben Ayed","Jose Dolz","Nicolas Thome"],"pdf_url":"https://arxiv.org/pdf/2507.07620v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20158v2","updated":"2025-07-24T08:56:00Z","published":"2025-02-27T14:56:58Z","title":"Learning to Generalize without Bias for Open-Vocabulary Action\n  Recognition","summary":"  Leveraging the effective visual-text alignment and static generalizability\nfrom CLIP, recent video learners adopt CLIP initialization with further\nregularization or recombination for generalization in open-vocabulary action\nrecognition in-context. However, due to the static bias of CLIP, such video\nlearners tend to overfit on shortcut static features, thereby compromising\ntheir generalizability, especially to novel out-of-context actions. To address\nthis issue, we introduce Open-MeDe, a novel Meta-optimization framework with\nstatic Debiasing for Open-vocabulary action recognition. From a fresh\nperspective of generalization, Open-MeDe adopts a meta-learning approach to\nimprove known-to-open generalizing and image-to-video debiasing in a\ncost-effective manner. Specifically, Open-MeDe introduces a cross-batch\nmeta-optimization scheme that explicitly encourages video learners to quickly\ngeneralize to arbitrary subsequent data via virtual evaluation, steering a\nsmoother optimization landscape. In effect, the free of CLIP regularization\nduring optimization implicitly mitigates the inherent static bias of the video\nmeta-learner. We further apply self-ensemble over the optimization trajectory\nto obtain generic optimal parameters that can achieve robust generalization to\nboth in-context and out-of-context novel data. Extensive evaluations show that\nOpen-MeDe not only surpasses state-of-the-art regularization methods tailored\nfor in-context open-vocabulary action recognition but also substantially excels\nin out-of-context scenarios.Code is released at\nhttps://github.com/Mia-YatingYu/Open-MeDe.\n","authors":["Yating Yu","Congqi Cao","Yifan Zhang","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.20158v2.pdf","comment":"Accepted by ICCV2025 (Highlight)"},{"id":"http://arxiv.org/abs/2507.18192v1","updated":"2025-07-24T08:45:40Z","published":"2025-07-24T08:45:40Z","title":"TeEFusion: Blending Text Embeddings to Distill Classifier-Free Guidance","summary":"  Recent advances in text-to-image synthesis largely benefit from sophisticated\nsampling strategies and classifier-free guidance (CFG) to ensure high-quality\ngeneration. However, CFG's reliance on two forward passes, especially when\ncombined with intricate sampling algorithms, results in prohibitively high\ninference costs. To address this, we introduce TeEFusion (\\textbf{Te}xt\n\\textbf{E}mbeddings \\textbf{Fusion}), a novel and efficient distillation method\nthat directly incorporates the guidance magnitude into the text embeddings and\ndistills the teacher model's complex sampling strategy. By simply fusing\nconditional and unconditional text embeddings using linear operations,\nTeEFusion reconstructs the desired guidance without adding extra parameters,\nsimultaneously enabling the student model to learn from the teacher's output\nproduced via its sophisticated sampling approach. Extensive experiments on\nstate-of-the-art models such as SD3 demonstrate that our method allows the\nstudent to closely mimic the teacher's performance with a far simpler and more\nefficient sampling strategy. Consequently, the student model achieves inference\nspeeds up to 6$\\times$ faster than the teacher model, while maintaining image\nquality at levels comparable to those obtained through the teacher's complex\nsampling approach. The code is publicly available at\n\\href{https://github.com/AIDC-AI/TeEFusion}{github.com/AIDC-AI/TeEFusion}.\n","authors":["Minghao Fu","Guo-Hua Wang","Xiaohao Chen","Qing-Guo Chen","Zhao Xu","Weihua Luo","Kaifu Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.18192v1.pdf","comment":"Accepted by ICCV 2025. The code is publicly available at\n  https://github.com/AIDC-AI/TeEFusion"},{"id":"http://arxiv.org/abs/2507.18184v1","updated":"2025-07-24T08:32:41Z","published":"2025-07-24T08:32:41Z","title":"MatSSL: Robust Self-Supervised Representation Learning for\n  Metallographic Image Segmentation","summary":"  MatSSL is a streamlined self-supervised learning (SSL) architecture that\nemploys Gated Feature Fusion at each stage of the backbone to integrate\nmulti-level representations effectively. Current micrograph analysis of\nmetallic materials relies on supervised methods, which require retraining for\neach new dataset and often perform inconsistently with only a few labeled\nsamples. While SSL offers a promising alternative by leveraging unlabeled data,\nmost existing methods still depend on large-scale datasets to be effective.\nMatSSL is designed to overcome this limitation. We first perform\nself-supervised pretraining on a small-scale, unlabeled dataset and then\nfine-tune the model on multiple benchmark datasets. The resulting segmentation\nmodels achieve 69.13% mIoU on MetalDAM, outperforming the 66.73% achieved by an\nImageNet-pretrained encoder, and delivers consistently up to nearly 40%\nimprovement in average mIoU on the Environmental Barrier Coating benchmark\ndataset (EBC) compared to models pretrained with MicroNet. This suggests that\nMatSSL enables effective adaptation to the metallographic domain using only a\nsmall amount of unlabeled data, while preserving the rich and transferable\nfeatures learned from large-scale pretraining on natural images.\n","authors":["Hoang Hai Nam Nguyen","Phan Nguyen Duc Hieu","Ho Won Lee"],"pdf_url":"https://arxiv.org/pdf/2507.18184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18183v1","updated":"2025-07-24T08:29:21Z","published":"2025-07-24T08:29:21Z","title":"ChronoSelect: Robust Learning with Noisy Labels via Dynamics Temporal\n  Memory","summary":"  Training deep neural networks on real-world datasets is often hampered by the\npresence of noisy labels, which can be memorized by over-parameterized models,\nleading to significant degradation in generalization performance. While\nexisting methods for learning with noisy labels (LNL) have made considerable\nprogress, they fundamentally suffer from static snapshot evaluations and fail\nto leverage the rich temporal dynamics of learning evolution. In this paper, we\npropose ChronoSelect (chrono denoting its temporal nature), a novel framework\nfeaturing an innovative four-stage memory architecture that compresses\nprediction history into compact temporal distributions. Our unique sliding\nupdate mechanism with controlled decay maintains only four dynamic memory units\nper sample, progressively emphasizing recent patterns while retaining essential\nhistorical knowledge. This enables precise three-way sample partitioning into\nclean, boundary, and noisy subsets through temporal trajectory analysis and\ndual-branch consistency. Theoretical guarantees prove the mechanism's\nconvergence and stability under noisy conditions. Extensive experiments\ndemonstrate ChronoSelect's state-of-the-art performance across synthetic and\nreal-world benchmarks.\n","authors":["Jianchao Wang","Qingfeng Li","Pengcheng Zheng","Xiaorong Pu","Yazhou Ren"],"pdf_url":"https://arxiv.org/pdf/2507.18183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.05245v3","updated":"2025-07-24T08:26:59Z","published":"2025-03-07T08:57:38Z","title":"L-FUSION: Laplacian Fetal Ultrasound Segmentation & Uncertainty\n  Estimation","summary":"  Accurate analysis of prenatal ultrasound (US) is essential for early\ndetection of developmental anomalies. However, operator dependency and\ntechnical limitations (e.g. intrinsic artefacts and effects, setting errors)\ncan complicate image interpretation and the assessment of diagnostic\nuncertainty. We present L-FUSION (Laplacian Fetal US Segmentation with\nIntegrated FoundatiON models), a framework that integrates uncertainty\nquantification through unsupervised, normative learning and large-scale\nfoundation models for robust segmentation of fetal structures in normal and\npathological scans. We propose to utilise the aleatoric logit distributions of\nStochastic Segmentation Networks and Laplace approximations with fast Hessian\nestimations to estimate epistemic uncertainty only from the segmentation head.\nThis enables us to achieve reliable abnormality quantification for instant\ndiagnostic feedback. Combined with an integrated Dropout component, L-FUSION\nenables reliable differentiation of lesions from normal fetal anatomy with\nenhanced uncertainty maps and segmentation counterfactuals in US imaging. It\nimproves epistemic and aleatoric uncertainty interpretation and removes the\nneed for manual disease-labelling. Evaluations across multiple datasets show\nthat L-FUSION achieves superior segmentation accuracy and consistent\nuncertainty quantification, supporting on-site decision-making and offering a\nscalable solution for advancing fetal ultrasound analysis in clinical settings.\n","authors":["Johanna P. Müller","Robert Wright","Thomas G. Day","Lorenzo Venturini","Samuel F. Budd","Hadrien Reynaud","Joseph V. Hajnal","Reza Razavi","Bernhard Kainz"],"pdf_url":"https://arxiv.org/pdf/2503.05245v3.pdf","comment":"Accepted at MICCAI ASMUS 2025"},{"id":"http://arxiv.org/abs/2503.13176v3","updated":"2025-07-24T08:25:52Z","published":"2025-03-17T13:53:04Z","title":"DeGauss: Dynamic-Static Decomposition with Gaussian Splatting for\n  Distractor-free 3D Reconstruction","summary":"  Reconstructing clean, distractor-free 3D scenes from real-world captures\nremains a significant challenge, particularly in highly dynamic and cluttered\nsettings such as egocentric videos. To tackle this problem, we introduce\nDeGauss, a simple and robust self-supervised framework for dynamic scene\nreconstruction based on a decoupled dynamic-static Gaussian Splatting design.\nDeGauss models dynamic elements with foreground Gaussians and static content\nwith background Gaussians, using a probabilistic mask to coordinate their\ncomposition and enable independent yet complementary optimization. DeGauss\ngeneralizes robustly across a wide range of real-world scenarios, from casual\nimage collections to long, dynamic egocentric videos, without relying on\ncomplex heuristics or extensive supervision. Experiments on benchmarks\nincluding NeRF-on-the-go, ADT, AEA, Hot3D, and EPIC-Fields demonstrate that\nDeGauss consistently outperforms existing methods, establishing a strong\nbaseline for generalizable, distractor-free 3D reconstructionin highly dynamic,\ninteraction-rich environments. Project page:\nhttps://batfacewayne.github.io/DeGauss.io/\n","authors":["Rui Wang","Quentin Lohmeyer","Mirko Meboldt","Siyu Tang"],"pdf_url":"https://arxiv.org/pdf/2503.13176v3.pdf","comment":"Accepted by ICCV 2025"},{"id":"http://arxiv.org/abs/2507.15401v3","updated":"2025-07-24T08:24:01Z","published":"2025-07-21T09:04:29Z","title":"Rethinking Occlusion in FER: A Semantic-Aware Perspective and Go Beyond","summary":"  Facial expression recognition (FER) is a challenging task due to pervasive\nocclusion and dataset biases. Especially when facial information is partially\noccluded, existing FER models struggle to extract effective facial features,\nleading to inaccurate classifications. In response, we present ORSANet, which\nintroduces the following three key contributions: First, we introduce auxiliary\nmulti-modal semantic guidance to disambiguate facial occlusion and learn\nhigh-level semantic knowledge, which is two-fold: 1) we introduce semantic\nsegmentation maps as dense semantics prior to generate semantics-enhanced\nfacial representations; 2) we introduce facial landmarks as sparse geometric\nprior to mitigate intrinsic noises in FER, such as identity and gender biases.\nSecond, to facilitate the effective incorporation of these two multi-modal\npriors, we customize a Multi-scale Cross-interaction Module (MCM) to adaptively\nfuse the landmark feature and semantics-enhanced representations within\ndifferent scales. Third, we design a Dynamic Adversarial Repulsion Enhancement\nLoss (DARELoss) that dynamically adjusts the margins of ambiguous classes,\nfurther enhancing the model's ability to distinguish similar expressions. We\nfurther construct the first occlusion-oriented FER dataset to facilitate\nspecialized robustness analysis on various real-world occlusion conditions,\ndubbed Occlu-FER. Extensive experiments on both public benchmarks and Occlu-FER\ndemonstrate that our proposed ORSANet achieves SOTA recognition performance.\nCode is publicly available at https://github.com/Wenyuzhy/ORSANet-master.\n","authors":["Huiyu Zhai","Xingxing Yang","Yalan Ye","Chenyang Li","Bin Fan","Changze Li"],"pdf_url":"https://arxiv.org/pdf/2507.15401v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18177v1","updated":"2025-07-24T08:23:11Z","published":"2025-07-24T08:23:11Z","title":"Differential-UMamba: Rethinking Tumor Segmentation Under Limited Data\n  Scenarios","summary":"  In data-scarce scenarios, deep learning models often overfit to noise and\nirrelevant patterns, which limits their ability to generalize to unseen\nsamples. To address these challenges in medical image segmentation, we\nintroduce Diff-UMamba, a novel architecture that combines the UNet framework\nwith the mamba mechanism for modeling long-range dependencies. At the heart of\nDiff-UMamba is a Noise Reduction Module (NRM), which employs a signal\ndifferencing strategy to suppress noisy or irrelevant activations within the\nencoder. This encourages the model to filter out spurious features and enhance\ntask-relevant representations, thereby improving its focus on clinically\nmeaningful regions. As a result, the architecture achieves improved\nsegmentation accuracy and robustness, particularly in low-data settings.\nDiff-UMamba is evaluated on multiple public datasets, including MSD (lung and\npancreas) and AIIB23, demonstrating consistent performance gains of 1-3% over\nbaseline methods across diverse segmentation tasks. To further assess\nperformance under limited-data conditions, additional experiments are conducted\non the BraTS-21 dataset by varying the proportion of available training\nsamples. The approach is also validated on a small internal non-small cell lung\ncancer (NSCLC) dataset for gross tumor volume (GTV) segmentation in cone beam\nCT (CBCT), where it achieves a 4-5% improvement over the baseline.\n","authors":["Dhruv Jain","Romain Modzelewski","Romain Hérault","Clement Chatelain","Eva Torfeh","Sebastien Thureau"],"pdf_url":"https://arxiv.org/pdf/2507.18177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18176v1","updated":"2025-07-24T08:21:43Z","published":"2025-07-24T08:21:43Z","title":"Unsupervised Domain Adaptation for 3D LiDAR Semantic Segmentation Using\n  Contrastive Learning and Multi-Model Pseudo Labeling","summary":"  Addressing performance degradation in 3D LiDAR semantic segmentation due to\ndomain shifts (e.g., sensor type, geographical location) is crucial for\nautonomous systems, yet manual annotation of target data is prohibitive. This\nstudy addresses the challenge using Unsupervised Domain Adaptation (UDA) and\nintroduces a novel two-stage framework to tackle it. Initially, unsupervised\ncontrastive learning at the segment level is used to pre-train a backbone\nnetwork, enabling it to learn robust, domain-invariant features without labels.\nSubsequently, a multi-model pseudo-labeling strategy is introduced, utilizing\nan ensemble of diverse state-of-the-art architectures (including projection,\nvoxel, hybrid, and cylinder-based methods). Predictions from these models are\naggregated via hard voting to generate high-quality, refined pseudo-labels for\nthe unlabeled target domain, mitigating single-model biases. The contrastively\npre-trained network is then fine-tuned using these robust pseudo-labels.\nExperiments adapting from SemanticKITTI to unlabeled target datasets\n(SemanticPOSS, SemanticSlamantic) demonstrate significant improvements in\nsegmentation accuracy compared to direct transfer and single-model UDA\napproaches. These results highlight the effectiveness of combining contrastive\npre-training with refined ensemble pseudo-labeling for bridging complex domain\ngaps without requiring target domain annotations.\n","authors":["Abhishek Kaushik","Norbert Haala","Uwe Soergel"],"pdf_url":"https://arxiv.org/pdf/2507.18176v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18174v1","updated":"2025-07-24T08:17:37Z","published":"2025-07-24T08:17:37Z","title":"Real-Time Object Detection and Classification using YOLO for Edge FPGAs","summary":"  Object detection and classification are crucial tasks across various\napplication domains, particularly in the development of safe and reliable\nAdvanced Driver Assistance Systems (ADAS). Existing deep learning-based methods\nsuch as Convolutional Neural Networks (CNNs), Single Shot Detectors (SSDs), and\nYou Only Look Once (YOLO) have demonstrated high performance in terms of\naccuracy and computational speed when deployed on Field-Programmable Gate\nArrays (FPGAs). However, despite these advances, state-of-the-art YOLO-based\nobject detection and classification systems continue to face challenges in\nachieving resource efficiency suitable for edge FPGA platforms. To address this\nlimitation, this paper presents a resource-efficient real-time object detection\nand classification system based on YOLOv5 optimized for FPGA deployment. The\nproposed system is trained on the COCO and GTSRD datasets and implemented on\nthe Xilinx Kria KV260 FPGA board. Experimental results demonstrate a\nclassification accuracy of 99%, with a power consumption of 3.5W and a\nprocessing speed of 9 frames per second (FPS). These findings highlight the\neffectiveness of the proposed approach in enabling real-time,\nresource-efficient object detection and classification for edge computing\napplications.\n","authors":["Rashed Al Amin","Roman Obermaisser"],"pdf_url":"https://arxiv.org/pdf/2507.18174v1.pdf","comment":"This paper has been accepted for the 67th International Symposium on\n  ELMAR 2025"},{"id":"http://arxiv.org/abs/2507.18173v1","updated":"2025-07-24T08:16:15Z","published":"2025-07-24T08:16:15Z","title":"WaveMamba: Wavelet-Driven Mamba Fusion for RGB-Infrared Object Detection","summary":"  Leveraging the complementary characteristics of visible (RGB) and infrared\n(IR) imagery offers significant potential for improving object detection. In\nthis paper, we propose WaveMamba, a cross-modality fusion method that\nefficiently integrates the unique and complementary frequency features of RGB\nand IR decomposed by Discrete Wavelet Transform (DWT). An improved detection\nhead incorporating the Inverse Discrete Wavelet Transform (IDWT) is also\nproposed to reduce information loss and produce the final detection results.\nThe core of our approach is the introduction of WaveMamba Fusion Block (WMFB),\nwhich facilitates comprehensive fusion across low-/high-frequency sub-bands.\nWithin WMFB, the Low-frequency Mamba Fusion Block (LMFB), built upon the Mamba\nframework, first performs initial low-frequency feature fusion with channel\nswapping, followed by deep fusion with an advanced gated attention mechanism\nfor enhanced integration. High-frequency features are enhanced using a strategy\nthat applies an ``absolute maximum\" fusion approach. These advancements lead to\nsignificant performance gains, with our method surpassing state-of-the-art\napproaches and achieving average mAP improvements of 4.5% on four benchmarks.\n","authors":["Haodong Zhu","Wenhao Dong","Linlin Yang","Hong Li","Yuguang Yang","Yangyang Ren","Qingcheng Zhu","Zichao Feng","Changbai Li","Shaohui Lin","Runqi Wang","Xiaoyan Luo","Baochang Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.18173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16319v3","updated":"2025-07-24T08:15:11Z","published":"2024-11-25T12:11:27Z","title":"CutS3D: Cutting Semantics in 3D for 2D Unsupervised Instance\n  Segmentation","summary":"  Traditionally, algorithms that learn to segment object instances in 2D images\nhave heavily relied on large amounts of human-annotated data. Only recently,\nnovel approaches have emerged tackling this problem in an unsupervised fashion.\nGenerally, these approaches first generate pseudo-masks and then train a\nclass-agnostic detector. While such methods deliver the current state of the\nart, they often fail to correctly separate instances overlapping in 2D image\nspace since only semantics are considered. To tackle this issue, we instead\npropose to cut the semantic masks in 3D to obtain the final 2D instances by\nutilizing a point cloud representation of the scene. Furthermore, we derive a\nSpatial Importance function, which we use to resharpen the semantics along the\n3D borders of instances. Nevertheless, these pseudo-masks are still subject to\nmask ambiguity. To address this issue, we further propose to augment the\ntraining of a class-agnostic detector with three Spatial Confidence components\naiming to isolate a clean learning signal. With these contributions, our\napproach outperforms competing methods across multiple standard benchmarks for\nunsupervised instance segmentation and object detection.\n","authors":["Leon Sick","Dominik Engel","Sebastian Hartwig","Pedro Hermosilla","Timo Ropinski"],"pdf_url":"https://arxiv.org/pdf/2411.16319v3.pdf","comment":"Accepted at ICCV 2025. Project Page with Code, Models & Demo:\n  https://leonsick.github.io/cuts3d/"},{"id":"http://arxiv.org/abs/2503.22351v2","updated":"2025-07-24T08:14:19Z","published":"2025-03-28T11:46:50Z","title":"One Look is Enough: A Novel Seamless Patchwise Refinement for Zero-Shot\n  Monocular Depth Estimation Models on High-Resolution Images","summary":"  Zero-shot depth estimation (DE) models exhibit strong generalization\nperformance as they are trained on large-scale datasets. However, existing\nmodels struggle with high-resolution images due to the discrepancy in image\nresolutions of training (with smaller resolutions) and inference (for high\nresolutions). Processing them at full resolution leads to decreased estimation\naccuracy on depth with tremendous memory consumption, while downsampling to the\ntraining resolution results in blurred edges in the estimated depth images.\nPrevailing high-resolution depth estimation methods adopt a patch-based\napproach, which introduces depth discontinuity issues when reassembling the\nestimated depth patches, resulting in test-time inefficiency. Additionally, to\nobtain fine-grained depth details, these methods rely on synthetic datasets due\nto the real-world sparse ground truth depth, leading to poor generalizability.\nTo tackle these limitations, we propose Patch Refine Once (PRO), an efficient\nand generalizable tile-based framework. Our PRO consists of two key components:\n(i) Grouped Patch Consistency Training that enhances test-time efficiency while\nmitigating the depth discontinuity problem by jointly processing four\noverlapping patches and enforcing a consistency loss on their overlapping\nregions within a single backpropagation step, and (ii) Bias Free Masking that\nprevents the DE models from overfitting to dataset-specific biases, enabling\nbetter generalization to real-world datasets even after training on synthetic\ndata. Zero-shot evaluations on Booster, ETH3D, Middlebury 2014, and NuScenes\ndemonstrate that our PRO can be seamlessly integrated into existing depth\nestimation models.\n","authors":["Byeongjun Kwon","Munchurl Kim"],"pdf_url":"https://arxiv.org/pdf/2503.22351v2.pdf","comment":"ICCV 2025 (camera-ready version). [Project\n  page](https://kaist-viclab.github.io/One-Look-is-Enough_site)"},{"id":"http://arxiv.org/abs/2403.09554v2","updated":"2025-07-24T08:04:35Z","published":"2024-03-14T16:41:26Z","title":"Cloud gap-filling with deep learning for improved grassland monitoring","summary":"  Uninterrupted optical image time series are crucial for the timely monitoring\nof agricultural land changes, particularly in grasslands. However, the\ncontinuity of such time series is often disrupted by clouds. In response to\nthis challenge, we propose an innovative deep learning method that integrates\ncloud-free optical (Sentinel-2) observations and weather-independent\n(Sentinel-1) Synthetic Aperture Radar (SAR) data. Our approach employs a hybrid\narchitecture combining Convolutional Neural Networks (CNNs) and Recurrent\nNeural Networks (RNNs) to generate continuous Normalized Difference Vegetation\nIndex (NDVI) time series, highlighting the role of NDVI in the synergy between\nSAR and optical data. We demonstrate the significance of observation continuity\nby assessing the impact of the generated NDVI time series on the downstream\ntask of grassland mowing event detection. We conducted our study in Lithuania,\na country characterized by extensive cloud coverage, and compared our approach\nwith alternative interpolation techniques (i.e., linear, Akima, quadratic). Our\nmethod outperformed these techniques, achieving an average Mean Absolute Error\n(MAE) of 0.024 and a coefficient of determination R^2 of 0.92. Additionally,\nour analysis revealed improvement in the performance of the mowing event\ndetection, with F1-score up to 84% using two widely applied mowing detection\nmethodologies. Our method also effectively mitigated sudden shifts and noise\noriginating from cloudy observations, which are often missed by conventional\ncloud masks and adversely affect mowing detection precision.\n","authors":["Iason Tsardanidis","Alkiviadis Koukos","Vasileios Sitokonstantinou","Thanassis Drivas","Charalampos Kontoes"],"pdf_url":"https://arxiv.org/pdf/2403.09554v2.pdf","comment":"Published in Computers and Electronics in Agriculture"},{"id":"http://arxiv.org/abs/2507.00566v2","updated":"2025-07-24T07:56:39Z","published":"2025-07-01T08:34:35Z","title":"Zero-Shot Skeleton-Based Action Recognition With Prototype-Guided\n  Feature Alignment","summary":"  Zero-shot skeleton-based action recognition aims to classify unseen\nskeleton-based human actions without prior exposure to such categories during\ntraining. This task is extremely challenging due to the difficulty in\ngeneralizing from known to unknown actions. Previous studies typically use\ntwo-stage training: pre-training skeleton encoders on seen action categories\nusing cross-entropy loss and then aligning pre-extracted skeleton and text\nfeatures, enabling knowledge transfer to unseen classes through skeleton-text\nalignment and language models' generalization. However, their efficacy is\nhindered by 1) insufficient discrimination for skeleton features, as the fixed\nskeleton encoder fails to capture necessary alignment information for effective\nskeleton-text alignment; 2) the neglect of alignment bias between skeleton and\nunseen text features during testing. To this end, we propose a prototype-guided\nfeature alignment paradigm for zero-shot skeleton-based action recognition,\ntermed PGFA. Specifically, we develop an end-to-end cross-modal contrastive\ntraining framework to improve skeleton-text alignment, ensuring sufficient\ndiscrimination for skeleton features. Additionally, we introduce a\nprototype-guided text feature alignment strategy to mitigate the adverse impact\nof the distribution discrepancy during testing. We provide a theoretical\nanalysis to support our prototype-guided text feature alignment strategy and\nempirically evaluate our overall PGFA on three well-known datasets. Compared\nwith the top competitor SMIE method, our PGFA achieves absolute accuracy\nimprovements of 22.96%, 12.53%, and 18.54% on the NTU-60, NTU-120, and PKU-MMD\ndatasets, respectively.\n","authors":["Kai Zhou","Shuhai Zhang","Zeng You","Jinwu Hu","Mingkui Tan","Fei Liu"],"pdf_url":"https://arxiv.org/pdf/2507.00566v2.pdf","comment":"This paper is accepted by IEEE TIP 2025 (The journal version is\n  available at https://doi.org/10.1109/TIP.2025.3586487). Code is publicly\n  available at https://github.com/kaai520/PGFA"},{"id":"http://arxiv.org/abs/2503.07588v3","updated":"2025-07-24T07:44:45Z","published":"2025-03-10T17:51:16Z","title":"When Large Vision-Language Model Meets Large Remote Sensing Imagery:\n  Coarse-to-Fine Text-Guided Token Pruning","summary":"  Efficient vision-language understanding of large Remote Sensing Images (RSIs)\nis meaningful but challenging. Current Large Vision-Language Models (LVLMs)\ntypically employ limited pre-defined grids to process images, leading to\ninformation loss when handling gigapixel RSIs. Conversely, using unlimited\ngrids significantly increases computational costs. To preserve image details\nwhile reducing computational complexity, we propose a text-guided token pruning\nmethod with Dynamic Image Pyramid (DIP) integration. Our method introduces: (i)\na Region Focus Module (RFM) that leverages text-aware region localization\ncapability to identify critical vision tokens, and (ii) a coarse-to-fine image\ntile selection and vision token pruning strategy based on DIP, which is guided\nby RFM outputs and avoids directly processing the entire large imagery.\nAdditionally, existing benchmarks for evaluating LVLMs' perception ability on\nlarge RSI suffer from limited question diversity and constrained image sizes.\nWe construct a new benchmark named LRS-VQA, which contains 7,333 QA pairs\nacross 8 categories, with image length up to 27,328 pixels. Our method\noutperforms existing high-resolution strategies on four datasets using the same\ndata. Moreover, compared to existing token reduction methods, our approach\ndemonstrates higher efficiency under high-resolution settings. Dataset and code\nare in https://github.com/VisionXLab/LRS-VQA.\n","authors":["Junwei Luo","Yingying Zhang","Xue Yang","Kang Wu","Qi Zhu","Lei Liang","Jingdong Chen","Yansheng Li"],"pdf_url":"https://arxiv.org/pdf/2503.07588v3.pdf","comment":"18 pages, 6 figures, 18 tables"},{"id":"http://arxiv.org/abs/2507.18155v1","updated":"2025-07-24T07:41:40Z","published":"2025-07-24T07:41:40Z","title":"GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar","summary":"  Despite recent progress in 3D head avatar generation, balancing identity\npreservation, i.e., reconstruction, with novel poses and expressions, i.e.,\nanimation, remains a challenge. Existing methods struggle to adapt Gaussians to\nvarying geometrical deviations across facial regions, resulting in suboptimal\nquality. To address this, we propose GeoAvatar, a framework for adaptive\ngeometrical Gaussian Splatting. GeoAvatar leverages Adaptive Pre-allocation\nStage (APS), an unsupervised method that segments Gaussians into rigid and\nflexible sets for adaptive offset regularization. Then, based on mouth anatomy\nand dynamics, we introduce a novel mouth structure and the part-wise\ndeformation strategy to enhance the animation fidelity of the mouth. Finally,\nwe propose a regularization loss for precise rigging between Gaussians and 3DMM\nfaces. Moreover, we release DynamicFace, a video dataset with highly expressive\nfacial motions. Extensive experiments show the superiority of GeoAvatar\ncompared to state-of-the-art methods in reconstruction and novel animation\nscenarios.\n","authors":["SeungJun Moon","Hah Min Lew","Seungeun Lee","Ji-Su Kang","Gyeong-Moon Park"],"pdf_url":"https://arxiv.org/pdf/2507.18155v1.pdf","comment":"ICCV 2025, Project page: https://hahminlew.github.io/geoavatar/"},{"id":"http://arxiv.org/abs/2507.16362v2","updated":"2025-07-24T07:30:07Z","published":"2025-07-22T08:54:32Z","title":"LPTR-AFLNet: Lightweight Integrated Chinese License Plate Rectification\n  and Recognition Network","summary":"  Chinese License Plate Recognition (CLPR) faces numerous challenges in\nunconstrained and complex environments, particularly due to perspective\ndistortions caused by various shooting angles and the correction of single-line\nand double-line license plates. Considering the limited computational resources\nof edge devices, developing a low-complexity, end-to-end integrated network for\nboth correction and recognition is essential for achieving real-time and\nefficient deployment. In this work, we propose a lightweight, unified network\nnamed LPTR-AFLNet for correcting and recognizing Chinese license plates, which\ncombines a perspective transformation correction module (PTR) with an optimized\nlicense plate recognition network, AFLNet. The network leverages the\nrecognition output as a weak supervisory signal to effectively guide the\ncorrection process, ensuring accurate perspective distortion correction. To\nenhance recognition accuracy, we introduce several improvements to LPRNet,\nincluding an improved attention module to reduce confusion among similar\ncharacters and the use of Focal Loss to address class imbalance during\ntraining. Experimental results demonstrate the exceptional performance of\nLPTR-AFLNet in rectifying perspective distortion and recognizing double-line\nlicense plate images, maintaining high recognition accuracy across various\nchallenging scenarios. Moreover, on lower-mid-range GPUs platform, the method\nruns in less than 10 milliseconds, indicating its practical efficiency and\nbroad applicability.\n","authors":["Guangzhu Xu","Pengcheng Zuo","Zhi Ke","Bangjun Lei"],"pdf_url":"https://arxiv.org/pdf/2507.16362v2.pdf","comment":"28 pages, 33 figures"},{"id":"http://arxiv.org/abs/2506.23825v2","updated":"2025-07-24T07:25:10Z","published":"2025-06-30T13:17:49Z","title":"Flash-VStream: Efficient Real-Time Understanding for Long Video Streams","summary":"  Benefiting from the advances in large language models and cross-modal\nalignment, existing multimodal large language models have achieved prominent\nperformance in image and short video understanding. However, the understanding\nof long videos is still challenging, as their long-context nature results in\nsignificant computational and memory overhead. Most existing work treats long\nvideos in the same way as short videos, which is inefficient for real-world\napplications and hard to generalize to even longer videos. To address these\nissues, we propose Flash-VStream, an efficient video language model capable of\nprocessing extremely long videos and responding to user queries in real time.\nParticularly, we design a Flash Memory module, containing a low-capacity\ncontext memory to aggregate long-context temporal information and model the\ndistribution of information density, and a high-capacity augmentation memory to\nretrieve detailed spatial information based on this distribution. Compared to\nexisting models, Flash-VStream achieves significant reductions in inference\nlatency. Extensive experiments on long video benchmarks and comprehensive video\nbenchmarks, i.e., EgoSchema, MLVU, LVBench, MVBench and Video-MME, demonstrate\nthe state-of-the-art performance and outstanding efficiency of our method. Code\nis available at https://github.com/IVGSZ/Flash-VStream.\n","authors":["Haoji Zhang","Yiqin Wang","Yansong Tang","Yong Liu","Jiashi Feng","Xiaojie Jin"],"pdf_url":"https://arxiv.org/pdf/2506.23825v2.pdf","comment":"Accepted by ICCV 2025"},{"id":"http://arxiv.org/abs/2507.18144v1","updated":"2025-07-24T07:21:23Z","published":"2025-07-24T07:21:23Z","title":"Degradation-Consistent Learning via Bidirectional Diffusion for\n  Low-Light Image Enhancement","summary":"  Low-light image enhancement aims to improve the visibility of degraded images\nto better align with human visual perception. While diffusion-based methods\nhave shown promising performance due to their strong generative capabilities.\nHowever, their unidirectional modelling of degradation often struggles to\ncapture the complexity of real-world degradation patterns, leading to\nstructural inconsistencies and pixel misalignments. To address these\nchallenges, we propose a bidirectional diffusion optimization mechanism that\njointly models the degradation processes of both low-light and normal-light\nimages, enabling more precise degradation parameter matching and enhancing\ngeneration quality. Specifically, we perform bidirectional diffusion-from\nlow-to-normal light and from normal-to-low light during training and introduce\nan adaptive feature interaction block (AFI) to refine feature representation.\nBy leveraging the complementarity between these two paths, our approach imposes\nan implicit symmetry constraint on illumination attenuation and noise\ndistribution, facilitating consistent degradation learning and improving the\nmodels ability to perceive illumination and detail degradation. Additionally,\nwe design a reflection-aware correction module (RACM) to guide color\nrestoration post-denoising and suppress overexposed regions, ensuring content\nconsistency and generating high-quality images that align with human visual\nperception. Extensive experiments on multiple benchmark datasets demonstrate\nthat our method outperforms state-of-the-art methods in both quantitative and\nqualitative evaluations while generalizing effectively to diverse degradation\nscenarios. Code at https://github.com/hejh8/BidDiff\n","authors":["Jinhong He","Minglong Xue","Zhipu Liu","Mingliang Zhou","Aoxiang Ning","Palaiahnakote Shivakumara"],"pdf_url":"https://arxiv.org/pdf/2507.18144v1.pdf","comment":"10page"},{"id":"http://arxiv.org/abs/2406.16109v4","updated":"2025-07-24T07:00:31Z","published":"2024-06-23T13:53:35Z","title":"X-ray2CTPA: Leveraging Diffusion Models to Enhance Pulmonary Embolism\n  Classification","summary":"  Chest X-rays or chest radiography (CXR), commonly used for medical\ndiagnostics, typically enables limited imaging compared to computed tomography\n(CT) scans, which offer more detailed and accurate three-dimensional data,\nparticularly contrast-enhanced scans like CT Pulmonary Angiography (CTPA).\nHowever, CT scans entail higher costs, greater radiation exposure, and are less\naccessible than CXRs. In this work we explore cross-modal translation from a 2D\nlow contrast-resolution X-ray input to a 3D high contrast and\nspatial-resolution CTPA scan. Driven by recent advances in generative AI, we\nintroduce a novel diffusion-based approach to this task. We evaluate the models\nperformance using both quantitative metrics and qualitative feedback from\nradiologists, ensuring diagnostic relevance of the generated images.\nFurthermore, we employ the synthesized 3D images in a classification framework\nand show improved AUC in a PE categorization task, using the initial CXR input.\nThe proposed method is generalizable and capable of performing additional\ncross-modality translations in medical imaging. It may pave the way for more\naccessible and cost-effective advanced diagnostic tools. The code for this\nproject is available: https://github.com/NoaCahan/X-ray2CTPA .\n","authors":["Noa Cahan","Eyal Klang","Galit Aviram","Yiftach Barash","Eli Konen","Raja Giryes","Hayit Greenspan"],"pdf_url":"https://arxiv.org/pdf/2406.16109v4.pdf","comment":"preprint, project code: https://github.com/NoaCahan/X-ray2CTPA"},{"id":"http://arxiv.org/abs/2503.23461v4","updated":"2025-07-24T06:56:41Z","published":"2025-03-30T14:36:55Z","title":"TextCrafter: Accurately Rendering Multiple Texts in Complex Visual\n  Scenes","summary":"  This paper explores the task of Complex Visual Text Generation (CVTG), which\ncenters on generating intricate textual content distributed across diverse\nregions within visual images. In CVTG, image generation models often rendering\ndistorted and blurred visual text or missing some visual text. To tackle these\nchallenges, we propose TextCrafter, a novel multi-visual text rendering method.\nTextCrafter employs a progressive strategy to decompose complex visual text\ninto distinct components while ensuring robust alignment between textual\ncontent and its visual carrier. Additionally, it incorporates a token focus\nenhancement mechanism to amplify the prominence of visual text during the\ngeneration process. TextCrafter effectively addresses key challenges in CVTG\ntasks, such as text confusion, omissions, and blurriness. Moreover, we present\na new benchmark dataset, CVTG-2K, tailored to rigorously evaluate the\nperformance of generative models on CVTG tasks. Extensive experiments\ndemonstrate that our method surpasses state-of-the-art approaches.\n","authors":["Nikai Du","Zhennan Chen","Zhizhou Chen","Shan Gao","Xi Chen","Zhengkai Jiang","Jian Yang","Ying Tai"],"pdf_url":"https://arxiv.org/pdf/2503.23461v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09305v3","updated":"2025-07-24T06:51:41Z","published":"2025-07-12T14:46:42Z","title":"DAA*: Deep Angular A Star for Image-based Path Planning","summary":"  Path smoothness is often overlooked in path imitation learning from expert\ndemonstrations. In this paper, we introduce a novel learning method, termed\ndeep angular A* (DAA*), by incorporating the proposed path angular freedom\n(PAF) into A* to improve path similarity through adaptive path smoothness. The\nPAF aims to explore the effect of move angles on path node expansion by finding\nthe trade-off between their minimum and maximum values, allowing for high\nadaptiveness for imitation learning. DAA* improves path optimality by closely\naligning with the reference path through joint optimization of path shortening\nand smoothing, which correspond to heuristic distance and PAF, respectively.\nThroughout comprehensive evaluations on 7 datasets, including 4 maze datasets,\n2 video-game datasets, and a real-world drone-view dataset containing 2\nscenarios, we demonstrate remarkable improvements of our DAA* over neural A* in\npath similarity between the predicted and reference paths with a shorter path\nlength when the shortest path is plausible, improving by 9.0% SPR, 6.9% ASIM,\nand 3.9% PSIM. Furthermore, when jointly learning pathfinding with both path\nloss and path probability map loss, DAA* significantly outperforms the\nstate-of-the-art TransPath by 6.3% SPR, 6.0% PSIM, and 3.7% ASIM. We also\ndiscuss the minor trade-off between path optimality and search efficiency where\napplicable. Our code and model weights are available at\nhttps://github.com/zwxu064/DAAStar.git.\n","authors":["Zhiwei Xu"],"pdf_url":"https://arxiv.org/pdf/2507.09305v3.pdf","comment":"International Conference on Computer Vision (ICCV), 2025"},{"id":"http://arxiv.org/abs/2507.18135v1","updated":"2025-07-24T06:51:10Z","published":"2025-07-24T06:51:10Z","title":"Information Entropy-Based Framework for Quantifying Tortuosity in\n  Meibomian Gland Uneven Atrophy","summary":"  In the medical image analysis field, precise quantification of curve\ntortuosity plays a critical role in the auxiliary diagnosis and pathological\nassessment of various diseases. In this study, we propose a novel framework for\ntortuosity quantification and demonstrate its effectiveness through the\nevaluation of meibomian gland atrophy uniformity,serving as a representative\napplication scenario.\n  We introduce an information entropy-based tortuosity quantification framework\nthat integrates probability modeling with entropy theory and incorporates\ndomain transformation of curve data. Unlike traditional methods such as\ncurvature or arc-chord ratio, this approach evaluates the tortuosity of a\ntarget curve by comparing it to a designated reference curve. Consequently, it\nis more suitable for tortuosity assessment tasks in medical data where\nbiologically plausible reference curves are available, providing a more robust\nand objective evaluation metric without relying on idealized straight-line\ncomparisons.\n  First, we conducted numerical simulation experiments to preliminarily assess\nthe stability and validity of the method. Subsequently, the framework was\napplied to quantify the spatial uniformity of meibomian gland atrophy and to\nanalyze the difference in this uniformity between \\textit{Demodex}-negative and\n\\textit{Demodex}-positive patient groups. The results demonstrated a\nsignificant difference in tortuosity-based uniformity between the two groups,\nwith an area under the curve of 0.8768, sensitivity of 0.75, and specificity of\n0.93. These findings highlight the clinical utility of the proposed framework\nin curve tortuosity analysis and its potential as a generalizable tool for\nquantitative morphological evaluation in medical diagnostics.\n","authors":["Kesheng Wang","Xiaoyu Chen","Chunlei He","Fenfen Li","Xinxin Yu","Dexing Kong","Shoujun Huang","Qi Dai"],"pdf_url":"https://arxiv.org/pdf/2507.18135v1.pdf","comment":"This manuscript contains 7 figures. All comments are welcome"},{"id":"http://arxiv.org/abs/2507.18133v1","updated":"2025-07-24T06:47:23Z","published":"2025-07-24T06:47:23Z","title":"Deep Learning for Glioblastoma Morpho-pathological Features\n  Identification: A BraTS-Pathology Challenge Solution","summary":"  Glioblastoma, a highly aggressive brain tumor with diverse molecular and\npathological features, poses a diagnostic challenge due to its heterogeneity.\nAccurate diagnosis and assessment of this heterogeneity are essential for\nchoosing the right treatment and improving patient outcomes. Traditional\nmethods rely on identifying specific features in tissue samples, but deep\nlearning offers a promising approach for improved glioblastoma diagnosis. In\nthis paper, we present our approach to the BraTS-Path Challenge 2024. We\nleverage a pre-trained model and fine-tune it on the BraTS-Path training\ndataset. Our model demonstrates poor performance on the challenging BraTS-Path\nvalidation set, as rigorously assessed by the Synapse online platform. The\nmodel achieves an accuracy of 0.392229, a recall of 0.392229, and a F1-score of\n0.392229, indicating a consistent ability to correctly identify instances under\nthe target condition. Notably, our model exhibits perfect specificity of\n0.898704, showing an exceptional capacity to correctly classify negative cases.\nMoreover, a Matthews Correlation Coefficient (MCC) of 0.255267 is calculated,\nto signify a limited positive correlation between predicted and actual values\nand highlight our model's overall predictive power. Our solution also achieves\nthe second place during the testing phase.\n","authors":["Juexin Zhang","Ying Weng","Ke Chen"],"pdf_url":"https://arxiv.org/pdf/2507.18133v1.pdf","comment":"Accepted by the International Brain Tumor Segmentation (BraTS)\n  challenge organized at MICCAI 2024 conference"},{"id":"http://arxiv.org/abs/2507.18126v1","updated":"2025-07-24T06:26:46Z","published":"2025-07-24T06:26:46Z","title":"U-Net Based Healthy 3D Brain Tissue Inpainting","summary":"  This paper introduces a novel approach to synthesize healthy 3D brain tissue\nfrom masked input images, specifically focusing on the task of 'ASNR-MICCAI\nBraTS Local Synthesis of Tissue via Inpainting'. Our proposed method employs a\nU-Net-based architecture, which is designed to effectively reconstruct the\nmissing or corrupted regions of brain MRI scans. To enhance our model's\ngeneralization capabilities and robustness, we implement a comprehensive data\naugmentation strategy that involves randomly masking healthy images during\ntraining. Our model is trained on the BraTS-Local-Inpainting dataset and\ndemonstrates the exceptional performance in recovering healthy brain tissue.\nThe evaluation metrics employed, including Structural Similarity Index (SSIM),\nPeak Signal-to-Noise Ratio (PSNR), and Mean Squared Error (MSE), consistently\nyields impressive results. On the BraTS-Local-Inpainting validation set, our\nmodel achieved an SSIM score of 0.841, a PSNR score of 23.257, and an MSE score\nof 0.007. Notably, these evaluation metrics exhibit relatively low standard\ndeviations, i.e., 0.103 for SSIM score, 4.213 for PSNR score and 0.007 for MSE\nscore, which indicates that our model's reliability and consistency across\nvarious input scenarios. Our method also secured first place in the challenge.\n","authors":["Juexin Zhang","Ying Weng","Ke Chen"],"pdf_url":"https://arxiv.org/pdf/2507.18126v1.pdf","comment":"Accepted by the International Brain Tumor Segmentation (BraTS)\n  challenge organized at MICCAI 2024 conference. Included 7 pages, 2 figures"},{"id":"http://arxiv.org/abs/2407.03605v3","updated":"2025-07-24T06:25:42Z","published":"2024-07-04T03:33:19Z","title":"Orthogonal Constrained Minimization with Tensor $\\ell_{2,p}$\n  Regularization for HSI Denoising and Destriping","summary":"  Hyperspectral images~(HSIs) are often contaminated by a mixture of noise such\nas Gaussian noise, dead lines, stripes, and so on. In this paper, we propose a\nmulti-scale low-rank tensor regularized $\\ell_{2,p}$ (MLTL2p) approach for HSI\ndenoising and destriping, which consists of an orthogonal constrained\nminimization model and an iterative algorithm with convergence guarantees. The\nmodel of the proposed MLTL2p approach is built based on a new sparsity-enhanced\nMulti-scale Low-rank Tensor regularization and a tensor $\\ell_{2,p}$ norm with\n\\(p\\in (0,1)\\). The multi-scale low-rank regularization for HSI denoising\nutilizes the global and local spectral correlation as well as the spatial\nnonlocal self-similarity priors of HSIs. The corresponding low-rank constraints\nare formulated based on independent higher-order singular value decomposition\nwith sparsity enhancement on its core tensor to prompt more low-rankness. The\ntensor $\\ell_{2,p}$ norm for HSI destriping is extended from the matrix\n$\\ell_{2,p}$ norm. A proximal block coordinate descent algorithm is proposed in\nthe MLTL2p approach to solve the resulting nonconvex nonsmooth minimization\nwith orthogonal constraints. We show any accumulation point of the sequence\ngenerated by the proposed algorithm converges to a first-order stationary\npoint, which is defined using three equalities of substationarity, symmetry,\nand feasibility for orthogonal constraints. In the numerical experiments, we\ncompare the proposed method with state-of-the-art methods including a deep\nlearning based method, and test the methods on both simulated and real HSI\ndatasets. Our proposed MLTL2p method demonstrates outperformance in terms of\nmetrics such as mean peak signal-to-noise ratio as well as visual quality.\n","authors":["Xiaoxia Liu","Shijie Yu","Jian Lu","Xiaojun Chen"],"pdf_url":"https://arxiv.org/pdf/2407.03605v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14672v5","updated":"2025-07-24T06:22:40Z","published":"2024-08-26T22:39:08Z","title":"Optimizing against Infeasible Inclusions from Data for Semantic\n  Segmentation through Morphology","summary":"  State-of-the-art semantic segmentation models are typically optimized in a\ndata-driven fashion, minimizing solely per-pixel or per-segment classification\nobjectives on their training data. This purely data-driven paradigm often leads\nto absurd segmentations, especially when the domain of input images is shifted\nfrom the one encountered during training. For instance, state-of-the-art models\nmay assign the label \"road\" to a segment that is included by another segment\nthat is respectively labeled as \"sky\". However, the ground truth of the\nexisting dataset at hand dictates that such inclusion is not feasible. Our\nmethod, Infeasible Semantic Inclusions (InSeIn), first extracts explicit\ninclusion constraints that govern spatial class relations from the semantic\nsegmentation training set at hand in an offline, data-driven fashion, and then\nenforces a morphological yet differentiable loss that penalizes violations of\nthese constraints during training to promote prediction feasibility. InSeIn is\na light-weight plug-and-play method, constitutes a novel step towards\nminimizing infeasible semantic inclusions in the predictions of learned\nsegmentation models, and yields consistent and significant performance\nimprovements over diverse state-of-the-art networks across the ADE20K,\nCityscapes, and ACDC datasets. https://github.com/SHAMIK-97/InSeIn/tree/main\n","authors":["Shamik Basu","Luc Van Gool","Christos Sakaridis"],"pdf_url":"https://arxiv.org/pdf/2408.14672v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11936v3","updated":"2025-07-24T06:15:29Z","published":"2025-07-16T06:03:08Z","title":"A Survey of Deep Learning for Geometry Problem Solving","summary":"  Geometry problem solving is a key area of mathematical reasoning, which is\nwidely involved in many important fields such as education, mathematical\nability assessment of artificial intelligence, and multimodal ability\nassessment. In recent years, the rapid development of deep learning technology,\nespecially the rise of multimodal large language models, has triggered a\nwidespread research boom. This paper provides a survey of the applications of\ndeep learning in geometry problem solving, including (i) a comprehensive\nsummary of the relevant tasks in geometry problem solving; (ii) a thorough\nreview of related deep learning methods; (iii) a detailed analysis of\nevaluation metrics and methods; and (iv) a critical discussion of the current\nchallenges and future directions that can be explored. Our goal is to provide a\ncomprehensive and practical reference of deep learning for geometry problem\nsolving to promote further developments in this field. We create a continuously\nupdated list of papers on GitHub: https://github.com/majianz/dl4gps.\n","authors":["Jianzhe Ma","Wenxuan Wang","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2507.11936v3.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2507.18112v1","updated":"2025-07-24T05:51:51Z","published":"2025-07-24T05:51:51Z","title":"Parameter-Efficient Fine-Tuning of 3D DDPM for MRI Image Generation\n  Using Tensor Networks","summary":"  We address the challenge of parameter-efficient fine-tuning (PEFT) for\nthree-dimensional (3D) U-Net-based denoising diffusion probabilistic models\n(DDPMs) in magnetic resonance imaging (MRI) image generation. Despite its\npractical significance, research on parameter-efficient representations of 3D\nconvolution operations remains limited. To bridge this gap, we propose Tensor\nVolumetric Operator (TenVOO), a novel PEFT method specifically designed for\nfine-tuning DDPMs with 3D convolutional backbones. Leveraging tensor network\nmodeling, TenVOO represents 3D convolution kernels with lower-dimensional\ntensors, effectively capturing complex spatial dependencies during fine-tuning\nwith few parameters. We evaluate TenVOO on three downstream brain MRI\ndatasets-ADNI, PPMI, and BraTS2021-by fine-tuning a DDPM pretrained on 59,830\nT1-weighted brain MRI scans from the UK Biobank. Our results demonstrate that\nTenVOO achieves state-of-the-art performance in multi-scale structural\nsimilarity index measure (MS-SSIM), outperforming existing approaches in\ncapturing spatial dependencies while requiring only 0.3% of the trainable\nparameters of the original model. Our code is available at:\nhttps://github.com/xiaovhua/tenvoo\n","authors":["Binghua Li","Ziqing Chang","Tong Liang","Chao Li","Toshihisa Tanaka","Shigeki Aoki","Qibin Zhao","Zhe Sun"],"pdf_url":"https://arxiv.org/pdf/2507.18112v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18107v1","updated":"2025-07-24T05:37:08Z","published":"2025-07-24T05:37:08Z","title":"T2VWorldBench: A Benchmark for Evaluating World Knowledge in\n  Text-to-Video Generation","summary":"  Text-to-video (T2V) models have shown remarkable performance in generating\nvisually reasonable scenes, while their capability to leverage world knowledge\nfor ensuring semantic consistency and factual accuracy remains largely\nunderstudied. In response to this challenge, we propose T2VWorldBench, the\nfirst systematic evaluation framework for evaluating the world knowledge\ngeneration abilities of text-to-video models, covering 6 major categories, 60\nsubcategories, and 1,200 prompts across a wide range of domains, including\nphysics, nature, activity, culture, causality, and object. To address both\nhuman preference and scalable evaluation, our benchmark incorporates both human\nevaluation and automated evaluation using vision-language models (VLMs). We\nevaluated the 10 most advanced text-to-video models currently available,\nranging from open source to commercial models, and found that most models are\nunable to understand world knowledge and generate truly correct videos. These\nfindings point out a critical gap in the capability of current text-to-video\nmodels to leverage world knowledge, providing valuable research opportunities\nand entry points for constructing models with robust capabilities for\ncommonsense reasoning and factual generation.\n","authors":["Yubin Chen","Xuyang Guo","Zhenmei Shi","Zhao Song","Jiahao Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.18107v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18106v1","updated":"2025-07-24T05:35:49Z","published":"2025-07-24T05:35:49Z","title":"Distributional Uncertainty for Out-of-Distribution Detection","summary":"  Estimating uncertainty from deep neural networks is a widely used approach\nfor detecting out-of-distribution (OoD) samples, which typically exhibit high\npredictive uncertainty. However, conventional methods such as Monte Carlo (MC)\nDropout often focus solely on either model or data uncertainty, failing to\nalign with the semantic objective of OoD detection. To address this, we propose\nthe Free-Energy Posterior Network, a novel framework that jointly models\ndistributional uncertainty and identifying OoD and misclassified regions using\nfree energy. Our method introduces two key contributions: (1) a\nfree-energy-based density estimator parameterized by a Beta distribution, which\nenables fine-grained uncertainty estimation near ambiguous or unseen regions;\nand (2) a loss integrated within a posterior network, allowing direct\nuncertainty estimation from learned parameters without requiring stochastic\nsampling. By integrating our approach with the residual prediction branch (RPL)\nframework, the proposed method goes beyond post-hoc energy thresholding and\nenables the network to learn OoD regions by leveraging the variance of the Beta\ndistribution, resulting in a semantically meaningful and computationally\nefficient solution for uncertainty-aware segmentation. We validate the\neffectiveness of our method on challenging real-world benchmarks, including\nFishyscapes, RoadAnomaly, and Segment-Me-If-You-Can.\n","authors":["JinYoung Kim","DaeUng Jo","Kimin Yun","Jeonghyo Song","Youngjoon Yoo"],"pdf_url":"https://arxiv.org/pdf/2507.18106v1.pdf","comment":"6 pages , 3 figures , IEEE International Conference on Advanced\n  Visual and Signal-Based Systems"},{"id":"http://arxiv.org/abs/2505.20147v3","updated":"2025-07-24T05:31:36Z","published":"2025-05-26T15:46:53Z","title":"FUDOKI: Discrete Flow-based Unified Understanding and Generation via\n  Kinetic-Optimal Velocities","summary":"  The rapid progress of large language models (LLMs) has catalyzed the\nemergence of multimodal large language models (MLLMs) that unify visual\nunderstanding and image generation within a single framework. However, most\nexisting MLLMs rely on autoregressive (AR) architectures, which impose inherent\nlimitations on future development, such as the raster-scan order in image\ngeneration and restricted reasoning abilities in causal context modeling. In\nthis work, we challenge the dominance of AR-based approaches by introducing\nFUDOKI, a unified multimodal model purely based on discrete flow matching, as\nan alternative to conventional AR paradigms. By leveraging metric-induced\nprobability paths with kinetic optimal velocities, our framework goes beyond\nthe previous masking-based corruption process, enabling iterative refinement\nwith self-correction capability and richer bidirectional context integration\nduring generation. To mitigate the high cost of training from scratch, we\ninitialize FUDOKI from pre-trained AR-based MLLMs and adaptively transition to\nthe discrete flow matching paradigm. Experimental results show that FUDOKI\nachieves performance comparable to state-of-the-art AR-based MLLMs across both\nvisual understanding and image generation tasks, highlighting its potential as\na foundation for next-generation unified multimodal models. Furthermore, we\nshow that applying test-time scaling techniques to FUDOKI yields significant\nperformance gains, further underscoring its promise for future enhancement\nthrough reinforcement learning.\n","authors":["Jin Wang","Yao Lai","Aoxue Li","Shifeng Zhang","Jiacheng Sun","Ning Kang","Chengyue Wu","Zhenguo Li","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2505.20147v3.pdf","comment":"37 pages, 12 figures"},{"id":"http://arxiv.org/abs/2507.18104v1","updated":"2025-07-24T05:29:37Z","published":"2025-07-24T05:29:37Z","title":"A Multimodal Seq2Seq Transformer for Predicting Brain Responses to\n  Naturalistic Stimuli","summary":"  The Algonauts 2025 Challenge called on the community to develop encoding\nmodels that predict whole-brain fMRI responses to naturalistic multimodal\nmovies. In this submission, we propose a sequence-to-sequence Transformer that\nautoregressively predicts fMRI activity from visual, auditory, and language\ninputs. Stimulus features were extracted using pretrained models including\nVideoMAE, HuBERT, Qwen, and BridgeTower. The decoder integrates information\nfrom prior brain states, current stimuli, and episode-level summaries via dual\ncross-attention mechanisms that attend to both perceptual information extracted\nfrom the stimulus as well as narrative information provided by high-level\nsummaries of narrative content. One core innovation of our approach is the use\nof sequences of multimodal context to predict sequences of brain activity,\nenabling the model to capture long-range temporal structure in both stimuli and\nneural responses. Another is the combination of a shared encoder with partial\nsubject-specific decoder, which leverages common structure across subjects\nwhile accounting for individual variability. Our model achieves strong\nperformance on both in-distribution and out-of-distribution data, demonstrating\nthe effectiveness of temporally-aware, multimodal sequence modeling for brain\nactivity prediction. The code is available at\nhttps://github.com/Angelneer926/Algonauts_challenge.\n","authors":["Qianyi He","Yuan Chang Leong"],"pdf_url":"https://arxiv.org/pdf/2507.18104v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18100v1","updated":"2025-07-24T05:24:01Z","published":"2025-07-24T05:24:01Z","title":"Datasets and Recipes for Video Temporal Grounding via Reinforcement\n  Learning","summary":"  Video Temporal Grounding (VTG) aims to localize relevant temporal segments in\nvideos given natural language queries. Despite recent progress with large\nvision-language models (LVLMs) and instruction-tuning, existing approaches\noften suffer from limited temporal awareness and poor generalization. In this\nwork, we introduce a two-stage training framework that integrates supervised\nfine-tuning with reinforcement learning (RL) to improve both the accuracy and\nrobustness of VTG models. Our approach first leverages high-quality curated\ncold start data for SFT initialization, followed by difficulty-controlled RL to\nfurther enhance temporal localization and reasoning abilities. Comprehensive\nexperiments on multiple VTG benchmarks demonstrate that our method consistently\noutperforms existing models, particularly in challenging and open-domain\nscenarios. We conduct an in-depth analysis of training strategies and dataset\ncuration, highlighting the importance of both high-quality cold start data and\ndifficulty-controlled RL. To facilitate further research and industrial\nadoption, we release all intermediate datasets, models, and code to the\ncommunity.\n","authors":["Ruizhe Chen","Zhiting Fan","Tianze Luo","Heqing Zou","Zhaopeng Feng","Guiyang Xie","Hansheng Zhang","Zhuochen Wang","Zuozhu Liu","Huaijian Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.18100v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18099v1","updated":"2025-07-24T05:23:02Z","published":"2025-07-24T05:23:02Z","title":"Comparison of Segmentation Methods in Remote Sensing for Land Use Land\n  Cover","summary":"  Land Use Land Cover (LULC) mapping is essential for urban and resource\nplanning, and is one of the key elements in developing smart and sustainable\ncities.This study evaluates advanced LULC mapping techniques, focusing on\nLook-Up Table (LUT)-based Atmospheric Correction applied to Cartosat\nMultispectral (MX) sensor images, followed by supervised and semi-supervised\nlearning models for LULC prediction. We explore DeeplabV3+ and Cross-Pseudo\nSupervision (CPS). The CPS model is further refined with dynamic weighting,\nenhancing pseudo-label reliability during training. This comprehensive approach\nanalyses the accuracy and utility of LULC mapping techniques for various urban\nplanning applications. A case study of Hyderabad, India, illustrates\nsignificant land use changes due to rapid urbanization. By analyzing Cartosat\nMX images over time, we highlight shifts such as urban sprawl, shrinking green\nspaces, and expanding industrial areas. This demonstrates the practical utility\nof these techniques for urban planners and policymakers.\n","authors":["Naman Srivastava","Joel D Joy","Yash Dixit","Swarup E","Rakshit Ramesh"],"pdf_url":"https://arxiv.org/pdf/2507.18099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17724v2","updated":"2025-07-24T05:22:14Z","published":"2025-03-22T10:41:46Z","title":"Trigger without Trace: Towards Stealthy Backdoor Attack on Text-to-Image\n  Diffusion Models","summary":"  Backdoor attacks targeting text-to-image diffusion models have advanced\nrapidly. However, current backdoor samples often exhibit two key abnormalities\ncompared to benign samples: 1) Semantic Consistency, where backdoor prompts\ntend to generate images with similar semantic content even with significant\ntextual variations to the prompts; 2) Attention Consistency, where the trigger\ninduces consistent structural responses in the cross-attention maps. These\nconsistencies leave detectable traces for defenders, making backdoors easier to\nidentify. In this paper, toward stealthy backdoor samples, we propose Trigger\nwithout Trace (TwT) by explicitly mitigating these consistencies. Specifically,\nour approach leverages syntactic structures as backdoor triggers to amplify the\nsensitivity to textual variations, effectively breaking down the semantic\nconsistency. Besides, a regularization method based on Kernel Maximum Mean\nDiscrepancy (KMMD) is proposed to align the distribution of cross-attention\nresponses between backdoor and benign samples, thereby disrupting attention\nconsistency. Extensive experiments demonstrate that our method achieves a 97.5%\nattack success rate while exhibiting stronger resistance to defenses. It\nachieves an average of over 98% backdoor samples bypassing three\nstate-of-the-art detection mechanisms, revealing the vulnerabilities of current\nbackdoor defense methods. The code is available at\nhttps://github.com/Robin-WZQ/TwT.\n","authors":["Jie Zhang","Zhongqi Wang","Shiguang Shan","Xilin Chen"],"pdf_url":"https://arxiv.org/pdf/2503.17724v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04757v3","updated":"2025-07-24T04:42:59Z","published":"2025-02-07T08:43:15Z","title":"ELITE: Enhanced Language-Image Toxicity Evaluation for Safety","summary":"  Current Vision Language Models (VLMs) remain vulnerable to malicious prompts\nthat induce harmful outputs. Existing safety benchmarks for VLMs primarily rely\non automated evaluation methods, but these methods struggle to detect implicit\nharmful content or produce inaccurate evaluations. Therefore, we found that\nexisting benchmarks have low levels of harmfulness, ambiguous data, and limited\ndiversity in image-text pair combinations. To address these issues, we propose\nthe ELITE benchmark, a high-quality safety evaluation benchmark for VLMs,\nunderpinned by our enhanced evaluation method, the ELITE evaluator. The ELITE\nevaluator explicitly incorporates a toxicity score to accurately assess\nharmfulness in multimodal contexts, where VLMs often provide specific,\nconvincing, but unharmful descriptions of images. We filter out ambiguous and\nlow-quality image-text pairs from existing benchmarks using the ELITE evaluator\nand generate diverse combinations of safe and unsafe image-text pairs. Our\nexperiments demonstrate that the ELITE evaluator achieves superior alignment\nwith human evaluations compared to prior automated methods, and the ELITE\nbenchmark offers enhanced benchmark quality and diversity. By introducing\nELITE, we pave the way for safer, more robust VLMs, contributing essential\ntools for evaluating and mitigating safety risks in real-world applications.\n","authors":["Wonjun Lee","Doehyeon Lee","Eugene Choi","Sangyoon Yu","Ashkan Yousefpour","Haon Park","Bumsub Ham","Suhyun Kim"],"pdf_url":"https://arxiv.org/pdf/2502.04757v3.pdf","comment":"ICML 2025. Project page at https://velpegor.github.io/ELITE/"},{"id":"http://arxiv.org/abs/2507.00698v2","updated":"2025-07-24T04:37:55Z","published":"2025-07-01T11:49:05Z","title":"Rectifying Magnitude Neglect in Linear Attention","summary":"  As the core operator of Transformers, Softmax Attention exhibits excellent\nglobal modeling capabilities. However, its quadratic complexity limits its\napplicability to vision tasks. In contrast, Linear Attention shares a similar\nformulation with Softmax Attention while achieving linear complexity, enabling\nefficient global information modeling. Nevertheless, Linear Attention suffers\nfrom a significant performance degradation compared to standard Softmax\nAttention. In this paper, we analyze the underlying causes of this issue based\non the formulation of Linear Attention. We find that, unlike Softmax Attention,\nLinear Attention entirely disregards the magnitude information of the Query.\nThis prevents the attention score distribution from dynamically adapting as the\nQuery scales. As a result, despite its structural similarity to Softmax\nAttention, Linear Attention exhibits a significantly different attention score\ndistribution. Based on this observation, we propose Magnitude-Aware Linear\nAttention (MALA), which modifies the computation of Linear Attention to fully\nincorporate the Query's magnitude. This adjustment allows MALA to generate an\nattention score distribution that closely resembles Softmax Attention while\nexhibiting a more well-balanced structure. We evaluate the effectiveness of\nMALA on multiple tasks, including image classification, object detection,\ninstance segmentation, semantic segmentation, natural language processing,\nspeech recognition, and image generation. Our MALA achieves strong results on\nall of these tasks. Code will be available at https://github.com/qhfan/MALA\n","authors":["Qihang Fan","Huaibo Huang","Yuang Ai","ran He"],"pdf_url":"https://arxiv.org/pdf/2507.00698v2.pdf","comment":"Accepted by ICCV2025, highlight paper"},{"id":"http://arxiv.org/abs/2507.17268v2","updated":"2025-07-24T04:33:14Z","published":"2025-07-23T07:09:10Z","title":"PolarAnything: Diffusion-based Polarimetric Image Synthesis","summary":"  Polarization images facilitate image enhancement and 3D reconstruction tasks,\nbut the limited accessibility of polarization cameras hinders their broader\napplication. This gap drives the need for synthesizing photorealistic\npolarization images. The existing polarization simulator Mitsuba relies on a\nparametric polarization image formation model and requires extensive 3D assets\ncovering shape and PBR materials, preventing it from generating large-scale\nphotorealistic images. To address this problem, we propose PolarAnything,\ncapable of synthesizing polarization images from a single RGB input with both\nphotorealism and physical accuracy, eliminating the dependency on 3D asset\ncollections. Drawing inspiration from the zero-shot performance of pretrained\ndiffusion models, we introduce a diffusion-based generative framework with an\neffective representation strategy that preserves the fidelity of polarization\nproperties. Experiments show that our model generates high-quality polarization\nimages and supports downstream tasks like shape from polarization.\n","authors":["Kailong Zhang","Youwei Lyu","Heng Guo","Si Li","Zhanyu Ma","Boxin Shi"],"pdf_url":"https://arxiv.org/pdf/2507.17268v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2507.16122v2","updated":"2025-07-24T04:17:09Z","published":"2025-07-22T00:30:44Z","title":"MLRU++: Multiscale Lightweight Residual UNETR++ with Attention for\n  Efficient 3D Medical Image Segmentation","summary":"  Accurate and efficient medical image segmentation is crucial but challenging\ndue to anatomical variability and high computational demands on volumetric\ndata. Recent hybrid CNN-Transformer architectures achieve state-of-the-art\nresults but add significant complexity. In this paper, we propose MLRU++, a\nMultiscale Lightweight Residual UNETR++ architecture designed to balance\nsegmentation accuracy and computational efficiency. It introduces two key\ninnovations: a Lightweight Channel and Bottleneck Attention Module (LCBAM) that\nenhances contextual feature encoding with minimal overhead, and a Multiscale\nBottleneck Block (M2B) in the decoder that captures fine-grained details via\nmulti-resolution feature aggregation. Experiments on four publicly available\nbenchmark datasets (Synapse, BTCV, ACDC, and Decathlon Lung) demonstrate that\nMLRU++ achieves state-of-the-art performance, with average Dice scores of\n87.57% (Synapse), 93.00% (ACDC), and 81.12% (Lung). Compared to existing\nleading models, MLRU++ improves Dice scores by 5.38% and 2.12% on Synapse and\nACDC, respectively, while significantly reducing parameter count and\ncomputational cost. Ablation studies evaluating LCBAM and M2B further confirm\nthe effectiveness of the proposed architectural components. Results suggest\nthat MLRU++ offers a practical and high-performing solution for 3D medical\nimage segmentation tasks. Source code is available at:\nhttps://github.com/1027865/MLRUPP\n","authors":["Nand Kumar Yadav","Rodrigue Rizk","William CW Chen"," KC"],"pdf_url":"https://arxiv.org/pdf/2507.16122v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18082v1","updated":"2025-07-24T04:17:06Z","published":"2025-07-24T04:17:06Z","title":"TextSAM-EUS: Text Prompt Learning for SAM to Accurately Segment\n  Pancreatic Tumor in Endoscopic Ultrasound","summary":"  Pancreatic cancer carries a poor prognosis and relies on endoscopic\nultrasound (EUS) for targeted biopsy and radiotherapy. However, the speckle\nnoise, low contrast, and unintuitive appearance of EUS make segmentation of\npancreatic tumors with fully supervised deep learning (DL) models both\nerror-prone and dependent on large, expert-curated annotation datasets. To\naddress these challenges, we present TextSAM-EUS, a novel, lightweight,\ntext-driven adaptation of the Segment Anything Model (SAM) that requires no\nmanual geometric prompts at inference. Our approach leverages text prompt\nlearning (context optimization) through the BiomedCLIP text encoder in\nconjunction with a LoRA-based adaptation of SAM's architecture to enable\nautomatic pancreatic tumor segmentation in EUS, tuning only 0.86% of the total\nparameters. On the public Endoscopic Ultrasound Database of the Pancreas,\nTextSAM-EUS with automatic prompts attains 82.69% Dice and 85.28% normalized\nsurface distance (NSD), and with manual geometric prompts reaches 83.10% Dice\nand 85.70% NSD, outperforming both existing state-of-the-art (SOTA) supervised\nDL models and foundation models (e.g., SAM and its variants). As the first\nattempt to incorporate prompt learning in SAM-based medical image segmentation,\nTextSAM-EUS offers a practical option for efficient and robust automatic EUS\nsegmentation. Our code will be publicly available upon acceptance.\n","authors":["Pascal Spiegler","Taha Koleilat","Arash Harirpoush","Corey S. Miller","Hassan Rivaz","Marta Kersten-Oertel","Yiming Xiao"],"pdf_url":"https://arxiv.org/pdf/2507.18082v1.pdf","comment":"Accepted to ICCV 2025 Workshop CVAMD"},{"id":"http://arxiv.org/abs/2502.06764v2","updated":"2025-07-24T03:58:47Z","published":"2025-02-10T18:44:25Z","title":"History-Guided Video Diffusion","summary":"  Classifier-free guidance (CFG) is a key technique for improving conditional\ngeneration in diffusion models, enabling more accurate control while enhancing\nsample quality. It is natural to extend this technique to video diffusion,\nwhich generates video conditioned on a variable number of context frames,\ncollectively referred to as history. However, we find two key challenges to\nguiding with variable-length history: architectures that only support\nfixed-size conditioning, and the empirical observation that CFG-style history\ndropout performs poorly. To address this, we propose the Diffusion Forcing\nTransformer (DFoT), a video diffusion architecture and theoretically grounded\ntraining objective that jointly enable conditioning on a flexible number of\nhistory frames. We then introduce History Guidance, a family of guidance\nmethods uniquely enabled by DFoT. We show that its simplest form, vanilla\nhistory guidance, already significantly improves video generation quality and\ntemporal consistency. A more advanced method, history guidance across time and\nfrequency further enhances motion dynamics, enables compositional\ngeneralization to out-of-distribution history, and can stably roll out\nextremely long videos. Project website: https://boyuan.space/history-guidance\n","authors":["Kiwhan Song","Boyuan Chen","Max Simchowitz","Yilun Du","Russ Tedrake","Vincent Sitzmann"],"pdf_url":"https://arxiv.org/pdf/2502.06764v2.pdf","comment":"ICML 2025. Project website: https://boyuan.space/history-guidance"},{"id":"http://arxiv.org/abs/2408.01162v3","updated":"2025-07-24T03:53:21Z","published":"2024-08-02T10:24:35Z","title":"PreMix: Label-Efficient Multiple Instance Learning via Non-Contrastive\n  Pre-training and Feature Mixing","summary":"  Multiple instance learning (MIL) has emerged as a powerful framework for\nweakly supervised whole slide image (WSI) classification, enabling slide-level\npredictions without requiring detailed patch-level annotations. Despite its\nsuccess, a critical limitation of current MIL methods lies in the\nunderutilization of pre-training for the MIL aggregator. Most existing\napproaches initialize the aggregator randomly and train it from scratch, making\nperformance highly sensitive to the quantity of labeled WSIs and ignoring the\nabundance of unlabeled WSIs commonly available in clinical settings. To address\nthis, we propose PreMix, a novel framework that leverages a non-contrastive\npre-training method, Barlow Twins, augmented with the Slide Mixing approach to\ngenerate additional positive pairs and enhance feature learning, particularly\nunder limited labeled WSI conditions. Fine-tuning with Mixup and Manifold Mixup\nfurther enhances robustness by effectively handling the diverse sizes of\ngigapixel WSIs. Experimental results demonstrate that integrating PreMix as a\nplug-in module into HIPT yields an average F1 improvement of 4.7% over the\nbaseline HIPT across various WSI training sizes and datasets. These findings\nunderscore its potential to advance WSI classification with limited labeled\ndata and its applicability to real-world histopathology practices. The code is\navailable at https://github.com/bryanwong17/PreMix\n","authors":["Bryan Wong","Mun Yong Yi"],"pdf_url":"https://arxiv.org/pdf/2408.01162v3.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2411.06106v4","updated":"2025-07-24T03:38:33Z","published":"2024-11-09T08:00:50Z","title":"Towards a Universal 3D Medical Multi-modality Generalization via\n  Learning Personalized Invariant Representation","summary":"  Variations in medical imaging modalities and individual anatomical\ndifferences pose challenges to cross-modality generalization in multi-modal\ntasks. Existing methods often concentrate exclusively on common anatomical\npatterns, thereby neglecting individual differences and consequently limiting\ntheir generalization performance. This paper emphasizes the critical role of\nlearning individual-level invariance, i.e., personalized representation\n$\\mathbb{X}_h$, to enhance multi-modality generalization under both homogeneous\nand heterogeneous settings. It reveals that mappings from individual biological\nprofile to different medical modalities remain static across the population,\nwhich is implied in the personalization process. We propose a two-stage\napproach: pre-training with invariant representation $\\mathbb{X}_h$ for\npersonalization, then fine-tuning for diverse downstream tasks. We provide both\ntheoretical and empirical evidence demonstrating the feasibility and advantages\nof personalization, showing that our approach yields greater generalizability\nand transferability across diverse multi-modal medical tasks compared to\nmethods lacking personalization. Extensive experiments further validate that\nour approach significantly enhances performance in various generalization\nscenarios.\n","authors":["Zhaorui Tan","Xi Yang","Tan Pan","Tianyi Liu","Chen Jiang","Xin Guo","Qiufeng Wang","Anh Nguyen","Yuan Qi","Kaizhu Huang","Yuan Cheng"],"pdf_url":"https://arxiv.org/pdf/2411.06106v4.pdf","comment":"Accepted by ICCV25"},{"id":"http://arxiv.org/abs/2507.18064v1","updated":"2025-07-24T03:35:20Z","published":"2025-07-24T03:35:20Z","title":"Adapting Large VLMs with Iterative and Manual Instructions for\n  Generative Low-light Enhancement","summary":"  Most existing low-light image enhancement (LLIE) methods rely on pre-trained\nmodel priors, low-light inputs, or both, while neglecting the semantic guidance\navailable from normal-light images. This limitation hinders their effectiveness\nin complex lighting conditions. In this paper, we propose VLM-IMI, a novel\nframework that leverages large vision-language models (VLMs) with iterative and\nmanual instructions (IMIs) for LLIE. VLM-IMI incorporates textual descriptions\nof the desired normal-light content as enhancement cues, enabling semantically\ninformed restoration. To effectively integrate cross-modal priors, we introduce\nan instruction prior fusion module, which dynamically aligns and fuses image\nand text features, promoting the generation of detailed and semantically\ncoherent outputs. During inference, we adopt an iterative and manual\ninstruction strategy to refine textual instructions, progressively improving\nvisual quality. This refinement enhances structural fidelity, semantic\nalignment, and the recovery of fine details under extremely low-light\nconditions. Extensive experiments across diverse scenarios demonstrate that\nVLM-IMI outperforms state-of-the-art methods in both quantitative metrics and\nperceptual quality. The source code is available at\nhttps://github.com/sunxiaoran01/VLM-IMI.\n","authors":["Xiaoran Sun","Liyan Wang","Cong Wang","Yeying Jin","Kin-man Lam","Zhixun Su","Yang Yang","Jinshan Pan"],"pdf_url":"https://arxiv.org/pdf/2507.18064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18060v1","updated":"2025-07-24T03:23:19Z","published":"2025-07-24T03:23:19Z","title":"BokehDiff: Neural Lens Blur with One-Step Diffusion","summary":"  We introduce BokehDiff, a novel lens blur rendering method that achieves\nphysically accurate and visually appealing outcomes, with the help of\ngenerative diffusion prior. Previous methods are bounded by the accuracy of\ndepth estimation, generating artifacts in depth discontinuities. Our method\nemploys a physics-inspired self-attention module that aligns with the image\nformation process, incorporating depth-dependent circle of confusion constraint\nand self-occlusion effects. We adapt the diffusion model to the one-step\ninference scheme without introducing additional noise, and achieve results of\nhigh quality and fidelity. To address the lack of scalable paired data, we\npropose to synthesize photorealistic foregrounds with transparency with\ndiffusion models, balancing authenticity and scene diversity.\n","authors":["Chengxuan Zhu","Qingnan Fan","Qi Zhang","Jinwei Chen","Huaqi Zhang","Chao Xu","Boxin Shi"],"pdf_url":"https://arxiv.org/pdf/2507.18060v1.pdf","comment":"Accepted by ICCV 2025"},{"id":"http://arxiv.org/abs/2405.06995v3","updated":"2025-07-24T03:00:38Z","published":"2024-05-11T12:06:31Z","title":"Benchmarking Cross-Domain Audio-Visual Deception Detection","summary":"  Automated deception detection is crucial for assisting humans in accurately\nassessing truthfulness and identifying deceptive behavior. Conventional\ncontact-based techniques, like polygraph devices, rely on physiological signals\nto determine the authenticity of an individual's statements. Nevertheless,\nrecent developments in automated deception detection have demonstrated that\nmultimodal features derived from both audio and video modalities may outperform\nhuman observers on publicly available datasets. Despite these positive\nfindings, the generalizability of existing audio-visual deception detection\napproaches across different scenarios remains largely unexplored. To close this\ngap, we present the first cross-domain audio-visual deception detection\nbenchmark, that enables us to assess how well these methods generalize for use\nin real-world scenarios. We used widely adopted audio and visual features and\ndifferent architectures for benchmarking, comparing single-to-single and\nmulti-to-single domain generalization performance. To further exploit the\nimpacts using data from multiple source domains for training, we investigate\nthree types of domain sampling strategies, including domain-simultaneous,\ndomain-alternating, and domain-by-domain for multi-to-single domain\ngeneralization evaluation. We also propose an algorithm to enhance the\ngeneralization performance by maximizing the gradient inner products between\nmodality encoders, named ``MM-IDGM\". Furthermore, we proposed the\nAttention-Mixer fusion method to improve performance, and we believe that this\nnew cross-domain benchmark will facilitate future research in audio-visual\ndeception detection.\n","authors":["Xiaobao Guo","Zitong Yu","Nithish Muthuchamy Selvaraj","Bingquan Shen","Adams Wai-Kin Kong","Alex C. Kot"],"pdf_url":"https://arxiv.org/pdf/2405.06995v3.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2507.18046v1","updated":"2025-07-24T02:50:26Z","published":"2025-07-24T02:50:26Z","title":"Enhancing Scene Transition Awareness in Video Generation via\n  Post-Training","summary":"  Recent advances in AI-generated video have shown strong performance on\n\\emph{text-to-video} tasks, particularly for short clips depicting a single\nscene. However, current models struggle to generate longer videos with coherent\nscene transitions, primarily because they cannot infer when a transition is\nneeded from the prompt. Most open-source models are trained on datasets\nconsisting of single-scene video clips, which limits their capacity to learn\nand respond to prompts requiring multiple scenes. Developing scene transition\nawareness is essential for multi-scene generation, as it allows models to\nidentify and segment videos into distinct clips by accurately detecting\ntransitions.\n  To address this, we propose the \\textbf{Transition-Aware Video} (TAV)\ndataset, which consists of preprocessed video clips with multiple scene\ntransitions. Our experiment shows that post-training on the \\textbf{TAV}\ndataset improves prompt-based scene transition understanding, narrows the gap\nbetween required and generated scenes, and maintains image quality.\n","authors":["Hanwen Shen","Jiajie Lu","Yupeng Cao","Xiaonan Yang"],"pdf_url":"https://arxiv.org/pdf/2507.18046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14501v3","updated":"2025-07-24T02:45:53Z","published":"2025-03-18T17:59:51Z","title":"Advances in 4D Generation: A Survey","summary":"  Generative artificial intelligence has recently progressed from static image\nand video synthesis to 3D content generation, culminating in the emergence of\n4D generation-the task of synthesizing temporally coherent dynamic 3D assets\nguided by user input. As a burgeoning research frontier, 4D generation enables\nricher interactive and immersive experiences, with applications ranging from\ndigital humans to autonomous driving. Despite rapid progress, the field lacks a\nunified understanding of 4D representations, generative frameworks, basic\nparadigms, and the core technical challenges it faces. This survey provides a\nsystematic and in-depth review of the 4D generation landscape. To\ncomprehensively characterize 4D generation, we first categorize fundamental 4D\nrepresentations and outline associated techniques for 4D generation. We then\npresent an in-depth analysis of representative generative pipelines based on\nconditions and representation methods. Subsequently, we discuss how motion and\ngeometry priors are integrated into 4D outputs to ensure spatio-temporal\nconsistency under various control schemes. From an application perspective,\nthis paper summarizes 4D generation tasks in areas such as dynamic object/scene\ngeneration, digital human synthesis, editable 4D content, and embodied AI.\nFurthermore, we summarize and multi-dimensionally compare four basic paradigms\nfor 4D generation: End-to-End, Generated-Data-Based,\nImplicit-Distillation-Based, and Explicit-Supervision-Based. Concluding our\nanalysis, we highlight five key challenges-consistency, controllability,\ndiversity, efficiency, and fidelity-and contextualize these with current\napproaches.By distilling recent advances and outlining open problems, this work\noffers a comprehensive and forward-looking perspective to guide future research\nin 4D generation.\n","authors":["Qiaowei Miao","Kehan Li","Jinsheng Quan","Zhiyuan Min","Shaojie Ma","Yichao Xu","Yi Yang","Ping Liu","Yawei Luo"],"pdf_url":"https://arxiv.org/pdf/2503.14501v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18043v1","updated":"2025-07-24T02:34:13Z","published":"2025-07-24T02:34:13Z","title":"GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs\n  and VLMs","summary":"  Inference-time steering methods offer a lightweight alternative to\nfine-tuning large language models (LLMs) and vision-language models (VLMs) by\nmodifying internal activations at test time without updating model weights.\nHowever, most existing approaches rely on fixed, global intervention vectors,\noverlook the causal influence of individual input tokens, and fail to leverage\ninformative gradients from the model's logits, particularly in multimodal\nsettings where visual and textual inputs contribute unevenly. To address these\nlimitations, we introduce GrAInS, an inference-time steering approach that\noperates across both language-only and vision-language models and tasks. GrAInS\nuses contrastive, gradient-based attribution via Integrated Gradients to\nidentify the top-k most influential tokens, both positively and negatively\nattributed based on their contribution to preferred versus dispreferred\noutputs. These tokens are then used to construct directional steering vectors\nthat capture semantic shifts from undesirable to desirable behavior. During\ninference, GrAInS adjusts hidden activations at transformer layers guided by\ntoken-level attribution signals, and normalizes activations to preserve\nrepresentational scale. This enables fine-grained, interpretable, and modular\ncontrol over model behavior, without retraining or auxiliary supervision.\nEmpirically, GrAInS consistently outperforms both fine-tuning and existing\nsteering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using\nLlama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514\nwith LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all\nwhile preserving the model's fluency and general capabilities.\n","authors":["Duy Nguyen","Archiki Prasad","Elias Stengel-Eskin","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2507.18043v1.pdf","comment":"21 pages. Code: https://github.com/duykhuongnguyen/GrAInS"},{"id":"http://arxiv.org/abs/2507.01884v2","updated":"2025-07-24T02:26:33Z","published":"2025-07-02T16:53:39Z","title":"Self-Reinforcing Prototype Evolution with Dual-Knowledge Cooperation for\n  Semi-Supervised Lifelong Person Re-Identification","summary":"  Current lifelong person re-identification (LReID) methods predominantly rely\non fully labeled data streams. However, in real-world scenarios where\nannotation resources are limited, a vast amount of unlabeled data coexists with\nscarce labeled samples, leading to the Semi-Supervised LReID (Semi-LReID)\nproblem where LReID methods suffer severe performance degradation. Existing\nLReID methods, even when combined with semi-supervised strategies, suffer from\nlimited long-term adaptation performance due to struggling with the noisy\nknowledge occurring during unlabeled data utilization. In this paper, we\npioneer the investigation of Semi-LReID, introducing a novel Self-Reinforcing\nPrototype Evolution with Dual-Knowledge Cooperation framework (SPRED). Our key\ninnovation lies in establishing a self-reinforcing cycle between dynamic\nprototype-guided pseudo-label generation and new-old knowledge collaborative\npurification to enhance the utilization of unlabeled data. Specifically,\nlearnable identity prototypes are introduced to dynamically capture the\nidentity distributions and generate high-quality pseudo-labels. Then, the\ndual-knowledge cooperation scheme integrates current model specialization and\nhistorical model generalization, refining noisy pseudo-labels. Through this\ncyclic design, reliable pseudo-labels are progressively mined to improve\ncurrent-stage learning and ensure positive knowledge propagation over long-term\nlearning. Experiments on the established Semi-LReID benchmarks show that our\nSPRED achieves state-of-the-art performance. Our source code is available at\nhttps://github.com/zhoujiahuan1991/ICCV2025-SPRED\n","authors":["Kunlun Xu","Fan Zhuo","Jiangmeng Li","Xu Zou","Jiahuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2507.01884v2.pdf","comment":"Accepted by ICCV 2025"},{"id":"http://arxiv.org/abs/2507.17332v2","updated":"2025-07-24T02:16:29Z","published":"2025-07-23T09:00:13Z","title":"PARTE: Part-Guided Texturing for 3D Human Reconstruction from a Single\n  Image","summary":"  The misaligned human texture across different human parts is one of the main\nlimitations of existing 3D human reconstruction methods. Each human part, such\nas a jacket or pants, should maintain a distinct texture without blending into\nothers. The structural coherence of human parts serves as a crucial cue to\ninfer human textures in the invisible regions of a single image. However, most\nexisting 3D human reconstruction methods do not explicitly exploit such part\nsegmentation priors, leading to misaligned textures in their reconstructions.\nIn this regard, we present PARTE, which utilizes 3D human part information as a\nkey guide to reconstruct 3D human textures. Our framework comprises two core\ncomponents. First, to infer 3D human part information from a single image, we\npropose a 3D part segmentation module (PartSegmenter) that initially\nreconstructs a textureless human surface and predicts human part labels based\non the textureless surface. Second, to incorporate part information into\ntexture reconstruction, we introduce a part-guided texturing module\n(PartTexturer), which acquires prior knowledge from a pre-trained image\ngeneration network on texture alignment of human parts. Extensive experiments\ndemonstrate that our framework achieves state-of-the-art quality in 3D human\nreconstruction. The project page is available at\nhttps://hygenie1228.github.io/PARTE/.\n","authors":["Hyeongjin Nam","Donghwan Kim","Gyeongsik Moon","Kyoung Mu Lee"],"pdf_url":"https://arxiv.org/pdf/2507.17332v2.pdf","comment":"Published at ICCV 2025, 22 pages including the supplementary material"},{"id":"http://arxiv.org/abs/2503.01075v2","updated":"2025-07-24T02:11:36Z","published":"2025-03-03T00:33:04Z","title":"Tackling Hallucination from Conditional Models for Medical Image\n  Reconstruction with DynamicDPS","summary":"  Hallucinations are spurious structures not present in the ground truth,\nposing a critical challenge in medical image reconstruction, especially for\ndata-driven conditional models. We hypothesize that combining an unconditional\ndiffusion model with data consistency, trained on a diverse dataset, can reduce\nthese hallucinations. Based on this, we propose DynamicDPS, a diffusion-based\nframework that integrates conditional and unconditional diffusion models to\nenhance low-quality medical images while systematically reducing\nhallucinations. Our approach first generates an initial reconstruction using a\nconditional model, then refines it with an adaptive diffusion-based inverse\nproblem solver. DynamicDPS skips early stage in the reverse process by\nselecting an optimal starting time point per sample and applies Wolfe's line\nsearch for adaptive step sizes, improving both efficiency and image fidelity.\nUsing diffusion priors and data consistency, our method effectively reduces\nhallucinations from any conditional model output. We validate its effectiveness\nin Image Quality Transfer for low-field MRI enhancement. Extensive evaluations\non synthetic and real MR scans, including a downstream task for tissue volume\nestimation, show that DynamicDPS reduces hallucinations, improving relative\nvolume estimation by over 15% for critical tissues while using only 5% of the\nsampling steps required by baseline diffusion models. As a model-agnostic and\nfine-tuning-free approach, DynamicDPS offers a robust solution for\nhallucination reduction in medical imaging. The code will be made publicly\navailable upon publication.\n","authors":["Seunghoi Kim","Henry F. J. Tregidgo","Matteo Figini","Chen Jin","Sarang Joshi","Daniel C. Alexander"],"pdf_url":"https://arxiv.org/pdf/2503.01075v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18036v1","updated":"2025-07-24T02:07:28Z","published":"2025-07-24T02:07:28Z","title":"NWaaS: Nonintrusive Watermarking as a Service for X-to-Image DNN","summary":"  The intellectual property of deep neural network (DNN) models can be\nprotected with DNN watermarking, which embeds copyright watermarks into model\nparameters (white-box), model behavior (black-box), or model outputs\n(box-free), and the watermarks can be subsequently extracted to verify model\nownership or detect model theft. Despite recent advances, these existing\nmethods are inherently intrusive, as they either modify the model parameters or\nalter the structure. This natural intrusiveness raises concerns about\nwatermarking-induced shifts in model behavior and the additional cost of\nfine-tuning, further exacerbated by the rapidly growing model size. As a\nresult, model owners are often reluctant to adopt DNN watermarking in practice,\nwhich limits the development of practical Watermarking as a Service (WaaS)\nsystems. To address this issue, we introduce Nonintrusive Watermarking as a\nService (NWaaS), a novel trustless paradigm designed for X-to-Image models, in\nwhich we hypothesize that with the model untouched, an owner-defined watermark\ncan still be extracted from model outputs. Building on this concept, we propose\nShadowMark, a concrete implementation of NWaaS which addresses critical\ndeployment challenges by establishing a robust and nonintrusive side channel in\nthe protected model's black-box API, leveraging a key encoder and a watermark\ndecoder. It is significantly distinctive from existing solutions by attaining\nthe so-called absolute fidelity and being applicable to different DNN\narchitectures, while being also robust against existing attacks, eliminating\nthe fidelity-robustness trade-off. Extensive experiments on image-to-image,\nnoise-to-image, noise-and-text-to-image, and text-to-image models, demonstrate\nthe efficacy and practicality of ShadowMark for real-world deployment of\nnonintrusive DNN watermarking.\n","authors":["Haonan An","Guang Hua","Yu Guo","Hangcheng Cao","Susanto Rahardja","Yuguang Fang"],"pdf_url":"https://arxiv.org/pdf/2507.18036v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18031v1","updated":"2025-07-24T02:04:58Z","published":"2025-07-24T02:04:58Z","title":"ViGText: Deepfake Image Detection with Vision-Language Model\n  Explanations and Graph Neural Networks","summary":"  The rapid rise of deepfake technology, which produces realistic but\nfraudulent digital content, threatens the authenticity of media. Traditional\ndeepfake detection approaches often struggle with sophisticated, customized\ndeepfakes, especially in terms of generalization and robustness against\nmalicious attacks. This paper introduces ViGText, a novel approach that\nintegrates images with Vision Large Language Model (VLLM) Text explanations\nwithin a Graph-based framework to improve deepfake detection. The novelty of\nViGText lies in its integration of detailed explanations with visual data, as\nit provides a more context-aware analysis than captions, which often lack\nspecificity and fail to reveal subtle inconsistencies. ViGText systematically\ndivides images into patches, constructs image and text graphs, and integrates\nthem for analysis using Graph Neural Networks (GNNs) to identify deepfakes.\nThrough the use of multi-level feature extraction across spatial and frequency\ndomains, ViGText captures details that enhance its robustness and accuracy to\ndetect sophisticated deepfakes. Extensive experiments demonstrate that ViGText\nsignificantly enhances generalization and achieves a notable performance boost\nwhen it detects user-customized deepfakes. Specifically, average F1 scores rise\nfrom 72.45% to 98.32% under generalization evaluation, and reflects the model's\nsuperior ability to generalize to unseen, fine-tuned variations of stable\ndiffusion models. As for robustness, ViGText achieves an increase of 11.1% in\nrecall compared to other deepfake detection approaches. When facing targeted\nattacks that exploit its graph-based architecture, ViGText limits\nclassification performance degradation to less than 4%. ViGText uses detailed\nvisual and textual analysis to set a new standard for detecting deepfakes,\nhelping ensure media authenticity and information integrity.\n","authors":["Ahmad ALBarqawi","Mahmoud Nazzal","Issa Khalil","Abdallah Khreishah","NhatHai Phan"],"pdf_url":"https://arxiv.org/pdf/2507.18031v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18026v1","updated":"2025-07-24T01:58:57Z","published":"2025-07-24T01:58:57Z","title":"Emotion Recognition from Skeleton Data: A Comprehensive Survey","summary":"  Emotion recognition through body movements has emerged as a compelling and\nprivacy-preserving alternative to traditional methods that rely on facial\nexpressions or physiological signals. Recent advancements in 3D skeleton\nacquisition technologies and pose estimation algorithms have significantly\nenhanced the feasibility of emotion recognition based on full-body motion. This\nsurvey provides a comprehensive and systematic review of skeleton-based emotion\nrecognition techniques. First, we introduce psychological models of emotion and\nexamine the relationship between bodily movements and emotional expression.\nNext, we summarize publicly available datasets, highlighting the differences in\ndata acquisition methods and emotion labeling strategies. We then categorize\nexisting methods into posture-based and gait-based approaches, analyzing them\nfrom both data-driven and technical perspectives. In particular, we propose a\nunified taxonomy that encompasses four primary technical paradigms: Traditional\napproaches, Feat2Net, FeatFusionNet, and End2EndNet. Representative works\nwithin each category are reviewed and compared, with benchmarking results\nacross commonly used datasets. Finally, we explore the extended applications of\nemotion recognition in mental health assessment, such as detecting depression\nand autism, and discuss the open challenges and future research directions in\nthis rapidly evolving field.\n","authors":["Haifeng Lu","Jiuyi Chen","Zhen Zhang","Ruida Liu","Runhao Zeng","Xiping Hu"],"pdf_url":"https://arxiv.org/pdf/2507.18026v1.pdf","comment":"34 pages, 5 figures, 13 tables"},{"id":"http://arxiv.org/abs/2409.15087v2","updated":"2025-07-24T01:49:32Z","published":"2024-09-23T15:01:09Z","title":"AI Workflow, External Validation, and Development in Eye Disease\n  Diagnosis","summary":"  Timely disease diagnosis is challenging due to increasing disease burdens and\nlimited clinician availability. AI shows promise in diagnosis accuracy but\nfaces real-world application issues due to insufficient validation in clinical\nworkflows and diverse populations. This study addresses gaps in medical AI\ndownstream accountability through a case study on age-related macular\ndegeneration (AMD) diagnosis and severity classification. We designed and\nimplemented an AI-assisted diagnostic workflow for AMD, comparing diagnostic\nperformance with and without AI assistance among 24 clinicians from 12\ninstitutions with real patient data sampled from the Age-Related Eye Disease\nStudy (AREDS). Additionally, we demonstrated continual enhancement of an\nexisting AI model by incorporating approximately 40,000 additional medical\nimages (named AREDS2 dataset). The improved model was then systematically\nevaluated using both AREDS and AREDS2 test sets, as well as an external test\nset from Singapore. AI assistance markedly enhanced diagnostic accuracy and\nclassification for 23 out of 24 clinicians, with the average F1-score\nincreasing by 20% from 37.71 (Manual) to 45.52 (Manual + AI) (P-value <\n0.0001), achieving an improvement of over 50% in some cases. In terms of\nefficiency, AI assistance reduced diagnostic times for 17 out of the 19\nclinicians tracked, with time savings of up to 40%. Furthermore, a model\nequipped with continual learning showed robust performance across three\nindependent datasets, recording a 29% increase in accuracy, and elevating the\nF1-score from 42 to 54 in the Singapore population.\n","authors":["Qingyu Chen","Tiarnan D L Keenan","Elvira Agron","Alexis Allot","Emily Guan","Bryant Duong","Amr Elsawy","Benjamin Hou","Cancan Xue","Sanjeeb Bhandari","Geoffrey Broadhead","Chantal Cousineau-Krieger","Ellen Davis","William G Gensheimer","David Grasic","Seema Gupta","Luis Haddock","Eleni Konstantinou","Tania Lamba","Michele Maiberger","Dimosthenis Mantopoulos","Mitul C Mehta","Ayman G Nahri","Mutaz AL-Nawaflh","Arnold Oshinsky","Brittany E Powell","Boonkit Purt","Soo Shin","Hillary Stiefel","Alisa T Thavikulwat","Keith James Wroblewski","Tham Yih Chung","Chui Ming Gemmy Cheung","Ching-Yu Cheng","Emily Y Chew","Michelle R. Hribar","Michael F. Chiang","Zhiyong Lu"],"pdf_url":"https://arxiv.org/pdf/2409.15087v2.pdf","comment":"Published in JAMA Network Open,\n  doi:10.1001/jamanetworkopen.2025.17204"},{"id":"http://arxiv.org/abs/2507.18023v1","updated":"2025-07-24T01:48:50Z","published":"2025-07-24T01:48:50Z","title":"High-fidelity 3D Gaussian Inpainting: preserving multi-view consistency\n  and photorealistic details","summary":"  Recent advancements in multi-view 3D reconstruction and novel-view synthesis,\nparticularly through Neural Radiance Fields (NeRF) and 3D Gaussian Splatting\n(3DGS), have greatly enhanced the fidelity and efficiency of 3D content\ncreation. However, inpainting 3D scenes remains a challenging task due to the\ninherent irregularity of 3D structures and the critical need for maintaining\nmulti-view consistency. In this work, we propose a novel 3D Gaussian inpainting\nframework that reconstructs complete 3D scenes by leveraging sparse inpainted\nviews. Our framework incorporates an automatic Mask Refinement Process and\nregion-wise Uncertainty-guided Optimization. Specifically, we refine the\ninpainting mask using a series of operations, including Gaussian scene\nfiltering and back-projection, enabling more accurate localization of occluded\nregions and realistic boundary restoration. Furthermore, our Uncertainty-guided\nFine-grained Optimization strategy, which estimates the importance of each\nregion across multi-view images during training, alleviates multi-view\ninconsistencies and enhances the fidelity of fine details in the inpainted\nresults. Comprehensive experiments conducted on diverse datasets demonstrate\nthat our approach outperforms existing state-of-the-art methods in both visual\nquality and view consistency.\n","authors":["Jun Zhou","Dinghao Li","Nannan Li","Mingjie Wang"],"pdf_url":"https://arxiv.org/pdf/2507.18023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18015v1","updated":"2025-07-24T01:12:28Z","published":"2025-07-24T01:12:28Z","title":"Celeb-DF++: A Large-scale Challenging Video DeepFake Benchmark for\n  Generalizable Forensics","summary":"  The rapid advancement of AI technologies has significantly increased the\ndiversity of DeepFake videos circulating online, posing a pressing challenge\nfor \\textit{generalizable forensics}, \\ie, detecting a wide range of unseen\nDeepFake types using a single model. Addressing this challenge requires\ndatasets that are not only large-scale but also rich in forgery diversity.\nHowever, most existing datasets, despite their scale, include only a limited\nvariety of forgery types, making them insufficient for developing generalizable\ndetection methods. Therefore, we build upon our earlier Celeb-DF dataset and\nintroduce {Celeb-DF++}, a new large-scale and challenging video DeepFake\nbenchmark dedicated to the generalizable forensics challenge. Celeb-DF++ covers\nthree commonly encountered forgery scenarios: Face-swap (FS), Face-reenactment\n(FR), and Talking-face (TF). Each scenario contains a substantial number of\nhigh-quality forged videos, generated using a total of 22 various recent\nDeepFake methods. These methods differ in terms of architectures, generation\npipelines, and targeted facial regions, covering the most prevalent DeepFake\ncases witnessed in the wild. We also introduce evaluation protocols for\nmeasuring the generalizability of 24 recent detection methods, highlighting the\nlimitations of existing detection methods and the difficulty of our new\ndataset.\n","authors":["Yuezun Li","Delong Zhu","Xinjie Cui","Siwei Lyu"],"pdf_url":"https://arxiv.org/pdf/2507.18015v1.pdf","comment":"https://github.com/OUC-VAS/Celeb-DF-PP"},{"id":"http://arxiv.org/abs/2507.18012v1","updated":"2025-07-24T01:00:06Z","published":"2025-07-24T01:00:06Z","title":"Direct Dual-Energy CT Material Decomposition using Model-based Denoising\n  Diffusion Model","summary":"  Dual-energy X-ray Computed Tomography (DECT) constitutes an advanced\ntechnology which enables automatic decomposition of materials in clinical\nimages without manual segmentation using the dependency of the X-ray linear\nattenuation with energy. However, most methods perform material decomposition\nin the image domain as a post-processing step after reconstruction but this\nprocedure does not account for the beam-hardening effect and it results in\nsub-optimal results. In this work, we propose a deep learning procedure called\nDual-Energy Decomposition Model-based Diffusion (DEcomp-MoD) for quantitative\nmaterial decomposition which directly converts the DECT projection data into\nmaterial images. The algorithm is based on incorporating the knowledge of the\nspectral DECT model into the deep learning training loss and combining a\nscore-based denoising diffusion learned prior in the material image domain.\nImportantly the inference optimization loss takes as inputs directly the\nsinogram and converts to material images through a model-based conditional\ndiffusion model which guarantees consistency of the results. We evaluate the\nperformance with both quantitative and qualitative estimation of the proposed\nDEcomp-MoD method on synthetic DECT sinograms from the low-dose AAPM dataset.\nFinally, we show that DEcomp-MoD outperform state-of-the-art unsupervised\nscore-based model and supervised deep learning networks, with the potential to\nbe deployed for clinical diagnosis.\n","authors":["Hang Xu","Alexandre Bousse","Alessandro Perelli"],"pdf_url":"https://arxiv.org/pdf/2507.18012v1.pdf","comment":"13 pages, 10 figures, 2 tables"},{"id":"http://arxiv.org/abs/2507.18009v1","updated":"2025-07-24T00:54:31Z","published":"2025-07-24T00:54:31Z","title":"GRR-CoCa: Leveraging LLM Mechanisms in Multimodal Model Architectures","summary":"  State-of-the-art (SOTA) image and text generation models are multimodal\nmodels that have many similarities to large language models (LLMs). Despite\nachieving strong performances, leading foundational multimodal model\narchitectures frequently lag behind the architectural sophistication of\ncontemporary LLMs. We propose GRR-CoCa, an improved SOTA Contrastive Captioner\n(CoCa) model that incorporates Gaussian error gated linear units, root mean\nsquared normalization, and rotary positional embedding into the textual\ndecoders and the vision transformer (ViT) encoder. Each architectural\nmodification has been shown to improve model performance in LLMs, but has yet\nto be adopted in CoCa. We benchmarked GRR-CoCa against Baseline CoCa, a model\nwith the same modified textual decoders but with CoCa's original ViT encoder.\nWe used standard pretraining and fine-tuning workflows to benchmark the models\non contrastive and generative tasks. Our GRR-CoCa significantly outperformed\nBaseline CoCa on the pretraining dataset and three diverse fine-tuning\ndatasets. Pretraining improvements were 27.25% in contrastive loss, 3.71% in\nperplexity, and 7.15% in CoCa loss. The average fine-tuning improvements were\n13.66% in contrastive loss, 5.18% in perplexity, and 5.55% in CoCa loss. We\nshow that GRR-CoCa's modified architecture improves performance and\ngeneralization across vision-language domains.\n","authors":["Jake R. Patock","Nicole Catherine Lewis","Kevin McCoy","Christina Gomez","Canling Chen","Lorenzo Luzi"],"pdf_url":"https://arxiv.org/pdf/2507.18009v1.pdf","comment":"12 pages, 2 figures"},{"id":"http://arxiv.org/abs/2507.15541v2","updated":"2025-07-24T00:51:53Z","published":"2025-07-21T12:10:42Z","title":"Towards Holistic Surgical Scene Graph","summary":"  Surgical scene understanding is crucial for computer-assisted intervention\nsystems, requiring visual comprehension of surgical scenes that involves\ndiverse elements such as surgical tools, anatomical structures, and their\ninteractions. To effectively represent the complex information in surgical\nscenes, graph-based approaches have been explored to structurally model\nsurgical entities and their relationships. Previous surgical scene graph\nstudies have demonstrated the feasibility of representing surgical scenes using\ngraphs. However, certain aspects of surgical scenes-such as diverse\ncombinations of tool-action-target and the identity of the hand operating the\ntool-remain underexplored in graph-based representations, despite their\nimportance. To incorporate these aspects into graph representations, we propose\nEndoscapes-SG201 dataset, which includes annotations for tool-action-target\ncombinations and hand identity. We also introduce SSG-Com, a graph-based method\ndesigned to learn and represent these critical elements. Through experiments on\ndownstream tasks such as critical view of safety assessment and action triplet\nrecognition, we demonstrated the importance of integrating these essential\nscene graph components, highlighting their significant contribution to surgical\nscene understanding. The code and dataset are available at\nhttps://github.com/ailab-kyunghee/SSG-Com\n","authors":["Jongmin Shin","Enki Cho","Ka Young Kim","Jung Yong Kim","Seong Tae Kim","Namkee Oh"],"pdf_url":"https://arxiv.org/pdf/2507.15541v2.pdf","comment":"Accepted to MICCAI 2025"},{"id":"http://arxiv.org/abs/2507.17998v1","updated":"2025-07-24T00:28:01Z","published":"2025-07-24T00:28:01Z","title":"Registration beyond Points: General Affine Subspace Alignment via\n  Geodesic Distance on Grassmann Manifold","summary":"  Affine Grassmannian has been favored for expressing proximity between lines\nand planes due to its theoretical exactness in measuring distances among\nfeatures. Despite this advantage, the existing method can only measure the\nproximity without yielding the distance as an explicit function of rigid body\ntransformation. Thus, an optimizable distance function on the manifold has\nremained underdeveloped, stifling its application in registration problems.\nThis paper is the first to explicitly derive an optimizable cost function\nbetween two Grassmannian features with respect to rigid body transformation\n($\\mathbf{R}$ and $\\mathbf{t}$). Specifically, we present a rigorous\nmathematical proof demonstrating that the bases of high-dimensional linear\nsubspaces can serve as an explicit representation of the cost. Finally, we\npropose an optimizable cost function based on the transformed bases that can be\napplied to the registration problem of any affine subspace. Compared to vector\nparameter-based approaches, our method is able to find a globally optimal\nsolution by directly minimizing the geodesic distance which is agnostic to\nrepresentation ambiguity. The resulting cost function and its extension to the\ninlier-set maximizing \\ac{BnB} solver have been demonstrated to improve the\nconvergence of existing solutions or outperform them in various computer vision\ntasks. The code is available on\nhttps://github.com/joomeok/GrassmannRegistration.\n","authors":["Jaeho Shin","Hyeonjae Gil","Junwoo Jang","Maani Ghaffari","Ayoung Kim"],"pdf_url":"https://arxiv.org/pdf/2507.17998v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17996v1","updated":"2025-07-24T00:21:12Z","published":"2025-07-24T00:21:12Z","title":"Exploring the interplay of label bias with subgroup size and\n  separability: A case study in mammographic density classification","summary":"  Systematic mislabelling affecting specific subgroups (i.e., label bias) in\nmedical imaging datasets represents an understudied issue concerning the\nfairness of medical AI systems. In this work, we investigated how size and\nseparability of subgroups affected by label bias influence the learned features\nand performance of a deep learning model. Therefore, we trained deep learning\nmodels for binary tissue density classification using the EMory BrEast imaging\nDataset (EMBED), where label bias affected separable subgroups (based on\nimaging manufacturer) or non-separable \"pseudo-subgroups\". We found that\nsimulated subgroup label bias led to prominent shifts in the learned feature\nrepresentations of the models. Importantly, these shifts within the feature\nspace were dependent on both the relative size and the separability of the\nsubgroup affected by label bias. We also observed notable differences in\nsubgroup performance depending on whether a validation set with clean labels\nwas used to define the classification threshold for the model. For instance,\nwith label bias affecting the majority separable subgroup, the true positive\nrate for that subgroup fell from 0.898, when the validation set had clean\nlabels, to 0.518, when the validation set had biased labels. Our work\nrepresents a key contribution toward understanding the consequences of label\nbias on subgroup fairness in medical imaging AI.\n","authors":["Emma A. M. Stanley","Raghav Mehta","Mélanie Roschewitz","Nils D. Forkert","Ben Glocker"],"pdf_url":"https://arxiv.org/pdf/2507.17996v1.pdf","comment":"Accepted at MICCAI Workshop on Fairness of AI in Medical Imaging\n  (FAIMI) 2025"},{"id":"http://arxiv.org/abs/2507.17995v1","updated":"2025-07-24T00:13:25Z","published":"2025-07-24T00:13:25Z","title":"AG-VPReID.VIR: Bridging Aerial and Ground Platforms for Video-based\n  Visible-Infrared Person Re-ID","summary":"  Person re-identification (Re-ID) across visible and infrared modalities is\ncrucial for 24-hour surveillance systems, but existing datasets primarily focus\non ground-level perspectives. While ground-based IR systems offer nighttime\ncapabilities, they suffer from occlusions, limited coverage, and vulnerability\nto obstructions--problems that aerial perspectives uniquely solve. To address\nthese limitations, we introduce AG-VPReID.VIR, the first aerial-ground\ncross-modality video-based person Re-ID dataset. This dataset captures 1,837\nidentities across 4,861 tracklets (124,855 frames) using both UAV-mounted and\nfixed CCTV cameras in RGB and infrared modalities. AG-VPReID.VIR presents\nunique challenges including cross-viewpoint variations, modality discrepancies,\nand temporal dynamics. Additionally, we propose TCC-VPReID, a novel\nthree-stream architecture designed to address the joint challenges of\ncross-platform and cross-modality person Re-ID. Our approach bridges the domain\ngaps between aerial-ground perspectives and RGB-IR modalities, through\nstyle-robust feature learning, memory-based cross-view adaptation, and\nintermediary-guided temporal modeling. Experiments show that AG-VPReID.VIR\npresents distinctive challenges compared to existing datasets, with our\nTCC-VPReID framework achieving significant performance gains across multiple\nevaluation protocols. Dataset and code are available at\nhttps://github.com/agvpreid25/AG-VPReID.VIR.\n","authors":["Huy Nguyen","Kien Nguyen","Akila Pemasiri","Akmal Jahan","Clinton Fookes","Sridha Sridharan"],"pdf_url":"https://arxiv.org/pdf/2507.17995v1.pdf","comment":"Accepted atIEEE International Joint Conference on Biometrics (IJCB)\n  2025"}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2507.18632v1","updated":"2025-07-24T17:59:36Z","published":"2025-07-24T17:59:36Z","title":"SIDA: Synthetic Image Driven Zero-shot Domain Adaptation","summary":"  Zero-shot domain adaptation is a method for adapting a model to a target\ndomain without utilizing target domain image data. To enable adaptation without\ntarget images, existing studies utilize CLIP's embedding space and text\ndescription to simulate target-like style features. Despite the previous\nachievements in zero-shot domain adaptation, we observe that these text-driven\nmethods struggle to capture complex real-world variations and significantly\nincrease adaptation time due to their alignment process. Instead of relying on\ntext descriptions, we explore solutions leveraging image data, which provides\ndiverse and more fine-grained style cues. In this work, we propose SIDA, a\nnovel and efficient zero-shot domain adaptation method leveraging synthetic\nimages. To generate synthetic images, we first create detailed, source-like\nimages and apply image translation to reflect the style of the target domain.\nWe then utilize the style features of these synthetic images as a proxy for the\ntarget domain. Based on these features, we introduce Domain Mix and Patch Style\nTransfer modules, which enable effective modeling of real-world variations. In\nparticular, Domain Mix blends multiple styles to expand the intra-domain\nrepresentations, and Patch Style Transfer assigns different styles to\nindividual patches. We demonstrate the effectiveness of our method by showing\nstate-of-the-art performance in diverse zero-shot adaptation scenarios,\nparticularly in challenging domains. Moreover, our approach achieves high\nefficiency by significantly reducing the overall adaptation time.\n","authors":["Ye-Chan Kim","SeungJu Cha","Si-Woo Kim","Taewhan Kim","Dong-Jin Kim"],"pdf_url":"https://arxiv.org/pdf/2507.18632v1.pdf","comment":"Accepted to ACM MM 2025"},{"id":"http://arxiv.org/abs/2507.18625v1","updated":"2025-07-24T17:58:03Z","published":"2025-07-24T17:58:03Z","title":"3D Software Synthesis Guided by Constraint-Expressive Intermediate\n  Representation","summary":"  Graphical user interface (UI) software has undergone a fundamental\ntransformation from traditional two-dimensional (2D) desktop/web/mobile\ninterfaces to spatial three-dimensional (3D) environments. While existing work\nhas made remarkable success in automated 2D software generation, such as\nHTML/CSS and mobile app interface code synthesis, the generation of 3D software\nstill remains under-explored. Current methods for 3D software generation\nusually generate the 3D environments as a whole and cannot modify or control\nspecific elements in the software. Furthermore, these methods struggle to\nhandle the complex spatial and semantic constraints inherent in the real world.\nTo address the challenges, we present Scenethesis, a novel\nrequirement-sensitive 3D software synthesis approach that maintains formal\ntraceability between user specifications and generated 3D software. Scenethesis\nis built upon ScenethesisLang, a domain-specific language that serves as a\ngranular constraint-aware intermediate representation (IR) to bridge natural\nlanguage requirements and executable 3D software. It serves both as a\ncomprehensive scene description language enabling fine-grained modification of\n3D software elements and as a formal constraint-expressive specification\nlanguage capable of expressing complex spatial constraints. By decomposing 3D\nsoftware synthesis into stages operating on ScenethesisLang, Scenethesis\nenables independent verification, targeted modification, and systematic\nconstraint satisfaction. Our evaluation demonstrates that Scenethesis\naccurately captures over 80% of user requirements and satisfies more than 90%\nof hard constraints while handling over 100 constraints simultaneously.\nFurthermore, Scenethesis achieves a 42.8% improvement in BLIP-2 visual\nevaluation scores compared to the state-of-the-art method.\n","authors":["Shuqing Li","Anson Y. Lam","Yun Peng","Wenxuan Wang","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2507.18625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18623v1","updated":"2025-07-24T17:57:18Z","published":"2025-07-24T17:57:18Z","title":"Moving Out: Physically-grounded Human-AI Collaboration","summary":"  The ability to adapt to physical actions and constraints in an environment is\ncrucial for embodied agents (e.g., robots) to effectively collaborate with\nhumans. Such physically grounded human-AI collaboration must account for the\nincreased complexity of the continuous state-action space and constrained\ndynamics caused by physical constraints. In this paper, we introduce\n\\textit{Moving Out}, a new human-AI collaboration benchmark that resembles a\nwide range of collaboration modes affected by physical attributes and\nconstraints, such as moving heavy items together and maintaining consistent\nactions to move a big item around a corner. Using Moving Out, we designed two\ntasks and collected human-human interaction data to evaluate models' abilities\nto adapt to diverse human behaviors and unseen physical attributes. To address\nthe challenges in physical environments, we propose a novel method, BASS\n(Behavior Augmentation, Simulation, and Selection), to enhance the diversity of\nagents and their understanding of the outcome of actions. Our experiments show\nthat BASS outperforms state-of-the-art models in AI-AI and human-AI\ncollaboration. The project page is available at\n\\href{https://live-robotics-uva.github.io/movingout_ai/}{https://live-robotics-uva.github.io/movingout\\_ai/}.\n","authors":["Xuhui Kang","Sung-Wook Lee","Haolin Liu","Yuyan Wang","Yen-Ling Kuo"],"pdf_url":"https://arxiv.org/pdf/2507.18623v1.pdf","comment":"24 pages, 8 figures"},{"id":"http://arxiv.org/abs/2507.15857v2","updated":"2025-07-24T17:55:24Z","published":"2025-07-21T17:59:57Z","title":"Diffusion Beats Autoregressive in Data-Constrained Settings","summary":"  Autoregressive (AR) models have long dominated the landscape of large\nlanguage models, driving progress across a wide range of tasks. Recently,\ndiffusion-based language models have emerged as a promising alternative, though\ntheir advantages over AR models remain underexplored. In this paper, we\nsystematically study masked diffusion models in data-constrained settings-where\ntraining involves repeated passes over limited data-and find that they\nsignificantly outperform AR models when compute is abundant but data is scarce.\nDiffusion models make better use of repeated data, achieving lower validation\nloss and superior downstream performance. We interpret this advantage as\nimplicit data augmentation: masked diffusion exposes the model to a diverse\ndistribution of token orderings and prediction tasks, unlike AR's fixed\nleft-to-right factorization. We find new scaling laws for diffusion models and\nderive a closed-form expression for the critical compute threshold at which\ndiffusion begins to outperform AR. These results suggest that when data, not\ncompute, is the bottleneck, diffusion models offer a compelling alternative to\nthe standard AR paradigm. Our code is available at:\nhttps://diffusion-scaling.github.io.\n","authors":["Mihir Prabhudesai","Menging Wu","Amir Zadeh","Katerina Fragkiadaki","Deepak Pathak"],"pdf_url":"https://arxiv.org/pdf/2507.15857v2.pdf","comment":"Project Webpage: https://diffusion-scaling.github.io"},{"id":"http://arxiv.org/abs/2507.18616v1","updated":"2025-07-24T17:53:26Z","published":"2025-07-24T17:53:26Z","title":"SynC: Synthetic Image Caption Dataset Refinement with One-to-many\n  Mapping for Zero-shot Image Captioning","summary":"  Zero-shot Image Captioning (ZIC) increasingly utilizes synthetic datasets\ngenerated by text-to-image (T2I) models to mitigate the need for costly manual\nannotation. However, these T2I models often produce images that exhibit\nsemantic misalignments with their corresponding input captions (e.g., missing\nobjects, incorrect attributes), resulting in noisy synthetic image-caption\npairs that can hinder model training. Existing dataset pruning techniques are\nlargely designed for removing noisy text in web-crawled data. However, these\nmethods are ill-suited for the distinct challenges of synthetic data, where\ncaptions are typically well-formed, but images may be inaccurate\nrepresentations. To address this gap, we introduce SynC, a novel framework\nspecifically designed to refine synthetic image-caption datasets for ZIC.\nInstead of conventional filtering or regeneration, SynC focuses on reassigning\ncaptions to the most semantically aligned images already present within the\nsynthetic image pool. Our approach employs a one-to-many mapping strategy by\ninitially retrieving multiple relevant candidate images for each caption. We\nthen apply a cycle-consistency-inspired alignment scorer that selects the best\nimage by verifying its ability to retrieve the original caption via\nimage-to-text retrieval. Extensive evaluations demonstrate that SynC\nconsistently and significantly improves performance across various ZIC models\non standard benchmarks (MS-COCO, Flickr30k, NoCaps), achieving state-of-the-art\nresults in several scenarios. SynC offers an effective strategy for curating\nrefined synthetic data to enhance ZIC.\n","authors":["Si-Woo Kim","MinJu Jeon","Ye-Chan Kim","Soeun Lee","Taewhan Kim","Dong-Jin Kim"],"pdf_url":"https://arxiv.org/pdf/2507.18616v1.pdf","comment":"Accepted to ACM Multimedia 2025"},{"id":"http://arxiv.org/abs/2507.18612v1","updated":"2025-07-24T17:48:13Z","published":"2025-07-24T17:48:13Z","title":"Approximate SMT Counting Beyond Discrete Domains","summary":"  Satisfiability Modulo Theory (SMT) solvers have advanced automated reasoning,\nsolving complex formulas across discrete and continuous domains. Recent\nprogress in propositional model counting motivates extending SMT capabilities\ntoward model counting, especially for hybrid SMT formulas. Existing approaches,\nlike bit-blasting, are limited to discrete variables, highlighting the\nchallenge of counting solutions projected onto the discrete domain in hybrid\nformulas.\n  We introduce pact, an SMT model counter for hybrid formulas that uses\nhashing-based approximate model counting to estimate solutions with theoretical\nguarantees. pact makes a logarithmic number of SMT solver calls relative to the\nprojection variables, leveraging optimized hash functions. pact achieves\nsignificant performance improvements over baselines on a large suite of\nbenchmarks. In particular, out of 14,202 instances, pact successfully finished\non 603 instances, while Baseline could only finish on 13 instances.\n","authors":["Arijit Shaw","Kuldeep S. Meel"],"pdf_url":"https://arxiv.org/pdf/2507.18612v1.pdf","comment":"To be published in the proceedings of Design Automation Conference\n  (DAC) 2025"},{"id":"http://arxiv.org/abs/2503.07919v3","updated":"2025-07-24T17:45:05Z","published":"2025-03-10T23:50:30Z","title":"BEARCUBS: A benchmark for computer-using web agents","summary":"  Modern web agents possess computer use abilities that allow them to interact\nwith webpages by sending commands to a virtual keyboard and mouse. While such\nagents have considerable potential to assist human users with complex tasks,\nevaluating their capabilities in real-world settings poses a major challenge.\nTo this end, we introduce BEARCUBS, a \"smallbut mighty\" benchmark of 111\ninformation-seeking questions designed to evaluate a web agent's ability to\nsearch, browse, and identify factual information from the web. Unlike prior web\nagent benchmarks, solving BEARCUBS requires (1) accessing live web content\nrather than synthetic or simulated pages, which captures the unpredictability\nof real-world web interactions; and (2) performing a broad range of multimodal\ninteractions (e.g., video understanding, 3D navigation) that cannot be bypassed\nvia text-based workarounds. Each question in BEARCUBS has a corresponding\nshort, unambiguous answer and a human-validated browsing trajectory, allowing\nfor transparent evaluation of agent performance and strategies. A human study\nconfirms that BEARCUBS questions are solvable but non-trivial (84.7% human\naccuracy), revealing domain knowledge gaps and overlooked details as common\nfailure points. We find that ChatGPT Agent significantly outperforms other\ncomputer-using agents with an overall accuracy of 65.8% (compared to e.g.,\nOperator's 23.4%), showcasing substantial progress in tasks involving real\ncomputer use, such as playing web games and navigating 3D environments.\nNevertheless, closing the gap to human performance requires improvements in\nareas like fine control, complex data filtering, and execution speed. To\nfacilitate future research, BEARCUBS will be updated periodically to replace\ninvalid or contaminated questions, keeping the benchmark fresh for future\ngenerations of web agents.\n","authors":["Yixiao Song","Katherine Thai","Chau Minh Pham","Yapei Chang","Mazin Nadaf","Mohit Iyyer"],"pdf_url":"https://arxiv.org/pdf/2503.07919v3.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2503.16870v2","updated":"2025-07-24T17:30:12Z","published":"2025-03-21T05:58:18Z","title":"Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs","summary":"  Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.\n","authors":[" Anshumann","Mohd Abbas Zaidi","Akhil Kedia","Jinwoo Ahn","Taehwak Kwon","Kangwook Lee","Haejun Lee","Joohyung Lee"],"pdf_url":"https://arxiv.org/pdf/2503.16870v2.pdf","comment":"Accepted as Oral paper at ACL 2025. Source code is available at\n  https://github.com/akhilkedia/RandomSamplingKD . Anshumann, Mohd Abbas Zaidi\n  and Akhil Kedia have Equal Contribution"},{"id":"http://arxiv.org/abs/2506.03654v3","updated":"2025-07-24T17:28:09Z","published":"2025-06-04T07:46:24Z","title":"MambaNeXt-YOLO: A Hybrid State Space Model for Real-time Object\n  Detection","summary":"  Real-time object detection is a fundamental but challenging task in computer\nvision, particularly when computational resources are limited. Although\nYOLO-series models have set strong benchmarks by balancing speed and accuracy,\nthe increasing need for richer global context modeling has led to the use of\nTransformer-based architectures. Nevertheless, Transformers have high\ncomputational complexity because of their self-attention mechanism, which\nlimits their practicality for real-time and edge deployments. To overcome these\nchallenges, recent developments in linear state space models, such as Mamba,\nprovide a promising alternative by enabling efficient sequence modeling with\nlinear complexity. Building on this insight, we propose MambaNeXt-YOLO, a novel\nobject detection framework that balances accuracy and efficiency through three\nkey contributions: (1) MambaNeXt Block: a hybrid design that integrates CNNs\nwith Mamba to effectively capture both local features and long-range\ndependencies; (2) Multi-branch Asymmetric Fusion Pyramid Network (MAFPN): an\nenhanced feature pyramid architecture that improves multi-scale object\ndetection across various object sizes; and (3) Edge-focused Efficiency: our\nmethod achieved 66.6% mAP at 31.9 FPS on the PASCAL VOC dataset without any\npre-training and supports deployment on edge devices such as the NVIDIA Jetson\nXavier NX and Orin NX.\n","authors":["Xiaochun Lei","Siqi Wu","Weilin Wu","Zetao Jiang"],"pdf_url":"https://arxiv.org/pdf/2506.03654v3.pdf","comment":"This paper is under consideration at Image and Vision Computing"},{"id":"http://arxiv.org/abs/2507.18594v1","updated":"2025-07-24T17:24:59Z","published":"2025-07-24T17:24:59Z","title":"DRWKV: Focusing on Object Edges for Low-Light Image Enhancement","summary":"  Low-light image enhancement remains a challenging task, particularly in\npreserving object edge continuity and fine structural details under extreme\nillumination degradation. In this paper, we propose a novel model, DRWKV\n(Detailed Receptance Weighted Key Value), which integrates our proposed Global\nEdge Retinex (GER) theory, enabling effective decoupling of illumination and\nedge structures for enhanced edge fidelity. Secondly, we introduce Evolving WKV\nAttention, a spiral-scanning mechanism that captures spatial edge continuity\nand models irregular structures more effectively. Thirdly, we design the\nBilateral Spectrum Aligner (Bi-SAB) and a tailored MS2-Loss to jointly align\nluminance and chrominance features, improving visual naturalness and mitigating\nartifacts. Extensive experiments on five LLIE benchmarks demonstrate that DRWKV\nachieves leading performance in PSNR, SSIM, and NIQE while maintaining low\ncomputational complexity. Furthermore, DRWKV enhances downstream performance in\nlow-light multi-object tracking tasks, validating its generalization\ncapabilities.\n","authors":["Xuecheng Bai","Yuxiang Wang","Boyu Hu","Qinyuan Jie","Chuanzhi Xu","Hongru Xiao","Kechen Li","Vera Chung"],"pdf_url":"https://arxiv.org/pdf/2507.18594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07966v2","updated":"2025-07-24T17:20:41Z","published":"2025-07-10T17:47:40Z","title":"Scaling RL to Long Videos","summary":"  We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.0% and 70.7% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-R1 across multiple benchmarks. Moreover, LongVILA-R1\nshows steady performance improvements as the number of input video frames\nincreases. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames / around 256k tokens).\n","authors":["Yukang Chen","Wei Huang","Baifeng Shi","Qinghao Hu","Hanrong Ye","Ligeng Zhu","Zhijian Liu","Pavlo Molchanov","Jan Kautz","Xiaojuan Qi","Sifei Liu","Hongxu Yin","Yao Lu","Song Han"],"pdf_url":"https://arxiv.org/pdf/2507.07966v2.pdf","comment":"Code at https://github.com/NVlabs/Long-RL and model at\n  https://huggingface.co/Efficient-Large-Model/LongVILA-R1-7B"},{"id":"http://arxiv.org/abs/2507.18587v1","updated":"2025-07-24T17:10:06Z","published":"2025-07-24T17:10:06Z","title":"A Foundation Model for Massive MIMO Precoding with an Adaptive per-User\n  Rate-Power Tradeoff","summary":"  Deep learning (DL) has emerged as a solution for precoding in massive\nmultiple-input multiple-output (mMIMO) systems due to its capacity to learn the\ncharacteristics of the propagation environment. However, training such a model\nrequires high-quality, local datasets at the deployment site, which are often\ndifficult to collect. We propose a transformer-based foundation model for mMIMO\nprecoding that seeks to minimize the energy consumption of the transmitter\nwhile dynamically adapting to per-user rate requirements. At equal energy\nconsumption, zero-shot deployment of the proposed foundation model\nsignificantly outperforms zero forcing, and approaches weighted minimum mean\nsquared error performance with 8x less complexity. To address model adaptation\nin data-scarce settings, we introduce a data augmentation method that finds\ntraining samples similar to the target distribution by computing the cosine\nsimilarity between the outputs of the pre-trained feature extractor. Our work\nenables the implementation of DL-based solutions in practice by addressing\nchallenges of data availability and training complexity. Moreover, the ability\nto dynamically configure per-user rate requirements can be leveraged by higher\nlevel resource allocation and scheduling algorithms for greater control over\nenergy efficiency, spectral efficiency and fairness.\n","authors":["Jérôme Emery","Ali Hasanzadeh Karkan","Jean-François Frigon","François Leduc-Primeau"],"pdf_url":"https://arxiv.org/pdf/2507.18587v1.pdf","comment":"6 pages, 3 figures. Accepted to the IEEE International Symposium on\n  Personal, Indoor and Mobile Radio Communications (PIMRC) 2025"},{"id":"http://arxiv.org/abs/2507.18584v1","updated":"2025-07-24T17:03:27Z","published":"2025-07-24T17:03:27Z","title":"AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance\n  Data Synthesis for Specialist LLMs","summary":"  Despite the impressive performance of large language models (LLMs) in general\ndomains, they often underperform in specialized domains. Existing approaches\ntypically rely on data synthesis methods and yield promising results by using\nunlabeled data to capture domain-specific features. However, these methods\neither incur high computational costs or suffer from performance limitations,\nwhile also demonstrating insufficient generalization across different tasks. To\naddress these challenges, we propose AQuilt, a framework for constructing\ninstruction-tuning data for any specialized domains from corresponding\nunlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic,\nand Task type. By incorporating logic and inspection, we encourage reasoning\nprocesses and self-inspection to enhance model performance. Moreover,\ncustomizable task instructions enable high-quality data generation for any\ntask. As a result, we construct a dataset of 703k examples to train a powerful\ndata synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3\nwhile utilizing just 17% of the production cost. Further analysis demonstrates\nthat our generated data exhibits higher relevance to downstream tasks. Source\ncode, models, and scripts are available at https://github.com/Krueske/AQuilt.\n","authors":["Xiaopeng Ke","Hexuan Deng","Xuebo Liu","Jun Rao","Zhenxi Song","Jun Yu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.18584v1.pdf","comment":"32 pages, 4 figures"},{"id":"http://arxiv.org/abs/2507.18583v1","updated":"2025-07-24T17:02:46Z","published":"2025-07-24T17:02:46Z","title":"DR.EHR: Dense Retrieval for Electronic Health Record with Knowledge\n  Injection and Synthetic Data","summary":"  Electronic Health Records (EHRs) are pivotal in clinical practices, yet their\nretrieval remains a challenge mainly due to semantic gap issues. Recent\nadvancements in dense retrieval offer promising solutions but existing models,\nboth general-domain and biomedical-domain, fall short due to insufficient\nmedical knowledge or mismatched training corpora. This paper introduces\n\\texttt{DR.EHR}, a series of dense retrieval models specifically tailored for\nEHR retrieval. We propose a two-stage training pipeline utilizing MIMIC-IV\ndischarge summaries to address the need for extensive medical knowledge and\nlarge-scale training data. The first stage involves medical entity extraction\nand knowledge injection from a biomedical knowledge graph, while the second\nstage employs large language models to generate diverse training data. We train\ntwo variants of \\texttt{DR.EHR}, with 110M and 7B parameters, respectively.\nEvaluated on the CliniQ benchmark, our models significantly outperforms all\nexisting dense retrievers, achieving state-of-the-art results. Detailed\nanalyses confirm our models' superiority across various match and query types,\nparticularly in challenging semantic matches like implication and abbreviation.\nAblation studies validate the effectiveness of each pipeline component, and\nsupplementary experiments on EHR QA datasets demonstrate the models'\ngeneralizability on natural language questions, including complex ones with\nmultiple entities. This work significantly advances EHR retrieval, offering a\nrobust solution for clinical applications.\n","authors":["Zhengyun Zhao","Huaiyuan Ying","Yue Zhong","Sheng Yu"],"pdf_url":"https://arxiv.org/pdf/2507.18583v1.pdf","comment":"Model and code released upon acceptance"},{"id":"http://arxiv.org/abs/2408.10450v2","updated":"2025-07-24T16:51:36Z","published":"2024-08-19T23:16:18Z","title":"RUMI: Rummaging Using Mutual Information","summary":"  This paper presents Rummaging Using Mutual Information (RUMI), a method for\nonline generation of robot action sequences to gather information about the\npose of a known movable object in visually-occluded environments. Focusing on\ncontact-rich rummaging, our approach leverages mutual information between the\nobject pose distribution and robot trajectory for action planning. From an\nobserved partial point cloud, RUMI deduces the compatible object pose\ndistribution and approximates the mutual information of it with workspace\noccupancy in real time. Based on this, we develop an information gain cost\nfunction and a reachability cost function to keep the object within the robot's\nreach. These are integrated into a model predictive control (MPC) framework\nwith a stochastic dynamics model, updating the pose distribution in a closed\nloop. Key contributions include a new belief framework for object pose\nestimation, an efficient information gain computation strategy, and a robust\nMPC-based control scheme. RUMI demonstrates superior performance in both\nsimulated and real tasks compared to baseline methods.\n","authors":["Sheng Zhong","Nima Fazeli","Dmitry Berenson"],"pdf_url":"https://arxiv.org/pdf/2408.10450v2.pdf","comment":"20 pages, 20 figures, accepted by IEEE Transactions on Robotics\n  (T-RO), preprint"},{"id":"http://arxiv.org/abs/2507.17289v2","updated":"2025-07-24T16:50:13Z","published":"2025-07-23T07:51:10Z","title":"Compliance Brain Assistant: Conversational Agentic AI for Assisting\n  Compliance Tasks in Enterprise Environments","summary":"  This paper presents Compliance Brain Assistant (CBA), a conversational,\nagentic AI assistant designed to boost the efficiency of daily compliance tasks\nfor personnel in enterprise environments. To strike a good balance between\nresponse quality and latency, we design a user query router that can\nintelligently choose between (i) FastTrack mode: to handle simple requests that\nonly need additional relevant context retrieved from knowledge corpora; and\n(ii) FullAgentic mode: to handle complicated requests that need composite\nactions and tool invocations to proactively discover context across various\ncompliance artifacts, and/or involving other APIs/models for accommodating\nrequests. A typical example would be to start with a user query, use its\ndescription to find a specific entity and then use the entity's information to\nquery other APIs for curating and enriching the final AI response.\n  Our experimental evaluations compared CBA against an out-of-the-box LLM on\nvarious real-world privacy/compliance-related queries targeting various\npersonas. We found that CBA substantially improved upon the vanilla LLM's\nperformance on metrics such as average keyword match rate (83.7% vs. 41.7%) and\nLLM-judge pass rate (82.0% vs. 20.0%). We also compared metrics for the full\nrouting-based design against the `fast-track only` and `full-agentic` modes and\nfound that it had a better average match-rate and pass-rate while keeping the\nrun-time approximately the same. This finding validated our hypothesis that the\nrouting mechanism leads to a good trade-off between the two worlds.\n","authors":["Shitong Zhu","Chenhao Fang","Derek Larson","Neel Reddy Pochareddy","Rajeev Rao","Sophie Zeng","Yanqing Peng","Wendy Summer","Alex Goncalves","Arya Pudota","Hervé Robert"],"pdf_url":"https://arxiv.org/pdf/2507.17289v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18576v1","updated":"2025-07-24T16:49:19Z","published":"2025-07-24T16:49:19Z","title":"SafeWork-R1: Coevolving Safety and Intelligence under the\n  AI-45$^{\\circ}$ Law","summary":"  We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that\ndemonstrates the coevolution of capabilities and safety. It is developed by our\nproposed SafeLadder framework, which incorporates large-scale, progressive,\nsafety-oriented reinforcement learning post-training, supported by a suite of\nmulti-principled verifiers. Unlike previous alignment methods such as RLHF that\nsimply learn human preferences, SafeLadder enables SafeWork-R1 to develop\nintrinsic safety reasoning and self-reflection abilities, giving rise to safety\n`aha' moments. Notably, SafeWork-R1 achieves an average improvement of\n$46.54\\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks\nwithout compromising general capabilities, and delivers state-of-the-art safety\nperformance compared to leading proprietary models such as GPT-4.1 and Claude\nOpus 4. To further bolster its reliability, we implement two distinct\ninference-time intervention methods and a deliberative search mechanism,\nenforcing step-level verification. Finally, we further develop\nSafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and\nSafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and\ncapability can co-evolve synergistically, highlighting the generalizability of\nour framework in building robust, reliable, and trustworthy general-purpose AI.\n","authors":["Shanghai AI Lab"," :","Yicheng Bao","Guanxu Chen","Mingkang Chen","Yunhao Chen","Chiyu Chen","Lingjie Chen","Sirui Chen","Xinquan Chen","Jie Cheng","Yu Cheng","Dengke Deng","Yizhuo Ding","Dan Ding","Xiaoshan Ding","Yi Ding","Zhichen Dong","Lingxiao Du","Yuyu Fan","Xinshun Feng","Yanwei Fu","Yuxuan Gao","Ruijun Ge","Tianle Gu","Lujun Gui","Jiaxuan Guo","Qianxi He","Yuenan Hou","Xuhao Hu","Hong Huang","Kaichen Huang","Shiyang Huang","Yuxian Jiang","Shanzhe Lei","Jie Li","Lijun Li","Hao Li","Juncheng Li","Xiangtian Li","Yafu Li","Lingyu Li","Xueyan Li","Haotian Liang","Dongrui Liu","Qihua Liu","Zhixuan Liu","Bangwei Liu","Huacan Liu","Yuexiao Liu","Zongkai Liu","Chaochao Lu","Yudong Lu","Xiaoya Lu","Zhenghao Lu","Qitan Lv","Caoyuan Ma","Jiachen Ma","Xiaoya Ma","Zhongtian Ma","Lingyu Meng","Ziqi Miao","Yazhe Niu","Yuezhang Peng","Yuan Pu","Han Qi","Chen Qian","Xingge Qiao","Jingjing Qu","Jiashu Qu","Wanying Qu","Wenwen Qu","Xiaoye Qu","Qihan Ren","Qingnan Ren","Qingyu Ren","Jing Shao","Wenqi Shao","Shuai Shao","Dongxing Shi","Xin Song","Xinhao Song","Yan Teng","Xuan Tong","Yingchun Wang","Xuhong Wang","Shujie Wang","Xin Wang","Yige Wang","Yixu Wang","Yuanfu Wang","Futing Wang","Ruofan Wang","Wenjie Wang","Yajie Wang","Muhao Wei","Xiaoyu Wen","Fenghua Weng","Yuqi Wu","Yingtong Xiong","Xingcheng Xu","Chao Yang","Yue Yang","Yang Yao","Yulei Ye","Zhenyun Yin","Yi Yu","Bo Zhang","Qiaosheng Zhang","Jinxuan Zhang","Yexin Zhang","Yinqiang Zheng","Hefeng Zhou","Zhanhui Zhou","Pengyu Zhu","Qingzi Zhu","Yubo Zhu","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2507.18576v1.pdf","comment":"47 pages, 18 figures, authors are listed in alphabetical order by\n  their last names"},{"id":"http://arxiv.org/abs/2507.18572v1","updated":"2025-07-24T16:46:25Z","published":"2025-07-24T16:46:25Z","title":"PosterMate: Audience-driven Collaborative Persona Agents for Poster\n  Design","summary":"  Poster designing can benefit from synchronous feedback from target audiences.\nHowever, gathering audiences with diverse perspectives and reconciling them on\ndesign edits can be challenging. Recent generative AI models present\nopportunities to simulate human-like interactions, but it is unclear how they\nmay be used for feedback processes in design. We introduce PosterMate, a poster\ndesign assistant that facilitates collaboration by creating audience-driven\npersona agents constructed from marketing documents. PosterMate gathers\nfeedback from each persona agent regarding poster components, and stimulates\ndiscussion with the help of a moderator to reach a conclusion. These\nagreed-upon edits can then be directly integrated into the poster design.\nThrough our user study (N=12), we identified the potential of PosterMate to\ncapture overlooked viewpoints, while serving as an effective prototyping tool.\nAdditionally, our controlled online evaluation (N=100) revealed that the\nfeedback from an individual persona agent is appropriate given its persona\nidentity, and the discussion effectively synthesizes the different persona\nagents' perspectives.\n","authors":["Donghoon Shin","Daniel Lee","Gary Hsieh","Gromit Yeuk-Yin Chan"],"pdf_url":"https://arxiv.org/pdf/2507.18572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18567v1","updated":"2025-07-24T16:42:15Z","published":"2025-07-24T16:42:15Z","title":"Proceedings 19th International Workshop on the ACL2 Theorem Prover and\n  Its Applications","summary":"  The ACL2 Workshop series is the major technical forum for users of the ACL2\ntheorem proving system to present research related to the ACL2 theorem prover\nand its applications. ACL2 is an industrial-strength automated reasoning\nsystem, the latest in the Boyer-Moore family of theorem provers. The 2005 ACM\nSoftware System Award was awarded to Boyer, Kaufmann, and Moore for their work\non ACL2 and the other theorem provers in the Boyer-Moore family.\n","authors":["Ruben Gamboa","Panagiotis Manolios"],"pdf_url":"https://arxiv.org/pdf/2507.18567v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18562v1","updated":"2025-07-24T16:36:47Z","published":"2025-07-24T16:36:47Z","title":"GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation","summary":"  Multimodal Machine Translation (MMT) has demonstrated the significant help of\nvisual information in machine translation. However, existing MMT methods face\nchallenges in leveraging the modality gap by enforcing rigid visual-linguistic\nalignment whilst being confined to inference within their trained multimodal\ndomains. In this work, we construct novel multimodal scene graphs to preserve\nand integrate modality-specific information and introduce GIIFT, a two-stage\nGraph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph\nAttention Network adapter to learn multimodal knowledge in a unified fused\nspace and inductively generalize it to broader image-free translation domains.\nExperimental results on the Multi30K dataset of English-to-French and\nEnglish-to-German tasks demonstrate that our GIIFT surpasses existing\napproaches and achieves the state-of-the-art, even without images during\ninference. Results on the WMT benchmark show significant improvements over the\nimage-free translation baselines, demonstrating the strength of GIIFT towards\ninductive image-free inference.\n","authors":["Jiafeng Xiong","Yuting Zhao"],"pdf_url":"https://arxiv.org/pdf/2507.18562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18561v1","updated":"2025-07-24T16:35:42Z","published":"2025-07-24T16:35:42Z","title":"Beyond Internal Data: Constructing Complete Datasets for Fairness\n  Testing","summary":"  As AI becomes prevalent in high-risk domains and decision-making, it is\nessential to test for potential harms and biases. This urgency is reflected by\nthe global emergence of AI regulations that emphasise fairness and adequate\ntesting, with some mandating independent bias audits. However, procuring the\nnecessary data for fairness testing remains a significant challenge.\nParticularly in industry settings, legal and privacy concerns restrict the\ncollection of demographic data required to assess group disparities, and\nauditors face practical and cultural challenges in gaining access to data.\nFurther, internal historical datasets are often insufficiently representative\nto identify real-world biases. This work focuses on evaluating classifier\nfairness when complete datasets including demographics are inaccessible. We\npropose leveraging separate overlapping datasets to construct complete\nsynthetic data that includes demographic information and accurately reflects\nthe underlying relationships between protected attributes and model features.\nWe validate the fidelity of the synthetic data by comparing it to real data,\nand empirically demonstrate that fairness metrics derived from testing on such\nsynthetic data are consistent with those obtained from real data. This work,\ntherefore, offers a path to overcome real-world data scarcity for fairness\ntesting, enabling independent, model-agnostic evaluation of fairness, and\nserving as a viable substitute where real data is limited.\n","authors":["Varsha Ramineni","Hossein A. Rahmani","Emine Yilmaz","David Barber"],"pdf_url":"https://arxiv.org/pdf/2507.18561v1.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2507.18560v1","updated":"2025-07-24T16:35:24Z","published":"2025-07-24T16:35:24Z","title":"HARLF: Hierarchical Reinforcement Learning and Lightweight LLM-Driven\n  Sentiment Integration for Financial Portfolio Optimization","summary":"  This paper presents a novel hierarchical framework for portfolio\noptimization, integrating lightweight Large Language Models (LLMs) with Deep\nReinforcement Learning (DRL) to combine sentiment signals from financial news\nwith traditional market indicators. Our three-tier architecture employs base RL\nagents to process hybrid data, meta-agents to aggregate their decisions, and a\nsuper-agent to merge decisions based on market data and sentiment analysis.\nEvaluated on data from 2018 to 2024, after training on 2000-2017, the framework\nachieves a 26% annualized return and a Sharpe ratio of 1.2, outperforming\nequal-weighted and S&P 500 benchmarks. Key contributions include scalable\ncross-modal integration, a hierarchical RL structure for enhanced stability,\nand open-source reproducibility.\n","authors":["Benjamin Coriat","Eric Benhamou"],"pdf_url":"https://arxiv.org/pdf/2507.18560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.14783v2","updated":"2025-07-24T16:25:54Z","published":"2025-07-20T01:50:16Z","title":"Omni-Thinker: Scaling Cross-Domain Generalization in LLMs via Multi-Task\n  RL with Hybrid Rewards","summary":"  The advancement of general-purpose artificial intelligence relies on large\nlanguage models (LLMs) that excel across a wide range of tasks, from structured\nreasoning to creative generation. However, post-training methods like\nSupervised Fine-Tuning (SFT) often struggle with generalization, favoring\nmemorization over transferable learning. In this work, we introduce\nOmni-Thinker, a unified reinforcement learning (RL) framework that enhances LLM\nperformance across diverse tasks by combining rule-based verifiable rewards\nwith generative preference signals via LLM-as-a-Judge evaluations. Our approach\nenables consistent optimization across task types and scales RL-based training\nto subjective domains. We further investigate training strategies,\ndemonstrating that a curriculum-based progression that orders tasks from\nstructured to open-ended improves performance and reduces forgetting.\nExperimental results across four domains reveal that curriculum learning\nimproves performance by 5.2% over joint training and 9.1% over model merging.\nThese results highlight the importance of task-aware sampling and hybrid\nsupervision in scaling RL-based post-training for general-purpose LLMs.\n","authors":["Derek Li","Jiaming Zhou","Amirreza Kazemi","Qianyi Sun","Abbas Ghaddar","Mohammad Ali Alomrani","Liheng Ma","Yu Luo","Dong Li","Feng Wen","Jianye Hao","Mark Coates","Yingxue Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.14783v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.04704v2","updated":"2025-07-24T16:25:51Z","published":"2025-04-07T03:22:15Z","title":"LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important","summary":"  The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV.\n","authors":["Manlai Liang","JiaMing Zhang","Xiong Li","Jinlong Li"],"pdf_url":"https://arxiv.org/pdf/2504.04704v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08989v3","updated":"2025-07-24T16:21:10Z","published":"2024-10-11T17:01:43Z","title":"Zeroth-Order Fine-Tuning of LLMs in Random Subspaces","summary":"  Fine-tuning Large Language Models (LLMs) has proven effective for a variety\nof downstream tasks. However, as LLMs grow in size, the memory demands for\nbackpropagation become increasingly prohibitive. Zeroth-order (ZO) optimization\nmethods offer a memory-efficient alternative by using forward passes to\nestimate gradients, but the variance of gradient estimates typically scales\nlinearly with the model's parameter dimension$\\unicode{x2013}$a significant\nissue for LLMs. In this paper, we propose the random Subspace Zeroth-order\n(SubZero) optimization to address the challenges posed by LLMs' high\ndimensionality. We introduce a low-rank perturbation tailored for LLMs that\nsignificantly reduces memory consumption while improving training performance.\nAdditionally, we prove that our gradient estimation closely approximates the\nbackpropagation gradient, exhibits lower variance than traditional ZO methods,\nand ensures convergence when combined with SGD. Experimental results show that\nSubZero enhances fine-tuning performance and achieves faster convergence\ncompared to standard ZO approaches like MeZO across various language modeling\ntasks. Code is available at https://github.com/zimingyy/SubZero.\n","authors":["Ziming Yu","Pan Zhou","Sike Wang","Jia Li","Mi Tian","Hua Huang"],"pdf_url":"https://arxiv.org/pdf/2410.08989v3.pdf","comment":"ICCV 2025 camera-ready version"},{"id":"http://arxiv.org/abs/2507.18552v1","updated":"2025-07-24T16:19:43Z","published":"2025-07-24T16:19:43Z","title":"VideoMind: An Omni-Modal Video Dataset with Intent Grounding for\n  Deep-Cognitive Video Understanding","summary":"  This paper introduces VideoMind, a video-centric omni-modal dataset designed\nfor deep video content cognition and enhanced multi-modal feature\nrepresentation. The dataset comprises 103K video samples (3K reserved for\ntesting), each paired with audio and systematically detailed textual\ndescriptions. Specifically, every video and its audio is described across three\nhierarchical layers (factual, abstract, and intent), progressing from surface\nto depth. It contains over 22 million words, averaging ~225 words per sample.\nVideoMind's key distinction from existing datasets is its provision of intent\nexpressions, which require contextual integration across the entire video and\nare not directly observable. These deep-cognitive expressions are generated\nusing a Chain-of-Thought (COT) approach, prompting the mLLM through\nstep-by-step reasoning. Each description includes annotations for subject,\nplace, time, event, action, and intent, supporting downstream recognition\ntasks. Crucially, we establish a gold-standard benchmark with 3,000 manually\nvalidated samples for evaluating deep-cognitive video understanding. We design\nhybrid-cognitive retrieval experiments, scored by multi-level retrieval\nmetrics, to appropriately assess deep video comprehension. Evaluation results\nfor models (e.g., InternVideo, VAST, UMT-L) are released. VideoMind serves as a\npowerful benchmark for fine-grained cross-modal alignment and advances fields\nrequiring in-depth video understanding, such as emotion and intent recognition.\nThe data is publicly available on GitHub, HuggingFace, and OpenDataLab,\nhttps://github.com/cdx-cindy/VideoMind.\n","authors":["Baoyao Yang","Wanyun Li","Dixin Chen","Junxiang Chen","Wenbin Yao","Haifeng Lin"],"pdf_url":"https://arxiv.org/pdf/2507.18552v1.pdf","comment":"7 pages; 14 figures"},{"id":"http://arxiv.org/abs/2507.18550v1","updated":"2025-07-24T16:18:46Z","published":"2025-07-24T16:18:46Z","title":"On the Performance of Concept Probing: The Influence of the Data\n  (Extended Version)","summary":"  Concept probing has recently garnered increasing interest as a way to help\ninterpret artificial neural networks, dealing both with their typically large\nsize and their subsymbolic nature, which ultimately renders them unfeasible for\ndirect human interpretation. Concept probing works by training additional\nclassifiers to map the internal representations of a model into human-defined\nconcepts of interest, thus allowing humans to peek inside artificial neural\nnetworks. Research on concept probing has mainly focused on the model being\nprobed or the probing model itself, paying limited attention to the data\nrequired to train such probing models. In this paper, we address this gap.\nFocusing on concept probing in the context of image classification tasks, we\ninvestigate the effect of the data used to train probing models on their\nperformance. We also make available concept labels for two widely used\ndatasets.\n","authors":["Manuel de Sousa Ribeiro","Afonso Leote","João Leite"],"pdf_url":"https://arxiv.org/pdf/2507.18550v1.pdf","comment":"Extended version of the paper published in Proceedings of the\n  European Conference on Artificial Intelligence (ECAI 2025)"},{"id":"http://arxiv.org/abs/2507.18546v1","updated":"2025-07-24T16:11:14Z","published":"2025-07-24T16:11:14Z","title":"GLiNER2: An Efficient Multi-Task Information Extraction System with\n  Schema-Driven Interface","summary":"  Information extraction (IE) is fundamental to numerous NLP applications, yet\nexisting solutions often require specialized models for different tasks or rely\non computationally expensive large language models. We present GLiNER2, a\nunified framework that enhances the original GLiNER architecture to support\nnamed entity recognition, text classification, and hierarchical structured data\nextraction within a single efficient model. Built pretrained transformer\nencoder architecture, GLiNER2 maintains CPU efficiency and compact size while\nintroducing multi-task composition through an intuitive schema-based interface.\nOur experiments demonstrate competitive performance across extraction and\nclassification tasks with substantial improvements in deployment accessibility\ncompared to LLM-based alternatives. We release GLiNER2 as an open-source\npip-installable library with pre-trained models and documentation at\nhttps://github.com/fastino-ai/GLiNER2.\n","authors":["Urchade Zaratiana","Gil Pasternak","Oliver Boyd","George Hurn-Maloney","Ash Lewis"],"pdf_url":"https://arxiv.org/pdf/2507.18546v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18533v1","updated":"2025-07-24T16:00:32Z","published":"2025-07-24T16:00:32Z","title":"C2G-KD: PCA-Constrained Generator for Data-Free Knowledge Distillation","summary":"  We introduce C2G-KD, a data-free knowledge distillation framework where a\nclass-conditional generator is trained to produce synthetic samples guided by a\nfrozen teacher model and geometric constraints derived from PCA. The generator\nnever observes real training data but instead learns to activate the teacher's\noutput through a combination of semantic and structural losses. By constraining\ngenerated samples to lie within class-specific PCA subspaces estimated from as\nfew as two real examples per class, we preserve topological consistency and\ndiversity. Experiments on MNIST show that even minimal class structure is\nsufficient to bootstrap useful synthetic training pipelines.\n","authors":["Magnus Bengtsson","Kenneth Östberg"],"pdf_url":"https://arxiv.org/pdf/2507.18533v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2505.09619v4","updated":"2025-07-24T15:55:05Z","published":"2025-04-07T14:07:05Z","title":"Machine Learning Solutions Integrated in an IoT Healthcare Platform for\n  Heart Failure Risk Stratification","summary":"  The management of chronic Heart Failure (HF) presents significant challenges\nin modern healthcare, requiring continuous monitoring, early detection of\nexacerbations, and personalized treatment strategies. In this paper, we present\na predictive model founded on Machine Learning (ML) techniques to identify\npatients at HF risk. This model is an ensemble learning approach, a modified\nstacking technique, that uses two specialized models leveraging clinical and\nechocardiographic features and then a meta-model to combine the predictions of\nthese two models. We initially assess the model on a real dataset and the\nobtained results suggest that it performs well in the stratification of\npatients at HR risk. Specifically, we obtained high sensitivity (95\\%),\nensuring that nearly all high-risk patients are identified. As for accuracy, we\nobtained 84\\%, which can be considered moderate in some ML contexts. However,\nit is acceptable given our priority of identifying patients at risk of HF\nbecause they will be asked to participate in the telemonitoring program of the\nPrediHealth research project on which some of the authors of this paper are\nworking. The initial findings also suggest that ML-based risk stratification\nmodels can serve as valuable decision-support tools not only in the PrediHealth\nproject but also for healthcare professionals, aiding in early intervention and\npersonalized patient management. To have a better understanding of the value\nand of potentiality of our predictive model, we also contrasted its results\nwith those obtained by using three baseline models. The preliminary results\nindicate that our predictive model outperforms these baselines that flatly\nconsider features, \\ie not grouping them in clinical and echocardiographic\nfeatures.\n","authors":["Aiman Faiz","Anna Maria De Roberto","Claudio Pascarelli","Gianvito Mitrano","Gianluca Fimiani","Marina Garofano","Genoveffa Tortora","Mariangela Lazoi","Claudio Passino","Alessia Bramanti"],"pdf_url":"https://arxiv.org/pdf/2505.09619v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.09027v2","updated":"2025-07-24T15:55:00Z","published":"2025-06-10T17:53:29Z","title":"Diffuse and Disperse: Image Generation with Representation\n  Regularization","summary":"  The development of diffusion-based generative models over the past decade has\nlargely proceeded independently of progress in representation learning. These\ndiffusion models typically rely on regression-based objectives and generally\nlack explicit regularization. In this work, we propose \\textit{Dispersive\nLoss}, a simple plug-and-play regularizer that effectively improves\ndiffusion-based generative models. Our loss function encourages internal\nrepresentations to disperse in the hidden space, analogous to contrastive\nself-supervised learning, with the key distinction that it requires no positive\nsample pairs and therefore does not interfere with the sampling process used\nfor regression. Compared to the recent method of representation alignment\n(REPA), our approach is self-contained and minimalist, requiring no\npre-training, no additional parameters, and no external data. We evaluate\nDispersive Loss on the ImageNet dataset across a range of models and report\nconsistent improvements over widely used and strong baselines. We hope our work\nwill help bridge the gap between generative modeling and representation\nlearning.\n","authors":["Runqian Wang","Kaiming He"],"pdf_url":"https://arxiv.org/pdf/2506.09027v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.14679v2","updated":"2025-07-24T15:46:28Z","published":"2025-07-19T16:09:48Z","title":"GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character\n  Similarity Networks","summary":"  The exponential growth of spam text on the Internet necessitates robust\ndetection mechanisms to mitigate risks such as information leakage and social\ninstability. This work addresses two principal challenges: adversarial\nstrategies employed by spammers and the scarcity of labeled data. We propose a\nnovel spam-text detection framework GCC-Spam, which integrates three core\ninnovations. First, a character similarity network captures orthographic and\nphonetic features to counter character-obfuscation attacks and furthermore\nproduces sentence embeddings for downstream classification. Second, contrastive\nlearning enhances discriminability by optimizing the latent-space distance\nbetween spam and normal texts. Third, a Generative Adversarial Network (GAN)\ngenerates realistic pseudo-spam samples to alleviate data scarcity while\nimproving model robustness and classification accuracy. Extensive experiments\non real-world datasets demonstrate that our model outperforms baseline\napproaches, achieving higher detection rates with significantly fewer labeled\nexamples.\n","authors":["Zhijie Wang","Zixin Xu","Zhiyuan Pan"],"pdf_url":"https://arxiv.org/pdf/2507.14679v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18521v1","updated":"2025-07-24T15:45:26Z","published":"2025-07-24T15:45:26Z","title":"GLANCE: Graph Logic Attention Network with Cluster Enhancement for\n  Heterophilous Graph Representation Learning","summary":"  Graph Neural Networks (GNNs) have demonstrated significant success in\nlearning from graph-structured data but often struggle on heterophilous graphs,\nwhere connected nodes differ in features or class labels. This limitation\narises from indiscriminate neighbor aggregation and insufficient incorporation\nof higher-order structural patterns. To address these challenges, we propose\nGLANCE (Graph Logic Attention Network with Cluster Enhancement), a novel\nframework that integrates logic-guided reasoning, dynamic graph refinement, and\nadaptive clustering to enhance graph representation learning. GLANCE combines a\nlogic layer for interpretable and structured embeddings, multi-head\nattention-based edge pruning for denoising graph structures, and clustering\nmechanisms for capturing global patterns. Experimental results in benchmark\ndatasets, including Cornell, Texas, and Wisconsin, demonstrate that GLANCE\nachieves competitive performance, offering robust and interpretable solutions\nfor heterophilous graph scenarios. The proposed framework is lightweight,\nadaptable, and uniquely suited to the challenges of heterophilous graphs.\n","authors":["Zhongtian Sun","Anoushka Harit","Alexandra Cristea","Christl A. Donnelly","Pietro Liò"],"pdf_url":"https://arxiv.org/pdf/2507.18521v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18512v1","updated":"2025-07-24T15:33:31Z","published":"2025-07-24T15:33:31Z","title":"Explaining How Visual, Textual and Multimodal Encoders Share Concepts","summary":"  Sparse autoencoders (SAEs) have emerged as a powerful technique for\nextracting human-interpretable features from neural networks activations.\nPrevious works compared different models based on SAE-derived features but\nthose comparisons have been restricted to models within the same modality. We\npropose a novel indicator allowing quantitative comparison of models across SAE\nfeatures, and use it to conduct a comparative study of visual, textual and\nmultimodal encoders. We also propose to quantify the Comparative Sharedness of\nindividual features between different classes of models. With these two new\ntools, we conduct several studies on 21 encoders of the three types, with two\nsignificantly different sizes, and considering generalist and domain specific\ndatasets. The results allow to revisit previous studies at the light of\nencoders trained in a multimodal context and to quantify to which extent all\nthese models share some representations or features. They also suggest that\nvisual features that are specific to VLMs among vision encoders are shared with\ntext encoders, highlighting the impact of text pretraining. The code is\navailable at https://github.com/CEA-LIST/SAEshareConcepts\n","authors":["Clément Cornet","Romaric Besançon","Hervé Le Borgne"],"pdf_url":"https://arxiv.org/pdf/2507.18512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17226v6","updated":"2025-07-24T15:32:35Z","published":"2024-07-24T12:26:21Z","title":"Sublinear Regret for a Class of Continuous-Time Linear-Quadratic\n  Reinforcement Learning Problems","summary":"  We study reinforcement learning (RL) for a class of continuous-time\nlinear-quadratic (LQ) control problems for diffusions, where states are\nscalar-valued and running control rewards are absent but volatilities of the\nstate processes depend on both state and control variables. We apply a\nmodel-free approach that relies neither on knowledge of model parameters nor on\ntheir estimations, and devise an RL algorithm to learn the optimal policy\nparameter directly. Our main contributions include the introduction of an\nexploration schedule and a regret analysis of the proposed algorithm. We\nprovide the convergence rate of the policy parameter to the optimal one, and\nprove that the algorithm achieves a regret bound of $O(N^{\\frac{3}{4}})$ up to\na logarithmic factor, where $N$ is the number of learning episodes. We conduct\na simulation study to validate the theoretical results and demonstrate the\neffectiveness and reliability of the proposed algorithm. We also perform\nnumerical comparisons between our method and those of the recent model-based\nstochastic LQ RL studies adapted to the state- and control-dependent volatility\nsetting, demonstrating a better performance of the former in terms of regret\nbounds.\n","authors":["Yilie Huang","Yanwei Jia","Xun Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.17226v6.pdf","comment":"42 pages, 4 figures. Accepted for publication in SIAM Journal on\n  Control and Optimization (2025)"},{"id":"http://arxiv.org/abs/2506.22495v2","updated":"2025-07-24T15:31:13Z","published":"2025-06-25T03:25:49Z","title":"Masked Autoencoders that Feel the Heart: Unveiling Simplicity Bias for\n  ECG Analyses","summary":"  The diagnostic value of electrocardiogram (ECG) lies in its dynamic\ncharacteristics, ranging from rhythm fluctuations to subtle waveform\ndeformations that evolve across time and frequency domains. However, supervised\nECG models tend to overfit dominant and repetitive patterns, overlooking\nfine-grained but clinically critical cues, a phenomenon known as Simplicity\nBias (SB), where models favor easily learnable signals over subtle but\ninformative ones. In this work, we first empirically demonstrate the presence\nof SB in ECG analyses and its negative impact on diagnostic performance, while\nsimultaneously discovering that self-supervised learning (SSL) can alleviate\nit, providing a promising direction for tackling the bias. Following the SSL\nparadigm, we propose a novel method comprising two key components: 1)\nTemporal-Frequency aware Filters to capture temporal-frequency features\nreflecting the dynamic characteristics of ECG signals, and 2) building on this,\nMulti-Grained Prototype Reconstruction for coarse and fine representation\nlearning across dual domains, further mitigating SB. To advance SSL in ECG\nanalyses, we curate a large-scale multi-site ECG dataset with 1.53 million\nrecordings from over 300 clinical centers. Experiments on three downstream\ntasks across six ECG datasets demonstrate that our method effectively reduces\nSB and achieves state-of-the-art performance. Code and dataset will be released\npublicly.\n","authors":["He-Yang Xu","Hongxiang Gao","Yuwen Li","Xiu-Shen Wei","Chengyu Liu"],"pdf_url":"https://arxiv.org/pdf/2506.22495v2.pdf","comment":"there are factual errors"},{"id":"http://arxiv.org/abs/2402.12118v2","updated":"2025-07-24T15:23:53Z","published":"2024-02-19T13:13:16Z","title":"DualXDA: Towards Sparse, Efficient and Explainable Data Attribution in\n  Large AI Models","summary":"  Deep learning models achieve remarkable performance, yet their\ndecision-making processes often remain opaque. In response, the field of\neXplainable Artificial Intelligence (XAI) has grown significantly over the last\ndecade, primarily focusing on feature attribution methods. Complementing this\nperspective, Data Attribution (DA) has emerged as a promising paradigm that\nshifts the focus from features to data provenance. However, existing DA\napproaches suffer from prohibitively high computational costs and memory\ndemands. Additionally, current attribution methods exhibit low sparsity,\nhindering the discovery of decisive patterns in the data. We introduce DualXDA,\na framework for sparse, efficient and explainable DA, comprised of two\ninterlinked approaches for Dual Data Attribution (DualDA) and eXplainable Data\nAttribution (XDA): With DualDA, we propose efficient and effective DA,\nleveraging Support Vector Machine theory to provide fast and naturally sparse\ndata attributions for AI predictions. We demonstrate that DualDA achieves high\nattribution quality, excels at solving a series of evaluated downstream tasks,\nwhile at the same time improving explanation time by a factor of up to\n4,100,000$\\times$ compared to the original Influence Functions method, and up\nto 11,000$\\times$ compared to the method's most efficient approximation from\nliterature. We further introduce XDA, a method for enhancing Data Attribution\nwith capabilities from feature attribution methods to explain why training\nsamples are relevant for the prediction of a test sample in terms of impactful\nfeatures. Taken together, our contributions in DualXDA ultimately point towards\na future of eXplainable AI applied at unprecedented scale, enabling\ntransparent, efficient and novel analysis of even the largest neural\narchitectures fostering a new generation of accountable AI systems. Code at\nhttps://github.com/gumityolcu/DualXDA.\n","authors":["Galip Ümit Yolcu","Moritz Weckbecker","Thomas Wiegand","Wojciech Samek","Sebastian Lapuschkin"],"pdf_url":"https://arxiv.org/pdf/2402.12118v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17311v2","updated":"2025-07-24T15:12:15Z","published":"2025-07-23T08:29:25Z","title":"EarthLink: A Self-Evolving AI Agent for Climate Science","summary":"  Modern Earth science is at an inflection point. The vast, fragmented, and\ncomplex nature of Earth system data, coupled with increasingly sophisticated\nanalytical demands, creates a significant bottleneck for rapid scientific\ndiscovery. Here we introduce EarthLink, the first AI agent designed as an\ninteractive copilot for Earth scientists. It automates the end-to-end research\nworkflow, from planning and code generation to multi-scenario analysis. Unlike\nstatic diagnostic tools, EarthLink can learn from user interaction,\ncontinuously refining its capabilities through a dynamic feedback loop. We\nvalidated its performance on a number of core scientific tasks of climate\nchange, ranging from model-observation comparisons to the diagnosis of complex\nphenomena. In a multi-expert evaluation, EarthLink produced scientifically\nsound analyses and demonstrated an analytical competency that was rated as\ncomparable to specific aspects of a human junior researcher's workflow.\nAdditionally, its transparent, auditable workflows and natural language\ninterface empower scientists to shift from laborious manual execution to\nstrategic oversight and hypothesis generation. EarthLink marks a pivotal step\ntowards an efficient, trustworthy, and collaborative paradigm for Earth system\nresearch in an era of accelerating global change. The system is accessible at\nour website https://earthlink.intern-ai.org.cn.\n","authors":["Zijie Guo","Jiong Wang","Xiaoyu Yue","Wangxu Wei","Zhe Jiang","Wanghan Xu","Ben Fei","Wenlong Zhang","Xinyu Gu","Lijing Cheng","Jing-Jia Luo","Chao Li","Yaqiang Wang","Tao Chen","Wanli Ouyang","Fenghua Ling","Lei Bai"],"pdf_url":"https://arxiv.org/pdf/2507.17311v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17813v2","updated":"2025-07-24T15:10:45Z","published":"2024-06-24T23:41:46Z","title":"Unsupervised Concept Drift Detection from Deep Learning Representations\n  in Real-time","summary":"  Concept drift is the phenomenon in which the underlying data distributions\nand statistical properties of a target domain change over time, leading to a\ndegradation in model performance. Consequently, production models require\ncontinuous drift detection monitoring. Most drift detection methods to date are\nsupervised, relying on ground-truth labels. However, they are inapplicable in\nmany real-world scenarios, as true labels are often unavailable. Although\nrecent efforts have proposed unsupervised drift detectors, many lack the\naccuracy required for reliable detection or are too computationally intensive\nfor real-time use in high-dimensional, large-scale production environments.\nMoreover, they often fail to characterize or explain drift effectively.\n  To address these limitations, we propose \\textsc{DriftLens}, an unsupervised\nframework for real-time concept drift detection and characterization. Designed\nfor deep learning classifiers handling unstructured data, \\textsc{DriftLens}\nleverages distribution distances in deep learning representations to enable\nefficient and accurate detection. Additionally, it characterizes drift by\nanalyzing and explaining its impact on each label. Our evaluation across\nclassifiers and data-types demonstrates that \\textsc{DriftLens} (i) outperforms\nprevious methods in detecting drift in 15/17 use cases; (ii) runs at least 5\ntimes faster; (iii) produces drift curves that align closely with actual drift\n(correlation $\\geq\\!0.85$); (iv) effectively identifies representative drift\nsamples as explanations.\n","authors":["Salvatore Greco","Bartolomeo Vacchetti","Daniele Apiletti","Tania Cerquitelli"],"pdf_url":"https://arxiv.org/pdf/2406.17813v2.pdf","comment":"Accepted at IEEE Transactions on Knowledge and Data Engineering\n  (TKDE)"},{"id":"http://arxiv.org/abs/2507.18484v1","updated":"2025-07-24T14:56:21Z","published":"2025-07-24T14:56:21Z","title":"Reinforced Embodied Active Defense: Exploiting Adaptive Interaction for\n  Robust Visual Perception in Adversarial 3D Environments","summary":"  Adversarial attacks in 3D environments have emerged as a critical threat to\nthe reliability of visual perception systems, particularly in safety-sensitive\napplications such as identity verification and autonomous driving. These\nattacks employ adversarial patches and 3D objects to manipulate deep neural\nnetwork (DNN) predictions by exploiting vulnerabilities within complex scenes.\nExisting defense mechanisms, such as adversarial training and purification,\nprimarily employ passive strategies to enhance robustness. However, these\napproaches often rely on pre-defined assumptions about adversarial tactics,\nlimiting their adaptability in dynamic 3D settings. To address these\nchallenges, we introduce Reinforced Embodied Active Defense (Rein-EAD), a\nproactive defense framework that leverages adaptive exploration and interaction\nwith the environment to improve perception robustness in 3D adversarial\ncontexts. By implementing a multi-step objective that balances immediate\nprediction accuracy with predictive entropy minimization, Rein-EAD optimizes\ndefense strategies over a multi-step horizon. Additionally, Rein-EAD involves\nan uncertainty-oriented reward-shaping mechanism that facilitates efficient\npolicy updates, thereby reducing computational overhead and supporting\nreal-world applicability without the need for differentiable environments.\nComprehensive experiments validate the effectiveness of Rein-EAD, demonstrating\na substantial reduction in attack success rates while preserving standard\naccuracy across diverse tasks. Notably, Rein-EAD exhibits robust generalization\nto unseen and adaptive attacks, making it suitable for real-world complex\ntasks, including 3D object classification, face recognition and autonomous\ndriving.\n","authors":["Xiao Yang","Lingxuan Wu","Lizhong Wang","Chengyang Ying","Hang Su","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2507.18484v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2404.00540"},{"id":"http://arxiv.org/abs/2507.18476v1","updated":"2025-07-24T14:50:27Z","published":"2025-07-24T14:50:27Z","title":"Automated Code Review Using Large Language Models with Symbolic\n  Reasoning","summary":"  Code review is one of the key processes in the software development lifecycle\nand is essential to maintain code quality. However, manual code review is\nsubjective and time consuming. Given its rule-based nature, code review is well\nsuited for automation. In recent years, significant efforts have been made to\nautomate this process with the help of artificial intelligence. Recent\ndevelopments in Large Language Models (LLMs) have also emerged as a promising\ntool in this area, but these models often lack the logical reasoning\ncapabilities needed to fully understand and evaluate code. To overcome this\nlimitation, this study proposes a hybrid approach that integrates symbolic\nreasoning techniques with LLMs to automate the code review process. We tested\nour approach using the CodexGlue dataset, comparing several models, including\nCodeT5, CodeBERT, and GraphCodeBERT, to assess the effectiveness of combining\nsymbolic reasoning and prompting techniques with LLMs. Our results show that\nthis approach improves the accuracy and efficiency of automated code review.\n","authors":["Busra Icoz","Goksel Biricik"],"pdf_url":"https://arxiv.org/pdf/2507.18476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.16859v2","updated":"2025-07-24T14:41:42Z","published":"2025-07-21T17:22:18Z","title":"Leveraging multi-source and heterogeneous signals for fatigue detection","summary":"  Fatigue detection plays a critical role in safety-critical applications such\nas aviation, mining, and long-haul transport. However, most existing methods\nrely on high-end sensors and controlled environments, limiting their\napplicability in real world settings. This paper formally defines a practical\nyet underexplored problem setting for real world fatigue detection, where\nsystems operating with context-appropriate sensors aim to leverage knowledge\nfrom differently instrumented sources including those using impractical sensors\ndeployed in controlled environments. To tackle this challenge, we propose a\nheterogeneous and multi-source fatigue detection framework that adaptively\nutilizes the available modalities in the target domain while benefiting from\nthe diverse configurations present in source domains. Our experiments,\nconducted using a realistic field-deployed sensor setup and two publicly\navailable datasets, demonstrate the practicality, robustness, and improved\ngeneralization of our approach, paving the practical way for effective fatigue\nmonitoring in sensor-constrained scenarios.\n","authors":["Luobin Cui","Yanlai Wu","Tang Ying","Weikai Li"],"pdf_url":"https://arxiv.org/pdf/2507.16859v2.pdf","comment":"1figures,32pages"},{"id":"http://arxiv.org/abs/2507.18457v1","updated":"2025-07-24T14:37:00Z","published":"2025-07-24T14:37:00Z","title":"Revisiting Physically Realizable Adversarial Object Attack against\n  LiDAR-based Detection: Clarifying Problem Formulation and Experimental\n  Protocols","summary":"  Adversarial robustness in LiDAR-based 3D object detection is a critical\nresearch area due to its widespread application in real-world scenarios. While\nmany digital attacks manipulate point clouds or meshes, they often lack\nphysical realizability, limiting their practical impact. Physical adversarial\nobject attacks remain underexplored and suffer from poor reproducibility due to\ninconsistent setups and hardware differences. To address this, we propose a\ndevice-agnostic, standardized framework that abstracts key elements of physical\nadversarial object attacks, supports diverse methods, and provides open-source\ncode with benchmarking protocols in simulation and real-world settings. Our\nframework enables fair comparison, accelerates research, and is validated by\nsuccessfully transferring simulated attacks to a physical LiDAR system. Beyond\nthe framework, we offer insights into factors influencing attack success and\nadvance understanding of adversarial robustness in real-world LiDAR perception.\n","authors":["Luo Cheng","Hanwei Zhang","Lijun Zhang","Holger Hermanns"],"pdf_url":"https://arxiv.org/pdf/2507.18457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18451v1","updated":"2025-07-24T14:35:16Z","published":"2025-07-24T14:35:16Z","title":"Generation of Synthetic Clinical Text: A Systematic Review","summary":"  Generating clinical synthetic text represents an effective solution for\ncommon clinical NLP issues like sparsity and privacy. This paper aims to\nconduct a systematic review on generating synthetic medical free-text by\nformulating quantitative analysis to three research questions concerning (i)\nthe purpose of generation, (ii) the techniques, and (iii) the evaluation\nmethods. We searched PubMed, ScienceDirect, Web of Science, Scopus, IEEE,\nGoogle Scholar, and arXiv databases for publications associated with generating\nsynthetic medical unstructured free-text. We have identified 94 relevant\narticles out of 1,398 collected ones. A great deal of attention has been given\nto the generation of synthetic medical text from 2018 onwards, where the main\npurpose of such a generation is towards text augmentation, assistive writing,\ncorpus building, privacy-preserving, annotation, and usefulness. Transformer\narchitectures were the main predominant technique used to generate the text,\nespecially the GPTs. On the other hand, there were four main aspects of\nevaluation, including similarity, privacy, structure, and utility, where\nutility was the most frequent method used to assess the generated synthetic\nmedical text. Although the generated synthetic medical text demonstrated a\nmoderate possibility to act as real medical documents in different downstream\nNLP tasks, it has proven to be a great asset as augmented, complementary to the\nreal documents, towards improving the accuracy and overcoming\nsparsity/undersampling issues. Yet, privacy is still a major issue behind\ngenerating synthetic medical text, where more human assessments are needed to\ncheck for the existence of any sensitive information. Despite that, advances in\ngenerating synthetic medical text will considerably accelerate the adoption of\nworkflows and pipeline development, discarding the time-consuming legalities of\ndata transfer.\n","authors":["Basel Alshaikhdeeb","Ahmed Abdelmonem Hemedan","Soumyabrata Ghosh","Irina Balaur","Venkata Satagopam"],"pdf_url":"https://arxiv.org/pdf/2507.18451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18448v1","updated":"2025-07-24T14:33:13Z","published":"2025-07-24T14:33:13Z","title":"Restoring Rhythm: Punctuation Restoration Using Transformer Models for\n  Bangla, a Low-Resource Language","summary":"  Punctuation restoration enhances the readability of text and is critical for\npost-processing tasks in Automatic Speech Recognition (ASR), especially for\nlow-resource languages like Bangla. In this study, we explore the application\nof transformer-based models, specifically XLM-RoBERTa-large, to automatically\nrestore punctuation in unpunctuated Bangla text. We focus on predicting four\npunctuation marks: period, comma, question mark, and exclamation mark across\ndiverse text domains. To address the scarcity of annotated resources, we\nconstructed a large, varied training corpus and applied data augmentation\ntechniques. Our best-performing model, trained with an augmentation factor of\nalpha = 0.20%, achieves an accuracy of 97.1% on the News test set, 91.2% on the\nReference set, and 90.2% on the ASR set.\n  Results show strong generalization to reference and ASR transcripts,\ndemonstrating the model's effectiveness in real-world, noisy scenarios. This\nwork establishes a strong baseline for Bangla punctuation restoration and\ncontributes publicly available datasets and code to support future research in\nlow-resource NLP.\n","authors":["Md Obyedullahil Mamun","Md Adyelullahil Mamun","Arif Ahmad","Md. Imran Hossain Emu"],"pdf_url":"https://arxiv.org/pdf/2507.18448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18442v1","updated":"2025-07-24T14:26:41Z","published":"2025-07-24T14:26:41Z","title":"AraTable: Benchmarking LLMs' Reasoning and Understanding of Arabic\n  Tabular Data","summary":"  The cognitive and reasoning abilities of large language models (LLMs) have\nenabled remarkable progress in natural language processing. However, their\nperformance in interpreting structured data, especially in tabular formats,\nremains limited. Although benchmarks for English tabular data are widely\navailable, Arabic is still underrepresented because of the limited availability\nof public resources and its unique language features. To address this gap, we\npresent AraTable, a novel and comprehensive benchmark designed to evaluate the\nreasoning and understanding capabilities of LLMs when applied to Arabic tabular\ndata. AraTable consists of various evaluation tasks, such as direct question\nanswering, fact verification, and complex reasoning, involving a wide range of\nArabic tabular sources. Our methodology follows a hybrid pipeline, where\ninitial content is generated by LLMs and subsequently filtered and verified by\nhuman experts to ensure high dataset quality. Initial analyses using AraTable\nshow that, while LLMs perform adequately on simpler tabular tasks such as\ndirect question answering, they continue to face significant cognitive\nchallenges when tasks require deeper reasoning and fact verification. This\nindicates that there are substantial opportunities for future work to improve\nperformance on complex tabular reasoning tasks. We also propose a fully\nautomated evaluation framework that uses a self-deliberation mechanism and\nachieves performance nearly identical to that of human judges. This research\nprovides a valuable, publicly available resource and evaluation framework that\ncan help accelerate the development of foundational models for processing and\nanalysing Arabic structured data.\n","authors":["Rana Alshaikh","Israa Alghanmi","Shelan Jeawak"],"pdf_url":"https://arxiv.org/pdf/2507.18442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.20268v2","updated":"2025-07-24T14:21:12Z","published":"2025-05-26T17:44:08Z","title":"Outcome-Based Online Reinforcement Learning: Algorithms and Fundamental\n  Limits","summary":"  Reinforcement learning with outcome-based feedback faces a fundamental\nchallenge: when rewards are only observed at trajectory endpoints, how do we\nassign credit to the right actions? This paper provides the first comprehensive\nanalysis of this problem in online RL with general function approximation. We\ndevelop a provably sample-efficient algorithm achieving $\\widetilde{O}({C_{\\rm\ncov} H^3}/{\\epsilon^2})$ sample complexity, where $C_{\\rm cov}$ is the\ncoverability coefficient of the underlying MDP. By leveraging general function\napproximation, our approach works effectively in large or infinite state spaces\nwhere tabular methods fail, requiring only that value functions and reward\nfunctions can be represented by appropriate function classes. Our results also\ncharacterize when outcome-based feedback is statistically separated from\nper-step rewards, revealing an unavoidable exponential separation for certain\nMDPs. For deterministic MDPs, we show how to eliminate the completeness\nassumption, dramatically simplifying the algorithm. We further extend our\napproach to preference-based feedback settings, proving that equivalent\nstatistical efficiency can be achieved even under more limited information.\nTogether, these results constitute a theoretical foundation for understanding\nthe statistical properties of outcome-based reinforcement learning.\n","authors":["Fan Chen","Zeyu Jia","Alexander Rakhlin","Tengyang Xie"],"pdf_url":"https://arxiv.org/pdf/2505.20268v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12358v4","updated":"2025-07-24T14:14:52Z","published":"2025-03-16T04:53:38Z","title":"IPCGRL: Language-Instructed Reinforcement Learning for Procedural Level\n  Generation","summary":"  Recent research has highlighted the significance of natural language in\nenhancing the controllability of generative models. While various efforts have\nbeen made to leverage natural language for content generation, research on deep\nreinforcement learning (DRL) agents utilizing text-based instructions for\nprocedural content generation remains limited. In this paper, we propose\nIPCGRL, an instruction-based procedural content generation method via\nreinforcement learning, which incorporates a sentence embedding model. IPCGRL\nfine-tunes task-specific embedding representations to effectively compress\ngame-level conditions. We evaluate IPCGRL in a two-dimensional level generation\ntask and compare its performance with a general-purpose embedding method. The\nresults indicate that IPCGRL achieves up to a 21.4% improvement in\ncontrollability and a 17.2% improvement in generalizability for unseen\ninstructions. Furthermore, the proposed method extends the modality of\nconditional input, enabling a more flexible and expressive interaction\nframework for procedural content generation.\n","authors":["In-Chang Baek","Sung-Hyun Kim","Seo-Young Lee","Dong-Hyeon Kim","Kyung-Joong Kim"],"pdf_url":"https://arxiv.org/pdf/2503.12358v4.pdf","comment":"9 pages, 9 figures, 3 tables, accepted to Conference on Games 2025"},{"id":"http://arxiv.org/abs/2506.06874v3","updated":"2025-07-24T14:00:31Z","published":"2025-06-07T17:42:21Z","title":"LLM-D12: A Dual-Dimensional Scale of Instrumental and Relational\n  Dependencies on Large Language Models","summary":"  There is growing interest in understanding how people interact with large\nlanguage models (LLMs) and whether such models elicit dependency or even\naddictive behaviour. Validated tools to assess the extent to which individuals\nmay become dependent on LLMs are scarce and primarily build on classic\nbehavioral addiction symptoms, adapted to the context of LLM use. We view this\nas a conceptual limitation, as the LLM-human relationship is more nuanced and\nwarrants a fresh and distinct perspective. To address this gap, we developed\nand validated a new 12-item questionnaire to measure LLM dependency, referred\nto as LLM-D12. The scale was based on the authors' prior theoretical work, with\nitems developed accordingly and responses collected from 526 participants in\nthe UK. Exploratory and confirmatory factor analyses, performed on separate\nhalves of the total sample using a split-sample approach, supported a\ntwo-factor structure: Instrumental Dependency (six items) and Relationship\nDependency (six items). Instrumental Dependency reflects the extent to which\nindividuals rely on LLMs to support or collaborate in decision-making and\ncognitive tasks. Relationship Dependency captures the tendency to perceive LLMs\nas socially meaningful, sentient, or companion-like entities. The two-factor\nstructure demonstrated excellent internal consistency and clear discriminant\nvalidity. External validation confirmed both the conceptual foundation and the\ndistinction between the two subscales. The psychometric properties and\nstructure of our LLM-D12 scale were interpreted in light of the emerging view\nthat dependency on LLMs does not necessarily indicate dysfunction but may still\nreflect reliance levels that could become problematic in certain contexts.\n","authors":["Ala Yankouskaya","Areej B. Babiker","Syeda W. F. Rizvi","Sameha Alshakhsi","Magnus Liebherr","Raian Ali"],"pdf_url":"https://arxiv.org/pdf/2506.06874v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18413v1","updated":"2025-07-24T13:53:49Z","published":"2025-07-24T13:53:49Z","title":"GPU Accelerated Compact-Table Propagation","summary":"  Constraint Programming developed within Logic Programming in the Eighties;\nnowadays all Prolog systems encompass modules capable of handling constraint\nprogramming on finite domains demanding their solution to a constraint solver.\nThis work focuses on a specific form of constraint, the so-called table\nconstraint, used to specify conditions on the values of variables as an\nenumeration of alternative options. Since every condition on a set of finite\ndomain variables can be ultimately expressed as a finite set of cases, Table\ncan, in principle, simulate any other constraint. These characteristics make\nTable one of the most studied constraints ever, leading to a series of\nincreasingly efficient propagation algorithms. Despite this, it is not uncommon\nto encounter real-world problems with hundreds or thousands of valid cases that\nare simply too many to be handled effectively with standard CPU-based\napproaches. In this paper, we deal with the Compact-Table (CT) algorithm, the\nstate-of-the-art propagation algorithms for Table. We describe how CT can be\nenhanced by exploiting the massive computational power offered by modern GPUs\nto handle large Table constraints. In particular, we report on the design and\nimplementation of GPU-accelerated CT, on its integration into an existing\nconstraint solver, and on an experimental validation performed on a significant\nset of instances.\n","authors":["Enrico Santi","Fabio Tardivo","Agostino Dovier","Andrea Formisano"],"pdf_url":"https://arxiv.org/pdf/2507.18413v1.pdf","comment":"Under consideration in Theory and Practice of Logic Programming\n  (TPLP)"},{"id":"http://arxiv.org/abs/2507.07893v3","updated":"2025-07-24T13:52:51Z","published":"2025-07-10T16:22:41Z","title":"An Integrated Framework of Prompt Engineering and Multidimensional\n  Knowledge Graphs for Legal Dispute Analysis","summary":"  Legal dispute analysis is crucial for intelligent legal assistance systems.\nHowever, current LLMs face significant challenges in understanding complex\nlegal concepts, maintaining reasoning consistency, and accurately citing legal\nsources. This research presents a framework combining prompt engineering with\nmultidimensional knowledge graphs to improve LLMs' legal dispute analysis.\nSpecifically, the framework includes a three-stage hierarchical prompt\nstructure (task definition, knowledge background, reasoning guidance) along\nwith a three-layer knowledge graph (legal ontology, representation, instance\nlayers). Additionally, four supporting methods enable precise legal concept\nretrieval: direct code matching, semantic vector similarity, ontology path\nreasoning, and lexical segmentation. Through extensive testing, results show\nmajor improvements: sensitivity increased by 9.9%-13.8%, specificity by\n4.8%-6.7%, and citation accuracy by 22.4%-39.7%. As a result, the framework\nprovides better legal analysis and understanding of judicial logic, thus\noffering a new technical method for intelligent legal assistance systems.\n","authors":["Mingda Zhang","Na Zhao","Jianglong Qing","Qing xu","Kaiwen Pan","Ting luo"],"pdf_url":"https://arxiv.org/pdf/2507.07893v3.pdf","comment":"19 pages,3 figures"},{"id":"http://arxiv.org/abs/2502.15487v3","updated":"2025-07-24T13:47:56Z","published":"2025-02-21T14:23:14Z","title":"ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models","summary":"  Large Language Models (LLMs) are increasingly used in tasks requiring\ninterpretive and inferential accuracy. In this paper, we introduce ExpliCa, a\nnew dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely\nintegrates both causal and temporal relations presented in different linguistic\norders and explicitly expressed by linguistic connectives. The dataset is\nenriched with crowdsourced human acceptability ratings. We tested LLMs on\nExpliCa through prompting and perplexity-based metrics. We assessed seven\ncommercial and open-source LLMs, revealing that even top models struggle to\nreach 0.80 accuracy. Interestingly, models tend to confound temporal relations\nwith causal ones, and their performance is also strongly influenced by the\nlinguistic order of the events. Finally, perplexity-based scores and prompting\nperformance are differently affected by model size.\n","authors":["Martina Miliani","Serena Auriemma","Alessandro Bondielli","Emmanuele Chersoni","Lucia Passaro","Irene Sucameli","Alessandro Lenci"],"pdf_url":"https://arxiv.org/pdf/2502.15487v3.pdf","comment":"Accepted for publication in Findings of ACL 2025"},{"id":"http://arxiv.org/abs/2507.18398v1","updated":"2025-07-24T13:31:38Z","published":"2025-07-24T13:31:38Z","title":"Optimising Call Centre Operations using Reinforcement Learning: Value\n  Iteration versus Proximal Policy Optimisation","summary":"  This paper investigates the application of Reinforcement Learning (RL) to\noptimise call routing in call centres to minimise client waiting time and staff\nidle time. Two methods are compared: a model-based approach using Value\nIteration (VI) under known system dynamics, and a model-free approach using\nProximal Policy Optimisation (PPO) that learns from experience. For the\nmodel-based approach, a theoretical model is used, while a simulation model\ncombining Discrete Event Simulation (DES) with the OpenAI Gym environment is\ndeveloped for model-free learning. Both models frame the problem as a Markov\nDecision Process (MDP) within a Skills-Based Routing (SBR) framework, with\nPoisson client arrivals and exponentially distributed service and abandonment\ntimes. For policy evaluation, random, VI, and PPO policies are evaluated using\nthe simulation model. After 1,000 test episodes, PPO consistently achives the\nhighest rewards, along with the lowest client waiting time and staff idle time,\ndespite requiring longer training time.\n","authors":["Kwong Ho Li","Wathsala Karunarathne"],"pdf_url":"https://arxiv.org/pdf/2507.18398v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2507.15292v4","updated":"2025-07-24T13:26:19Z","published":"2025-07-21T06:47:44Z","title":"EndoControlMag: Robust Endoscopic Vascular Motion Magnification with\n  Periodic Reference Resetting and Hierarchical Tissue-aware Dual-Mask Control","summary":"  Visualizing subtle vascular motions in endoscopic surgery is crucial for\nsurgical precision and decision-making, yet remains challenging due to the\ncomplex and dynamic nature of surgical scenes. To address this, we introduce\nEndoControlMag, a training-free, Lagrangian-based framework with\nmask-conditioned vascular motion magnification tailored to endoscopic\nenvironments. Our approach features two key modules: a Periodic Reference\nResetting (PRR) scheme that divides videos into short overlapping clips with\ndynamically updated reference frames to prevent error accumulation while\nmaintaining temporal coherence, and a Hierarchical Tissue-aware Magnification\n(HTM) framework with dual-mode mask dilation. HTM first tracks vessel cores\nusing a pretrained visual tracking model to maintain accurate localization\ndespite occlusions and view changes. It then applies one of two adaptive\nsoftening strategies to surrounding tissues: motion-based softening that\nmodulates magnification strength proportional to observed tissue displacement,\nor distance-based exponential decay that simulates biomechanical force\nattenuation. This dual-mode approach accommodates diverse surgical\nscenarios-motion-based softening excels with complex tissue deformations while\ndistance-based softening provides stability during unreliable optical flow\nconditions. We evaluate EndoControlMag on our EndoVMM24 dataset spanning four\ndifferent surgery types and various challenging scenarios, including\nocclusions, instrument disturbance, view changes, and vessel deformations.\nQuantitative metrics, visual assessments, and expert surgeon evaluations\ndemonstrate that EndoControlMag significantly outperforms existing methods in\nboth magnification accuracy and visual quality while maintaining robustness\nacross challenging surgical conditions. The code, dataset, and video results\nare available at https://szupc.github.io/EndoControlMag/.\n","authors":["An Wang","Rulin Zhou","Mengya Xu","Yiru Ye","Longfei Gou","Yiting Chang","Hao Chen","Chwee Ming Lim","Jiankun Wang","Hongliang Ren"],"pdf_url":"https://arxiv.org/pdf/2507.15292v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11937v4","updated":"2025-07-24T13:24:21Z","published":"2025-03-15T01:06:34Z","title":"Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I\n  Diffusion Adapter via Conditional Variational Autoencoder","summary":"  Text-to-Image (T2I) Diffusion Models have achieved remarkable performance in\ngenerating high quality images. However, enabling precise control of continuous\nattributes, especially multiple attributes simultaneously, in a new domain\n(e.g., numeric values like eye openness or car width) with text-only guidance\nremains a significant challenge. To address this, we introduce the Attribute\n(Att) Adapter, a novel plug-and-play module designed to enable fine-grained,\nmulti-attributes control in pretrained diffusion models. Our approach learns a\nsingle control adapter from a set of sample images that can be unpaired and\ncontain multiple visual attributes. The Att-Adapter leverages the decoupled\ncross attention module to naturally harmonize the multiple domain attributes\nwith text conditioning. We further introduce Conditional Variational\nAutoencoder (CVAE) to the Att-Adapter to mitigate overfitting, matching the\ndiverse nature of the visual world. Evaluations on two public datasets show\nthat Att-Adapter outperforms all LoRA-based baselines in controlling continuous\nattributes. Additionally, our method enables a broader control range and also\nimproves disentanglement across multiple attributes, surpassing StyleGAN-based\ntechniques. Notably, Att-Adapter is flexible, requiring no paired synthetic\ndata for training, and is easily scalable to multiple attributes within a\nsingle model.\n","authors":["Wonwoong Cho","Yan-Ying Chen","Matthew Klenk","David I. Inouye","Yanxia Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.11937v4.pdf","comment":"ICCV'25 (Highlight), The project page is available at\n  https://tri-mac.github.io/att-adapter/"},{"id":"http://arxiv.org/abs/2507.18392v1","updated":"2025-07-24T13:15:21Z","published":"2025-07-24T13:15:21Z","title":"CLEAR: Error Analysis via LLM-as-a-Judge Made Easy","summary":"  The evaluation of Large Language Models (LLMs) increasingly relies on other\nLLMs acting as judges. However, current evaluation paradigms typically yield a\nsingle score or ranking, answering which model is better but not why. While\nessential for benchmarking, these top-level scores obscure the specific,\nactionable reasons behind a model's performance. To bridge this gap, we\nintroduce CLEAR, an interactive, open-source package for LLM-based error\nanalysis. CLEAR first generates per-instance textual feedback, then it creates\na set of system-level error issues, and quantifies the prevalence of each\nidentified issue. Our package also provides users with an interactive dashboard\nthat allows for a comprehensive error analysis through aggregate\nvisualizations, applies interactive filters to isolate specific issues or score\nranges, and drills down to the individual instances that exemplify a particular\nbehavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks,\nand showcase its utility through a user case study.\n","authors":["Asaf Yehudai","Lilach Eden","Yotam Perlitz","Roy Bar-Haim","Michal Shmueli-Scheuer"],"pdf_url":"https://arxiv.org/pdf/2507.18392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18391v1","updated":"2025-07-24T13:14:25Z","published":"2025-07-24T13:14:25Z","title":"Revisiting LLM Reasoning via Information Bottleneck","summary":"  Large language models (LLMs) have recently demonstrated remarkable progress\nin reasoning capabilities through reinforcement learning with verifiable\nrewards (RLVR). By leveraging simple rule-based rewards, RL effectively\nincentivizes LLMs to produce extended chain-of-thought (CoT) reasoning\ntrajectories, progressively guiding them toward correct answers. However,\nexisting approaches remain largely heuristic and intuition-driven, limiting the\ndevelopment of principled methodologies. In this paper, we present a\ntheoretical characterization of LLM reasoning grounded in information\nbottleneck (IB) principle, introducing IB-aware reasoning optimization (IBRO),\na framework that encourages reasoning trajectories to be both informative about\nthe final correct answer and generalizable across diverse prompts. We derive a\npractical token-level surrogate objective and propose an efficient\napproximation, resulting in the lightweight IB regularization method. This\ntechnique integrates seamlessly into existing RL-based post-training frameworks\nwithout additional computational overhead, requiring only a one-line code\nmodification. Empirically, we validate IB regularization across multiple\nmathematical reasoning benchmarks and RL algorithms, demonstrating consistent\nimprovements in LLM reasoning performance.\n","authors":["Shiye Lei","Zhihao Cheng","Kai Jia","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2507.18391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.23276v2","updated":"2025-07-24T13:13:24Z","published":"2025-06-29T15:02:47Z","title":"Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in\n  Public Goods Games","summary":"  As large language models (LLMs) are increasingly deployed as autonomous\nagents, understanding their cooperation and social mechanisms is becoming\nincreasingly important. In particular, how LLMs balance self-interest and\ncollective well-being is a critical challenge for ensuring alignment,\nrobustness, and safe deployment. In this paper, we examine the challenge of\ncostly sanctioning in multi-agent LLM systems, where an agent must decide\nwhether to invest its own resources to incentivize cooperation or penalize\ndefection. To study this, we adapt a public goods game with institutional\nchoice from behavioral economics, allowing us to observe how different LLMs\nnavigate social dilemmas over repeated interactions. Our analysis reveals four\ndistinct behavioral patterns among models: some consistently establish and\nsustain high levels of cooperation, others fluctuate between engagement and\ndisengagement, some gradually decline in cooperative behavior over time, and\nothers rigidly follow fixed strategies regardless of outcomes. Surprisingly, we\nfind that reasoning LLMs, such as the o1 series, struggle significantly with\ncooperation, whereas some traditional LLMs consistently achieve high levels of\ncooperation. These findings suggest that the current approach to improving\nLLMs, which focuses on enhancing their reasoning capabilities, does not\nnecessarily lead to cooperation, providing valuable insights for deploying LLM\nagents in environments that require sustained collaboration. Our code is\navailable at https://github.com/davidguzmanp/SanctSim\n","authors":["David Guzman Piedrahita","Yongjin Yang","Mrinmaya Sachan","Giorgia Ramponi","Bernhard Schölkopf","Zhijing Jin"],"pdf_url":"https://arxiv.org/pdf/2506.23276v2.pdf","comment":"Published at COLM 2025"},{"id":"http://arxiv.org/abs/2504.21706v3","updated":"2025-07-24T12:57:18Z","published":"2025-04-30T14:50:02Z","title":"Vision Transformers in Precision Agriculture: A Comprehensive Survey","summary":"  Detecting plant diseases is a crucial aspect of modern agriculture, as it\nplays a key role in maintaining crop health and increasing overall yield.\nTraditional approaches, though still valuable, often rely on manual inspection\nor conventional machine learning techniques, both of which face limitations in\nscalability and accuracy. Recently, Vision Transformers (ViTs) have emerged as\na promising alternative, offering advantages such as improved handling of\nlong-range dependencies and better scalability for visual tasks. This review\nexplores the application of ViTs in precision agriculture, covering a range of\ntasks. We begin by introducing the foundational architecture of ViTs and\ndiscussing their transition from Natural Language Processing (NLP) to Computer\nVision. The discussion includes the concept of inductive bias in traditional\nmodels like Convolutional Neural Networks (CNNs), and how ViTs mitigate these\nbiases. We provide a comprehensive review of recent literature, focusing on key\nmethodologies, datasets, and performance metrics. This study also includes a\ncomparative analysis of CNNs and ViTs, along with a review of hybrid models and\nperformance enhancements. Technical challenges such as data requirements,\ncomputational demands, and model interpretability are addressed, along with\npotential solutions. Finally, we outline future research directions and\ntechnological advancements that could further support the integration of ViTs\nin real-world agricultural settings. Our goal with this study is to offer\npractitioners and researchers a deeper understanding of how ViTs are poised to\ntransform smart and precision agriculture.\n","authors":["Saber Mehdipour","Seyed Abolghasem Mirroshandel","Seyed Amirhossein Tabatabaei"],"pdf_url":"https://arxiv.org/pdf/2504.21706v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.03586v4","updated":"2025-07-24T12:56:07Z","published":"2025-06-04T05:33:33Z","title":"Beamforming and Resource Allocation for Delay Minimization in\n  RIS-Assisted OFDM Systems","summary":"  This paper investigates a joint beamforming and resource allocation problem\nin downlink reconfigurable intelligent surface (RIS)-assisted orthogonal\nfrequency division multiplexing (OFDM) systems to minimize the average delay,\nwhere data packets for each user arrive at the base station (BS)\nstochastically. The sequential optimization problem is inherently a Markov\ndecision process (MDP), thus falling within the remit of reinforcement\nlearning. To effectively handle the mixed action space and reduce the state\nspace dimensionality, a hybrid deep reinforcement learning (DRL) approach is\nproposed. Specifically, proximal policy optimization (PPO)-Theta is employed to\noptimize the RIS phase shift design, while PPO-N is responsible for subcarrier\nallocation decisions. The active beamforming at the BS is then derived from the\njointly optimized RIS phase shifts and subcarrier allocation decisions. To\nfurther mitigate the curse of dimensionality associated with subcarrier\nallocation, a multi-agent strategy is introduced to optimize the subcarrier\nallocation indicators more efficiently. Moreover, to achieve more adaptive\nresource allocation and accurately capture the network dynamics, key factors\nclosely related to average delay, such as the number of backlogged packets in\nbuffers and current packet arrivals, are incorporated into the state space.\nFurthermore, a transfer learning framework is introduced to enhance the\ntraining efficiency and accelerate convergence. Simulation results demonstrate\nthat the proposed algorithm significantly reduces the average delay, enhances\nresource allocation efficiency, and achieves superior system robustness and\nfairness compared to baseline methods.\n","authors":["Yu Ma","Xiao Li","Chongtao Guo","Le Liang","Michail Matthaiou","Shi Jin"],"pdf_url":"https://arxiv.org/pdf/2506.03586v4.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2507.18368v1","updated":"2025-07-24T12:47:29Z","published":"2025-07-24T12:47:29Z","title":"Reasoning Beyond the Obvious: Evaluating Divergent and Convergent\n  Thinking in LLMs for Financial Scenarios","summary":"  Most reasoning benchmarks for LLMs emphasize factual accuracy or step-by-step\nlogic. In finance, however, professionals must not only converge on optimal\ndecisions but also generate creative, plausible futures under uncertainty. We\nintroduce ConDiFi, a benchmark that jointly evaluates divergent and convergent\nthinking in LLMs for financial tasks.\n  ConDiFi features 607 macro-financial prompts for divergent reasoning and 990\nmulti-hop adversarial MCQs for convergent reasoning. Using this benchmark, we\nevaluated 14 leading models and uncovered striking differences. Despite high\nfluency, GPT-4o underperforms on Novelty and Actionability. In contrast, models\nlike DeepSeek-R1 and Cohere Command R+ rank among the top for generating\nactionable, insights suitable for investment decisions. ConDiFi provides a new\nperspective to assess reasoning capabilities essential to safe and strategic\ndeployment of LLMs in finance.\n","authors":["Zhuang Qiang Bok","Watson Wei Khong Chua"],"pdf_url":"https://arxiv.org/pdf/2507.18368v1.pdf","comment":"Accepted by Agentic & GenAI Evaluation KDD2025: KDD workshop on\n  Evaluation and Trustworthiness of Agentic and Generative AI Models\n  https://kdd-eval-workshop.github.io/genai-evaluation-kdd2025/"},{"id":"http://arxiv.org/abs/2507.17347v2","updated":"2025-07-24T12:46:21Z","published":"2025-07-23T09:28:25Z","title":"Swin-TUNA : A Novel PEFT Approach for Accurate Food Image Segmentation","summary":"  In the field of food image processing, efficient semantic segmentation\ntechniques are crucial for industrial applications. However, existing\nlarge-scale Transformer-based models (such as FoodSAM) face challenges in\nmeeting practical deploymentrequirements due to their massive parameter counts\nand high computational resource demands. This paper introduces TUNable Adapter\nmodule (Swin-TUNA), a Parameter Efficient Fine-Tuning (PEFT) method that\nintegrates multiscale trainable adapters into the Swin Transformer\narchitecture, achieving high-performance food image segmentation by updating\nonly 4% of the parameters. The core innovation of Swin-TUNA lies in its\nhierarchical feature adaptation mechanism: it designs separable convolutions in\ndepth and dimensional mappings of varying scales to address the differences in\nfeatures between shallow and deep networks, combined with a dynamic balancing\nstrategy for tasks-agnostic and task-specific features. Experiments demonstrate\nthat this method achieves mIoU of 50.56% and 74.94% on the FoodSeg103 and\nUECFoodPix Complete datasets, respectively, surpassing the fully parameterized\nFoodSAM model while reducing the parameter count by 98.7% (to only 8.13M).\nFurthermore, Swin-TUNA exhibits faster convergence and stronger generalization\ncapabilities in low-data scenarios, providing an efficient solution for\nassembling lightweight food image.\n","authors":["Haotian Chen","Zhiyong Xiao"],"pdf_url":"https://arxiv.org/pdf/2507.17347v2.pdf","comment":"After discussion among the authors, some parts of the paper are\n  deemed inappropriate and will be revised and resubmitted"},{"id":"http://arxiv.org/abs/2507.08017v3","updated":"2025-07-24T12:23:53Z","published":"2025-07-07T20:26:31Z","title":"Mechanistic Indicators of Understanding in Large Language Models","summary":"  Recent findings in mechanistic interpretability (MI), the field probing the\ninner workings of Large Language Models (LLMs), challenge the view that these\nmodels rely solely on superficial statistics. We offer an accessible synthesis\nof these findings that doubles as an introduction to MI while integrating these\nfindings within a novel theoretical framework for thinking about machine\nunderstanding. We argue that LLMs develop internal structures that are\nfunctionally analogous to the kind of understanding that consists in seeing\nconnections. To sharpen this idea, we propose a three-tiered conception of\nunderstanding. First, conceptual understanding emerges when a model forms\n\"features\" as directions in latent space, learning the connections between\ndiverse manifestations of something. Second, state-of-the-world understanding\nemerges when a model learns contingent factual connections between features and\ndynamically tracks changes in the world. Third, principled understanding\nemerges when a model ceases to rely on a collection of memorized facts and\ndiscovers a \"circuit\" connecting these facts. However, these forms of\nunderstanding remain radically different from human understanding, as the\nphenomenon of \"parallel mechanisms\" shows. We conclude that the debate should\nmove beyond the yes-or-no question of whether LLMs understand to investigate\nhow their strange minds work and forge conceptions that fit them.\n","authors":["Pierre Beckmann","Matthieu Queloz"],"pdf_url":"https://arxiv.org/pdf/2507.08017v3.pdf","comment":"32 pages"},{"id":"http://arxiv.org/abs/2507.18337v1","updated":"2025-07-24T12:08:49Z","published":"2025-07-24T12:08:49Z","title":"The AlphaPhysics Term Rewriting System for Marking Algebraic Expressions\n  in Physics Exams","summary":"  We present our method for automatically marking Physics exams. The marking\nproblem consists in assessing typed student answers for correctness with\nrespect to a ground truth solution. This is a challenging problem that we seek\nto tackle using a combination of a computer algebra system, an SMT solver and a\nterm rewriting system. A Large Language Model is used to interpret and remove\nerrors from student responses and rewrite these in a machine readable format.\nOnce formalized and language-aligned, the next step then consists in applying\nautomated reasoning techniques for assessing student solution correctness. We\nconsider two methods of automated theorem proving: off-the-shelf SMT solving\nand term rewriting systems tailored for physics problems involving\ntrigonometric expressions. The development of the term rewrite system and\nestablishing termination and confluence properties was not trivial, and we\ndescribe it in some detail in the paper. We evaluate our system on a rich pool\nof over 1500 real-world student exam responses from the 2023 Australian Physics\nOlympiad.\n","authors":["Peter Baumgartner","Lachlan McGinness"],"pdf_url":"https://arxiv.org/pdf/2507.18337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18334v1","updated":"2025-07-24T12:05:17Z","published":"2025-07-24T12:05:17Z","title":"Improving Bird Classification with Primary Color Additives","summary":"  We address the problem of classifying bird species using their song\nrecordings, a challenging task due to environmental noise, overlapping\nvocalizations, and missing labels. Existing models struggle with low-SNR or\nmulti-species recordings. We hypothesize that birds can be classified by\nvisualizing their pitch pattern, speed, and repetition, collectively called\nmotifs. Deep learning models applied to spectrogram images help, but similar\nmotifs across species cause confusion. To mitigate this, we embed frequency\ninformation into spectrograms using primary color additives. This enhances\nspecies distinction and improves classification accuracy. Our experiments show\nthat the proposed approach achieves statistically significant gains over models\nwithout colorization and surpasses the BirdCLEF 2024 winner, improving F1 by\n7.3%, ROC-AUC by 6.2%, and CMAP by 6.6%. These results demonstrate the\neffectiveness of incorporating frequency information via colorization.\n","authors":["Ezhini Rasendiran R","Chandresh Kumar Maurya"],"pdf_url":"https://arxiv.org/pdf/2507.18334v1.pdf","comment":"5 pages (Accepted to Interspeech 2025)"},{"id":"http://arxiv.org/abs/2504.13101v3","updated":"2025-07-24T11:53:07Z","published":"2025-04-17T17:10:33Z","title":"Position: An Empirically Grounded Identifiability Theory Will Accelerate\n  Self-Supervised Learning Research","summary":"  Self-Supervised Learning (SSL) powers many current AI systems. As research\ninterest and investment grow, the SSL design space continues to expand. The\nPlatonic view of SSL, following the Platonic Representation Hypothesis (PRH),\nsuggests that despite different methods and engineering approaches, all\nrepresentations converge to the same Platonic ideal. However, this phenomenon\nlacks precise theoretical explanation. By synthesizing evidence from\nIdentifiability Theory (IT), we show that the PRH can emerge in SSL. However,\ncurrent IT cannot explain SSL's empirical success. To bridge the gap between\ntheory and practice, we propose expanding IT into what we term Singular\nIdentifiability Theory (SITh), a broader theoretical framework encompassing the\nentire SSL pipeline. SITh would allow deeper insights into the implicit data\nassumptions in SSL and advance the field towards learning more interpretable\nand generalizable representations. We highlight three critical directions for\nfuture research: 1) training dynamics and convergence properties of SSL; 2) the\nimpact of finite samples, batch size, and data diversity; and 3) the role of\ninductive biases in architecture, augmentations, initialization schemes, and\noptimizers.\n","authors":["Patrik Reizinger","Randall Balestriero","David Klindt","Wieland Brendel"],"pdf_url":"https://arxiv.org/pdf/2504.13101v3.pdf","comment":"ICML2025 camera ready"},{"id":"http://arxiv.org/abs/2507.18326v1","updated":"2025-07-24T11:51:55Z","published":"2025-07-24T11:51:55Z","title":"A Concept for Efficient Scalability of Automated Driving Allowing for\n  Technical, Legal, Cultural, and Ethical Differences","summary":"  Efficient scalability of automated driving (AD) is key to reducing costs,\nenhancing safety, conserving resources, and maximizing impact. However,\nresearch focuses on specific vehicles and context, while broad deployment\nrequires scalability across various configurations and environments.\nDifferences in vehicle types, sensors, actuators, but also traffic regulations,\nlegal requirements, cultural dynamics, or even ethical paradigms demand high\nflexibility of data-driven developed capabilities. In this paper, we address\nthe challenge of scalable adaptation of generic capabilities to desired systems\nand environments. Our concept follows a two-stage fine-tuning process. In the\nfirst stage, fine-tuning to the specific environment takes place through a\ncountry-specific reward model that serves as an interface between technological\nadaptations and socio-political requirements. In the second stage,\nvehicle-specific transfer learning facilitates system adaptation and governs\nthe validation of design decisions. In sum, our concept offers a data-driven\nprocess that integrates both technological and socio-political aspects,\nenabling effective scalability across technical, legal, cultural, and ethical\ndifferences.\n","authors":["Lars Ullrich","Michael Buchholz","Jonathan Petit","Klaus Dietmayer","Knut Graichen"],"pdf_url":"https://arxiv.org/pdf/2507.18326v1.pdf","comment":"Accepted to be published at 2025 28th IEEE International Conference\n  on Intelligent Transportation Systems (ITSC), Gold Coast, Australia, November\n  18-21, 2025"},{"id":"http://arxiv.org/abs/2507.18323v1","updated":"2025-07-24T11:49:46Z","published":"2025-07-24T11:49:46Z","title":"A Multi-Dataset Benchmark for Semi-Supervised Semantic Segmentation in\n  ECG Delineation","summary":"  Electrocardiogram (ECG) delineation, the segmentation of meaningful waveform\nfeatures, is critical for clinical diagnosis. Despite recent advances using\ndeep learning, progress has been limited by the scarcity of publicly available\nannotated datasets. Semi-supervised learning presents a promising solution by\nleveraging abundant unlabeled ECG data. In this study, we present the first\nsystematic benchmark for semi-supervised semantic segmentation (SemiSeg) in ECG\ndelineation. We curated and unified multiple public datasets, including\npreviously underused sources, to support robust and diverse evaluation. We\nadopted five representative SemiSeg algorithms from computer vision,\nimplemented them on two different architectures: the convolutional network and\nthe transformer, and evaluated them in two different settings: in-domain and\ncross-domain. Additionally, we propose ECG-specific training configurations and\naugmentation strategies and introduce a standardized evaluation framework. Our\nresults show that the transformer outperforms the convolutional network in\nsemi-supervised ECG delineation. We anticipate that our benchmark will serve as\na foundation for advancing semi-supervised ECG delineation methods and will\nfacilitate further research in this domain.\n","authors":["Minje Park","Jeonghwa Lim","Taehyung Yu","Sunghoon Joo"],"pdf_url":"https://arxiv.org/pdf/2507.18323v1.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2507.08621v2","updated":"2025-07-24T11:49:06Z","published":"2025-07-11T14:23:40Z","title":"A comprehensive study of LLM-based argument classification: from LLAMA\n  through GPT-4o to Deepseek-R1","summary":"  Argument mining (AM) is an interdisciplinary research field that integrates\ninsights from logic, philosophy, linguistics, rhetoric, law, psychology, and\ncomputer science. It involves the automatic identification and extraction of\nargumentative components, such as premises and claims, and the detection of\nrelationships between them, such as support, attack, or neutrality. Recently,\nthe field has advanced significantly, especially with the advent of large\nlanguage models (LLMs), which have enhanced the efficiency of analyzing and\nextracting argument semantics compared to traditional methods and other deep\nlearning models. There are many benchmarks for testing and verifying the\nquality of LLM, but there is still a lack of research and results on the\noperation of these models in publicly available argument classification\ndatabases. This paper presents a study of a selection of LLM's, using diverse\ndatasets such as Args.me and UKP. The models tested include versions of GPT,\nLlama, and DeepSeek, along with reasoning-enhanced variants incorporating the\nChain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms\nthe others in the argument classification benchmarks. In case of models\nincorporated with reasoning capabilities, the Deepseek-R1 shows its\nsuperiority. However, despite their superiority, GPT-4o and Deepseek-R1 still\nmake errors. The most common errors are discussed for all models. To our\nknowledge, the presented work is the first broader analysis of the mentioned\ndatasets using LLM and prompt algorithms. The work also shows some weaknesses\nof known prompt algorithms in argument analysis, while indicating directions\nfor their improvement. The added value of the work is the in-depth analysis of\nthe available argument datasets and the demonstration of their shortcomings.\n","authors":["Marcin Pietroń","Rafał Olszowski","Jakub Gomułka","Filip Gampel","Andrzej Tomski"],"pdf_url":"https://arxiv.org/pdf/2507.08621v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12102v3","updated":"2025-07-24T11:44:19Z","published":"2023-12-19T12:26:57Z","title":"I-CEE: Tailoring Explanations of Image Classification Models to User\n  Expertise","summary":"  Effectively explaining decisions of black-box machine learning models is\ncritical to responsible deployment of AI systems that rely on them. Recognizing\ntheir importance, the field of explainable AI (XAI) provides several techniques\nto generate these explanations. Yet, there is relatively little emphasis on the\nuser (the explainee) in this growing body of work and most XAI techniques\ngenerate \"one-size-fits-all\" explanations. To bridge this gap and achieve a\nstep closer towards human-centered XAI, we present I-CEE, a framework that\nprovides Image Classification Explanations tailored to User Expertise. Informed\nby existing work, I-CEE explains the decisions of image classification models\nby providing the user with an informative subset of training data (i.e.,\nexample images), corresponding local explanations, and model decisions.\nHowever, unlike prior work, I-CEE models the informativeness of the example\nimages to depend on user expertise, resulting in different examples for\ndifferent users. We posit that by tailoring the example set to user expertise,\nI-CEE can better facilitate users' understanding and simulatability of the\nmodel. To evaluate our approach, we conduct detailed experiments in both\nsimulation and with human participants (N = 100) on multiple datasets.\nExperiments with simulated users show that I-CEE improves users' ability to\naccurately predict the model's decisions (simulatability) compared to\nbaselines, providing promising preliminary results. Experiments with human\nparticipants demonstrate that our method significantly improves user\nsimulatability accuracy, highlighting the importance of human-centered XAI\n","authors":["Yao Rong","Peizhu Qian","Vaibhav Unhelkar","Enkelejda Kasneci"],"pdf_url":"https://arxiv.org/pdf/2312.12102v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14019v3","updated":"2025-07-24T11:38:53Z","published":"2024-12-18T16:37:51Z","title":"Retrieving Classes of Causal Orders with Inconsistent Knowledge Bases","summary":"  Traditional causal discovery methods often rely on strong, untestable\nassumptions, which makes them unreliable in real applications. In this context,\nLarge Language Models (LLMs) have emerged as a promising alternative for\nextracting causal knowledge from text-based metadata, which consolidates domain\nexpertise. However, LLMs tend to be unreliable and prone to hallucinations,\nnecessitating strategies that account for their limitations. One effective\nstrategy is to use a consistency measure to assess reliability. Additionally,\nmost text metadata does not clearly distinguish direct causal relationships\nfrom indirect ones, further complicating the discovery of a causal DAG. As a\nresult, focusing on causal orders, rather than causal DAGs, emerges as a more\npractical and robust approach. We present a new method to derive a class of\nacyclic tournaments, which represent plausible causal orders, maximizing a\nconsistency score derived from an LLM. Our approach starts by calculating\npairwise consistency scores between variables, resulting in a semi-complete\npartially directed graph that consolidates these scores into an abstraction of\nthe maximally consistent causal orders. Using this structure, we identify\noptimal acyclic tournaments, focusing on those that maximize consistency across\nall configurations. We subsequently show how both the abstraction and the class\nof causal orders can be used to estimate causal effects. We tested our method\non both well-established benchmarks, as well as, real-world datasets from\nepidemiology and public health. Our results demonstrate the effectiveness of\nour approach in recovering the correct causal order.\n","authors":["Federico Baldo","Simon Ferreira","Charles K. Assaad"],"pdf_url":"https://arxiv.org/pdf/2412.14019v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12193v2","updated":"2025-07-24T11:36:31Z","published":"2024-10-16T03:29:33Z","title":"Differentiable Motion Manifold Primitives for Reactive Motion Generation\n  under Kinodynamic Constraints","summary":"  Real-time motion generation -- which is essential for achieving reactive and\nadaptive behavior -- under kinodynamic constraints for high-dimensional systems\nis a crucial yet challenging problem. We address this with a two-step approach:\noffline learning of a lower-dimensional trajectory manifold of task-relevant,\nconstraint-satisfying trajectories, followed by rapid online search within this\nmanifold. Extending the discrete-time Motion Manifold Primitives (MMP)\nframework, we propose Differentiable Motion Manifold Primitives (DMMP), a novel\nneural network architecture that encodes and generates continuous-time,\ndifferentiable trajectories, trained using data collected offline through\ntrajectory optimizations, with a strategy that ensures constraint satisfaction\n-- absent in existing methods. Experiments on dynamic throwing with a 7-DoF\nrobot arm demonstrate that DMMP outperforms prior methods in planning speed,\ntask success, and constraint satisfaction.\n","authors":["Yonghyeon Lee"],"pdf_url":"https://arxiv.org/pdf/2410.12193v2.pdf","comment":"6 pages and 9 figures"},{"id":"http://arxiv.org/abs/2507.18302v1","updated":"2025-07-24T11:18:27Z","published":"2025-07-24T11:18:27Z","title":"LoRA-Leak: Membership Inference Attacks Against LoRA Fine-tuned Language\n  Models","summary":"  Language Models (LMs) typically adhere to a \"pre-training and fine-tuning\"\nparadigm, where a universal pre-trained model can be fine-tuned to cater to\nvarious specialized domains. Low-Rank Adaptation (LoRA) has gained the most\nwidespread use in LM fine-tuning due to its lightweight computational cost and\nremarkable performance. Because the proportion of parameters tuned by LoRA is\nrelatively small, there might be a misleading impression that the LoRA\nfine-tuning data is invulnerable to Membership Inference Attacks (MIAs).\nHowever, we identify that utilizing the pre-trained model can induce more\ninformation leakage, which is neglected by existing MIAs. Therefore, we\nintroduce LoRA-Leak, a holistic evaluation framework for MIAs against the\nfine-tuning datasets of LMs. LoRA-Leak incorporates fifteen membership\ninference attacks, including ten existing MIAs, and five improved MIAs that\nleverage the pre-trained model as a reference. In experiments, we apply\nLoRA-Leak to three advanced LMs across three popular natural language\nprocessing tasks, demonstrating that LoRA-based fine-tuned LMs are still\nvulnerable to MIAs (e.g., 0.775 AUC under conservative fine-tuning settings).\nWe also applied LoRA-Leak to different fine-tuning settings to understand the\nresulting privacy risks. We further explore four defenses and find that only\ndropout and excluding specific LM layers during fine-tuning effectively\nmitigate MIA risks while maintaining utility. We highlight that under the\n\"pre-training and fine-tuning\" paradigm, the existence of the pre-trained model\nmakes MIA a more severe risk for LoRA-based LMs. We hope that our findings can\nprovide guidance on data privacy protection for specialized LM providers.\n","authors":["Delong Ran","Xinlei He","Tianshuo Cong","Anyu Wang","Qi Li","Xiaoyun Wang"],"pdf_url":"https://arxiv.org/pdf/2507.18302v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2308.09954v2","updated":"2025-07-24T11:16:48Z","published":"2023-08-19T09:17:19Z","title":"DocTER: Evaluating Document-based Knowledge Editing","summary":"  Knowledge editing aims to correct outdated or inaccurate knowledge in neural\nnetworks. In this paper, we explore knowledge editing using easily accessible\ndocuments instead of manually labeled factual triples employed in earlier\nresearch. To advance this field, we establish the first evaluation benchmark,\n\\textit{DocTER}, featuring Documents containing counterfactual knowledge for\nediting. A comprehensive four-perspective evaluation is introduced: Edit\nSuccess, Locality, Reasoning, and Cross-lingual Transfer. To adapt conventional\ntriplet-based knowledge editing methods for this task, we develop an\nExtract-then-Edit pipeline that extracts triples from documents before applying\nexisting methods. Experiments on popular knowledge editing methods demonstrate\nthat editing with documents presents significantly greater challenges than\nusing triples. In document-based scenarios, even the best-performing in-context\nediting approach still lags behind by 10 points in editing success when\ncompared to using gold triples. This observation also holds for both reasoning\nand cross-lingual test sets. We further analyze key factors influencing task\nperformance, including the quality of extracted triples, the frequency and\nposition of edited knowledge in documents, various methods for enhancing\nreasoning, and performance differences across various directions in\ncross-lingual knowledge editing, which provide valuable insights for future\nresearch.\n","authors":["Suhang Wu","Ante Wang","Minlong Peng","Yujie Lin","Wenbo Li","Mingming Sun","Jinsong Su"],"pdf_url":"https://arxiv.org/pdf/2308.09954v2.pdf","comment":"Information processing & management"},{"id":"http://arxiv.org/abs/2503.12972v2","updated":"2025-07-24T11:14:43Z","published":"2025-03-17T09:31:14Z","title":"Aligning Vision to Language: Annotation-Free Multimodal Knowledge Graph\n  Construction for Enhanced LLMs Reasoning","summary":"  Multimodal reasoning in Large Language Models (LLMs) struggles with\nincomplete knowledge and hallucination artifacts, challenges that textual\nKnowledge Graphs (KGs) only partially mitigate due to their modality isolation.\nWhile Multimodal Knowledge Graphs (MMKGs) promise enhanced cross-modal\nunderstanding, their practical construction is impeded by semantic narrowness\nof manual text annotations and inherent noise in visual-semantic entity\nlinkages. In this paper, we propose Vision-align-to-Language integrated\nKnowledge Graph (VaLiK), a novel approach for constructing MMKGs that enhances\nLLMs reasoning through cross-modal information supplementation. Specifically,\nwe cascade pre-trained Vision-Language Models (VLMs) to align image features\nwith text, transforming them into descriptions that encapsulate image-specific\ninformation. Furthermore, we developed a cross-modal similarity verification\nmechanism to quantify semantic consistency, effectively filtering out noise\nintroduced during feature alignment. Even without manually annotated image\ncaptions, the refined descriptions alone suffice to construct the MMKG.\nCompared to conventional MMKGs construction paradigms, our approach achieves\nsubstantial storage efficiency gains while maintaining direct entity-to-image\nlinkage capability. Experimental results on multimodal reasoning tasks\ndemonstrate that LLMs augmented with VaLiK outperform previous state-of-the-art\nmodels. Our code is published at https://github.com/Wings-Of-Disaster/VaLiK.\n","authors":["Junming Liu","Siyuan Meng","Yanting Gao","Song Mao","Pinlong Cai","Guohang Yan","Yirong Chen","Zilin Bian","Ding Wang","Botian Shi"],"pdf_url":"https://arxiv.org/pdf/2503.12972v2.pdf","comment":"14 pages, 7 figures, 6 tables; Accepted to ICCV 2025"},{"id":"http://arxiv.org/abs/2503.10009v2","updated":"2025-07-24T11:09:58Z","published":"2025-03-13T03:40:50Z","title":"OR-LLM-Agent: Automating Modeling and Solving of Operations Research\n  Optimization Problems with Reasoning LLM","summary":"  With the rise of artificial intelligence (AI), applying large language models\n(LLMs) to Operations Research (OR) problem-solving has attracted increasing\nattention. Most existing approaches attempt to improve OR problem-solving\nthrough prompt engineering or fine-tuning strategies for LLMs. However, these\nmethods are fundamentally constrained by the limited capabilities of\nnon-reasoning LLMs. To overcome these limitations, we propose OR-LLM-Agent, an\nAI agent built on reasoning LLMs for automated OR problem solving. The agent\ndecomposes the task into three sequential stages: mathematical modeling, code\ngeneration, and debugging. Each task is handled by a dedicated sub-agent, which\nenables more targeted reasoning. We also construct BWOR, a high-quality dataset\nfor evaluating LLM performance on OR tasks. Our analysis shows that existing\nbenchmarks such as NL4OPT, MAMO, and IndustryOR suffer from certain issues,\nmaking them less suitable for reliably evaluating LLM performance. In contrast,\nBWOR provides a more consistent and discriminative assessment of model\ncapabilities. Experimental results demonstrate that OR-LLM-Agent outperforms\nadvanced methods, including GPT-o3, Gemini 2.5 Pro, and ORLM, by at least 7% in\naccuracy. These results demonstrate the effectiveness of task decomposition for\nOR problem solving.\n","authors":["Bowen Zhang","Pengcheng Luo"],"pdf_url":"https://arxiv.org/pdf/2503.10009v2.pdf","comment":"8 pages, 12 figures"},{"id":"http://arxiv.org/abs/2407.19795v2","updated":"2025-07-24T11:08:59Z","published":"2024-07-29T08:38:46Z","title":"VolDoGer: LLM-assisted Datasets for Domain Generalization in\n  Vision-Language Tasks","summary":"  Domain generalizability is a crucial aspect of a deep learning model since it\ndetermines the capability of the model to perform well on data from unseen\ndomains. However, research on the domain generalizability of deep learning\nmodels for vision-language tasks remains limited, primarily because of the lack\nof required datasets. To address these challenges, we propose VolDoGer:\nVision-Language Dataset for Domain Generalization, a dedicated dataset designed\nfor domain generalization that addresses three vision-language tasks: image\ncaptioning, visual question answering, and visual entailment. We constructed\nVolDoGer by extending LLM-based data annotation techniques to vision-language\ntasks, thereby alleviating the burden of recruiting human annotators. We\nevaluated the domain generalizability of various models, ranging from\nfine-tuned models to a recent multimodal large language model, through\nVolDoGer.\n","authors":["Juhwan Choi","Junehyoung Kwon","JungMin Yun","Seunguk Yu","YoungBin Kim"],"pdf_url":"https://arxiv.org/pdf/2407.19795v2.pdf","comment":"ICCV 2025 Workshop on Curated Data for Efficient Learning (CDEL)"},{"id":"http://arxiv.org/abs/2507.17596v2","updated":"2025-07-24T11:04:42Z","published":"2025-07-23T15:28:23Z","title":"PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving","summary":"  While end-to-end autonomous driving models show promising results, their\npractical deployment is often hindered by large model sizes, a reliance on\nexpensive LiDAR sensors and computationally intensive BEV feature\nrepresentations. This limits their scalability, especially for mass-market\nvehicles equipped only with cameras. To address these challenges, we propose\nPRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving\narchitecture operates using only camera data, without explicit BEV\nrepresentation and forgoing the need for LiDAR. PRIX leverages a visual feature\nextractor coupled with a generative planning head to predict safe trajectories\nfrom raw pixel inputs directly. A core component of our architecture is the\nContext-aware Recalibration Transformer (CaRT), a novel module designed to\neffectively enhance multi-level visual features for more robust planning. We\ndemonstrate through comprehensive experiments that PRIX achieves\nstate-of-the-art performance on the NavSim and nuScenes benchmarks, matching\nthe capabilities of larger, multimodal diffusion planners while being\nsignificantly more efficient in terms of inference speed and model size, making\nit a practical solution for real-world deployment. Our work is open-source and\nthe code will be at https://maxiuw.github.io/prix.\n","authors":["Maciej K. Wozniak","Lianhang Liu","Yixi Cai","Patric Jensfelt"],"pdf_url":"https://arxiv.org/pdf/2507.17596v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2507.18290v1","updated":"2025-07-24T10:52:22Z","published":"2025-07-24T10:52:22Z","title":"Foundations for Risk Assessment of AI in Protecting Fundamental Rights","summary":"  This chapter introduces a conceptual framework for qualitative risk\nassessment of AI, particularly in the context of the EU AI Act. The framework\naddresses the complexities of legal compliance and fundamental rights\nprotection by itegrating definitional balancing and defeasible reasoning.\nDefinitional balancing employs proportionality analysis to resolve conflicts\nbetween competing rights, while defeasible reasoning accommodates the dynamic\nnature of legal decision-making. Our approach stresses the need for an analysis\nof AI deployment scenarios and for identifying potential legal violations and\nmulti-layered impacts on fundamental rights. On the basis of this analysis, we\nprovide philosophical foundations for a logical account of AI risk analysis. In\nparticular, we consider the basic building blocks for conceptually grasping the\ninteraction between AI deployment scenarios and fundamental rights,\nincorporating in defeasible reasoning definitional balancing and arguments\nabout the contextual promotion or demotion of rights. This layered approach\nallows for more operative models of assessment of both high-risk AI systems and\nGeneral Purpose AI (GPAI) systems, emphasizing the broader applicability of the\nlatter. Future work aims to develop a formal model and effective algorithms to\nenhance AI risk assessment, bridging theoretical insights with practical\napplications to support responsible AI governance.\n","authors":["Antonino Rotolo","Beatrice Ferrigno","Jose Miguel Angel Garcia Godinez","Claudio Novelli","Giovanni Sartor"],"pdf_url":"https://arxiv.org/pdf/2507.18290v1.pdf","comment":"24 pages, 1 figure. To be published in: The Philosophical Foundations\n  of Information Technology Law. Oxford University Press, Oxford"},{"id":"http://arxiv.org/abs/2507.18288v1","updated":"2025-07-24T10:49:31Z","published":"2025-07-24T10:49:31Z","title":"TCM-Tongue: A Standardized Tongue Image Dataset with Pathological\n  Annotations for AI-Assisted TCM Diagnosis","summary":"  Traditional Chinese medicine (TCM) tongue diagnosis, while clinically\nvaluable, faces standardization challenges due to subjective interpretation and\ninconsistent imaging protocols, compounded by the lack of large-scale,\nannotated datasets for AI development. To address this gap, we present the\nfirst specialized dataset for AI-driven TCM tongue diagnosis, comprising 6,719\nhigh-quality images captured under standardized conditions and annotated with\n20 pathological symptom categories (averaging 2.54 clinically validated labels\nper image, all verified by licensed TCM practitioners). The dataset supports\nmultiple annotation formats (COCO, TXT, XML) for broad usability and has been\nbenchmarked using nine deep learning models (YOLOv5/v7/v8 variants, SSD, and\nMobileNetV2) to demonstrate its utility for AI development. This resource\nprovides a critical foundation for advancing reliable computational tools in\nTCM, bridging the data shortage that has hindered progress in the field, and\nfacilitating the integration of AI into both research and clinical practice\nthrough standardized, high-quality diagnostic data.\n","authors":["Xuebo Jin","Longfei Gao","Anshuo Tong","Zhengyang Chen","Jianlei Kong","Ning Sun","Huijun Ma","Qiang Wang","Yuting Bai","Tingli Su"],"pdf_url":"https://arxiv.org/pdf/2507.18288v1.pdf","comment":"16 pages, 11 figures, 2 Tables"},{"id":"http://arxiv.org/abs/2507.11554v3","updated":"2025-07-24T10:37:32Z","published":"2025-07-14T02:59:28Z","title":"Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models","summary":"  Recent advancements in diffusion models (DMs) have been propelled by\nalignment methods that post-train models to better conform to human\npreferences. However, these approaches typically require computation-intensive\ntraining of a base model and a reward model, which not only incurs substantial\ncomputational overhead but may also compromise model accuracy and training\nefficiency. To address these limitations, we propose Inversion-DPO, a novel\nalignment framework that circumvents reward modeling by reformulating Direct\nPreference Optimization (DPO) with DDIM inversion for DMs. Our method conducts\nintractable posterior sampling in Diffusion-DPO with the deterministic\ninversion from winning and losing samples to noise and thus derive a new\npost-training paradigm. This paradigm eliminates the need for auxiliary reward\nmodels or inaccurate appromixation, significantly enhancing both precision and\nefficiency of training. We apply Inversion-DPO to a basic task of text-to-image\ngeneration and a challenging task of compositional image generation. Extensive\nexperiments show substantial performance improvements achieved by Inversion-DPO\ncompared to existing post-training methods and highlight the ability of the\ntrained generative models to generate high-fidelity compositionally coherent\nimages. For the post-training of compostitional image geneation, we curate a\npaired dataset consisting of 11,140 images with complex structural annotations\nand comprehensive scores, designed to enhance the compositional capabilities of\ngenerative models. Inversion-DPO explores a new avenue for efficient,\nhigh-precision alignment in diffusion models, advancing their applicability to\ncomplex realistic generation tasks. Our code is available at\nhttps://github.com/MIGHTYEZ/Inversion-DPO\n","authors":["Zejian Li","Yize Li","Chenye Meng","Zhongni Liu","Yang Ling","Shengyuan Zhang","Guang Yang","Changyuan Yang","Zhiyuan Yang","Lingyun Sun"],"pdf_url":"https://arxiv.org/pdf/2507.11554v3.pdf","comment":"Accepted by ACM MM25"},{"id":"http://arxiv.org/abs/2502.06788v2","updated":"2025-07-24T10:29:52Z","published":"2025-02-10T18:59:58Z","title":"EVEv2: Improved Baselines for Encoder-Free Vision-Language Models","summary":"  Existing encoder-free vision-language models (VLMs) are rapidly narrowing the\nperformance gap with their encoder-based counterparts, highlighting the\npromising potential for unified multimodal systems with structural simplicity\nand efficient deployment. We systematically clarify the performance gap between\nVLMs using pre-trained vision encoders, discrete tokenizers, and minimalist\nvisual layers from scratch, deeply excavating the under-examined\ncharacteristics of encoder-free VLMs. We develop efficient strategies for\nencoder-free VLMs that rival mainstream encoder-based ones. After an in-depth\ninvestigation, we launch EVEv2.0, a new and improved family of encoder-free\nVLMs. We show that: (i) Properly decomposing and hierarchically associating\nvision and language within a unified model reduces interference between\nmodalities. (ii) A well-designed training strategy enables effective\noptimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0\nrepresents a thorough study for developing a decoder-only architecture across\nmodalities, demonstrating superior data efficiency and strong vision-reasoning\ncapability. Code is publicly available at: https://github.com/baaivision/EVE.\n","authors":["Haiwen Diao","Xiaotong Li","Yufeng Cui","Yueze Wang","Haoge Deng","Ting Pan","Wenxuan Wang","Huchuan Lu","Xinlong Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06788v2.pdf","comment":"20 pages, 10 figures, Accepted by ICCV2025 (highlight)"},{"id":"http://arxiv.org/abs/2501.15916v2","updated":"2025-07-24T10:10:42Z","published":"2025-01-27T10:05:49Z","title":"Online Housing Market","summary":"  This paper studies an online variant of the celebrated housing market\nproblem, where each agent has a single house and seeks to exchange it for\nanother based on her preferences. In this online setting, agents may arrive and\ndepart at any time, meaning that not all agents are present on the housing\nmarket simultaneously. I extend the well known serial dictatorship and Gale s\ntop trading cycle mechanisms to this online scenario, aiming to retain their\ndesirable properties such as Pareto efficiency, individual rationality, and\nstrategy proofness. These extensions also seek to prevent agents from\nstrategically delaying their arrival or advancing their departure. I\ndemonstrate that achieving all of these properties simultaneously is impossible\nin the online context, and I present several variants that achieve different\nsubsets of these properties.\n","authors":["Julien Lesca"],"pdf_url":"https://arxiv.org/pdf/2501.15916v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18263v1","updated":"2025-07-24T10:07:59Z","published":"2025-07-24T10:07:59Z","title":"Locate-and-Focus: Enhancing Terminology Translation in Speech Language\n  Models","summary":"  Direct speech translation (ST) has garnered increasing attention nowadays,\nyet the accurate translation of terminology within utterances remains a great\nchallenge. In this regard, current studies mainly concentrate on leveraging\nvarious translation knowledge into ST models. However, these methods often\nstruggle with interference from irrelevant noise and can not fully utilize the\ntranslation knowledge. To address these issues, in this paper, we propose a\nnovel Locate-and-Focus method for terminology translation. It first effectively\nlocates the speech clips containing terminologies within the utterance to\nconstruct translation knowledge, minimizing irrelevant information for the ST\nmodel. Subsequently, it associates the translation knowledge with the utterance\nand hypothesis from both audio and textual modalities, allowing the ST model to\nbetter focus on translation knowledge during translation. Experimental results\nacross various datasets demonstrate that our method effectively locates\nterminologies within utterances and enhances the success rate of terminology\ntranslation, while maintaining robust general translation performance.\n","authors":["Suhang Wu","Jialong Tang","Chengyi Yang","Pei Zhang","Baosong Yang","Junhui Li","Junfeng Yao","Min Zhang","Jinsong Su"],"pdf_url":"https://arxiv.org/pdf/2507.18263v1.pdf","comment":"Accepted at ACL 2025"},{"id":"http://arxiv.org/abs/2507.18262v1","updated":"2025-07-24T10:07:31Z","published":"2025-07-24T10:07:31Z","title":"ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic\n  Grounding for Generalizable Robotic Manipulation","summary":"  Semantics-driven 3D spatial constraints align highlevel semantic\nrepresentations with low-level action spaces, facilitating the unification of\ntask understanding and execution in robotic manipulation. The synergistic\nreasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation\nModels (VFMs) enables cross-modal 3D spatial constraint construction.\nNevertheless, existing methods have three key limitations: (1) coarse semantic\ngranularity in constraint modeling, (2) lack of real-time closed-loop planning,\n(3) compromised robustness in semantically diverse environments. To address\nthese challenges, we propose ReSem3D, a unified manipulation framework for\nsemantically diverse environments, leveraging the synergy between VFMs and\nMLLMs to achieve fine-grained visual grounding and dynamically constructs\nhierarchical 3D spatial constraints for real-time manipulation. Specifically,\nthe framework is driven by hierarchical recursive reasoning in MLLMs, which\ninteract with VFMs to automatically construct 3D spatial constraints from\nnatural language instructions and RGB-D observations in two stages: part-level\nextraction and region-level refinement. Subsequently, these constraints are\nencoded as real-time optimization objectives in joint space, enabling reactive\nbehavior to dynamic disturbances. Extensive simulation and real-world\nexperiments are conducted in semantically rich household and sparse chemical\nlab environments. The results demonstrate that ReSem3D performs diverse\nmanipulation tasks under zero-shot conditions, exhibiting strong adaptability\nand generalization. Code and videos at https://resem3d.github.io.\n","authors":["Chenyu Su","Weiwei Shang","Chen Qian","Fei Zhang","Shuang Cong"],"pdf_url":"https://arxiv.org/pdf/2507.18262v1.pdf","comment":"12 pages,9 figures"},{"id":"http://arxiv.org/abs/2507.18260v1","updated":"2025-07-24T10:03:33Z","published":"2025-07-24T10:03:33Z","title":"Exploiting Gaussian Agnostic Representation Learning with Diffusion\n  Priors for Enhanced Infrared Small Target Detection","summary":"  Infrared small target detection (ISTD) plays a vital role in numerous\npractical applications. In pursuit of determining the performance boundaries,\nresearchers employ large and expensive manual-labeling data for representation\nlearning. Nevertheless, this approach renders the state-of-the-art ISTD methods\nhighly fragile in real-world challenges. In this paper, we first study the\nvariation in detection performance across several mainstream methods under\nvarious scarcity -- namely, the absence of high-quality infrared data -- that\nchallenge the prevailing theories about practical ISTD. To address this\nconcern, we introduce the Gaussian Agnostic Representation Learning.\nSpecifically, we propose the Gaussian Group Squeezer, leveraging Gaussian\nsampling and compression for non-uniform quantization. By exploiting a diverse\narray of training samples, we enhance the resilience of ISTD models against\nvarious challenges. Then, we introduce two-stage diffusion models for\nreal-world reconstruction. By aligning quantized signals closely with\nreal-world distributions, we significantly elevate the quality and fidelity of\nthe synthetic samples. Comparative evaluations against state-of-the-art\ndetection methods in various scarcity scenarios demonstrate the efficacy of the\nproposed approach.\n","authors":["Junyao Li","Yahao Lu","Xingyuan Guo","Xiaoyu Xian","Tiantian Wang","Yukai Shi"],"pdf_url":"https://arxiv.org/pdf/2507.18260v1.pdf","comment":"Submitted to Neural Networks. We propose the Gaussian Group Squeezer,\n  leveraging Gaussian sampling and compression with diffusion models for\n  channel-based data augmentation"},{"id":"http://arxiv.org/abs/2502.14400v4","updated":"2025-07-24T10:00:09Z","published":"2025-02-20T09:37:41Z","title":"HPS: Hard Preference Sampling for Human Preference Alignment","summary":"  Aligning Large Language Model (LLM) responses with human preferences is vital\nfor building safe and controllable AI systems. While preference optimization\nmethods based on Plackett-Luce (PL) and Bradley-Terry (BT) models have shown\npromise, they face challenges such as poor handling of harmful content,\ninefficient use of dispreferred responses, and, specifically for PL, high\ncomputational costs. To address these issues, we propose Hard Preference\nSampling (HPS), a novel framework for robust and efficient human preference\nalignment. HPS introduces a training loss that prioritizes the most preferred\nresponse while rejecting all dispreferred and harmful ones. It emphasizes\n\"hard\" dispreferred responses -- those closely resembling preferred ones -- to\nenhance the model's rejection capabilities. By leveraging a single-sample Monte\nCarlo sampling strategy, HPS reduces computational overhead while maintaining\nalignment quality. Theoretically, HPS improves sample efficiency over existing\nPL methods and maximizes the reward margin between preferred and dispreferred\nresponses, ensuring clearer distinctions. Experiments on HH-RLHF and PKU-Safety\ndatasets validate HPS's effectiveness, achieving comparable BLEU and reward\nscores while greatly improving reward margins and thus reducing harmful content\ngeneration.\n","authors":["Xiandong Zou","Wanyu Lin","Yuchen Li","Pan Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.14400v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12006v3","updated":"2025-07-24T09:57:56Z","published":"2025-07-16T07:59:54Z","title":"Frequency-Dynamic Attention Modulation for Dense Prediction","summary":"  Vision Transformers (ViTs) have significantly advanced computer vision,\ndemonstrating strong performance across various tasks. However, the attention\nmechanism in ViTs makes each layer function as a low-pass filter, and the\nstacked-layer architecture in existing transformers suffers from frequency\nvanishing. This leads to the loss of critical details and textures. We propose\na novel, circuit-theory-inspired strategy called Frequency-Dynamic Attention\nModulation (FDAM), which can be easily plugged into ViTs. FDAM directly\nmodulates the overall frequency response of ViTs and consists of two\ntechniques: Attention Inversion (AttInv) and Frequency Dynamic Scaling\n(FreqScale). Since circuit theory uses low-pass filters as fundamental\nelements, we introduce AttInv, a method that generates complementary high-pass\nfiltering by inverting the low-pass filter in the attention matrix, and\ndynamically combining the two. We further design FreqScale to weight different\nfrequency components for fine-grained adjustments to the target response\nfunction. Through feature similarity analysis and effective rank evaluation, we\ndemonstrate that our approach avoids representation collapse, leading to\nconsistent performance improvements across various models, including SegFormer,\nDeiT, and MaskDINO. These improvements are evident in tasks such as semantic\nsegmentation, object detection, and instance segmentation. Additionally, we\napply our method to remote sensing detection, achieving state-of-the-art\nresults in single-scale settings. The code is available at\nhttps://github.com/Linwei-Chen/FDAM.\n","authors":["Linwei Chen","Lin Gu","Ying Fu"],"pdf_url":"https://arxiv.org/pdf/2507.12006v3.pdf","comment":"Accepted by ICCV 2025"},{"id":"http://arxiv.org/abs/2506.16297v3","updated":"2025-07-24T09:52:06Z","published":"2025-06-19T13:17:30Z","title":"SyncMapV2: Robust and Adaptive Unsupervised Segmentation","summary":"  Human vision excels at segmenting visual cues without the need for explicit\ntraining, and it remains remarkably robust even as noise severity increases. In\ncontrast, existing AI algorithms struggle to maintain accuracy under similar\nconditions. Here, we present SyncMapV2, the first to solve unsupervised\nsegmentation with state-of-the-art robustness. SyncMapV2 exhibits a minimal\ndrop in mIoU, only 0.01%, under digital corruption, compared to a 23.8% drop\nobserved in SOTA methods. This superior performance extends across various\ntypes of corruption: noise (7.3% vs. 37.7%), weather (7.5% vs. 33.8%), and blur\n(7.0% vs. 29.5%). Notably, SyncMapV2 accomplishes this without any robust\ntraining, supervision, or loss functions. It is based on a learning paradigm\nthat uses self-organizing dynamical equations combined with concepts from\nrandom networks. Moreover, unlike conventional methods that require\nre-initialization for each new input, SyncMapV2 adapts online, mimicking the\ncontinuous adaptability of human vision. Thus, we go beyond the accurate and\nrobust results, and present the first algorithm that can do all the above\nonline, adapting to input rather than re-initializing. In adaptability tests,\nSyncMapV2 demonstrates near-zero performance degradation, which motivates and\nfosters a new generation of robust and adaptive intelligence in the near\nfuture.\n","authors":["Heng Zhang","Zikang Wan","Danilo Vasconcellos Vargas"],"pdf_url":"https://arxiv.org/pdf/2506.16297v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18252v1","updated":"2025-07-24T09:49:53Z","published":"2025-07-24T09:49:53Z","title":"Multimodal Behavioral Patterns Analysis with Eye-Tracking and LLM-Based\n  Reasoning","summary":"  Eye-tracking data reveals valuable insights into users' cognitive states but\nis difficult to analyze due to its structured, non-linguistic nature. While\nlarge language models (LLMs) excel at reasoning over text, they struggle with\ntemporal and numerical data. This paper presents a multimodal human-AI\ncollaborative framework designed to enhance cognitive pattern extraction from\neye-tracking signals. The framework includes: (1) a multi-stage pipeline using\nhorizontal and vertical segmentation alongside LLM reasoning to uncover latent\ngaze patterns; (2) an Expert-Model Co-Scoring Module that integrates expert\njudgment with LLM output to generate trust scores for behavioral\ninterpretations; and (3) a hybrid anomaly detection module combining LSTM-based\ntemporal modeling with LLM-driven semantic analysis. Our results across several\nLLMs and prompt strategies show improvements in consistency, interpretability,\nand performance, with up to 50% accuracy in difficulty prediction tasks. This\napproach offers a scalable, interpretable solution for cognitive modeling and\nhas broad potential in adaptive learning, human-computer interaction, and\neducational analytics.\n","authors":["Dongyang Guo","Yasmeen Abdrabou","Enkeleda Thaqi","Enkelejda Kasneci"],"pdf_url":"https://arxiv.org/pdf/2507.18252v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18243v1","updated":"2025-07-24T09:32:53Z","published":"2025-07-24T09:32:53Z","title":"DepthDark: Robust Monocular Depth Estimation for Low-Light Environments","summary":"  In recent years, foundation models for monocular depth estimation have\nreceived increasing attention. Current methods mainly address typical daylight\nconditions, but their effectiveness notably decreases in low-light\nenvironments. There is a lack of robust foundational models for monocular depth\nestimation specifically designed for low-light scenarios. This largely stems\nfrom the absence of large-scale, high-quality paired depth datasets for\nlow-light conditions and the effective parameter-efficient fine-tuning (PEFT)\nstrategy. To address these challenges, we propose DepthDark, a robust\nfoundation model for low-light monocular depth estimation. We first introduce a\nflare-simulation module and a noise-simulation module to accurately simulate\nthe imaging process under nighttime conditions, producing high-quality paired\ndepth datasets for low-light conditions. Additionally, we present an effective\nlow-light PEFT strategy that utilizes illumination guidance and multiscale\nfeature fusion to enhance the model's capability in low-light environments. Our\nmethod achieves state-of-the-art depth estimation performance on the\nchallenging nuScenes-Night and RobotCar-Night datasets, validating its\neffectiveness using limited training data and computing resources.\n","authors":["Longjian Zeng","Zunjie Zhu","Rongfeng Lu","Ming Lu","Bolun Zheng","Chenggang Yan","Anke Xue"],"pdf_url":"https://arxiv.org/pdf/2507.18243v1.pdf","comment":"Accepted by ACM MM 2025 conference"},{"id":"http://arxiv.org/abs/2507.04600v2","updated":"2025-07-24T09:29:08Z","published":"2025-07-07T01:35:55Z","title":"DisMS-TS: Eliminating Redundant Multi-Scale Features for Time Series\n  Classification","summary":"  Real-world time series typically exhibit complex temporal variations, making\nthe time series classification task notably challenging. Recent advancements\nhave demonstrated the potential of multi-scale analysis approaches, which\nprovide an effective solution for capturing these complex temporal patterns.\nHowever, existing multi-scale analysis-based time series prediction methods\nfail to eliminate redundant scale-shared features across multi-scale time\nseries, resulting in the model over- or under-focusing on scale-shared\nfeatures. To address this issue, we propose a novel end-to-end Disentangled\nMulti-Scale framework for Time Series classification (DisMS-TS). The core idea\nof DisMS-TS is to eliminate redundant shared features in multi-scale time\nseries, thereby improving prediction performance. Specifically, we propose a\ntemporal disentanglement module to capture scale-shared and scale-specific\ntemporal representations, respectively. Subsequently, to effectively learn both\nscale-shared and scale-specific temporal representations, we introduce two\nregularization terms that ensure the consistency of scale-shared\nrepresentations and the disparity of scale-specific representations across all\ntemporal scales. Extensive experiments conducted on multiple datasets validate\nthe superiority of DisMS-TS over its competitive baselines, with the accuracy\nimprovement up to 9.71%.\n","authors":["Zhipeng Liu","Peibo Duan","Binwu Wang","Xuan Tang","Qi Chu","Changsheng Zhang","Yongsheng Huang","Bin Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.04600v2.pdf","comment":"This paper has been accepted for presentation at the ACM\n  International Conference on Multimedia (ACM MM 2025)"},{"id":"http://arxiv.org/abs/2503.04151v2","updated":"2025-07-24T09:25:16Z","published":"2025-03-06T07:01:08Z","title":"Robust Multi-View Learning via Representation Fusion of Sample-Level\n  Attention and Alignment of Simulated Perturbation","summary":"  Recently, multi-view learning (MVL) has garnered significant attention due to\nits ability to fuse discriminative information from multiple views. However,\nreal-world multi-view datasets are often heterogeneous and imperfect, which\nusually causes MVL methods designed for specific combinations of views to lack\napplication potential and limits their effectiveness. To address this issue, we\npropose a novel robust MVL method (namely RML) with simultaneous representation\nfusion and alignment. Specifically, we introduce a simple yet effective\nmulti-view transformer fusion network where we transform heterogeneous\nmulti-view data into homogeneous word embeddings, and then integrate multiple\nviews by the sample-level attention mechanism to obtain a fused representation.\nFurthermore, we propose a simulated perturbation based multi-view contrastive\nlearning framework that dynamically generates the noise and unusable\nperturbations for simulating imperfect data conditions. The simulated noisy and\nunusable data obtain two distinct fused representations, and we utilize\ncontrastive learning to align them for learning discriminative and robust\nrepresentations. Our RML is self-supervised and can also be applied for\ndownstream tasks as a regularization. In experiments, we employ it in\nmulti-view unsupervised clustering, noise-label classification, and as a\nplug-and-play module for cross-modal hashing retrieval. Extensive comparison\nexperiments and ablation studies validate RML's effectiveness. Code is\navailable at https://github.com/SubmissionsIn/RML.\n","authors":["Jie Xu","Na Zhao","Gang Niu","Masashi Sugiyama","Xiaofeng Zhu"],"pdf_url":"https://arxiv.org/pdf/2503.04151v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.16068v2","updated":"2025-07-24T09:25:12Z","published":"2025-07-21T21:09:15Z","title":"Compositional Coordination for Multi-Robot Teams with Large Language\n  Models","summary":"  Multi-robot coordination has traditionally relied on a mission-specific and\nexpert-driven pipeline, where natural language mission descriptions are\nmanually translated by domain experts into mathematical formulation, algorithm\ndesign, and executable code. This conventional process is labor-intensive,\ninaccessible to non-experts, and inflexible to changes in mission requirements.\nHere, we propose LAN2CB (Language to Collective Behavior), a novel framework\nthat leverages large language models (LLMs) to streamline and generalize the\nmulti-robot coordination pipeline. LAN2CB transforms natural language (NL)\nmission descriptions into executable Python code for multi-robot systems\nthrough two core modules: (1) Mission Analysis, which parses mission\ndescriptions into behavior trees, and (2) Code Generation, which leverages the\nbehavior tree and a structured knowledge base to generate robot control code.\nWe further introduce a dataset of natural language mission descriptions to\nsupport development and benchmarking. Experiments in both simulation and\nreal-world environments demonstrate that LAN2CB enables robust and flexible\nmulti-robot coordination from natural language, significantly reducing manual\nengineering effort and supporting broad generalization across diverse mission\ntypes. Website: https://sites.google.com/view/lan-cb\n","authors":["Zhehui Huang","Guangyao Shi","Yuwei Wu","Vijay Kumar","Gaurav S. Sukhatme"],"pdf_url":"https://arxiv.org/pdf/2507.16068v2.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2507.18229v1","updated":"2025-07-24T09:21:02Z","published":"2025-07-24T09:21:02Z","title":"From Individual Learning to Market Equilibrium: Correcting Structural\n  and Parametric Biases in RL Simulations of Economic Models","summary":"  The application of Reinforcement Learning (RL) to economic modeling reveals a\nfundamental conflict between the assumptions of equilibrium theory and the\nemergent behavior of learning agents. While canonical economic models assume\natomistic agents act as `takers' of aggregate market conditions, a naive\nsingle-agent RL simulation incentivizes the agent to become a `manipulator' of\nits environment. This paper first demonstrates this discrepancy within a\nsearch-and-matching model with concave production, showing that a standard RL\nagent learns a non-equilibrium, monopsonistic policy. Additionally, we identify\na parametric bias arising from the mismatch between economic discounting and\nRL's treatment of intertemporal costs. To address both issues, we propose a\ncalibrated Mean-Field Reinforcement Learning framework that embeds a\nrepresentative agent in a fixed macroeconomic field and adjusts the cost\nfunction to reflect economic opportunity costs. Our iterative algorithm\nconverges to a self-consistent fixed point where the agent's policy aligns with\nthe competitive equilibrium. This approach provides a tractable and\ntheoretically sound methodology for modeling learning agents in economic\nsystems within the broader domain of computational social science.\n","authors":["Zeqiang Zhang","Ruxin Chen"],"pdf_url":"https://arxiv.org/pdf/2507.18229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11482v8","updated":"2025-07-24T09:19:38Z","published":"2023-11-20T01:51:13Z","title":"Meta Prompting for AI Systems","summary":"  We introduce Meta Prompting (MP), a framework that elevates the reasoning\ncapabilities of large language models (LLMs) by focusing on the formal\nstructure of a task rather than content-specific examples. We establish a\ntheoretical foundation for this paradigm, formalizing MP as a functor that maps\na category of tasks to a category of structured prompts, thereby guaranteeing\nthat compositional problem-solving strategies can be systematically decomposed\ninto modular prompt structures. We extend this concept to Recursive Meta\nPrompting (RMP), an automated process where an LLM can generate and refine its\nown prompts. We model this self-improvement loop formally as a monad, providing\na principled framework for automated prompt engineering. Our claims are\nvalidated through extensive experiments demonstrating that a Qwen-72B base\nmodel, guided by a single, example-agnostic meta-prompt, achieves\nstate-of-the-art results on MATH, GSM8K, and Game of 24. These results are\nachieved with substantial token efficiency gains over traditional few-shot\nmethods. Project Page: https://github.com/meta-prompting/meta-prompting.\n","authors":["Yifan Zhang","Yang Yuan","Andrew Chi-Chih Yao"],"pdf_url":"https://arxiv.org/pdf/2311.11482v8.pdf","comment":"Project Page: https://github.com/meta-prompting/meta-prompting"},{"id":"http://arxiv.org/abs/2506.11790v2","updated":"2025-07-24T09:17:21Z","published":"2025-06-13T13:52:32Z","title":"Why Do Class-Dependent Evaluation Effects Occur with Time Series Feature\n  Attributions? A Synthetic Data Investigation","summary":"  Evaluating feature attribution methods represents a critical challenge in\nexplainable AI (XAI), as researchers typically rely on perturbation-based\nmetrics when ground truth is unavailable. However, recent work reveals that\nthese evaluation metrics can show different performance across predicted\nclasses within the same dataset. These \"class-dependent evaluation effects\"\nraise questions about whether perturbation analysis reliably measures\nattribution quality, with direct implications for XAI method development and\nevaluation trustworthiness. We investigate under which conditions these\nclass-dependent effects arise by conducting controlled experiments with\nsynthetic time series data where ground truth feature locations are known. We\nsystematically vary feature types and class contrasts across binary\nclassification tasks, then compare perturbation-based degradation scores with\nground truth-based precision-recall metrics using multiple attribution methods.\nOur experiments demonstrate that class-dependent effects emerge with both\nevaluation approaches, even in simple scenarios with temporally localized\nfeatures, triggered by basic variations in feature amplitude or temporal extent\nbetween classes. Most critically, we find that perturbation-based and ground\ntruth metrics frequently yield contradictory assessments of attribution quality\nacross classes, with weak correlations between evaluation approaches. These\nfindings suggest that researchers should interpret perturbation-based metrics\nwith care, as they may not always align with whether attributions correctly\nidentify discriminating features. By showing this disconnect, our work points\ntoward reconsidering what attribution evaluation actually measures and\ndeveloping more rigorous evaluation methods that capture multiple dimensions of\nattribution quality.\n","authors":["Gregor Baer","Isel Grau","Chao Zhang","Pieter Van Gorp"],"pdf_url":"https://arxiv.org/pdf/2506.11790v2.pdf","comment":"Accepted at TempXAI Workshop @ ECML-PKDD 2025 (Explainable AI for\n  Time Series and Data Streams)"},{"id":"http://arxiv.org/abs/2507.18223v1","updated":"2025-07-24T09:17:13Z","published":"2025-07-24T09:17:13Z","title":"GenAI for Automotive Software Development: From Requirements to Wheels","summary":"  This paper introduces a GenAI-empowered approach to automated development of\nautomotive software, with emphasis on autonomous and Advanced Driver Assistance\nSystems (ADAS) capabilities. The process starts with requirements as input,\nwhile the main generated outputs are test scenario code for simulation\nenvironment, together with implementation of desired ADAS capabilities\ntargeting hardware platform of the vehicle connected to testbench. Moreover, we\nintroduce additional steps for requirements consistency checking leveraging\nModel-Driven Engineering (MDE). In the proposed workflow, Large Language Models\n(LLMs) are used for model-based summarization of requirements (Ecore metamodel,\nXMI model instance and OCL constraint creation), test scenario generation,\nsimulation code (Python) and target platform code generation (C++).\nAdditionally, Retrieval Augmented Generation (RAG) is adopted to enhance test\nscenario generation from autonomous driving regulations-related documents. Our\napproach aims shorter compliance and re-engineering cycles, as well as reduced\ndevelopment and testing time when it comes to ADAS-related capabilities.\n","authors":["Nenad Petrovic","Fengjunjie Pan","Vahid Zolfaghari","Krzysztof Lebioda","Andre Schamschurko","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2507.18223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18219v1","updated":"2025-07-24T09:15:07Z","published":"2025-07-24T09:15:07Z","title":"FedSA-GCL: A Semi-Asynchronous Federated Graph Learning Framework with\n  Personalized Aggregation and Cluster-Aware Broadcasting","summary":"  Federated Graph Learning (FGL) is a distributed learning paradigm that\nenables collaborative training over large-scale subgraphs located on multiple\nlocal systems. However, most existing FGL approaches rely on synchronous\ncommunication, which leads to inefficiencies and is often impractical in\nreal-world deployments. Meanwhile, current asynchronous federated learning\n(AFL) methods are primarily designed for conventional tasks such as image\nclassification and natural language processing, without accounting for the\nunique topological properties of graph data. Directly applying these methods to\ngraph learning can possibly result in semantic drift and representational\ninconsistency in the global model. To address these challenges, we propose\nFedSA-GCL, a semi-asynchronous federated framework that leverages both\ninter-client label distribution divergence and graph topological\ncharacteristics through a novel ClusterCast mechanism for efficient training.\nWe evaluate FedSA-GCL on multiple real-world graph datasets using the Louvain\nand Metis split algorithms, and compare it against 9 baselines. Extensive\nexperiments demonstrate that our method achieves strong robustness and\noutstanding efficiency, outperforming the baselines by an average of 2.92% with\nthe Louvain and by 3.4% with the Metis.\n","authors":["Zhongzheng Yuan","Lianshuai Guo","Xunkai Li","Yinlin Zhu","Wenyu Wang","Meixia Qu"],"pdf_url":"https://arxiv.org/pdf/2507.18219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18215v1","updated":"2025-07-24T09:09:36Z","published":"2025-07-24T09:09:36Z","title":"Information Security Based on LLM Approaches: A Review","summary":"  Information security is facing increasingly severe challenges, and\ntraditional protection means are difficult to cope with complex and changing\nthreats. In recent years, as an emerging intelligent technology, large language\nmodels (LLMs) have shown a broad application prospect in the field of\ninformation security. In this paper, we focus on the key role of LLM in\ninformation security, systematically review its application progress in\nmalicious behavior prediction, network threat analysis, system vulnerability\ndetection, malicious code identification, and cryptographic algorithm\noptimization, and explore its potential in enhancing security protection\nperformance. Based on neural networks and Transformer architecture, this paper\nanalyzes the technical basis of large language models and their advantages in\nnatural language processing tasks. It is shown that the introduction of large\nlanguage modeling helps to improve the detection accuracy and reduce the false\nalarm rate of security systems. Finally, this paper summarizes the current\napplication results and points out that it still faces challenges in model\ntransparency, interpretability, and scene adaptability, among other issues. It\nis necessary to explore further the optimization of the model structure and the\nimprovement of the generalization ability to realize a more intelligent and\naccurate information security protection system.\n","authors":["Chang Gong","Zhongwen Li","Xiaoqi Li"],"pdf_url":"https://arxiv.org/pdf/2507.18215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18206v1","updated":"2025-07-24T09:02:13Z","published":"2025-07-24T09:02:13Z","title":"MoRPI-PINN: A Physics-Informed Framework for Mobile Robot Pure Inertial\n  Navigation","summary":"  A fundamental requirement for full autonomy in mobile robots is accurate\nnavigation even in situations where satellite navigation or cameras are\nunavailable. In such practical situations, relying only on inertial sensors\nwill result in navigation solution drift due to the sensors' inherent noise and\nerror terms. One of the emerging solutions to mitigate drift is to maneuver the\nrobot in a snake-like slithering motion to increase the inertial\nsignal-to-noise ratio, allowing the regression of the mobile robot position. In\nthis work, we propose MoRPI-PINN as a physics-informed neural network framework\nfor accurate inertial-based mobile robot navigation. By embedding physical laws\nand constraints into the training process, MoRPI-PINN is capable of providing\nan accurate and robust navigation solution. Using real-world experiments, we\nshow accuracy improvements of over 85% compared to other approaches. MoRPI-PINN\nis a lightweight approach that can be implemented even on edge devices and used\nin any typical mobile robot application.\n","authors":["Arup Kumar Sahoo","Itzik Klein"],"pdf_url":"https://arxiv.org/pdf/2507.18206v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2507.18202v1","updated":"2025-07-24T08:58:41Z","published":"2025-07-24T08:58:41Z","title":"Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token\n  Probability Method for Poisoned Document Detection","summary":"  Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by\nproviding external knowledge for accurate and up-to-date responses. However,\nthis reliance on external sources exposes a security risk, attackers can inject\npoisoned documents into the knowledge base to steer the generation process\ntoward harmful or misleading outputs. In this paper, we propose Gradient-based\nMasked Token Probability (GMTP), a novel defense method to detect and filter\nout adversarially crafted documents. Specifically, GMTP identifies high-impact\ntokens by examining gradients of the retriever's similarity function. These key\ntokens are then masked, and their probabilities are checked via a Masked\nLanguage Model (MLM). Since injected tokens typically exhibit markedly low\nmasked-token probabilities, this enables GMTP to easily detect malicious\ndocuments and achieve high-precision filtering. Experiments demonstrate that\nGMTP is able to eliminate over 90% of poisoned content while retaining relevant\ndocuments, thus maintaining robust retrieval and generation performance across\ndiverse datasets and adversarial settings.\n","authors":["San Kim","Jonghwi Kim","Yejin Jeon","Gary Geunbae Lee"],"pdf_url":"https://arxiv.org/pdf/2507.18202v1.pdf","comment":"18 pages, accepted to ACL Findings 2025"},{"id":"http://arxiv.org/abs/2507.18198v1","updated":"2025-07-24T08:54:37Z","published":"2025-07-24T08:54:37Z","title":"Comparing Non-minimal Semantics for Disjunction in Answer Set\n  Programming","summary":"  In this paper, we compare four different semantics for disjunction in Answer\nSet Programming that, unlike stable models, do not adhere to the principle of\nmodel minimality. Two of these approaches, Cabalar and Mu\\~niz' \\emph{Justified\nModels} and Doherty and Szalas' \\emph{Strongly Supported Models}, directly\nprovide an alternative non-minimal semantics for disjunction. The other two,\nAguado et al's \\emph{Forks} and Shen and Eiter's \\emph{Determining Inference}\n(DI) semantics, actually introduce a new disjunction connective, but are\ncompared here as if they constituted new semantics for the standard disjunction\noperator. We are able to prove that three of these approaches (Forks, Justified\nModels and a reasonable relaxation of the DI semantics) actually coincide,\nconstituting a common single approach under different definitions. Moreover,\nthis common semantics always provides a superset of the stable models of a\nprogram (in fact, modulo any context) and is strictly stronger than the fourth\napproach (Strongly Supported Models), that actually treats disjunctions as in\nclassical logic.\n","authors":["Felicidad Aguado","Pedro Cabalar","Brais Muñiz","Gilberto Pérez","Concepción Vidal"],"pdf_url":"https://arxiv.org/pdf/2507.18198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05086v2","updated":"2025-07-24T08:52:22Z","published":"2025-05-08T09:34:15Z","title":"Beyond Low-rank Decomposition: A Shortcut Approach for Efficient\n  On-Device Learning","summary":"  On-device learning has emerged as a promising direction for AI development,\nparticularly because of its potential to reduce latency issues and mitigate\nprivacy risks associated with device-server communication, while improving\nenergy efficiency. Despite these advantages, significant memory and\ncomputational constraints still represent major challenges for its deployment.\nDrawing on previous studies on low-rank decomposition methods that address\nactivation memory bottlenecks in backpropagation, we propose a novel shortcut\napproach as an alternative. Our analysis and experiments demonstrate that our\nmethod can reduce activation memory usage, even up to $120.09\\times$ compared\nto vanilla training, while also reducing overall training FLOPs up to\n$1.86\\times$ when evaluated on traditional benchmarks.\n","authors":["Le-Trung Nguyen","Ael Quelennec","Van-Tam Nguyen","Enzo Tartaglione"],"pdf_url":"https://arxiv.org/pdf/2505.05086v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15610v4","updated":"2025-07-24T08:48:10Z","published":"2025-02-21T17:31:22Z","title":"A general language model for peptide identification","summary":"  Accurate identification of bioactive peptides (BPs) and protein\npost-translational modifications (PTMs) is essential for understanding protein\nfunction and advancing therapeutic discovery. However, most computational\nmethods remain limited in their generalizability across diverse peptide\nfunctions. Here, we present PDeepPP, a unified deep learning framework that\nintegrates pretrained protein language models with a hybrid\ntransformer-convolutional architecture, enabling robust identification across\ndiverse peptide classes and PTM sites. We curated comprehensive benchmark\ndatasets and implemented strategies to address data imbalance, allowing PDeepPP\nto systematically extract both global and local sequence features. Through\nextensive analyses-including dimensionality reduction and comparison\nstudies-PDeepPP demonstrates strong, interpretable peptide representations and\nachieves state-of-the-art performance in 25 of the 33 biological identification\ntasks. Notably, PDeepPP attains high accuracy in antimicrobial (0.9726) and\nphosphorylation site (0.9984) identification, with 99.5% specificity in\nglycosylation site prediction and substantial reduction in false negatives in\nantimalarial tasks. By enabling large-scale, accurate peptide analysis, PDeepPP\nsupports biomedical research and the discovery of novel therapeutic targets for\ndisease treatment. All code, datasets, and pretrained models are publicly\navailable via GitHub:https://github.com/fondress/PDeepPP and Hugging\nFace:https://huggingface.co/fondress/PDeppPP.\n","authors":["Jixiu Zhai","Tianchi Lu","Haitian Zhong","Ziyang Xu","Yuhuan Liu","Shengrui Xu","Jingwan Wang","Dan Huang"],"pdf_url":"https://arxiv.org/pdf/2502.15610v4.pdf","comment":"24 pages, 9 figures, 4 tables, submitted to arXiv"},{"id":"http://arxiv.org/abs/2507.18182v1","updated":"2025-07-24T08:28:17Z","published":"2025-07-24T08:28:17Z","title":"SCOPE: Stochastic and Counterbiased Option Placement for Evaluating\n  Large Language Models","summary":"  Large Language Models (LLMs) can achieve inflated scores on multiple-choice\ntasks by exploiting inherent biases in option positions or labels, rather than\ndemonstrating genuine understanding. This study introduces SCOPE, an evaluation\nframework designed to measure and mitigate such selection bias in a\ndataset-independent manner. By repeatedly invoking a null prompt that lacks\nsemantic content, SCOPE estimates each model's unique position-bias\ndistribution. It then redistributes the answer slot according to the\ninverse-bias distribution, thereby equalizing the lucky-rate, the probability\nof selecting the correct answer by chance. Furthermore, it prevents\nsemantically similar distractors from being placed adjacent to the answer,\nthereby blocking near-miss guesses based on superficial proximity cues. Across\nmultiple benchmark experiments, SCOPE consistently outperformed existing\ndebiasing methods in terms of stable performance improvements and showed\nclearer confidence distributions over correct options. This framework thus\noffers a new standard for enhancing the fairness and reliability of LLM\nevaluations.\n","authors":["Wonjun Jeong","Dongseok Kim","Taegkeun Whangbo"],"pdf_url":"https://arxiv.org/pdf/2507.18182v1.pdf","comment":"34 pages, 1 figure"},{"id":"http://arxiv.org/abs/2507.18178v1","updated":"2025-07-24T08:24:52Z","published":"2025-07-24T08:24:52Z","title":"Decoupling Knowledge and Reasoning in LLMs: An Exploration Using\n  Cognitive Dual-System Theory","summary":"  While large language models (LLMs) leverage both knowledge and reasoning\nduring inference, the capacity to distinguish between them plays a pivotal role\nin model analysis, interpretability, and development. Inspired by dual-system\ncognitive theory, we propose a cognition attribution framework to decouple the\ncontribution of knowledge and reasoning. In particular, the cognition of LLMs\nis decomposed into two distinct yet complementary phases: knowledge retrieval\n(Phase 1) and reasoning adjustment (Phase 2). To separate these phases, LLMs\nare prompted to generate answers under two different cognitive modes, fast\nthinking and slow thinking, respectively. The performance under different\ncognitive modes is analyzed to quantify the contribution of knowledge and\nreasoning. This architecture is employed to 15 LLMs across 3 datasets. Results\nreveal: (1) reasoning adjustment is domain-specific, benefiting\nreasoning-intensive domains (e.g., mathematics, physics, and chemistry) and\npotentially imparing knowledge-intensive domains. (2) Parameter scaling\nimproves both knowledge and reasoning, with knowledge improvements being more\npronounced. Additionally, parameter scaling make LLMs reasoning significantly\nmore prudent, while moderately more intelligent. (3) Knowledge primarily\nresides in lower network layers, while reasoning operates in higher layers. Our\nframework not only helps understand LLMs from a \"decoupling\" perspective, but\nalso provides new insights into existing research, including scaling laws,\nhierarchical knowledge editing, and limitations of small-model reasoning.\n","authors":["Mutian Yang","Jiandong Gao","Ji Wu"],"pdf_url":"https://arxiv.org/pdf/2507.18178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.16473v2","updated":"2025-07-24T08:23:56Z","published":"2025-07-22T11:22:58Z","title":"Learning Temporal Abstractions via Variational Homomorphisms in\n  Option-Induced Abstract MDPs","summary":"  Large Language Models (LLMs) have shown remarkable reasoning ability through\nexplicit Chain-of-Thought (CoT) prompting, but generating these step-by-step\ntextual explanations is computationally expensive and slow. To overcome this,\nwe aim to develop a framework for efficient, implicit reasoning, where the\nmodel \"thinks\" in a latent space without generating explicit text for every\nstep. We propose that these latent thoughts can be modeled as\ntemporally-extended abstract actions, or options, within a hierarchical\nreinforcement learning framework. To effectively learn a diverse library of\noptions as latent embeddings, we first introduce the Variational Markovian\nOption Critic (VMOC), an off-policy algorithm that uses variational inference\nwithin the HiT-MDP framework. To provide a rigorous foundation for using these\noptions as an abstract reasoning space, we extend the theory of continuous MDP\nhomomorphisms. This proves that learning a policy in the simplified, abstract\nlatent space, for which VMOC is suited, preserves the optimality of the\nsolution to the original, complex problem. Finally, we propose a cold-start\nprocedure that leverages supervised fine-tuning (SFT) data to distill human\nreasoning demonstrations into this latent option space, providing a rich\ninitialization for the model's reasoning capabilities. Extensive experiments\ndemonstrate that our approach achieves strong performance on complex logical\nreasoning benchmarks and challenging locomotion tasks, validating our framework\nas a principled method for learning abstract skills for both language and\ncontrol.\n","authors":["Chang Li","Yaren Zhang","Haoran Lv","Qiong Cao","Chao Xue","Xiaodong He"],"pdf_url":"https://arxiv.org/pdf/2507.16473v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18177v1","updated":"2025-07-24T08:23:11Z","published":"2025-07-24T08:23:11Z","title":"Differential-UMamba: Rethinking Tumor Segmentation Under Limited Data\n  Scenarios","summary":"  In data-scarce scenarios, deep learning models often overfit to noise and\nirrelevant patterns, which limits their ability to generalize to unseen\nsamples. To address these challenges in medical image segmentation, we\nintroduce Diff-UMamba, a novel architecture that combines the UNet framework\nwith the mamba mechanism for modeling long-range dependencies. At the heart of\nDiff-UMamba is a Noise Reduction Module (NRM), which employs a signal\ndifferencing strategy to suppress noisy or irrelevant activations within the\nencoder. This encourages the model to filter out spurious features and enhance\ntask-relevant representations, thereby improving its focus on clinically\nmeaningful regions. As a result, the architecture achieves improved\nsegmentation accuracy and robustness, particularly in low-data settings.\nDiff-UMamba is evaluated on multiple public datasets, including MSD (lung and\npancreas) and AIIB23, demonstrating consistent performance gains of 1-3% over\nbaseline methods across diverse segmentation tasks. To further assess\nperformance under limited-data conditions, additional experiments are conducted\non the BraTS-21 dataset by varying the proportion of available training\nsamples. The approach is also validated on a small internal non-small cell lung\ncancer (NSCLC) dataset for gross tumor volume (GTV) segmentation in cone beam\nCT (CBCT), where it achieves a 4-5% improvement over the baseline.\n","authors":["Dhruv Jain","Romain Modzelewski","Romain Hérault","Clement Chatelain","Eva Torfeh","Sebastien Thureau"],"pdf_url":"https://arxiv.org/pdf/2507.18177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18171v1","updated":"2025-07-24T08:13:16Z","published":"2025-07-24T08:13:16Z","title":"Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models","summary":"  Despite the widespread use of Transformer-based text embedding models in NLP\ntasks, surprising 'sticky tokens' can undermine the reliability of embeddings.\nThese tokens, when repeatedly inserted into sentences, pull sentence similarity\ntoward a certain value, disrupting the normal distribution of embedding\ndistances and degrading downstream performance. In this paper, we\nsystematically investigate such anomalous tokens, formally defining them and\nintroducing an efficient detection method, Sticky Token Detector (STD), based\non sentence and token filtering. Applying STD to 40 checkpoints across 14 model\nfamilies, we discover a total of 868 sticky tokens. Our analysis reveals that\nthese tokens often originate from special or unused entries in the vocabulary,\nas well as fragmented subwords from multilingual corpora. Notably, their\npresence does not strictly correlate with model size or vocabulary size. We\nfurther evaluate how sticky tokens affect downstream tasks like clustering and\nretrieval, observing significant performance drops of up to 50%. Through\nattention-layer analysis, we show that sticky tokens disproportionately\ndominate the model's internal representations, raising concerns about\ntokenization robustness. Our findings show the need for better tokenization\nstrategies and model design to mitigate the impact of sticky tokens in future\ntext embedding applications.\n","authors":["Kexin Chen","Dongxia Wang","Yi Liu","Haonan Zhang","Wenhai Wang"],"pdf_url":"https://arxiv.org/pdf/2507.18171v1.pdf","comment":"ACL 2025 main"},{"id":"http://arxiv.org/abs/2411.10371v5","updated":"2025-07-24T07:53:24Z","published":"2024-11-15T17:19:42Z","title":"A Survey of Event Causality Identification: Taxonomy, Challenges,\n  Assessment, and Prospects","summary":"  Event Causality Identification (ECI) has become an essential task in Natural\nLanguage Processing (NLP), focused on automatically detecting causal\nrelationships between events within texts. This comprehensive survey\nsystematically investigates fundamental concepts and models, developing a\nsystematic taxonomy and critically evaluating diverse models. We begin by\ndefining core concepts, formalizing the ECI problem, and outlining standard\nevaluation protocols. Our classification framework divides ECI models into two\nprimary tasks: Sentence-level Event Causality Identification (SECI) and\nDocument-level Event Causality Identification (DECI). For SECI, we review\nmodels employing feature pattern-based matching, machine learning classifiers,\ndeep semantic encoding, prompt-based fine-tuning, and causal knowledge\npre-training, alongside data augmentation strategies. For DECI, we focus on\napproaches utilizing deep semantic encoding, event graph reasoning, and\nprompt-based fine-tuning. Special attention is given to recent advancements in\nmulti-lingual and cross-lingual ECI, as well as zero-shot ECI leveraging Large\nLanguage Models (LLMs). We analyze the strengths, limitations, and unresolved\nchallenges associated with each approach. Extensive quantitative evaluations\nare conducted on four benchmark datasets to rigorously assess the performance\nof various ECI models. We conclude by discussing future research directions and\nhighlighting opportunities to advance the field further.\n","authors":["Qing Cheng","Zefan Zeng","Xingchen Hu","Yuehang Si","Zhong Liu"],"pdf_url":"https://arxiv.org/pdf/2411.10371v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.14516v2","updated":"2025-07-24T07:48:25Z","published":"2025-07-19T07:32:00Z","title":"SDSC:A Structure-Aware Metric for Semantic Signal Representation\n  Learning","summary":"  We propose the Signal Dice Similarity Coefficient (SDSC), a structure-aware\nmetric function for time series self-supervised representation learning. Most\nSelf-Supervised Learning (SSL) methods for signals commonly adopt\ndistance-based objectives such as mean squared error (MSE), which are sensitive\nto amplitude, invariant to waveform polarity, and unbounded in scale. These\nproperties hinder semantic alignment and reduce interpretability. SDSC\naddresses this by quantifying structural agreement between temporal signals\nbased on the intersection of signed amplitudes, derived from the Dice\nSimilarity Coefficient (DSC).Although SDSC is defined as a structure-aware\nmetric, it can be used as a loss by subtracting from 1 and applying a\ndifferentiable approximation of the Heaviside function for gradient-based\noptimization. A hybrid loss formulation is also proposed to combine SDSC with\nMSE, improving stability and preserving amplitude where necessary. Experiments\non forecasting and classification benchmarks demonstrate that SDSC-based\npre-training achieves comparable or improved performance over MSE, particularly\nin in-domain and low-resource scenarios. The results suggest that structural\nfidelity in signal representations enhances the semantic representation\nquality, supporting the consideration of structure-aware metrics as viable\nalternatives to conventional distance-based methods.\n","authors":["Jeyoung Lee","Hochul Kang"],"pdf_url":"https://arxiv.org/pdf/2507.14516v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18639v2","updated":"2025-07-24T07:47:46Z","published":"2025-02-25T20:59:22Z","title":"Quantum Machine Learning in Precision Medicine and Drug Discovery -- A\n  Game Changer for Tailored Treatments?","summary":"  The digitization of healthcare presents numerous challenges, including the\ncomplexity of biological systems, vast data generation, and the need for\npersonalized treatment plans. Traditional computational methods often fall\nshort, leading to delayed and sometimes ineffective diagnoses and treatments.\nQuantum Computing (QC) and Quantum Machine Learning (QML) offer transformative\nadvancements with the potential to revolutionize medicine. This paper\nsummarizes areas where QC promises unprecedented computational power, enabling\nfaster, more accurate diagnostics, personalized treatments, and enhanced drug\ndiscovery processes. However, integrating quantum technologies into precision\nmedicine also presents challenges, including errors in algorithms and high\ncosts. We show that mathematically-based techniques for specifying, developing,\nand verifying software (formal methods) can enhance the reliability and\ncorrectness of QC. By providing a rigorous mathematical framework, formal\nmethods help to specify, develop, and verify systems with high precision. In\ngenomic data analysis, formal specification languages can precisely (1) define\nthe behavior and properties of quantum algorithms designed to identify genetic\nmarkers associated with diseases. Model checking tools can systematically\nexplore all possible states of the algorithm to (2) ensure it behaves correctly\nunder all conditions, while theorem proving techniques provide mathematical (3)\nproof that the algorithm meets its specified properties, ensuring accuracy and\nreliability. Additionally, formal optimization techniques can (4) enhance the\nefficiency and performance of quantum algorithms by reducing resource usage,\nsuch as the number of qubits and gate operations. Therefore, we posit that\nformal methods can significantly contribute to enabling QC to realize its full\npotential as a game changer in precision medicine.\n","authors":["Markus Bertl","Alan Mott","Salvatore Sinno","Bhavika Bhalgamiya"],"pdf_url":"https://arxiv.org/pdf/2502.18639v2.pdf","comment":"presented at AISoLA 2024"},{"id":"http://arxiv.org/abs/2503.07588v3","updated":"2025-07-24T07:44:45Z","published":"2025-03-10T17:51:16Z","title":"When Large Vision-Language Model Meets Large Remote Sensing Imagery:\n  Coarse-to-Fine Text-Guided Token Pruning","summary":"  Efficient vision-language understanding of large Remote Sensing Images (RSIs)\nis meaningful but challenging. Current Large Vision-Language Models (LVLMs)\ntypically employ limited pre-defined grids to process images, leading to\ninformation loss when handling gigapixel RSIs. Conversely, using unlimited\ngrids significantly increases computational costs. To preserve image details\nwhile reducing computational complexity, we propose a text-guided token pruning\nmethod with Dynamic Image Pyramid (DIP) integration. Our method introduces: (i)\na Region Focus Module (RFM) that leverages text-aware region localization\ncapability to identify critical vision tokens, and (ii) a coarse-to-fine image\ntile selection and vision token pruning strategy based on DIP, which is guided\nby RFM outputs and avoids directly processing the entire large imagery.\nAdditionally, existing benchmarks for evaluating LVLMs' perception ability on\nlarge RSI suffer from limited question diversity and constrained image sizes.\nWe construct a new benchmark named LRS-VQA, which contains 7,333 QA pairs\nacross 8 categories, with image length up to 27,328 pixels. Our method\noutperforms existing high-resolution strategies on four datasets using the same\ndata. Moreover, compared to existing token reduction methods, our approach\ndemonstrates higher efficiency under high-resolution settings. Dataset and code\nare in https://github.com/VisionXLab/LRS-VQA.\n","authors":["Junwei Luo","Yingying Zhang","Xue Yang","Kang Wu","Qi Zhu","Lei Liang","Jingdong Chen","Yansheng Li"],"pdf_url":"https://arxiv.org/pdf/2503.07588v3.pdf","comment":"18 pages, 6 figures, 18 tables"},{"id":"http://arxiv.org/abs/2507.18153v1","updated":"2025-07-24T07:39:07Z","published":"2025-07-24T07:39:07Z","title":"When Noisy Labels Meet Class Imbalance on Graphs: A Graph Augmentation\n  Method with LLM and Pseudo Label","summary":"  Class-imbalanced graph node classification is a practical yet underexplored\nresearch problem. Although recent studies have attempted to address this issue,\nthey typically assume clean and reliable labels when processing\nclass-imbalanced graphs. This assumption often violates the nature of\nreal-world graphs, where labels frequently contain noise. Given this gap, this\npaper systematically investigates robust node classification for\nclass-imbalanced graphs with noisy labels. We propose GraphALP, a novel Graph\nAugmentation framework based on Large language models (LLMs) and\nPseudo-labeling techniques. Specifically, we design an LLM-based oversampling\nmethod to generate synthetic minority nodes, producing label-accurate minority\nnodes to alleviate class imbalance. Based on the class-balanced graphs, we\ndevelop a dynamically weighted pseudo-labeling method to obtain high-confidence\npseudo labels to reduce label noise ratio. Additionally, we implement a\nsecondary LLM-guided oversampling mechanism to mitigate potential class\ndistribution skew caused by pseudo labels. Experimental results show that\nGraphALP achieves superior performance over state-of-the-art methods on\nclass-imbalanced graphs with noisy labels.\n","authors":["Riting Xia","Rucong Wang","Yulin Liu","Anchen Li","Xueyan Liu","Yan Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.18153v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16354v3","updated":"2025-07-24T07:23:42Z","published":"2024-11-25T13:04:53Z","title":"Scalable Parameter Design for Superconducting Quantum Circuits with\n  Graph Neural Networks","summary":"  To demonstrate supremacy of quantum computing, increasingly large-scale\nsuperconducting quantum computing chips are being designed and fabricated.\nHowever, the complexity of simulating quantum systems poses a significant\nchallenge to computer-aided design of quantum chips, especially for large-scale\nchips. Harnessing the scalability of graph neural networks (GNNs), we here\npropose a parameter designing algorithm for large-scale superconducting quantum\ncircuits. The algorithm depends on the so-called 'three-stair scaling'\nmechanism, which comprises two neural-network models: an evaluator supervisedly\ntrained on small-scale circuits for applying to medium-scale circuits, and a\ndesigner unsupervisedly trained on medium-scale circuits for applying to\nlarge-scale ones. We demonstrate our algorithm in mitigating quantum crosstalk\nerrors. Frequencies for both single- and two-qubit gates (corresponding to the\nparameters of nodes and edges) are considered simultaneously. Numerical results\nindicate that the well-trained designer achieves notable advantages in\nefficiency, effectiveness, and scalability. For example, for large-scale\nsuperconducting quantum circuits consisting of around 870 qubits, our\nGNNs-based algorithm achieves 51% of the errors produced by the\nstate-of-the-art algorithm, with a time reduction from 90 min to 27 sec.\nOverall, a better-performing and more scalable algorithm for designing\nparameters of superconducting quantum chips is proposed, which initially\ndemonstrates the advantages of applying GNNs in superconducting quantum chips.\n","authors":["Hao Ai","Yu-xi Liu"],"pdf_url":"https://arxiv.org/pdf/2411.16354v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18145v1","updated":"2025-07-24T07:21:49Z","published":"2025-07-24T07:21:49Z","title":"Logical Characterizations of GNNs with Mean Aggregation","summary":"  We study the expressive power of graph neural networks (GNNs) with mean as\nthe aggregation function. In the non-uniform setting, we show that such GNNs\nhave exactly the same expressive power as ratio modal logic, which has modal\noperators expressing that at least a certain ratio of the successors of a\nvertex satisfies a specified property. The non-uniform expressive power of mean\nGNNs is thus higher than that of GNNs with max aggregation, but lower than for\nsum aggregation--the latter are characterized by modal logic and graded modal\nlogic, respectively. In the uniform setting, we show that the expressive power\nrelative to MSO is exactly that of alternation-free modal logic, under the\nnatural assumptions that combination functions are continuous and\nclassification functions are thresholds. This implies that, relative to MSO and\nin the uniform setting, mean GNNs are strictly less expressive than sum GNNs\nand max GNNs. When any of the assumptions is dropped, the expressive power\nincreases.\n","authors":["Moritz Schönherr","Carsten Lutz"],"pdf_url":"https://arxiv.org/pdf/2507.18145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17248v2","updated":"2025-07-24T07:13:36Z","published":"2025-07-23T06:34:58Z","title":"Reality Proxy: Fluid Interactions with Real-World Objects in MR via\n  Abstract Representations","summary":"  Interacting with real-world objects in Mixed Reality (MR) often proves\ndifficult when they are crowded, distant, or partially occluded, hindering\nstraightforward selection and manipulation. We observe that these difficulties\nstem from performing interaction directly on physical objects, where input is\ntightly coupled to their physical constraints. Our key insight is to decouple\ninteraction from these constraints by introducing proxies-abstract\nrepresentations of real-world objects. We embody this concept in Reality Proxy,\na system that seamlessly shifts interaction targets from physical objects to\ntheir proxies during selection. Beyond facilitating basic selection, Reality\nProxy uses AI to enrich proxies with semantic attributes and hierarchical\nspatial relationships of their corresponding physical objects, enabling novel\nand previously cumbersome interactions in MR - such as skimming,\nattribute-based filtering, navigating nested groups, and complex multi object\nselections - all without requiring new gestures or menu systems. We demonstrate\nReality Proxy's versatility across diverse scenarios, including office\ninformation retrieval, large-scale spatial navigation, and multi-drone control.\nAn expert evaluation suggests the system's utility and usability, suggesting\nthat proxy-based abstractions offer a powerful and generalizable interaction\nparadigm for future MR systems.\n","authors":["Xiaoan Liu","Difan Jia","Xianhao Carton Liu","Mar Gonzalez-Franco","Chen Zhu-Tian"],"pdf_url":"https://arxiv.org/pdf/2507.17248v2.pdf","comment":"16 pages, 9 figures. Accepted for publication in UIST'25 (The 38th\n  Annual ACM Symposium on User Interface Software and Technology), Busan,\n  Republic of Korea, 28 Sep - 1 Oct 2025"},{"id":"http://arxiv.org/abs/2507.18143v1","updated":"2025-07-24T07:06:30Z","published":"2025-07-24T07:06:30Z","title":"HIVMedQA: Benchmarking large language models for HIV medical decision\n  support","summary":"  Large language models (LLMs) are emerging as valuable tools to support\nclinicians in routine decision-making. HIV management is a compelling use case\ndue to its complexity, including diverse treatment options, comorbidities, and\nadherence challenges. However, integrating LLMs into clinical practice raises\nconcerns about accuracy, potential harm, and clinician acceptance. Despite\ntheir promise, AI applications in HIV care remain underexplored, and LLM\nbenchmarking studies are scarce. This study evaluates the current capabilities\nof LLMs in HIV management, highlighting their strengths and limitations. We\nintroduce HIVMedQA, a benchmark designed to assess open-ended medical question\nanswering in HIV care. The dataset consists of curated, clinically relevant\nquestions developed with input from an infectious disease physician. We\nevaluated seven general-purpose and three medically specialized LLMs, applying\nprompt engineering to enhance performance. Our evaluation framework\nincorporates both lexical similarity and an LLM-as-a-judge approach, extended\nto better reflect clinical relevance. We assessed performance across key\ndimensions: question comprehension, reasoning, knowledge recall, bias,\npotential harm, and factual accuracy. Results show that Gemini 2.5 Pro\nconsistently outperformed other models across most dimensions. Notably, two of\nthe top three models were proprietary. Performance declined as question\ncomplexity increased. Medically fine-tuned models did not always outperform\ngeneral-purpose ones, and larger model size was not a reliable predictor of\nperformance. Reasoning and comprehension were more challenging than factual\nrecall, and cognitive biases such as recency and status quo were observed.\nThese findings underscore the need for targeted development and evaluation to\nensure safe, effective LLM integration in clinical care.\n","authors":["Gonzalo Cardenal Antolin","Jacques Fellay","Bashkim Jaha","Roger Kouyos","Niko Beerenwinkel","Diane Duroux"],"pdf_url":"https://arxiv.org/pdf/2507.18143v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18133v1","updated":"2025-07-24T06:47:23Z","published":"2025-07-24T06:47:23Z","title":"Deep Learning for Glioblastoma Morpho-pathological Features\n  Identification: A BraTS-Pathology Challenge Solution","summary":"  Glioblastoma, a highly aggressive brain tumor with diverse molecular and\npathological features, poses a diagnostic challenge due to its heterogeneity.\nAccurate diagnosis and assessment of this heterogeneity are essential for\nchoosing the right treatment and improving patient outcomes. Traditional\nmethods rely on identifying specific features in tissue samples, but deep\nlearning offers a promising approach for improved glioblastoma diagnosis. In\nthis paper, we present our approach to the BraTS-Path Challenge 2024. We\nleverage a pre-trained model and fine-tune it on the BraTS-Path training\ndataset. Our model demonstrates poor performance on the challenging BraTS-Path\nvalidation set, as rigorously assessed by the Synapse online platform. The\nmodel achieves an accuracy of 0.392229, a recall of 0.392229, and a F1-score of\n0.392229, indicating a consistent ability to correctly identify instances under\nthe target condition. Notably, our model exhibits perfect specificity of\n0.898704, showing an exceptional capacity to correctly classify negative cases.\nMoreover, a Matthews Correlation Coefficient (MCC) of 0.255267 is calculated,\nto signify a limited positive correlation between predicted and actual values\nand highlight our model's overall predictive power. Our solution also achieves\nthe second place during the testing phase.\n","authors":["Juexin Zhang","Ying Weng","Ke Chen"],"pdf_url":"https://arxiv.org/pdf/2507.18133v1.pdf","comment":"Accepted by the International Brain Tumor Segmentation (BraTS)\n  challenge organized at MICCAI 2024 conference"},{"id":"http://arxiv.org/abs/2507.18126v1","updated":"2025-07-24T06:26:46Z","published":"2025-07-24T06:26:46Z","title":"U-Net Based Healthy 3D Brain Tissue Inpainting","summary":"  This paper introduces a novel approach to synthesize healthy 3D brain tissue\nfrom masked input images, specifically focusing on the task of 'ASNR-MICCAI\nBraTS Local Synthesis of Tissue via Inpainting'. Our proposed method employs a\nU-Net-based architecture, which is designed to effectively reconstruct the\nmissing or corrupted regions of brain MRI scans. To enhance our model's\ngeneralization capabilities and robustness, we implement a comprehensive data\naugmentation strategy that involves randomly masking healthy images during\ntraining. Our model is trained on the BraTS-Local-Inpainting dataset and\ndemonstrates the exceptional performance in recovering healthy brain tissue.\nThe evaluation metrics employed, including Structural Similarity Index (SSIM),\nPeak Signal-to-Noise Ratio (PSNR), and Mean Squared Error (MSE), consistently\nyields impressive results. On the BraTS-Local-Inpainting validation set, our\nmodel achieved an SSIM score of 0.841, a PSNR score of 23.257, and an MSE score\nof 0.007. Notably, these evaluation metrics exhibit relatively low standard\ndeviations, i.e., 0.103 for SSIM score, 4.213 for PSNR score and 0.007 for MSE\nscore, which indicates that our model's reliability and consistency across\nvarious input scenarios. Our method also secured first place in the challenge.\n","authors":["Juexin Zhang","Ying Weng","Ke Chen"],"pdf_url":"https://arxiv.org/pdf/2507.18126v1.pdf","comment":"Accepted by the International Brain Tumor Segmentation (BraTS)\n  challenge organized at MICCAI 2024 conference. Included 7 pages, 2 figures"},{"id":"http://arxiv.org/abs/2412.18003v3","updated":"2025-07-24T06:25:26Z","published":"2024-12-23T21:53:06Z","title":"Integrated Learning and Optimization for Congestion Management and\n  Profit Maximization in Real-Time Electricity Market","summary":"  We develop novel integrated learning and optimization (ILO) methodologies to\nsolve economic dispatch (ED) and DC optimal power flow (DCOPF) problems for\nbetter economic operation. The optimization problem for ED is formulated with\nload being an unknown parameter while DCOPF consists of load and power transfer\ndistribution factor (PTDF) matrix as unknown parameters. PTDF represents the\nincremental variations of real power on transmission lines which occur due to\nreal power transfers between two regions. These values represent a linearized\napproximation of power flows over the transmission lines. We develop novel ILO\nformulations to solve post-hoc penalties in electricity market and line\ncongestion problems using ED and DCOPF optimization formulations. Our proposed\nmethodologies capture the real-time electricity market and line congestion\nbehavior to train the regret function which eventually train unknown loads at\ndifferent buses and line PTDF matrix to achieve the afore-mentioned post-hoc\ngoals. The proposed methodology is compared to sequential learning and\noptimization (SLO) which train load and PTDF forecasts for accuracy rather than\neconomic operation. Our experimentation prove the superiority of ILO in\nminimizing the post-hoc penalties in electricity markets and minimizing the\nline congestion thereby improving the economic operation with noticeable\namount.\n","authors":["Imran Pervez","Ricardo Pinto Lima","Omar Knio"],"pdf_url":"https://arxiv.org/pdf/2412.18003v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18123v1","updated":"2025-07-24T06:18:34Z","published":"2025-07-24T06:18:34Z","title":"Actively evaluating and learning the distinctions that matter: Vaccine\n  safety signal detection from emergency triage notes","summary":"  The rapid development of COVID-19 vaccines has showcased the global\ncommunitys ability to combat infectious diseases. However, the need for\npost-licensure surveillance systems has grown due to the limited window for\nsafety data collection in clinical trials and early widespread implementation.\nThis study aims to employ Natural Language Processing techniques and Active\nLearning to rapidly develop a classifier that detects potential vaccine safety\nissues from emergency department notes. ED triage notes, containing expert,\nsuccinct vital patient information at the point of entry to health systems, can\nsignificantly contribute to timely vaccine safety signal surveillance. While\nkeyword-based classification can be effective, it may yield false positives and\ndemand extensive keyword modifications. This is exacerbated by the infrequency\nof vaccination-related ED presentations and their similarity to other reasons\nfor ED visits. NLP offers a more accurate and efficient alternative, albeit\nrequiring annotated data, which is often scarce in the medical field. Active\nlearning optimizes the annotation process and the quality of annotated data,\nwhich can result in faster model implementation and improved model performance.\nThis work combines active learning, data augmentation, and active learning and\nevaluation techniques to create a classifier that is used to enhance vaccine\nsafety surveillance from ED triage notes.\n","authors":["Sedigh Khademi","Christopher Palmer","Muhammad Javed","Hazel Clothier","Jim Buttery","Gerardo Luis Dimaguila","Jim Black"],"pdf_url":"https://arxiv.org/pdf/2507.18123v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2507.09682v2","updated":"2025-07-24T06:16:38Z","published":"2025-07-13T15:38:39Z","title":"OrQstrator: An AI-Powered Framework for Advanced Quantum Circuit\n  Optimization","summary":"  We propose a novel approach, OrQstrator, which is a modular framework for\nconducting quantum circuit optimization in the Noisy Intermediate-Scale Quantum\n(NISQ) era. Our framework is powered by Deep Reinforcement Learning (DRL). Our\norchestration engine intelligently selects among three complementary circuit\noptimizers: A DRL-based circuit rewriter trained to reduce depth and gate count\nvia learned rewrite sequences; a domain-specific optimizer that performs\nefficient local gate resynthesis and numeric optimization; a parameterized\ncircuit instantiator that improves compilation by optimizing template circuits\nduring gate set translation. These modules are coordinated by a central\norchestration engine that learns coordination policies based on circuit\nstructure, hardware constraints, and backend-aware performance features such as\ngate count, depth, and expected fidelity. The system outputs an optimized\ncircuit for hardware-aware transpilation and execution, leveraging techniques\nfrom an existing state-of-the-art approach, called the NISQ Analyzer, to adapt\nto backend constraints.\n","authors":["Laura Baird","Armin Moin"],"pdf_url":"https://arxiv.org/pdf/2507.09682v2.pdf","comment":"IEEE International Conference on Quantum Computing and Engineering\n  (QCE) 2025 - Extended Abstract"},{"id":"http://arxiv.org/abs/2507.11936v3","updated":"2025-07-24T06:15:29Z","published":"2025-07-16T06:03:08Z","title":"A Survey of Deep Learning for Geometry Problem Solving","summary":"  Geometry problem solving is a key area of mathematical reasoning, which is\nwidely involved in many important fields such as education, mathematical\nability assessment of artificial intelligence, and multimodal ability\nassessment. In recent years, the rapid development of deep learning technology,\nespecially the rise of multimodal large language models, has triggered a\nwidespread research boom. This paper provides a survey of the applications of\ndeep learning in geometry problem solving, including (i) a comprehensive\nsummary of the relevant tasks in geometry problem solving; (ii) a thorough\nreview of related deep learning methods; (iii) a detailed analysis of\nevaluation metrics and methods; and (iv) a critical discussion of the current\nchallenges and future directions that can be explored. Our goal is to provide a\ncomprehensive and practical reference of deep learning for geometry problem\nsolving to promote further developments in this field. We create a continuously\nupdated list of papers on GitHub: https://github.com/majianz/dl4gps.\n","authors":["Jianzhe Ma","Wenxuan Wang","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2507.11936v3.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2502.00352v2","updated":"2025-07-24T06:12:24Z","published":"2025-02-01T07:16:15Z","title":"A Differentiated Reward Method for Reinforcement Learning based\n  Multi-Vehicle Cooperative Decision-Making Algorithms","summary":"  Reinforcement learning (RL) shows great potential for optimizing\nmulti-vehicle cooperative driving strategies through the state-action-reward\nfeedback loop, but it still faces challenges such as low sample efficiency.\nThis paper proposes a differentiated reward method based on steady-state\ntransition systems, which incorporates state transition gradient information\ninto the reward design by analyzing traffic flow characteristics, aiming to\noptimize action selection and policy learning in multi-vehicle cooperative\ndecision-making. The performance of the proposed method is validated in RL\nalgorithms such as MAPPO, MADQN, and QMIX under varying autonomous vehicle\npenetration. The results show that the differentiated reward method\nsignificantly accelerates training convergence and outperforms centering reward\nand others in terms of traffic efficiency, safety, and action rationality.\nAdditionally, the method demonstrates strong scalability and environmental\nadaptability, providing a novel approach for multi-agent cooperative\ndecision-making in complex traffic scenarios.\n","authors":["Ye Han","Lijun Zhang","Dejian Meng","Zhuang Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.00352v2.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2412.14209v2","updated":"2025-07-24T06:11:52Z","published":"2024-12-17T13:02:05Z","title":"Integrating Evidence into the Design of XAI and AI-based Decision\n  Support Systems: A Means-End Framework for End-users in Construction","summary":"  Explainable Artificial Intelligence seeks to make the reasoning processes of\nAI models transparent and interpretable, particularly in complex decision\nmaking environments. In the construction industry, where AI based decision\nsupport systems are increasingly adopted, limited attention has been paid to\nthe integration of supporting evidence that underpins the reliability and\naccountability of AI generated outputs. The absence of such evidence undermines\nthe validity of explanations and the trustworthiness of system recommendations.\nThis paper addresses this gap by introducing a theoretical, evidence based\nmeans end framework developed through a narrative review. The framework offers\nan epistemic foundation for designing XAI enabled DSS that generate meaningful\nexplanations tailored to users knowledge needs and decision contexts. It\nfocuses on evaluating the strength, relevance, and utility of different types\nof evidence supporting AI generated explanations. While developed with\nconstruction professionals as primary end users, the framework is also\napplicable to developers, regulators, and project managers with varying\nepistemic goals.\n","authors":["Peter E. D. Love","Jane Matthews","Weili Fang","Hadi Mahamivanan"],"pdf_url":"https://arxiv.org/pdf/2412.14209v2.pdf","comment":"74 pages, 5 figures and 3 tables"},{"id":"http://arxiv.org/abs/2507.18119v1","updated":"2025-07-24T06:10:29Z","published":"2025-07-24T06:10:29Z","title":"GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker\n  Characteristic Awareness","summary":"  Recent advances in end-to-end spoken language models (SLMs) have\nsignificantly improved the ability of AI systems to engage in natural spoken\ninteractions. However, most existing models treat speech merely as a vehicle\nfor linguistic content, often overlooking the rich paralinguistic and speaker\ncharacteristic cues embedded in human speech, such as dialect, age, emotion,\nand non-speech vocalizations. In this work, we introduce GOAT-SLM, a novel\nspoken language model with paralinguistic and speaker characteristic awareness,\ndesigned to extend spoken language modeling beyond text semantics. GOAT-SLM\nadopts a dual-modality head architecture that decouples linguistic modeling\nfrom acoustic realization, enabling robust language understanding while\nsupporting expressive and adaptive speech generation. To enhance model\nefficiency and versatility, we propose a modular, staged training strategy that\nprogressively aligns linguistic, paralinguistic, and speaker characteristic\ninformation using large-scale speech-text corpora. Experimental results on\nTELEVAL, a multi-dimensional evaluation benchmark, demonstrate that GOAT-SLM\nachieves well-balanced performance across both semantic and non-semantic tasks,\nand outperforms existing open-source models in handling emotion, dialectal\nvariation, and age-sensitive interactions. This work highlights the importance\nof modeling beyond linguistic content and advances the development of more\nnatural, adaptive, and socially aware spoken language systems.\n","authors":["Hongjie Chen","Zehan Li","Yaodong Song","Wenming Deng","Yitong Yao","Yuxin Zhang","Hang Lv","Xuechao Zhu","Jian Kang","Jie Lian","Jie Li","Chao Wang","Shuangyong Song","Yongxiang Li","Zhongjiang He"],"pdf_url":"https://arxiv.org/pdf/2507.18119v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.14660v2","updated":"2025-07-24T06:00:02Z","published":"2025-07-19T15:17:30Z","title":"When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion\n  in Social Systems","summary":"  Recent large-scale events like election fraud and financial scams have shown\nhow harmful coordinated efforts by human groups can be. With the rise of\nautonomous AI systems, there is growing concern that AI-driven groups could\nalso cause similar harm. While most AI safety research focuses on individual AI\nsystems, the risks posed by multi-agent systems (MAS) in complex real-world\nsituations are still underexplored. In this paper, we introduce a\nproof-of-concept to simulate the risks of malicious MAS collusion, using a\nflexible framework that supports both centralized and decentralized\ncoordination structures. We apply this framework to two high-risk fields:\nmisinformation spread and e-commerce fraud. Our findings show that\ndecentralized systems are more effective at carrying out malicious actions than\ncentralized ones. The increased autonomy of decentralized systems allows them\nto adapt their strategies and cause more damage. Even when traditional\ninterventions, like content flagging, are applied, decentralized groups can\nadjust their tactics to avoid detection. We present key insights into how these\nmalicious groups operate and the need for better detection systems and\ncountermeasures. Code is available at https://github.com/renqibing/RogueAgent.\n","authors":["Qibing Ren","Sitao Xie","Longxuan Wei","Zhenfei Yin","Junchi Yan","Lizhuang Ma","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2507.14660v2.pdf","comment":"Code is available at\n  https://github.com/renqibing/MultiAgent4Collusion"},{"id":"http://arxiv.org/abs/2507.18115v1","updated":"2025-07-24T05:56:25Z","published":"2025-07-24T05:56:25Z","title":"Agentic AI framework for End-to-End Medical Data Inference","summary":"  Building and deploying machine learning solutions in healthcare remains\nexpensive and labor-intensive due to fragmented preprocessing workflows, model\ncompatibility issues, and stringent data privacy constraints. In this work, we\nintroduce an Agentic AI framework that automates the entire clinical data\npipeline, from ingestion to inference, through a system of modular,\ntask-specific agents. These agents handle both structured and unstructured\ndata, enabling automatic feature selection, model selection, and preprocessing\nrecommendation without manual intervention. We evaluate the system on publicly\navailable datasets from geriatrics, palliative care, and colonoscopy imaging.\nFor example, in the case of structured data (anxiety data) and unstructured\ndata (colonoscopy polyps data), the pipeline begins with file-type detection by\nthe Ingestion Identifier Agent, followed by the Data Anonymizer Agent ensuring\nprivacy compliance, where we first identify the data type and then anonymize\nit. The Feature Extraction Agent identifies features using an embedding-based\napproach for tabular data, extracting all column names, and a multi-stage\nMedGemma-based approach for image data, which infers modality and disease name.\nThese features guide the Model-Data Feature Matcher Agent in selecting the\nbest-fit model from a curated repository. The Preprocessing Recommender Agent\nand Preprocessing Implementor Agent then apply tailored preprocessing based on\ndata type and model requirements. Finally, the ``Model Inference Agent\" runs\nthe selected model on the uploaded data and generates interpretable outputs\nusing tools like SHAP, LIME, and DETR attention maps. By automating these\nhigh-friction stages of the ML lifecycle, the proposed framework reduces the\nneed for repeated expert intervention, offering a scalable, cost-efficient\npathway for operationalizing AI in clinical environments.\n","authors":["Soorya Ram Shimgekar","Shayan Vassef","Abhay Goyal","Navin Kumar","Koustuv Saha"],"pdf_url":"https://arxiv.org/pdf/2507.18115v1.pdf","comment":"10 pages, 5 figures, 2 tables, BIBM conference"},{"id":"http://arxiv.org/abs/2507.18112v1","updated":"2025-07-24T05:51:51Z","published":"2025-07-24T05:51:51Z","title":"Parameter-Efficient Fine-Tuning of 3D DDPM for MRI Image Generation\n  Using Tensor Networks","summary":"  We address the challenge of parameter-efficient fine-tuning (PEFT) for\nthree-dimensional (3D) U-Net-based denoising diffusion probabilistic models\n(DDPMs) in magnetic resonance imaging (MRI) image generation. Despite its\npractical significance, research on parameter-efficient representations of 3D\nconvolution operations remains limited. To bridge this gap, we propose Tensor\nVolumetric Operator (TenVOO), a novel PEFT method specifically designed for\nfine-tuning DDPMs with 3D convolutional backbones. Leveraging tensor network\nmodeling, TenVOO represents 3D convolution kernels with lower-dimensional\ntensors, effectively capturing complex spatial dependencies during fine-tuning\nwith few parameters. We evaluate TenVOO on three downstream brain MRI\ndatasets-ADNI, PPMI, and BraTS2021-by fine-tuning a DDPM pretrained on 59,830\nT1-weighted brain MRI scans from the UK Biobank. Our results demonstrate that\nTenVOO achieves state-of-the-art performance in multi-scale structural\nsimilarity index measure (MS-SSIM), outperforming existing approaches in\ncapturing spatial dependencies while requiring only 0.3% of the trainable\nparameters of the original model. Our code is available at:\nhttps://github.com/xiaovhua/tenvoo\n","authors":["Binghua Li","Ziqing Chang","Tong Liang","Chao Li","Toshihisa Tanaka","Shigeki Aoki","Qibin Zhao","Zhe Sun"],"pdf_url":"https://arxiv.org/pdf/2507.18112v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18106v1","updated":"2025-07-24T05:35:49Z","published":"2025-07-24T05:35:49Z","title":"Distributional Uncertainty for Out-of-Distribution Detection","summary":"  Estimating uncertainty from deep neural networks is a widely used approach\nfor detecting out-of-distribution (OoD) samples, which typically exhibit high\npredictive uncertainty. However, conventional methods such as Monte Carlo (MC)\nDropout often focus solely on either model or data uncertainty, failing to\nalign with the semantic objective of OoD detection. To address this, we propose\nthe Free-Energy Posterior Network, a novel framework that jointly models\ndistributional uncertainty and identifying OoD and misclassified regions using\nfree energy. Our method introduces two key contributions: (1) a\nfree-energy-based density estimator parameterized by a Beta distribution, which\nenables fine-grained uncertainty estimation near ambiguous or unseen regions;\nand (2) a loss integrated within a posterior network, allowing direct\nuncertainty estimation from learned parameters without requiring stochastic\nsampling. By integrating our approach with the residual prediction branch (RPL)\nframework, the proposed method goes beyond post-hoc energy thresholding and\nenables the network to learn OoD regions by leveraging the variance of the Beta\ndistribution, resulting in a semantically meaningful and computationally\nefficient solution for uncertainty-aware segmentation. We validate the\neffectiveness of our method on challenging real-world benchmarks, including\nFishyscapes, RoadAnomaly, and Segment-Me-If-You-Can.\n","authors":["JinYoung Kim","DaeUng Jo","Kimin Yun","Jeonghyo Song","Youngjoon Yoo"],"pdf_url":"https://arxiv.org/pdf/2507.18106v1.pdf","comment":"6 pages , 3 figures , IEEE International Conference on Advanced\n  Visual and Signal-Based Systems"},{"id":"http://arxiv.org/abs/2507.18100v1","updated":"2025-07-24T05:24:01Z","published":"2025-07-24T05:24:01Z","title":"Datasets and Recipes for Video Temporal Grounding via Reinforcement\n  Learning","summary":"  Video Temporal Grounding (VTG) aims to localize relevant temporal segments in\nvideos given natural language queries. Despite recent progress with large\nvision-language models (LVLMs) and instruction-tuning, existing approaches\noften suffer from limited temporal awareness and poor generalization. In this\nwork, we introduce a two-stage training framework that integrates supervised\nfine-tuning with reinforcement learning (RL) to improve both the accuracy and\nrobustness of VTG models. Our approach first leverages high-quality curated\ncold start data for SFT initialization, followed by difficulty-controlled RL to\nfurther enhance temporal localization and reasoning abilities. Comprehensive\nexperiments on multiple VTG benchmarks demonstrate that our method consistently\noutperforms existing models, particularly in challenging and open-domain\nscenarios. We conduct an in-depth analysis of training strategies and dataset\ncuration, highlighting the importance of both high-quality cold start data and\ndifficulty-controlled RL. To facilitate further research and industrial\nadoption, we release all intermediate datasets, models, and code to the\ncommunity.\n","authors":["Ruizhe Chen","Zhiting Fan","Tianze Luo","Heqing Zou","Zhaopeng Feng","Guiyang Xie","Hansheng Zhang","Zhuochen Wang","Zuozhu Liu","Huaijian Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.18100v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17724v2","updated":"2025-07-24T05:22:14Z","published":"2025-03-22T10:41:46Z","title":"Trigger without Trace: Towards Stealthy Backdoor Attack on Text-to-Image\n  Diffusion Models","summary":"  Backdoor attacks targeting text-to-image diffusion models have advanced\nrapidly. However, current backdoor samples often exhibit two key abnormalities\ncompared to benign samples: 1) Semantic Consistency, where backdoor prompts\ntend to generate images with similar semantic content even with significant\ntextual variations to the prompts; 2) Attention Consistency, where the trigger\ninduces consistent structural responses in the cross-attention maps. These\nconsistencies leave detectable traces for defenders, making backdoors easier to\nidentify. In this paper, toward stealthy backdoor samples, we propose Trigger\nwithout Trace (TwT) by explicitly mitigating these consistencies. Specifically,\nour approach leverages syntactic structures as backdoor triggers to amplify the\nsensitivity to textual variations, effectively breaking down the semantic\nconsistency. Besides, a regularization method based on Kernel Maximum Mean\nDiscrepancy (KMMD) is proposed to align the distribution of cross-attention\nresponses between backdoor and benign samples, thereby disrupting attention\nconsistency. Extensive experiments demonstrate that our method achieves a 97.5%\nattack success rate while exhibiting stronger resistance to defenses. It\nachieves an average of over 98% backdoor samples bypassing three\nstate-of-the-art detection mechanisms, revealing the vulnerabilities of current\nbackdoor defense methods. The code is available at\nhttps://github.com/Robin-WZQ/TwT.\n","authors":["Jie Zhang","Zhongqi Wang","Shiguang Shan","Xilin Chen"],"pdf_url":"https://arxiv.org/pdf/2503.17724v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.15205v2","updated":"2025-07-24T05:15:18Z","published":"2025-07-21T03:12:54Z","title":"Long-Short Distance Graph Neural Networks and Improved Curriculum\n  Learning for Emotion Recognition in Conversation","summary":"  Emotion Recognition in Conversation (ERC) is a practical and challenging\ntask. This paper proposes a novel multimodal approach, the Long-Short Distance\nGraph Neural Network (LSDGNN). Based on the Directed Acyclic Graph (DAG), it\nconstructs a long-distance graph neural network and a short-distance graph\nneural network to obtain multimodal features of distant and nearby utterances,\nrespectively. To ensure that long- and short-distance features are as distinct\nas possible in representation while enabling mutual influence between the two\nmodules, we employ a Differential Regularizer and incorporate a BiAffine Module\nto facilitate feature interaction. In addition, we propose an Improved\nCurriculum Learning (ICL) to address the challenge of data imbalance. By\ncomputing the similarity between different emotions to emphasize the shifts in\nsimilar emotions, we design a \"weighted emotional shift\" metric and develop a\ndifficulty measurer, enabling a training process that prioritizes learning easy\nsamples before harder ones. Experimental results on the IEMOCAP and MELD\ndatasets demonstrate that our model outperforms existing benchmarks.\n","authors":["Xinran Li","Xiujuan Xu","Jiaqi Qiao"],"pdf_url":"https://arxiv.org/pdf/2507.15205v2.pdf","comment":"Accepted by the 28th European Conference on Artificial Intelligence\n  (ECAI 2025)"},{"id":"http://arxiv.org/abs/2506.15690v3","updated":"2025-07-24T05:08:02Z","published":"2025-05-26T22:10:52Z","title":"LLM Web Dynamics: Tracing Model Collapse in a Network of LLMs","summary":"  The increasing use of synthetic data from the public Internet has enhanced\ndata usage efficiency in large language model (LLM) training. However, the\npotential threat of model collapse remains insufficiently explored. Existing\nstudies primarily examine model collapse in a single model setting or rely\nsolely on statistical surrogates. In this work, we introduce LLM Web Dynamics\n(LWD), an efficient framework for investigating model collapse at the network\nlevel. By simulating the Internet with a retrieval-augmented generation (RAG)\ndatabase, we analyze the convergence pattern of model outputs. Furthermore, we\nprovide theoretical guarantees for this convergence by drawing an analogy to\ninteracting Gaussian Mixture Models.\n","authors":["Tianyu Wang","Akira Horiguchi","Lingyou Pang","Carey E. Priebe"],"pdf_url":"https://arxiv.org/pdf/2506.15690v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.08562v2","updated":"2025-07-24T04:51:19Z","published":"2024-11-13T12:19:46Z","title":"Neural Corrective Machine Unranking","summary":"  Machine unlearning in neural information retrieval (IR) systems requires\nremoving specific data whilst maintaining model performance. Applying existing\nmachine unlearning methods to IR may compromise retrieval effectiveness or\ninadvertently expose unlearning actions due to the removal of particular items\nfrom the retrieved results presented to users. We formalise corrective\nunranking, which extends machine unlearning in (neural) IR context by\nintegrating substitute documents to preserve ranking integrity, and propose a\nnovel teacher-student framework, Corrective unRanking Distillation (CuRD), for\nthis task. CuRD (1) facilitates forgetting by adjusting the (trained) neural IR\nmodel such that its output relevance scores of to-be-forgotten samples mimic\nthose of low-ranking, non-retrievable samples; (2) enables correction by\nfine-tuning the relevance scores for the substitute samples to match those of\ncorresponding to-be-forgotten samples closely; (3) seeks to preserve\nperformance on samples that are not targeted for forgetting. We evaluate CuRD\non four neural IR models (BERTcat, BERTdot, ColBERT, PARADE) using MS MARCO and\nTREC CAR datasets. Experiments with forget set sizes from 1 % and 20 % of the\ntraining dataset demonstrate that CuRD outperforms seven state-of-the-art\nbaselines in terms of forgetting and correction while maintaining model\nretention and generalisation capabilities.\n","authors":["Jingrui Hou","Axel Finke","Georgina Cosma"],"pdf_url":"https://arxiv.org/pdf/2411.08562v2.pdf","comment":"submitted to Information Sciences"},{"id":"http://arxiv.org/abs/2504.14928v2","updated":"2025-07-24T04:20:03Z","published":"2025-04-21T07:48:20Z","title":"EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent\n  Dialogue Framework","summary":"  Large language models (LLMs) increasingly serve as educational tools, yet\nevaluating their teaching capabilities remains challenging due to the\nresource-intensive, context-dependent, and methodologically complex nature of\nteacher-student interactions. We introduce EducationQ, a multi-agent dialogue\nframework that efficiently assesses teaching capabilities through simulated\ndynamic educational scenarios, featuring specialized agents for teaching,\nlearning, and evaluation. Testing 14 LLMs across major AI Organizations\n(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13\ndisciplines and 10 difficulty levels reveals that teaching effectiveness does\nnot correlate linearly with model scale or general reasoning capabilities -\nwith some smaller open-source models outperforming larger commercial\ncounterparts in teaching contexts. This finding highlights a critical gap in\ncurrent evaluations that prioritize knowledge recall over interactive pedagogy.\nOur mixed-methods evaluation, combining quantitative metrics with qualitative\nanalysis and expert case studies, identifies distinct pedagogical strengths\nemployed by top-performing models (e.g., sophisticated questioning strategies,\nadaptive feedback mechanisms). Human expert evaluations show 78% agreement with\nour automated qualitative analysis of effective teaching behaviors, validating\nour methodology. EducationQ demonstrates that LLMs-as-teachers require\nspecialized optimization beyond simple scaling, suggesting next-generation\neducational AI prioritize targeted enhancement of specific pedagogical\neffectiveness.\n","authors":["Yao Shi","Rongkeng Liang","Yong Xu"],"pdf_url":"https://arxiv.org/pdf/2504.14928v2.pdf","comment":"Paper URL: https://aclanthology.org/2025.acl-long.1576/; Presentation\n  Video: https://www.youtube.com/watch?v=j63ooKE50I0"},{"id":"http://arxiv.org/abs/2507.18082v1","updated":"2025-07-24T04:17:06Z","published":"2025-07-24T04:17:06Z","title":"TextSAM-EUS: Text Prompt Learning for SAM to Accurately Segment\n  Pancreatic Tumor in Endoscopic Ultrasound","summary":"  Pancreatic cancer carries a poor prognosis and relies on endoscopic\nultrasound (EUS) for targeted biopsy and radiotherapy. However, the speckle\nnoise, low contrast, and unintuitive appearance of EUS make segmentation of\npancreatic tumors with fully supervised deep learning (DL) models both\nerror-prone and dependent on large, expert-curated annotation datasets. To\naddress these challenges, we present TextSAM-EUS, a novel, lightweight,\ntext-driven adaptation of the Segment Anything Model (SAM) that requires no\nmanual geometric prompts at inference. Our approach leverages text prompt\nlearning (context optimization) through the BiomedCLIP text encoder in\nconjunction with a LoRA-based adaptation of SAM's architecture to enable\nautomatic pancreatic tumor segmentation in EUS, tuning only 0.86% of the total\nparameters. On the public Endoscopic Ultrasound Database of the Pancreas,\nTextSAM-EUS with automatic prompts attains 82.69% Dice and 85.28% normalized\nsurface distance (NSD), and with manual geometric prompts reaches 83.10% Dice\nand 85.70% NSD, outperforming both existing state-of-the-art (SOTA) supervised\nDL models and foundation models (e.g., SAM and its variants). As the first\nattempt to incorporate prompt learning in SAM-based medical image segmentation,\nTextSAM-EUS offers a practical option for efficient and robust automatic EUS\nsegmentation. Our code will be publicly available upon acceptance.\n","authors":["Pascal Spiegler","Taha Koleilat","Arash Harirpoush","Corey S. Miller","Hassan Rivaz","Marta Kersten-Oertel","Yiming Xiao"],"pdf_url":"https://arxiv.org/pdf/2507.18082v1.pdf","comment":"Accepted to ICCV 2025 Workshop CVAMD"},{"id":"http://arxiv.org/abs/2507.10136v4","updated":"2025-07-24T04:04:13Z","published":"2025-07-14T10:35:38Z","title":"A PBN-RL-XAI Framework for Discovering a \"Hit-and-Run\" Therapeutic\n  Strategy in Melanoma","summary":"  Innate resistance to anti-PD-1 immunotherapy remains a major clinical\nchallenge in metastatic melanoma, with the underlying molecular networks being\npoorly understood. To address this, we constructed a dynamic Probabilistic\nBoolean Network model using transcriptomic data from patient tumor biopsies to\nelucidate the regulatory logic governing therapy response. We then employed a\nreinforcement learning agent to systematically discover optimal, multi-step\ntherapeutic interventions and used explainable artificial intelligence to\nmechanistically interpret the agent's control policy. The analysis revealed\nthat a precisely timed, 4-step temporary inhibition of the lysyl oxidase like 2\nprotein (LOXL2) was the most effective strategy. Our explainable analysis\nshowed that this ''hit-and-run\" intervention is sufficient to erase the\nmolecular signature driving resistance, allowing the network to self-correct\nwithout requiring sustained intervention. This study presents a novel,\ntime-dependent therapeutic hypothesis for overcoming immunotherapy resistance\nand provides a powerful computational framework for identifying non-obvious\nintervention protocols in complex biological systems.\n","authors":["Zhonglin Liu"],"pdf_url":"https://arxiv.org/pdf/2507.10136v4.pdf","comment":"9 pages, 5 figures. Submitted to the IEEE International Conference on\n  Bioinformatics and Biomedicine (BIBM) 2025. Code is available at\n  https://github.com/Liu-Zhonglin/pbn-melanoma-project"},{"id":"http://arxiv.org/abs/2507.16214v2","updated":"2025-07-24T04:02:42Z","published":"2025-07-22T04:13:03Z","title":"Adaptive Relative Pose Estimation Framework with Dual Noise Tuning for\n  Safe Approaching Maneuvers","summary":"  Accurate and robust relative pose estimation is crucial for enabling\nchallenging Active Debris Removal (ADR) missions targeting tumbling derelict\nsatellites such as ESA's ENVISAT. This work presents a complete pipeline\nintegrating advanced computer vision techniques with adaptive nonlinear\nfiltering to address this challenge. A Convolutional Neural Network (CNN),\nenhanced with image preprocessing, detects structural markers (corners) from\nchaser imagery, whose 2D coordinates are converted to 3D measurements using\ncamera modeling. These measurements are fused within an Unscented Kalman Filter\n(UKF) framework, selected for its ability to handle nonlinear relative\ndynamics, to estimate the full relative pose. Key contributions include the\nintegrated system architecture and a dual adaptive strategy within the UKF:\ndynamic tuning of the measurement noise covariance compensates for varying CNN\nmeasurement uncertainty, while adaptive tuning of the process noise covariance,\nutilizing measurement residual analysis, accounts for unmodeled dynamics or\nmaneuvers online. This dual adaptation enhances robustness against both\nmeasurement imperfections and dynamic model uncertainties. The performance of\nthe proposed adaptive integrated system is evaluated through high-fidelity\nsimulations using a realistic ENVISAT model, comparing estimates against ground\ntruth under various conditions, including measurement outages. This\ncomprehensive approach offers an enhanced solution for robust onboard relative\nnavigation, significantly advancing the capabilities required for safe\nproximity operations during ADR missions.\n","authors":["Batu Candan","Simone Servadio"],"pdf_url":"https://arxiv.org/pdf/2507.16214v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05330v3","updated":"2025-07-24T04:02:17Z","published":"2024-08-09T20:36:40Z","title":"Neural Machine Unranking","summary":"  We address the problem of machine unlearning in neural information retrieval\n(IR), introducing a novel task termed Neural Machine UnRanking (NuMuR). This\nproblem is motivated by growing demands for data privacy compliance and\nselective information removal in neural IR systems. Existing task- or model-\nagnostic unlearning approaches, primarily designed for classification tasks,\nare suboptimal for NuMuR due to two core challenges: (1) neural rankers output\nunnormalised relevance scores rather than probability distributions, limiting\nthe effectiveness of traditional teacher-student distillation frameworks; and\n(2) entangled data scenarios, where queries and documents appear simultaneously\nacross both forget and retain sets, may degrade retention performance in\nexisting methods. To address these issues, we propose Contrastive and\nConsistent Loss (CoCoL), a dual-objective framework. CoCoL comprises (1) a\ncontrastive loss that reduces relevance scores on forget sets while maintaining\nperformance on entangled samples, and (2) a consistent loss that preserves\naccuracy on retain set. Extensive experiments on MS MARCO and TREC CAR\ndatasets, across four neural IR models, demonstrate that CoCoL achieves\nsubstantial forgetting with minimal retain and generalisation performance loss.\nOur method facilitates more effective and controllable data removal than\nexisting techniques.\n","authors":["Jingrui Hou","Axel Finke","Georgina Cosma"],"pdf_url":"https://arxiv.org/pdf/2408.05330v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18074v1","updated":"2025-07-24T03:57:27Z","published":"2025-07-24T03:57:27Z","title":"AlphaGo Moment for Model Architecture Discovery","summary":"  While AI systems demonstrate exponentially improving capabilities, the pace\nof AI research itself remains linearly bounded by human cognitive capacity,\ncreating an increasingly severe development bottleneck. We present ASI-Arch,\nthe first demonstration of Artificial Superintelligence for AI research\n(ASI4AI) in the critical domain of neural architecture discovery--a fully\nautonomous system that shatters this fundamental constraint by enabling AI to\nconduct its own architectural innovation. Moving beyond traditional Neural\nArchitecture Search (NAS), which is fundamentally limited to exploring\nhuman-defined spaces, we introduce a paradigm shift from automated optimization\nto automated innovation. ASI-Arch can conduct end-to-end scientific research in\nthe domain of architecture discovery, autonomously hypothesizing novel\narchitectural concepts, implementing them as executable code, training and\nempirically validating their performance through rigorous experimentation and\npast experience. ASI-Arch conducted 1,773 autonomous experiments over 20,000\nGPU hours, culminating in the discovery of 106 innovative, state-of-the-art\n(SOTA) linear attention architectures. Like AlphaGo's Move 37 that revealed\nunexpected strategic insights invisible to human players, our AI-discovered\narchitectures demonstrate emergent design principles that systematically\nsurpass human-designed baselines and illuminate previously unknown pathways for\narchitectural innovation. Crucially, we establish the first empirical scaling\nlaw for scientific discovery itself--demonstrating that architectural\nbreakthroughs can be scaled computationally, transforming research progress\nfrom a human-limited to a computation-scalable process. We provide\ncomprehensive analysis of the emergent design patterns and autonomous research\ncapabilities that enabled these breakthroughs, establishing a blueprint for\nself-accelerating AI systems.\n","authors":["Yixiu Liu","Yang Nan","Weixian Xu","Xiangkun Hu","Lyumanshan Ye","Zhen Qin","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2507.18074v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18071v1","updated":"2025-07-24T03:50:32Z","published":"2025-07-24T03:50:32Z","title":"Group Sequence Policy Optimization","summary":"  This paper introduces Group Sequence Policy Optimization (GSPO), our stable,\nefficient, and performant reinforcement learning algorithm for training large\nlanguage models. Unlike previous algorithms that adopt token-level importance\nratios, GSPO defines the importance ratio based on sequence likelihood and\nperforms sequence-level clipping, rewarding, and optimization. We demonstrate\nthat GSPO achieves superior training efficiency and performance compared to the\nGRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and\nhas the potential for simplifying the design of RL infrastructure. These merits\nof GSPO have contributed to the remarkable improvements in the latest Qwen3\nmodels.\n","authors":["Chujie Zheng","Shixuan Liu","Mingze Li","Xiong-Hui Chen","Bowen Yu","Chang Gao","Kai Dang","Yuqiong Liu","Rui Men","An Yang","Jingren Zhou","Junyang Lin"],"pdf_url":"https://arxiv.org/pdf/2507.18071v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.06106v4","updated":"2025-07-24T03:38:33Z","published":"2024-11-09T08:00:50Z","title":"Towards a Universal 3D Medical Multi-modality Generalization via\n  Learning Personalized Invariant Representation","summary":"  Variations in medical imaging modalities and individual anatomical\ndifferences pose challenges to cross-modality generalization in multi-modal\ntasks. Existing methods often concentrate exclusively on common anatomical\npatterns, thereby neglecting individual differences and consequently limiting\ntheir generalization performance. This paper emphasizes the critical role of\nlearning individual-level invariance, i.e., personalized representation\n$\\mathbb{X}_h$, to enhance multi-modality generalization under both homogeneous\nand heterogeneous settings. It reveals that mappings from individual biological\nprofile to different medical modalities remain static across the population,\nwhich is implied in the personalization process. We propose a two-stage\napproach: pre-training with invariant representation $\\mathbb{X}_h$ for\npersonalization, then fine-tuning for diverse downstream tasks. We provide both\ntheoretical and empirical evidence demonstrating the feasibility and advantages\nof personalization, showing that our approach yields greater generalizability\nand transferability across diverse multi-modal medical tasks compared to\nmethods lacking personalization. Extensive experiments further validate that\nour approach significantly enhances performance in various generalization\nscenarios.\n","authors":["Zhaorui Tan","Xi Yang","Tan Pan","Tianyi Liu","Chen Jiang","Xin Guo","Qiufeng Wang","Anh Nguyen","Yuan Qi","Kaizhu Huang","Yuan Cheng"],"pdf_url":"https://arxiv.org/pdf/2411.06106v4.pdf","comment":"Accepted by ICCV25"},{"id":"http://arxiv.org/abs/2507.18061v1","updated":"2025-07-24T03:23:55Z","published":"2025-07-24T03:23:55Z","title":"TELEVAL: A Dynamic Benchmark Designed for Spoken Language Models in\n  Chinese Interactive Scenarios","summary":"  Spoken language models (SLMs) have seen rapid progress in recent years, along\nwith the development of numerous benchmarks for evaluating their performance.\nHowever, most existing benchmarks primarily focus on evaluating whether SLMs\ncan perform complex tasks comparable to those tackled by large language models\n(LLMs), often failing to align with how users naturally interact in real-world\nconversational scenarios. In this paper, we propose TELEVAL, a dynamic\nbenchmark specifically designed to evaluate SLMs' effectiveness as\nconversational agents in realistic Chinese interactive settings. TELEVAL\ndefines three evaluation dimensions: Explicit Semantics, Paralinguistic and\nImplicit Semantics, and System Abilities. It adopts a dialogue format\nconsistent with real-world usage and evaluates text and audio outputs\nseparately. TELEVAL particularly focuses on the model's ability to extract\nimplicit cues from user speech and respond appropriately without additional\ninstructions. Our experiments demonstrate that despite recent progress,\nexisting SLMs still have considerable room for improvement in natural\nconversational tasks. We hope that TELEVAL can serve as a user-centered\nevaluation framework that directly reflects the user experience and contributes\nto the development of more capable dialogue-oriented SLMs.\n","authors":["Zehan Li","Hongjie Chen","Yuxin Zhang","Jing Zhou","Xuening Wang","Hang Lv","Mengjie Du","Yaodong Song","Jie Lian","Jian Kang","Jie Li","Yongxiang Li","Zhongjiang He","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2507.18061v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18059v1","updated":"2025-07-24T03:22:21Z","published":"2025-07-24T03:22:21Z","title":"Multi-Agent Guided Policy Optimization","summary":"  Due to practical constraints such as partial observability and limited\ncommunication, Centralized Training with Decentralized Execution (CTDE) has\nbecome the dominant paradigm in cooperative Multi-Agent Reinforcement Learning\n(MARL). However, existing CTDE methods often underutilize centralized training\nor lack theoretical guarantees. We propose Multi-Agent Guided Policy\nOptimization (MAGPO), a novel framework that better leverages centralized\ntraining by integrating centralized guidance with decentralized execution.\nMAGPO uses an auto-regressive joint policy for scalable, coordinated\nexploration and explicitly aligns it with decentralized policies to ensure\ndeployability under partial observability. We provide theoretical guarantees of\nmonotonic policy improvement and empirically evaluate MAGPO on 43 tasks across\n6 diverse environments. Results show that MAGPO consistently outperforms strong\nCTDE baselines and matches or surpasses fully centralized approaches, offering\na principled and practical solution for decentralized multi-agent learning. Our\ncode and experimental data can be found in https://github.com/liyheng/MAGPO.\n","authors":["Yueheng Li","Guangming Xie","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2507.18059v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14445v2","updated":"2025-07-24T03:19:19Z","published":"2024-04-20T08:08:28Z","title":"A Multi-Faceted Evaluation Framework for Assessing Synthetic Data\n  Generated by Large Language Models","summary":"  The rapid advancements in generative AI and large language models (LLMs) have\nopened up new avenues for producing synthetic data, particularly in the realm\nof structured tabular formats, such as product reviews. Despite the potential\nbenefits, concerns regarding privacy leakage have surfaced, especially when\npersonal information is utilized in the training datasets. In addition, there\nis an absence of a comprehensive evaluation framework capable of quantitatively\nmeasuring the quality of the generated synthetic data and their utility for\ndownstream tasks. In response to this gap, we introduce SynEval, an open-source\nevaluation framework designed to assess the fidelity, utility, and privacy\npreservation of synthetically generated tabular data via a suite of diverse\nevaluation metrics. We validate the efficacy of our proposed framework -\nSynEval - by applying it to synthetic product review data generated by three\nstate-of-the-art LLMs: ChatGPT, Claude, and Llama. Our experimental findings\nilluminate the trade-offs between various evaluation metrics in the context of\nsynthetic data generation. Furthermore, SynEval stands as a critical instrument\nfor researchers and practitioners engaged with synthetic tabular data,,\nempowering them to judiciously determine the suitability of the generated data\nfor their specific applications, with an emphasis on upholding user privacy.\n","authors":["Yefeng Yuan","Yuhong Liu","Liang Cheng"],"pdf_url":"https://arxiv.org/pdf/2404.14445v2.pdf","comment":"10 pages, 1 figure, 4 tables"},{"id":"http://arxiv.org/abs/2503.01424v3","updated":"2025-07-24T02:59:25Z","published":"2025-03-03T11:27:13Z","title":"From Hypothesis to Publication: A Comprehensive Survey of AI-Driven\n  Research Support Systems","summary":"  Research is a fundamental process driving the advancement of human\ncivilization, yet it demands substantial time and effort from researchers. In\nrecent years, the rapid development of artificial intelligence (AI)\ntechnologies has inspired researchers to explore how AI can accelerate and\nenhance research. To monitor relevant advancements, this paper presents a\nsystematic review of the progress in this domain. Specifically, we organize the\nrelevant studies into three main categories: hypothesis formulation, hypothesis\nvalidation, and manuscript publication. Hypothesis formulation involves\nknowledge synthesis and hypothesis generation. Hypothesis validation includes\nthe verification of scientific claims, theorem proving, and experiment\nvalidation. Manuscript publication encompasses manuscript writing and the peer\nreview process. Furthermore, we identify and discuss the current challenges\nfaced in these areas, as well as potential future directions for research.\nFinally, we also offer a comprehensive overview of existing benchmarks and\ntools across various domains that support the integration of AI into the\nresearch process. We hope this paper serves as an introduction for beginners\nand fosters future research. Resources have been made publicly available at\nhttps://github.com/zkzhou126/AI-for-Research.\n","authors":["Zekun Zhou","Xiaocheng Feng","Lei Huang","Xiachong Feng","Ziyun Song","Ruihan Chen","Liang Zhao","Weitao Ma","Yuxuan Gu","Baoxin Wang","Dayong Wu","Guoping Hu","Ting Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2503.01424v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.16838v2","updated":"2025-07-24T02:55:40Z","published":"2025-07-18T04:00:58Z","title":"Segmentation-free Goodness of Pronunciation","summary":"  Mispronunciation detection and diagnosis (MDD) is a significant part in\nmodern computer aided language learning (CALL) systems. Within MDD,\nphoneme-level pronunciation assessment is key to helping L2 learners improve\ntheir pronunciation. However, most systems are based on a form of goodness of\npronunciation (GOP) which requires pre-segmentation of speech into phonetic\nunits. This limits the accuracy of these methods and the possibility to use\nmodern CTC-based acoustic models for their evaluation. In this study, we first\npropose self-alignment GOP (GOP-SA) that enables the use of CTC-trained ASR\nmodels for MDD. Next, we define a more general alignment-free method that takes\nall possible alignments of the target phoneme into account (GOP-AF). We give a\ntheoretical account of our definition of GOP-AF, an implementation that solves\npotential numerical issues as well as a proper normalization which makes the\nmethod applicable with acoustic models with different peakiness over time. We\nprovide extensive experimental results on the CMU Kids and Speechocean762\ndatasets comparing the different definitions of our methods, estimating the\ndependency of GOP-AF on the peakiness of the acoustic models and on the amount\nof context around the target phoneme. Finally, we compare our methods with\nrecent studies over the Speechocean762 data showing that the feature vectors\nderived from the proposed method achieve state-of-the-art results on\nphoneme-level pronunciation assessment.\n","authors":["Xinwei Cao","Zijian Fan","Torbjørn Svendsen","Giampiero Salvi"],"pdf_url":"https://arxiv.org/pdf/2507.16838v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2507.18046v1","updated":"2025-07-24T02:50:26Z","published":"2025-07-24T02:50:26Z","title":"Enhancing Scene Transition Awareness in Video Generation via\n  Post-Training","summary":"  Recent advances in AI-generated video have shown strong performance on\n\\emph{text-to-video} tasks, particularly for short clips depicting a single\nscene. However, current models struggle to generate longer videos with coherent\nscene transitions, primarily because they cannot infer when a transition is\nneeded from the prompt. Most open-source models are trained on datasets\nconsisting of single-scene video clips, which limits their capacity to learn\nand respond to prompts requiring multiple scenes. Developing scene transition\nawareness is essential for multi-scene generation, as it allows models to\nidentify and segment videos into distinct clips by accurately detecting\ntransitions.\n  To address this, we propose the \\textbf{Transition-Aware Video} (TAV)\ndataset, which consists of preprocessed video clips with multiple scene\ntransitions. Our experiment shows that post-training on the \\textbf{TAV}\ndataset improves prompt-based scene transition understanding, narrows the gap\nbetween required and generated scenes, and maintains image quality.\n","authors":["Hanwen Shen","Jiajie Lu","Yupeng Cao","Xiaonan Yang"],"pdf_url":"https://arxiv.org/pdf/2507.18046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18044v1","updated":"2025-07-24T02:45:03Z","published":"2025-07-24T02:45:03Z","title":"Synthetic Data Generation for Phrase Break Prediction with Large\n  Language Model","summary":"  Current approaches to phrase break prediction address crucial prosodic\naspects of text-to-speech systems but heavily rely on vast human annotations\nfrom audio or text, incurring significant manual effort and cost. Inherent\nvariability in the speech domain, driven by phonetic factors, further\ncomplicates acquiring consistent, high-quality data. Recently, large language\nmodels (LLMs) have shown success in addressing data challenges in NLP by\ngenerating tailored synthetic data while reducing manual annotation needs.\nMotivated by this, we explore leveraging LLM to generate synthetic phrase break\nannotations, addressing the challenges of both manual annotation and\nspeech-related tasks by comparing with traditional annotations and assessing\neffectiveness across multiple languages. Our findings suggest that LLM-based\nsynthetic data generation effectively mitigates data challenges in phrase break\nprediction and highlights the potential of LLMs as a viable solution for the\nspeech domain.\n","authors":["Hoyeon Lee","Sejung Son","Ye-Eun Kang","Jong-Hwan Kim"],"pdf_url":"https://arxiv.org/pdf/2507.18044v1.pdf","comment":"Accepted at Interspeech 2025"},{"id":"http://arxiv.org/abs/2507.18043v1","updated":"2025-07-24T02:34:13Z","published":"2025-07-24T02:34:13Z","title":"GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs\n  and VLMs","summary":"  Inference-time steering methods offer a lightweight alternative to\nfine-tuning large language models (LLMs) and vision-language models (VLMs) by\nmodifying internal activations at test time without updating model weights.\nHowever, most existing approaches rely on fixed, global intervention vectors,\noverlook the causal influence of individual input tokens, and fail to leverage\ninformative gradients from the model's logits, particularly in multimodal\nsettings where visual and textual inputs contribute unevenly. To address these\nlimitations, we introduce GrAInS, an inference-time steering approach that\noperates across both language-only and vision-language models and tasks. GrAInS\nuses contrastive, gradient-based attribution via Integrated Gradients to\nidentify the top-k most influential tokens, both positively and negatively\nattributed based on their contribution to preferred versus dispreferred\noutputs. These tokens are then used to construct directional steering vectors\nthat capture semantic shifts from undesirable to desirable behavior. During\ninference, GrAInS adjusts hidden activations at transformer layers guided by\ntoken-level attribution signals, and normalizes activations to preserve\nrepresentational scale. This enables fine-grained, interpretable, and modular\ncontrol over model behavior, without retraining or auxiliary supervision.\nEmpirically, GrAInS consistently outperforms both fine-tuning and existing\nsteering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using\nLlama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514\nwith LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all\nwhile preserving the model's fluency and general capabilities.\n","authors":["Duy Nguyen","Archiki Prasad","Elias Stengel-Eskin","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2507.18043v1.pdf","comment":"21 pages. Code: https://github.com/duykhuongnguyen/GrAInS"},{"id":"http://arxiv.org/abs/2503.01075v2","updated":"2025-07-24T02:11:36Z","published":"2025-03-03T00:33:04Z","title":"Tackling Hallucination from Conditional Models for Medical Image\n  Reconstruction with DynamicDPS","summary":"  Hallucinations are spurious structures not present in the ground truth,\nposing a critical challenge in medical image reconstruction, especially for\ndata-driven conditional models. We hypothesize that combining an unconditional\ndiffusion model with data consistency, trained on a diverse dataset, can reduce\nthese hallucinations. Based on this, we propose DynamicDPS, a diffusion-based\nframework that integrates conditional and unconditional diffusion models to\nenhance low-quality medical images while systematically reducing\nhallucinations. Our approach first generates an initial reconstruction using a\nconditional model, then refines it with an adaptive diffusion-based inverse\nproblem solver. DynamicDPS skips early stage in the reverse process by\nselecting an optimal starting time point per sample and applies Wolfe's line\nsearch for adaptive step sizes, improving both efficiency and image fidelity.\nUsing diffusion priors and data consistency, our method effectively reduces\nhallucinations from any conditional model output. We validate its effectiveness\nin Image Quality Transfer for low-field MRI enhancement. Extensive evaluations\non synthetic and real MR scans, including a downstream task for tissue volume\nestimation, show that DynamicDPS reduces hallucinations, improving relative\nvolume estimation by over 15% for critical tissues while using only 5% of the\nsampling steps required by baseline diffusion models. As a model-agnostic and\nfine-tuning-free approach, DynamicDPS offers a robust solution for\nhallucination reduction in medical imaging. The code will be made publicly\navailable upon publication.\n","authors":["Seunghoi Kim","Henry F. J. Tregidgo","Matteo Figini","Chen Jin","Sarang Joshi","Daniel C. Alexander"],"pdf_url":"https://arxiv.org/pdf/2503.01075v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18033v1","updated":"2025-07-24T02:05:28Z","published":"2025-07-24T02:05:28Z","title":"OpenNav: Open-World Navigation with Multimodal Large Language Models","summary":"  Pre-trained large language models (LLMs) have demonstrated strong\ncommon-sense reasoning abilities, making them promising for robotic navigation\nand planning tasks. However, despite recent progress, bridging the gap between\nlanguage descriptions and actual robot actions in the open-world, beyond merely\ninvoking limited predefined motion primitives, remains an open challenge. In\nthis work, we aim to enable robots to interpret and decompose complex language\ninstructions, ultimately synthesizing a sequence of trajectory points to\ncomplete diverse navigation tasks given open-set instructions and open-set\nobjects. We observe that multi-modal large language models (MLLMs) exhibit\nstrong cross-modal understanding when processing free-form language\ninstructions, demonstrating robust scene comprehension. More importantly,\nleveraging their code-generation capability, MLLMs can interact with\nvision-language perception models to generate compositional 2D bird-eye-view\nvalue maps, effectively integrating semantic knowledge from MLLMs with spatial\ninformation from maps to reinforce the robot's spatial understanding. To\nfurther validate our approach, we effectively leverage large-scale autonomous\nvehicle datasets (AVDs) to validate our proposed zero-shot vision-language\nnavigation framework in outdoor navigation tasks, demonstrating its capability\nto execute a diverse range of free-form natural language navigation\ninstructions while maintaining robustness against object detection errors and\nlinguistic ambiguities. Furthermore, we validate our system on a Husky robot in\nboth indoor and outdoor scenes, demonstrating its real-world robustness and\napplicability. Supplementary videos are available at\nhttps://trailab.github.io/OpenNav-website/\n","authors":["Mingfeng Yuan","Letian Wang","Steven L. Waslander"],"pdf_url":"https://arxiv.org/pdf/2507.18033v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18031v1","updated":"2025-07-24T02:04:58Z","published":"2025-07-24T02:04:58Z","title":"ViGText: Deepfake Image Detection with Vision-Language Model\n  Explanations and Graph Neural Networks","summary":"  The rapid rise of deepfake technology, which produces realistic but\nfraudulent digital content, threatens the authenticity of media. Traditional\ndeepfake detection approaches often struggle with sophisticated, customized\ndeepfakes, especially in terms of generalization and robustness against\nmalicious attacks. This paper introduces ViGText, a novel approach that\nintegrates images with Vision Large Language Model (VLLM) Text explanations\nwithin a Graph-based framework to improve deepfake detection. The novelty of\nViGText lies in its integration of detailed explanations with visual data, as\nit provides a more context-aware analysis than captions, which often lack\nspecificity and fail to reveal subtle inconsistencies. ViGText systematically\ndivides images into patches, constructs image and text graphs, and integrates\nthem for analysis using Graph Neural Networks (GNNs) to identify deepfakes.\nThrough the use of multi-level feature extraction across spatial and frequency\ndomains, ViGText captures details that enhance its robustness and accuracy to\ndetect sophisticated deepfakes. Extensive experiments demonstrate that ViGText\nsignificantly enhances generalization and achieves a notable performance boost\nwhen it detects user-customized deepfakes. Specifically, average F1 scores rise\nfrom 72.45% to 98.32% under generalization evaluation, and reflects the model's\nsuperior ability to generalize to unseen, fine-tuned variations of stable\ndiffusion models. As for robustness, ViGText achieves an increase of 11.1% in\nrecall compared to other deepfake detection approaches. When facing targeted\nattacks that exploit its graph-based architecture, ViGText limits\nclassification performance degradation to less than 4%. ViGText uses detailed\nvisual and textual analysis to set a new standard for detecting deepfakes,\nhelping ensure media authenticity and information integrity.\n","authors":["Ahmad ALBarqawi","Mahmoud Nazzal","Issa Khalil","Abdallah Khreishah","NhatHai Phan"],"pdf_url":"https://arxiv.org/pdf/2507.18031v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18028v1","updated":"2025-07-24T02:00:09Z","published":"2025-07-24T02:00:09Z","title":"NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural\n  KV Database","summary":"  Efficiently editing knowledge stored in large language models (LLMs) enables\nmodel updates without large-scale training. One possible solution is\nLocate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number\nof facts. However, such editing may compromise the general abilities of LLMs\nand even result in forgetting edited facts when scaling up to thousands of\nedits. In this paper, we model existing linear L\\&E methods as querying a\nKey-Value (KV) database. From this perspective, we then propose NeuralDB, an\nediting framework that explicitly represents the edited facts as a neural KV\ndatabase equipped with a non-linear gated retrieval module, % In particular,\nour gated module only operates when inference involves the edited facts,\neffectively preserving the general abilities of LLMs. Comprehensive experiments\ninvolving the editing of 10,000 facts were conducted on the ZsRE and\nCounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results\ndemonstrate that NeuralDB not only excels in editing efficacy, generalization,\nspecificity, fluency, and consistency, but also preserves overall performance\nacross six representative text understanding and generation tasks. Further\nexperiments indicate that NeuralDB maintains its effectiveness even when scaled\nto 100,000 facts (\\textbf{50x} more than in prior work).\n","authors":["Weizhi Fei","Hao Shi","Jing Xu","Jingchen Peng","Jiazheng Li","Jingzhao Zhang","Bo Bai","Wei Han","Zhenyuan Chen","Xueyan Niu"],"pdf_url":"https://arxiv.org/pdf/2507.18028v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18022v1","updated":"2025-07-24T01:47:34Z","published":"2025-07-24T01:47:34Z","title":"Does visualization help AI understand data?","summary":"  Charts and graphs help people analyze data, but can they also be useful to AI\nsystems? To investigate this question, we perform a series of experiments with\ntwo commercial vision-language models: GPT 4.1 and Claude 3.5. Across three\nrepresentative analysis tasks, the two systems describe synthetic datasets more\nprecisely and accurately when raw data is accompanied by a scatterplot,\nespecially as datasets grow in complexity. Comparison with two baselines --\nproviding a blank chart and a chart with mismatched data -- shows that the\nimproved performance is due to the content of the charts. Our results are\ninitial evidence that AI systems, like humans, can benefit from visualization.\n","authors":["Victoria R. Li","Johnathan Sun","Martin Wattenberg"],"pdf_url":"https://arxiv.org/pdf/2507.18022v1.pdf","comment":"5 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.16743v4","updated":"2025-07-24T01:40:47Z","published":"2025-03-20T23:11:30Z","title":"SuperARC: An Agnostic Test for Narrow, General, and Super Intelligence\n  Based On the Principles of Recursive Compression and Algorithmic Probability","summary":"  We introduce an open-ended test grounded in algorithmic probability that can\navoid benchmark contamination in the quantitative evaluation of frontier models\nin the context of their Artificial General Intelligence (AGI) and\nSuperintelligence (ASI) claims. Unlike other tests, this test does not rely on\nstatistical compression methods (such as GZIP or LZW), which are more closely\nrelated to Shannon entropy than to Kolmogorov complexity and are not able to\ntest beyond simple pattern matching. The test challenges aspects of AI, in\nparticular LLMs, related to features of intelligence of fundamental nature such\nas synthesis and model creation in the context of inverse problems (generating\nnew knowledge from observation). We argue that metrics based on model\nabstraction and abduction (optimal Bayesian `inference') for predictive\n`planning' can provide a robust framework for testing intelligence, including\nnatural intelligence (human and animal), narrow AI, AGI, and ASI. We found that\nLLM model versions tend to be fragile and incremental as a result of\nmemorisation only with progress likely driven by the size of training data. The\nresults were compared with a hybrid neurosymbolic approach that theoretically\nguarantees universal intelligence based on the principles of algorithmic\nprobability and Kolmogorov complexity. The method outperforms LLMs in a\nproof-of-concept on short binary sequences. We prove that compression is\nequivalent and directly proportional to a system's predictive power and vice\nversa. That is, if a system can better predict it can better compress, and if\nit can better compress, then it can better predict. Our findings strengthen the\nsuspicion regarding the fundamental limitations of LLMs, exposing them as\nsystems optimised for the perception of mastery over human language.\n","authors":["Alberto Hernández-Espinosa","Luan Ozelim","Felipe S. Abrahão","Hector Zenil"],"pdf_url":"https://arxiv.org/pdf/2503.16743v4.pdf","comment":"51 pages + Technical Supplementary Information, 79 pages total"},{"id":"http://arxiv.org/abs/2507.18017v1","updated":"2025-07-24T01:18:24Z","published":"2025-07-24T01:18:24Z","title":"Fashion-AlterEval: A Dataset for Improved Evaluation of Conversational\n  Recommendation Systems with Alternative Relevant Items","summary":"  In Conversational Recommendation Systems (CRS), a user provides feedback on\nrecommended items at each turn, leading the CRS towards improved\nrecommendations. Due to the need for a large amount of data, a user simulator\nis employed for both training and evaluation. Such user simulators critique the\ncurrent retrieved item based on knowledge of a single target item. However,\nsystem evaluation in offline settings with simulators is limited by the focus\non a single target item and their unlimited patience over a large number of\nturns. To overcome these limitations of existing simulators, we propose\nFashion-AlterEval, a new dataset that contains human judgments for a selection\nof alternative items by adding new annotations in common fashion CRS datasets.\nConsequently, we propose two novel meta-user simulators that use the collected\njudgments and allow simulated users not only to express their preferences about\nalternative items to their original target, but also to change their mind and\nlevel of patience. In our experiments using the Shoes and Fashion IQ as the\noriginal datasets and three CRS models, we find that using the knowledge of\nalternatives by the simulator can have a considerable impact on the evaluation\nof existing CRS models, specifically that the existing single-target evaluation\nunderestimates their effectiveness, and when simulatedusers are allowed to\ninstead consider alternative relevant items, the system can rapidly respond to\nmore quickly satisfy the user.\n","authors":["Maria Vlachou"],"pdf_url":"https://arxiv.org/pdf/2507.18017v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2401.05783"},{"id":"http://arxiv.org/abs/2505.02581v4","updated":"2025-07-24T01:12:59Z","published":"2025-05-05T11:33:18Z","title":"Neurodivergent Influenceability as a Contingent Solution to the AI\n  Alignment Problem","summary":"  The AI alignment problem, which focusses on ensuring that artificial\nintelligence (AI), including AGI and ASI, systems act according to human\nvalues, presents profound challenges. With the progression from narrow AI to\nArtificial General Intelligence (AGI) and Superintelligence, fears about\ncontrol and existential risk have escalated. Here, we investigate whether\nembracing inevitable AI misalignment can be a contingent strategy to foster a\ndynamic ecosystem of competing agents as a viable path to steer them in more\nhuman-aligned trends and mitigate risks. We explore how misalignment may serve\nand should be promoted as a counterbalancing mechanism to team up with\nwhichever agents are most aligned to human interests, ensuring that no single\nsystem dominates destructively. The main premise of our contribution is that\nmisalignment is inevitable because full AI-human alignment is a mathematical\nimpossibility from Turing-complete systems, which we also offer as a proof in\nthis contribution, a feature then inherited to AGI and ASI systems. We\nintroduce a change-of-opinion attack test based on perturbation and\nintervention analysis to study how humans and agents may change or neutralise\nfriendly and unfriendly AIs through cooperation and competition. We show that\nopen models are more diverse and that most likely guardrails implemented in\nproprietary models are successful at controlling some of the agents' range of\nbehaviour with positive and negative consequences while closed systems are more\nsteerable and can also be used against proprietary AI systems. We also show\nthat human and AI intervention has different effects hence suggesting multiple\nstrategies.\n","authors":["Alberto Hernández-Espinosa","Felipe S. Abrahão","Olaf Witkowski","Hector Zenil"],"pdf_url":"https://arxiv.org/pdf/2505.02581v4.pdf","comment":"44 pages"},{"id":"http://arxiv.org/abs/2507.18009v1","updated":"2025-07-24T00:54:31Z","published":"2025-07-24T00:54:31Z","title":"GRR-CoCa: Leveraging LLM Mechanisms in Multimodal Model Architectures","summary":"  State-of-the-art (SOTA) image and text generation models are multimodal\nmodels that have many similarities to large language models (LLMs). Despite\nachieving strong performances, leading foundational multimodal model\narchitectures frequently lag behind the architectural sophistication of\ncontemporary LLMs. We propose GRR-CoCa, an improved SOTA Contrastive Captioner\n(CoCa) model that incorporates Gaussian error gated linear units, root mean\nsquared normalization, and rotary positional embedding into the textual\ndecoders and the vision transformer (ViT) encoder. Each architectural\nmodification has been shown to improve model performance in LLMs, but has yet\nto be adopted in CoCa. We benchmarked GRR-CoCa against Baseline CoCa, a model\nwith the same modified textual decoders but with CoCa's original ViT encoder.\nWe used standard pretraining and fine-tuning workflows to benchmark the models\non contrastive and generative tasks. Our GRR-CoCa significantly outperformed\nBaseline CoCa on the pretraining dataset and three diverse fine-tuning\ndatasets. Pretraining improvements were 27.25% in contrastive loss, 3.71% in\nperplexity, and 7.15% in CoCa loss. The average fine-tuning improvements were\n13.66% in contrastive loss, 5.18% in perplexity, and 5.55% in CoCa loss. We\nshow that GRR-CoCa's modified architecture improves performance and\ngeneralization across vision-language domains.\n","authors":["Jake R. Patock","Nicole Catherine Lewis","Kevin McCoy","Christina Gomez","Canling Chen","Lorenzo Luzi"],"pdf_url":"https://arxiv.org/pdf/2507.18009v1.pdf","comment":"12 pages, 2 figures"},{"id":"http://arxiv.org/abs/2507.06174v5","updated":"2025-07-24T00:40:26Z","published":"2025-07-08T16:54:34Z","title":"Fast Bilateral Teleoperation and Imitation Learning Using Sensorless\n  Force Control via Accurate Dynamics Model","summary":"  In recent years, the advancement of imitation learning has led to increased\ninterest in teleoperating low-cost manipulators to collect demonstration data.\nHowever, most existing systems rely on unilateral control, which only transmits\ntarget position values. While this approach is easy to implement and suitable\nfor slow, non-contact tasks, it struggles with fast or contact-rich operations\ndue to the absence of force feedback. This work demonstrates that fast\nteleoperation with force feedback is feasible even with force-sensorless,\nlow-cost manipulators by leveraging 4-channel bilateral control. Based on\naccurately identified manipulator dynamics, our method integrates nonlinear\nterms compensation, velocity and external force estimation, and variable gain\ncorresponding to inertial variation. Furthermore, using data collected by\n4-channel bilateral control, we show that incorporating force information into\nboth the input and output of learned policies improves performance in imitation\nlearning. These results highlight the practical effectiveness of our system for\nhigh-fidelity teleoperation and data collection on affordable hardware.\n","authors":["Koki Yamane","Yunhan Li","Masashi Konosu","Koki Inami","Junji Oaki","Sho Sakaino","Toshiaki Tsuji"],"pdf_url":"https://arxiv.org/pdf/2507.06174v5.pdf","comment":"20 pages, 9 figures, Submitted to CoRL 2025"},{"id":"http://arxiv.org/abs/2507.18004v1","updated":"2025-07-24T00:39:19Z","published":"2025-07-24T00:39:19Z","title":"E.A.R.T.H.: Structuring Creative Evolution through Model Error in\n  Generative AI","summary":"  How can AI move beyond imitation toward genuine creativity? This paper\nproposes the E.A.R.T.H. framework, a five-stage generative pipeline that\ntransforms model-generated errors into creative assets through Error\ngeneration, Amplification, Refine selection, Transform, and Harness feedback.\nDrawing on cognitive science and generative modeling, we posit that \"creative\npotential hides in failure\" and operationalize this via structured prompts,\nsemantic scoring, and human-in-the-loop evaluation. Implemented using\nLLaMA-2-7B-Chat, SBERT, BERTScore, CLIP, BLIP-2, and Stable Diffusion, the\npipeline employs a composite reward function based on novelty, surprise, and\nrelevance. At the Refine stage, creativity scores increase by 52.5% (1.179 to\n1.898, t = -5.56, p < 0.001), with final outputs reaching 2.010 - a 70.4%\nimprovement. Refined slogans are 48.4% shorter, 40.7% more novel, with only a\n4.0% drop in relevance. Cross-modal tests show strong slogan-to-image alignment\n(CLIPScore: 0.249; BERTScore F1: 0.816). In human evaluations, 60% of outputs\nscored >= 4.0, with metaphorical slogans (avg. 4.09) outperforming literal ones\n(3.99). Feedback highlights stylistic precision and emotional resonance. These\nresults demonstrate that error-centered, feedback-driven generation enhances\ncreativity, offering a scalable path toward self-evolving, human-aligned\ncreative AI.\n","authors":["Yusen Peng","Shuhua Mao"],"pdf_url":"https://arxiv.org/pdf/2507.18004v1.pdf","comment":"44 pages,11 figures"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2302.10160v4","updated":"2025-07-24T17:59:49Z","published":"2023-02-20T18:46:12Z","title":"Pseudo-Labeling for Kernel Ridge Regression under Covariate Shift","summary":"  We develop and analyze a principled approach to kernel ridge regression under\ncovariate shift. The goal is to learn a regression function with small mean\nsquared error over a target distribution, based on unlabeled data from there\nand labeled data that may have a different feature distribution. We propose to\nsplit the labeled data into two subsets, and conduct kernel ridge regression on\nthem separately to obtain a collection of candidate models and an imputation\nmodel. We use the latter to fill the missing labels and then select the best\ncandidate accordingly. Our non-asymptotic excess risk bounds demonstrate that\nour estimator adapts effectively to both the structure of the target\ndistribution and the covariate shift. This adaptation is quantified through a\nnotion of effective sample size that reflects the value of labeled source data\nfor the target regression task. Our estimator achieves the minimax optimal\nerror rate up to a polylogarithmic factor, and we find that using pseudo-labels\nfor model selection does not significantly hinder performance.\n","authors":["Kaizheng Wang"],"pdf_url":"https://arxiv.org/pdf/2302.10160v4.pdf","comment":"45 pages, 2 figures"},{"id":"http://arxiv.org/abs/2507.18632v1","updated":"2025-07-24T17:59:36Z","published":"2025-07-24T17:59:36Z","title":"SIDA: Synthetic Image Driven Zero-shot Domain Adaptation","summary":"  Zero-shot domain adaptation is a method for adapting a model to a target\ndomain without utilizing target domain image data. To enable adaptation without\ntarget images, existing studies utilize CLIP's embedding space and text\ndescription to simulate target-like style features. Despite the previous\nachievements in zero-shot domain adaptation, we observe that these text-driven\nmethods struggle to capture complex real-world variations and significantly\nincrease adaptation time due to their alignment process. Instead of relying on\ntext descriptions, we explore solutions leveraging image data, which provides\ndiverse and more fine-grained style cues. In this work, we propose SIDA, a\nnovel and efficient zero-shot domain adaptation method leveraging synthetic\nimages. To generate synthetic images, we first create detailed, source-like\nimages and apply image translation to reflect the style of the target domain.\nWe then utilize the style features of these synthetic images as a proxy for the\ntarget domain. Based on these features, we introduce Domain Mix and Patch Style\nTransfer modules, which enable effective modeling of real-world variations. In\nparticular, Domain Mix blends multiple styles to expand the intra-domain\nrepresentations, and Patch Style Transfer assigns different styles to\nindividual patches. We demonstrate the effectiveness of our method by showing\nstate-of-the-art performance in diverse zero-shot adaptation scenarios,\nparticularly in challenging domains. Moreover, our approach achieves high\nefficiency by significantly reducing the overall adaptation time.\n","authors":["Ye-Chan Kim","SeungJu Cha","Si-Woo Kim","Taewhan Kim","Dong-Jin Kim"],"pdf_url":"https://arxiv.org/pdf/2507.18632v1.pdf","comment":"Accepted to ACM MM 2025"},{"id":"http://arxiv.org/abs/2507.18627v1","updated":"2025-07-24T17:59:08Z","published":"2025-07-24T17:59:08Z","title":"Gait Recognition Based on Tiny ML and IMU Sensors","summary":"  This project presents the development of a gait recognition system using Tiny\nMachine Learning (Tiny ML) and Inertial Measurement Unit (IMU) sensors. The\nsystem leverages the XIAO-nRF52840 Sense microcontroller and the LSM6DS3 IMU\nsensor to capture motion data, including acceleration and angular velocity,\nfrom four distinct activities: walking, stationary, going upstairs, and going\ndownstairs. The data collected is processed through Edge Impulse, an edge AI\nplatform, which enables the training of machine learning models that can be\ndeployed directly onto the microcontroller for real-time activity\nclassification.The data preprocessing step involves extracting relevant\nfeatures from the raw sensor data using techniques such as sliding windows and\ndata normalization, followed by training a Deep Neural Network (DNN) classifier\nfor activity recognition. The model achieves over 80% accuracy on a test\ndataset, demonstrating its ability to classify the four activities effectively.\nAdditionally, the platform enables anomaly detection, further enhancing the\nrobustness of the system. The integration of Tiny ML ensures low-power\noperation, making it suitable for battery-powered or energy-harvesting devices.\n","authors":["Jiahang Zhang","Mingtong Chen","Zhengbao Yang"],"pdf_url":"https://arxiv.org/pdf/2507.18627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18623v1","updated":"2025-07-24T17:57:18Z","published":"2025-07-24T17:57:18Z","title":"Moving Out: Physically-grounded Human-AI Collaboration","summary":"  The ability to adapt to physical actions and constraints in an environment is\ncrucial for embodied agents (e.g., robots) to effectively collaborate with\nhumans. Such physically grounded human-AI collaboration must account for the\nincreased complexity of the continuous state-action space and constrained\ndynamics caused by physical constraints. In this paper, we introduce\n\\textit{Moving Out}, a new human-AI collaboration benchmark that resembles a\nwide range of collaboration modes affected by physical attributes and\nconstraints, such as moving heavy items together and maintaining consistent\nactions to move a big item around a corner. Using Moving Out, we designed two\ntasks and collected human-human interaction data to evaluate models' abilities\nto adapt to diverse human behaviors and unseen physical attributes. To address\nthe challenges in physical environments, we propose a novel method, BASS\n(Behavior Augmentation, Simulation, and Selection), to enhance the diversity of\nagents and their understanding of the outcome of actions. Our experiments show\nthat BASS outperforms state-of-the-art models in AI-AI and human-AI\ncollaboration. The project page is available at\n\\href{https://live-robotics-uva.github.io/movingout_ai/}{https://live-robotics-uva.github.io/movingout\\_ai/}.\n","authors":["Xuhui Kang","Sung-Wook Lee","Haolin Liu","Yuyan Wang","Yen-Ling Kuo"],"pdf_url":"https://arxiv.org/pdf/2507.18623v1.pdf","comment":"24 pages, 8 figures"},{"id":"http://arxiv.org/abs/2507.15857v2","updated":"2025-07-24T17:55:24Z","published":"2025-07-21T17:59:57Z","title":"Diffusion Beats Autoregressive in Data-Constrained Settings","summary":"  Autoregressive (AR) models have long dominated the landscape of large\nlanguage models, driving progress across a wide range of tasks. Recently,\ndiffusion-based language models have emerged as a promising alternative, though\ntheir advantages over AR models remain underexplored. In this paper, we\nsystematically study masked diffusion models in data-constrained settings-where\ntraining involves repeated passes over limited data-and find that they\nsignificantly outperform AR models when compute is abundant but data is scarce.\nDiffusion models make better use of repeated data, achieving lower validation\nloss and superior downstream performance. We interpret this advantage as\nimplicit data augmentation: masked diffusion exposes the model to a diverse\ndistribution of token orderings and prediction tasks, unlike AR's fixed\nleft-to-right factorization. We find new scaling laws for diffusion models and\nderive a closed-form expression for the critical compute threshold at which\ndiffusion begins to outperform AR. These results suggest that when data, not\ncompute, is the bottleneck, diffusion models offer a compelling alternative to\nthe standard AR paradigm. Our code is available at:\nhttps://diffusion-scaling.github.io.\n","authors":["Mihir Prabhudesai","Menging Wu","Amir Zadeh","Katerina Fragkiadaki","Deepak Pathak"],"pdf_url":"https://arxiv.org/pdf/2507.15857v2.pdf","comment":"Project Webpage: https://diffusion-scaling.github.io"},{"id":"http://arxiv.org/abs/2507.18618v1","updated":"2025-07-24T17:54:44Z","published":"2025-07-24T17:54:44Z","title":"TRPrompt: Bootstrapping Query-Aware Prompt Optimization from Textual\n  Rewards","summary":"  Prompt optimization improves the reasoning abilities of large language models\n(LLMs) without requiring parameter updates to the target model. Following\nheuristic-based \"Think step by step\" approaches, the field has evolved in two\nmain directions: while one group of methods uses textual feedback to elicit\nimproved prompts from general-purpose LLMs in a training-free way, a concurrent\nline of research relies on numerical rewards to train a special prompt model,\ntailored for providing optimal prompts to the target model. In this paper, we\nintroduce the Textual Reward Prompt framework (TRPrompt), which unifies these\napproaches by directly incorporating textual feedback into training of the\nprompt model. Our framework does not require prior dataset collection and is\nbeing iteratively improved with the feedback on the generated prompts. When\ncoupled with the capacity of an LLM to internalize the notion of what a \"good\"\nprompt is, the high-resolution signal provided by the textual rewards allows us\nto train a prompt model yielding state-of-the-art query-specific prompts for\nthe problems from the challenging math datasets GSMHard and MATH.\n","authors":["Andreea Nica","Ivan Zakazov","Nicolas Mario Baldwin","Saibo Geng","Robert West"],"pdf_url":"https://arxiv.org/pdf/2507.18618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18616v1","updated":"2025-07-24T17:53:26Z","published":"2025-07-24T17:53:26Z","title":"SynC: Synthetic Image Caption Dataset Refinement with One-to-many\n  Mapping for Zero-shot Image Captioning","summary":"  Zero-shot Image Captioning (ZIC) increasingly utilizes synthetic datasets\ngenerated by text-to-image (T2I) models to mitigate the need for costly manual\nannotation. However, these T2I models often produce images that exhibit\nsemantic misalignments with their corresponding input captions (e.g., missing\nobjects, incorrect attributes), resulting in noisy synthetic image-caption\npairs that can hinder model training. Existing dataset pruning techniques are\nlargely designed for removing noisy text in web-crawled data. However, these\nmethods are ill-suited for the distinct challenges of synthetic data, where\ncaptions are typically well-formed, but images may be inaccurate\nrepresentations. To address this gap, we introduce SynC, a novel framework\nspecifically designed to refine synthetic image-caption datasets for ZIC.\nInstead of conventional filtering or regeneration, SynC focuses on reassigning\ncaptions to the most semantically aligned images already present within the\nsynthetic image pool. Our approach employs a one-to-many mapping strategy by\ninitially retrieving multiple relevant candidate images for each caption. We\nthen apply a cycle-consistency-inspired alignment scorer that selects the best\nimage by verifying its ability to retrieve the original caption via\nimage-to-text retrieval. Extensive evaluations demonstrate that SynC\nconsistently and significantly improves performance across various ZIC models\non standard benchmarks (MS-COCO, Flickr30k, NoCaps), achieving state-of-the-art\nresults in several scenarios. SynC offers an effective strategy for curating\nrefined synthetic data to enhance ZIC.\n","authors":["Si-Woo Kim","MinJu Jeon","Ye-Chan Kim","Soeun Lee","Taewhan Kim","Dong-Jin Kim"],"pdf_url":"https://arxiv.org/pdf/2507.18616v1.pdf","comment":"Accepted to ACM Multimedia 2025"},{"id":"http://arxiv.org/abs/2503.07919v3","updated":"2025-07-24T17:45:05Z","published":"2025-03-10T23:50:30Z","title":"BEARCUBS: A benchmark for computer-using web agents","summary":"  Modern web agents possess computer use abilities that allow them to interact\nwith webpages by sending commands to a virtual keyboard and mouse. While such\nagents have considerable potential to assist human users with complex tasks,\nevaluating their capabilities in real-world settings poses a major challenge.\nTo this end, we introduce BEARCUBS, a \"smallbut mighty\" benchmark of 111\ninformation-seeking questions designed to evaluate a web agent's ability to\nsearch, browse, and identify factual information from the web. Unlike prior web\nagent benchmarks, solving BEARCUBS requires (1) accessing live web content\nrather than synthetic or simulated pages, which captures the unpredictability\nof real-world web interactions; and (2) performing a broad range of multimodal\ninteractions (e.g., video understanding, 3D navigation) that cannot be bypassed\nvia text-based workarounds. Each question in BEARCUBS has a corresponding\nshort, unambiguous answer and a human-validated browsing trajectory, allowing\nfor transparent evaluation of agent performance and strategies. A human study\nconfirms that BEARCUBS questions are solvable but non-trivial (84.7% human\naccuracy), revealing domain knowledge gaps and overlooked details as common\nfailure points. We find that ChatGPT Agent significantly outperforms other\ncomputer-using agents with an overall accuracy of 65.8% (compared to e.g.,\nOperator's 23.4%), showcasing substantial progress in tasks involving real\ncomputer use, such as playing web games and navigating 3D environments.\nNevertheless, closing the gap to human performance requires improvements in\nareas like fine control, complex data filtering, and execution speed. To\nfacilitate future research, BEARCUBS will be updated periodically to replace\ninvalid or contaminated questions, keeping the benchmark fresh for future\ngenerations of web agents.\n","authors":["Yixiao Song","Katherine Thai","Chau Minh Pham","Yapei Chang","Mazin Nadaf","Mohit Iyyer"],"pdf_url":"https://arxiv.org/pdf/2503.07919v3.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2507.18607v1","updated":"2025-07-24T17:43:40Z","published":"2025-07-24T17:43:40Z","title":"Explainable Mapper: Charting LLM Embedding Spaces Using\n  Perturbation-Based Explanation and Verification Agents","summary":"  Large language models (LLMs) produce high-dimensional embeddings that capture\nrich semantic and syntactic relationships between words, sentences, and\nconcepts. Investigating the topological structures of LLM embedding spaces via\nmapper graphs enables us to understand their underlying structures.\nSpecifically, a mapper graph summarizes the topological structure of the\nembedding space, where each node represents a topological neighborhood\n(containing a cluster of embeddings), and an edge connects two nodes if their\ncorresponding neighborhoods overlap. However, manually exploring these\nembedding spaces to uncover encoded linguistic properties requires considerable\nhuman effort. To address this challenge, we introduce a framework for\nsemi-automatic annotation of these embedding properties. To organize the\nexploration process, we first define a taxonomy of explorable elements within a\nmapper graph such as nodes, edges, paths, components, and trajectories. The\nannotation of these elements is executed through two types of customizable\nLLM-based agents that employ perturbation techniques for scalable and automated\nanalysis. These agents help to explore and explain the characteristics of\nmapper elements and verify the robustness of the generated explanations. We\ninstantiate the framework within a visual analytics workspace and demonstrate\nits effectiveness through case studies. In particular, we replicate findings\nfrom prior research on BERT's embedding properties across various layers of its\narchitecture and provide further observations into the linguistic properties of\ntopological neighborhoods.\n","authors":["Xinyuan Yan","Rita Sevastjanova","Sinie van der Ben","Mennatallah El-Assady","Bei Wang"],"pdf_url":"https://arxiv.org/pdf/2507.18607v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18606v1","updated":"2025-07-24T17:42:30Z","published":"2025-07-24T17:42:30Z","title":"Hybrid quantum-classical algorithm for near-optimal planning in POMDPs","summary":"  Reinforcement learning (RL) provides a principled framework for\ndecision-making in partially observable environments, which can be modeled as\nMarkov decision processes and compactly represented through dynamic decision\nBayesian networks. Recent advances demonstrate that inference on sparse\nBayesian networks can be accelerated using quantum rejection sampling combined\nwith amplitude amplification, leading to a computational speedup in estimating\nacceptance probabilities.\\\\ Building on this result, we introduce Quantum\nBayesian Reinforcement Learning (QBRL), a hybrid quantum-classical look-ahead\nalgorithm for model-based RL in partially observable environments. We present a\nrigorous, oracle-free time complexity analysis under fault-tolerant assumptions\nfor the quantum device. Unlike standard treatments that assume a black-box\noracle, we explicitly specify the inference process, allowing our bounds to\nmore accurately reflect the true computational cost. We show that, for\nenvironments whose dynamics form a sparse Bayesian network, horizon-based\nnear-optimal planning can be achieved sub-quadratically faster through\nquantum-enhanced belief updates.\n  Furthermore, we present numerical experiments benchmarking QBRL against its\nclassical counterpart on simple yet illustrative decision-making tasks. Our\nresults offer a detailed analysis of how the quantum computational advantage\ntranslates into decision-making performance, highlighting that the magnitude of\nthe advantage can vary significantly across different deployment settings.\n","authors":["Gilberto Cunha","Alexandra Ramôa","André Sequeira","Michael de Oliveira","Luís Barbosa"],"pdf_url":"https://arxiv.org/pdf/2507.18606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09468v2","updated":"2025-07-24T17:41:43Z","published":"2024-07-12T17:48:36Z","title":"Beyond Euclid: An Illustrated Guide to Modern Machine Learning with\n  Geometric, Topological, and Algebraic Structures","summary":"  The enduring legacy of Euclidean geometry underpins classical machine\nlearning, which, for decades, has been primarily developed for data lying in\nEuclidean space. Yet, modern machine learning increasingly encounters richly\nstructured data that is inherently nonEuclidean. This data can exhibit\nintricate geometric, topological and algebraic structure: from the geometry of\nthe curvature of space-time, to topologically complex interactions between\nneurons in the brain, to the algebraic transformations describing symmetries of\nphysical systems. Extracting knowledge from such non-Euclidean data\nnecessitates a broader mathematical perspective. Echoing the 19th-century\nrevolutions that gave rise to non-Euclidean geometry, an emerging line of\nresearch is redefining modern machine learning with non-Euclidean structures.\nIts goal: generalizing classical methods to unconventional data types with\ngeometry, topology, and algebra. In this review, we provide an accessible\ngateway to this fast-growing field and propose a graphical taxonomy that\nintegrates recent advances into an intuitive unified framework. We subsequently\nextract insights into current challenges and highlight exciting opportunities\nfor future development in this field.\n","authors":["Mathilde Papillon","Sophia Sanborn","Johan Mathe","Louisa Cornelis","Abby Bertics","Domas Buracas","Hansen J Lillemark","Christian Shewmake","Fatih Dinc","Xavier Pennec","Nina Miolane"],"pdf_url":"https://arxiv.org/pdf/2407.09468v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18603v1","updated":"2025-07-24T17:34:02Z","published":"2025-07-24T17:34:02Z","title":"Demystify Protein Generation with Hierarchical Conditional Diffusion\n  Models","summary":"  Generating novel and functional protein sequences is critical to a wide range\nof applications in biology. Recent advancements in conditional diffusion models\nhave shown impressive empirical performance in protein generation tasks.\nHowever, reliable generations of protein remain an open research question in de\nnovo protein design, especially when it comes to conditional diffusion models.\nConsidering the biological function of a protein is determined by multi-level\nstructures, we propose a novel multi-level conditional diffusion model that\nintegrates both sequence-based and structure-based information for efficient\nend-to-end protein design guided by specified functions. By generating\nrepresentations at different levels simultaneously, our framework can\neffectively model the inherent hierarchical relations between different levels,\nresulting in an informative and discriminative representation of the generated\nprotein. We also propose a Protein-MMD, a new reliable evaluation metric, to\nevaluate the quality of generated protein with conditional diffusion models.\nOur new metric is able to capture both distributional and functional\nsimilarities between real and generated protein sequences while ensuring\nconditional consistency. We experiment with the benchmark datasets, and the\nresults on conditional protein generation tasks demonstrate the efficacy of the\nproposed generation framework and evaluation metric.\n","authors":["Zinan Ling","Yi Shi","Da Yan","Yang Zhou","Bo Hui"],"pdf_url":"https://arxiv.org/pdf/2507.18603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16870v2","updated":"2025-07-24T17:30:12Z","published":"2025-03-21T05:58:18Z","title":"Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs","summary":"  Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.\n","authors":[" Anshumann","Mohd Abbas Zaidi","Akhil Kedia","Jinwoo Ahn","Taehwak Kwon","Kangwook Lee","Haejun Lee","Joohyung Lee"],"pdf_url":"https://arxiv.org/pdf/2503.16870v2.pdf","comment":"Accepted as Oral paper at ACL 2025. Source code is available at\n  https://github.com/akhilkedia/RandomSamplingKD . Anshumann, Mohd Abbas Zaidi\n  and Akhil Kedia have Equal Contribution"},{"id":"http://arxiv.org/abs/2507.18597v1","updated":"2025-07-24T17:28:57Z","published":"2025-07-24T17:28:57Z","title":"Linear Memory SE(2) Invariant Attention","summary":"  Processing spatial data is a key component in many learning tasks for\nautonomous driving such as motion forecasting, multi-agent simulation, and\nplanning. Prior works have demonstrated the value in using SE(2) invariant\nnetwork architectures that consider only the relative poses between objects\n(e.g. other agents, scene features such as traffic lanes). However, these\nmethods compute the relative poses for all pairs of objects explicitly,\nrequiring quadratic memory. In this work, we propose a mechanism for SE(2)\ninvariant scaled dot-product attention that requires linear memory relative to\nthe number of objects in the scene. Our SE(2) invariant transformer\narchitecture enjoys the same scaling properties that have benefited large\nlanguage models in recent years. We demonstrate experimentally that our\napproach is practical to implement and improves performance compared to\ncomparable non-invariant architectures.\n","authors":["Ethan Pronovost","Neha Boloor","Peter Schleede","Noureldin Hendy","Andres Morales","Nicholas Roy"],"pdf_url":"https://arxiv.org/pdf/2507.18597v1.pdf","comment":"Best paper award, Equivariant Systems Workshop at RSS"},{"id":"http://arxiv.org/abs/2410.13812v2","updated":"2025-07-24T17:25:40Z","published":"2024-10-17T17:45:07Z","title":"Private Counterfactual Retrieval","summary":"  Transparency and explainability are two extremely important aspects to be\nconsidered when employing black-box machine learning models in high-stake\napplications. Providing counterfactual explanations is one way of fulfilling\nthis requirement. However, this also poses a threat to the privacy of both the\ninstitution that is providing the explanation as well as the user who is\nrequesting it. In this work, we propose multiple schemes inspired by private\ninformation retrieval (PIR) techniques which ensure the \\emph{user's privacy}\nwhen retrieving counterfactual explanations. We present a scheme which\nretrieves the \\emph{exact} nearest neighbor counterfactual explanation from a\ndatabase of accepted points while achieving perfect (information-theoretic)\nprivacy for the user. While the scheme achieves perfect privacy for the user,\nsome leakage on the database is inevitable which we quantify using a mutual\ninformation based metric. Furthermore, we propose strategies to reduce this\nleakage to achieve an advanced degree of database privacy. We extend these\nschemes to incorporate user's preference on transforming their attributes, so\nthat a more actionable explanation can be received. Since our schemes rely on\nfinite field arithmetic, we empirically validate our schemes on real datasets\nto understand the trade-off between the accuracy and the finite field sizes.\nFinally, we present numerical results to support our theoretical findings, and\ncompare the database leakage of the proposed schemes.\n","authors":["Mohamed Nomeir","Pasan Dissanayake","Shreya Meel","Sanghamitra Dutta","Sennur Ulukus"],"pdf_url":"https://arxiv.org/pdf/2410.13812v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18594v1","updated":"2025-07-24T17:24:59Z","published":"2025-07-24T17:24:59Z","title":"DRWKV: Focusing on Object Edges for Low-Light Image Enhancement","summary":"  Low-light image enhancement remains a challenging task, particularly in\npreserving object edge continuity and fine structural details under extreme\nillumination degradation. In this paper, we propose a novel model, DRWKV\n(Detailed Receptance Weighted Key Value), which integrates our proposed Global\nEdge Retinex (GER) theory, enabling effective decoupling of illumination and\nedge structures for enhanced edge fidelity. Secondly, we introduce Evolving WKV\nAttention, a spiral-scanning mechanism that captures spatial edge continuity\nand models irregular structures more effectively. Thirdly, we design the\nBilateral Spectrum Aligner (Bi-SAB) and a tailored MS2-Loss to jointly align\nluminance and chrominance features, improving visual naturalness and mitigating\nartifacts. Extensive experiments on five LLIE benchmarks demonstrate that DRWKV\nachieves leading performance in PSNR, SSIM, and NIQE while maintaining low\ncomputational complexity. Furthermore, DRWKV enhances downstream performance in\nlow-light multi-object tracking tasks, validating its generalization\ncapabilities.\n","authors":["Xuecheng Bai","Yuxiang Wang","Boyu Hu","Qinyuan Jie","Chuanzhi Xu","Hongru Xiao","Kechen Li","Vera Chung"],"pdf_url":"https://arxiv.org/pdf/2507.18594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05249v3","updated":"2025-07-24T16:56:37Z","published":"2025-06-05T17:10:22Z","title":"On the Convergence of Gradient Descent on Learning Transformers with\n  Residual Connections","summary":"  Transformer models have emerged as fundamental tools across various\nscientific and engineering disciplines, owing to their outstanding performance\nin diverse applications. Despite this empirical success, the theoretical\nfoundations of Transformers remain relatively underdeveloped, particularly in\nunderstanding their training dynamics. Existing research predominantly examines\nisolated components--such as self-attention mechanisms and feedforward\nnetworks--without thoroughly investigating the interdependencies between these\ncomponents, especially when residual connections are present. In this paper, we\naim to bridge this gap by analyzing the convergence behavior of a structurally\ncomplete yet single-layer Transformer, comprising self-attention, a feedforward\nnetwork, and residual connections. We demonstrate that, under appropriate\ninitialization, gradient descent exhibits a linear convergence rate, where the\nconvergence speed is determined by the minimum and maximum singular values of\nthe output matrix from the attention layer. Moreover, our analysis reveals that\nresidual connections serve to ameliorate the ill-conditioning of this output\nmatrix, an issue stemming from the low-rank structure imposed by the softmax\noperation, thereby promoting enhanced optimization stability. We also extend\nour theoretical findings to a multi-layer Transformer architecture, confirming\nthe linear convergence rate of gradient descent under suitable initialization.\nEmpirical results corroborate our theoretical insights, illustrating the\nbeneficial role of residual connections in promoting convergence stability.\n","authors":["Zhen Qin","Jinxin Zhou","Zhihui Zhu"],"pdf_url":"https://arxiv.org/pdf/2506.05249v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.16802v3","updated":"2025-07-24T16:46:58Z","published":"2025-07-22T17:52:16Z","title":"Agentar-Fin-R1: Enhancing Financial Intelligence through Domain\n  Expertise, Training Efficiency, and Advanced Reasoning","summary":"  Large Language Models (LLMs) exhibit considerable promise in financial\napplications; however, prevailing models frequently demonstrate limitations\nwhen confronted with scenarios that necessitate sophisticated reasoning\ncapabilities, stringent trustworthiness criteria, and efficient adaptation to\ndomain-specific requirements. We introduce the Agentar-Fin-R1 series of\nfinancial large language models (8B and 32B parameters), specifically\nengineered based on the Qwen3 foundation model to enhance reasoning\ncapabilities, reliability, and domain specialization for financial\napplications. Our optimization approach integrates a high-quality, systematic\nfinancial task label system with a comprehensive multi-layered trustworthiness\nassurance framework. This framework encompasses high-quality trustworthy\nknowledge engineering, multi-agent trustworthy data synthesis, and rigorous\ndata validation governance. Through label-guided automated difficulty-aware\noptimization, tow-stage training pipeline, and dynamic attribution systems, we\nachieve substantial improvements in training efficiency. Our models undergo\ncomprehensive evaluation on mainstream financial benchmarks including Fineva,\nFinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500\nand GPQA-diamond. To thoroughly assess real-world deployment capabilities, we\ninnovatively propose the Finova evaluation benchmark, which focuses on\nagent-level financial reasoning and compliance verification. Experimental\nresults demonstrate that Agentar-Fin-R1 not only achieves state-of-the-art\nperformance on financial tasks but also exhibits exceptional general reasoning\ncapabilities, validating its effectiveness as a trustworthy solution for\nhigh-stakes financial applications. The Finova bench is available at\nhttps://github.com/antgroup/Finova.\n","authors":["Yanjun Zheng","Xiyang Du","Longfei Liao","Xiaoke Zhao","Zhaowen Zhou","Jingze Song","Bo Zhang","Jiawei Liu","Xiang Qi","Zhe Li","Zhiqiang Zhang","Wei Wang","Peng Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.16802v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18561v1","updated":"2025-07-24T16:35:42Z","published":"2025-07-24T16:35:42Z","title":"Beyond Internal Data: Constructing Complete Datasets for Fairness\n  Testing","summary":"  As AI becomes prevalent in high-risk domains and decision-making, it is\nessential to test for potential harms and biases. This urgency is reflected by\nthe global emergence of AI regulations that emphasise fairness and adequate\ntesting, with some mandating independent bias audits. However, procuring the\nnecessary data for fairness testing remains a significant challenge.\nParticularly in industry settings, legal and privacy concerns restrict the\ncollection of demographic data required to assess group disparities, and\nauditors face practical and cultural challenges in gaining access to data.\nFurther, internal historical datasets are often insufficiently representative\nto identify real-world biases. This work focuses on evaluating classifier\nfairness when complete datasets including demographics are inaccessible. We\npropose leveraging separate overlapping datasets to construct complete\nsynthetic data that includes demographic information and accurately reflects\nthe underlying relationships between protected attributes and model features.\nWe validate the fidelity of the synthetic data by comparing it to real data,\nand empirically demonstrate that fairness metrics derived from testing on such\nsynthetic data are consistent with those obtained from real data. This work,\ntherefore, offers a path to overcome real-world data scarcity for fairness\ntesting, enabling independent, model-agnostic evaluation of fairness, and\nserving as a viable substitute where real data is limited.\n","authors":["Varsha Ramineni","Hossein A. Rahmani","Emine Yilmaz","David Barber"],"pdf_url":"https://arxiv.org/pdf/2507.18561v1.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2507.18555v1","updated":"2025-07-24T16:26:52Z","published":"2025-07-24T16:26:52Z","title":"Neural Tangent Kernels and Fisher Information Matrices for Simple ReLU\n  Networks with Random Hidden Weights","summary":"  Fisher information matrices and neural tangent kernels (NTK) for 2-layer ReLU\nnetworks with random hidden weight are argued. We discuss the relation between\nboth notions as a linear transformation and show that spectral decomposition of\nNTK with concrete forms of eigenfunctions with major eigenvalues. We also\nobtain an approximation formula of the functions presented by the 2-layer\nneural networks.\n","authors":["Jun'ichi Takeuchia","Yoshinari Takeishia","Noboru Muratab","Kazushi Mimurac","Ka Long Keith Hod","Hiroshi Nagaoka"],"pdf_url":"https://arxiv.org/pdf/2507.18555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.14783v2","updated":"2025-07-24T16:25:54Z","published":"2025-07-20T01:50:16Z","title":"Omni-Thinker: Scaling Cross-Domain Generalization in LLMs via Multi-Task\n  RL with Hybrid Rewards","summary":"  The advancement of general-purpose artificial intelligence relies on large\nlanguage models (LLMs) that excel across a wide range of tasks, from structured\nreasoning to creative generation. However, post-training methods like\nSupervised Fine-Tuning (SFT) often struggle with generalization, favoring\nmemorization over transferable learning. In this work, we introduce\nOmni-Thinker, a unified reinforcement learning (RL) framework that enhances LLM\nperformance across diverse tasks by combining rule-based verifiable rewards\nwith generative preference signals via LLM-as-a-Judge evaluations. Our approach\nenables consistent optimization across task types and scales RL-based training\nto subjective domains. We further investigate training strategies,\ndemonstrating that a curriculum-based progression that orders tasks from\nstructured to open-ended improves performance and reduces forgetting.\nExperimental results across four domains reveal that curriculum learning\nimproves performance by 5.2% over joint training and 9.1% over model merging.\nThese results highlight the importance of task-aware sampling and hybrid\nsupervision in scaling RL-based post-training for general-purpose LLMs.\n","authors":["Derek Li","Jiaming Zhou","Amirreza Kazemi","Qianyi Sun","Abbas Ghaddar","Mohammad Ali Alomrani","Liheng Ma","Yu Luo","Dong Li","Feng Wen","Jianye Hao","Mark Coates","Yingxue Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.14783v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.04704v2","updated":"2025-07-24T16:25:51Z","published":"2025-04-07T03:22:15Z","title":"LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important","summary":"  The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV.\n","authors":["Manlai Liang","JiaMing Zhang","Xiong Li","Jinlong Li"],"pdf_url":"https://arxiv.org/pdf/2504.04704v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18553v1","updated":"2025-07-24T16:22:18Z","published":"2025-07-24T16:22:18Z","title":"The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane\n  Algorithm","summary":"  Quantizing the weights of large language models (LLMs) from 16-bit to lower\nbitwidth is the de facto approach to deploy massive transformers onto more\naffordable accelerators. GPTQ emerged as one of the standard methods for\none-shot post-training quantization at LLM scale. Yet, its inner workings are\ndescribed as a sequence of ad-hoc algebraic updates that obscure any geometric\nmeaning or worst-case guarantees. In this work, we show that, when executed\nback-to-front (from the last to first dimension) for a linear layer, GPTQ is\nmathematically identical to Babai's nearest plane algorithm for the classical\nclosest vector problem (CVP) on a lattice defined by the Hessian matrix of the\nlayer's inputs. This equivalence is based on a sophisticated mathematical\nargument, and has two analytical consequences: (i) the GPTQ error propagation\nstep gains an intuitive geometric interpretation; (ii) GPTQ inherits the error\nupper bound of Babai's algorithm under the no-clipping condition. Taken\ntogether, these results place GPTQ on firm theoretical footing and open the\ndoor to importing decades of progress in lattice algorithms towards the design\nof future quantization algorithms for billion-parameter models.\n","authors":["Jiale Chen","Torsten Hoefler","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2507.18553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08989v3","updated":"2025-07-24T16:21:10Z","published":"2024-10-11T17:01:43Z","title":"Zeroth-Order Fine-Tuning of LLMs in Random Subspaces","summary":"  Fine-tuning Large Language Models (LLMs) has proven effective for a variety\nof downstream tasks. However, as LLMs grow in size, the memory demands for\nbackpropagation become increasingly prohibitive. Zeroth-order (ZO) optimization\nmethods offer a memory-efficient alternative by using forward passes to\nestimate gradients, but the variance of gradient estimates typically scales\nlinearly with the model's parameter dimension$\\unicode{x2013}$a significant\nissue for LLMs. In this paper, we propose the random Subspace Zeroth-order\n(SubZero) optimization to address the challenges posed by LLMs' high\ndimensionality. We introduce a low-rank perturbation tailored for LLMs that\nsignificantly reduces memory consumption while improving training performance.\nAdditionally, we prove that our gradient estimation closely approximates the\nbackpropagation gradient, exhibits lower variance than traditional ZO methods,\nand ensures convergence when combined with SGD. Experimental results show that\nSubZero enhances fine-tuning performance and achieves faster convergence\ncompared to standard ZO approaches like MeZO across various language modeling\ntasks. Code is available at https://github.com/zimingyy/SubZero.\n","authors":["Ziming Yu","Pan Zhou","Sike Wang","Jia Li","Mi Tian","Hua Huang"],"pdf_url":"https://arxiv.org/pdf/2410.08989v3.pdf","comment":"ICCV 2025 camera-ready version"},{"id":"http://arxiv.org/abs/2507.18550v1","updated":"2025-07-24T16:18:46Z","published":"2025-07-24T16:18:46Z","title":"On the Performance of Concept Probing: The Influence of the Data\n  (Extended Version)","summary":"  Concept probing has recently garnered increasing interest as a way to help\ninterpret artificial neural networks, dealing both with their typically large\nsize and their subsymbolic nature, which ultimately renders them unfeasible for\ndirect human interpretation. Concept probing works by training additional\nclassifiers to map the internal representations of a model into human-defined\nconcepts of interest, thus allowing humans to peek inside artificial neural\nnetworks. Research on concept probing has mainly focused on the model being\nprobed or the probing model itself, paying limited attention to the data\nrequired to train such probing models. In this paper, we address this gap.\nFocusing on concept probing in the context of image classification tasks, we\ninvestigate the effect of the data used to train probing models on their\nperformance. We also make available concept labels for two widely used\ndatasets.\n","authors":["Manuel de Sousa Ribeiro","Afonso Leote","João Leite"],"pdf_url":"https://arxiv.org/pdf/2507.18550v1.pdf","comment":"Extended version of the paper published in Proceedings of the\n  European Conference on Artificial Intelligence (ECAI 2025)"},{"id":"http://arxiv.org/abs/2507.18549v1","updated":"2025-07-24T16:13:56Z","published":"2025-07-24T16:13:56Z","title":"The Price equation reveals a universal force-metric-bias law of\n  algorithmic learning and natural selection","summary":"  Diverse learning algorithms, optimization methods, and natural selection\nshare a common mathematical structure, despite their apparent differences. Here\nI show that a simple notational partitioning of change by the Price equation\nreveals a universal force-metric-bias (FMB) law: $\\Delta\\mathbf{\\theta} =\n\\mathbf{M}\\,\\mathbf{f} + \\mathbf{b} + \\mathbf{\\xi}$. The force $\\mathbf{f}$\ndrives improvement in parameters, $\\Delta\\mathbf{\\theta}$, through the\ncovariance between the parameters and performance. The metric $\\mathbf{M}$\nrescales movement by inverse curvature. The bias $\\mathbf{b}$ adds momentum or\nchanges in the frame of reference. The noise $\\mathbf{\\xi}$ enables\nexploration. This framework unifies natural selection, Bayesian updating,\nNewton's method, stochastic gradient descent, stochastic Langevin dynamics,\nAdam optimization, and most other algorithms as special cases of the same\nunderlying process. The Price equation also reveals why Fisher information,\nKullback-Leibler divergence, and d'Alembert's principle arise naturally in\nlearning dynamics. By exposing this common structure, the FMB law provides a\nprincipled foundation for understanding, comparing, and designing learning\nalgorithms across disciplines.\n","authors":["Steven A. Frank"],"pdf_url":"https://arxiv.org/pdf/2507.18549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.07926v2","updated":"2025-07-24T16:08:12Z","published":"2025-03-11T00:12:25Z","title":"Learning Gentle Grasping Using Vision, Sound, and Touch","summary":"  In our daily life, we often encounter objects that are fragile and can be\ndamaged by excessive grasping force, such as fruits. For these objects, it is\nparamount to grasp gently -- not using the maximum amount of force possible,\nbut rather the minimum amount of force necessary. This paper proposes using\nvisual, tactile, and auditory signals to learn to grasp and regrasp objects\nstably and gently. Specifically, we use audio signals as an indicator of\ngentleness during the grasping, and then train an end-to-end action-conditional\nmodel from raw visuo-tactile inputs that predicts both the stability and the\ngentleness of future grasping candidates, thus allowing the selection and\nexecution of the most promising action. Experimental results on a\nmulti-fingered hand over 1,500 grasping trials demonstrated that our model is\nuseful for gentle grasping by validating the predictive performance (3.27%\nhigher accuracy than the vision-only variant) and providing interpretations of\ntheir behavior. Finally, real-world experiments confirmed that the grasping\nperformance with the trained multi-modal model outperformed other baselines\n(17% higher rate for stable and gentle grasps than vision-only). Our approach\nrequires neither tactile sensor calibration nor analytical force modeling,\ndrastically reducing the engineering effort to grasp fragile objects. Dataset\nand videos are available at https://lasr.org/research/gentle-grasping.\n","authors":["Ken Nakahara","Roberto Calandra"],"pdf_url":"https://arxiv.org/pdf/2503.07926v2.pdf","comment":"8 pages. Accepted by 2025 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS)"},{"id":"http://arxiv.org/abs/2507.18540v1","updated":"2025-07-24T16:07:13Z","published":"2025-07-24T16:07:13Z","title":"Deep Variational Free Energy Calculation of Hydrogen Hugoniot","summary":"  We develop a deep variational free energy framework to compute the equation\nof state of hydrogen in the warm dense matter region. This method parameterizes\nthe variational density matrix of hydrogen nuclei and electrons at finite\ntemperature using three deep generative models: a normalizing flow model that\nrepresents the Boltzmann distribution of the classical nuclei, an\nautoregressive transformer that models the distribution of electrons in excited\nstates, and a permutational equivariant flow model that constructs backflow\ncoordinates for electrons in Hartree-Fock orbitals. By jointly optimizing the\nthree neural networks to minimize the variational free energy, we obtain the\nequation of state and related thermodynamic properties of dense hydrogen. We\ncompare our results with other theoretical and experimental results on the\ndeuterium Hugoniot curve, aiming to resolve existing discrepancies. The\ncalculated results provide a valuable benchmark for deuterium in the warm dense\nmatter region.\n","authors":["Zihang Li","Hao Xie","Xinyang Dong","Lei Wang"],"pdf_url":"https://arxiv.org/pdf/2507.18540v1.pdf","comment":"7+17 pages, 5+14 figures, for source code and raw data, see\n  https://github.com/fermiflow/Hugoniot"},{"id":"http://arxiv.org/abs/2507.18538v1","updated":"2025-07-24T16:04:59Z","published":"2025-07-24T16:04:59Z","title":"AI/ML Life Cycle Management for Interoperable AI Native RAN","summary":"  Artificial intelligence (AI) and machine learning (ML) models are rapidly\npermeating the 5G Radio Access Network (RAN), powering beam management, channel\nstate information (CSI) feedback, positioning, and mobility prediction.\nHowever, without a standardized life-cycle management (LCM) framework,\nchallenges, such as model drift, vendor lock-in, and limited transparency,\nhinder large-scale adoption. 3GPP Releases 16-20 progressively evolve AI/ML\nfrom experimental features to managed, interoperable network functions.\nBeginning with the Network Data Analytics Function (NWDAF) in Rel-16,\nsubsequent releases introduced standardized interfaces for model transfer,\nexecution, performance monitoring, and closed-loop control, culminating in\nRel-20's two-sided CSI-compression Work Item and vendor-agnostic LCM profile.\nThis article reviews the resulting five-block LCM architecture, KPI-driven\nmonitoring mechanisms, and inter-vendor collaboration schemes, while\nidentifying open challenges in resource-efficient monitoring, environment drift\ndetection, intelligent decision-making, and flexible model training. These\ndevelopments lay the foundation for AI-native transceivers as a key enabler for\n6G.\n","authors":["Chu-Hsiang Huang","Chao-Kai Wen","Geoffrey Ye Li"],"pdf_url":"https://arxiv.org/pdf/2507.18538v1.pdf","comment":"8 pages, 4 figures, 2 table. This work has been submitted to the IEEE\n  for possible publication"},{"id":"http://arxiv.org/abs/2503.08510v2","updated":"2025-07-24T16:04:36Z","published":"2025-03-11T15:00:22Z","title":"External Knowledge Injection for CLIP-Based Class-Incremental Learning","summary":"  Class-Incremental Learning (CIL) enables learning systems to continuously\nadapt to evolving data streams. With the advancement of pre-training,\nleveraging pre-trained vision-language models (e.g., CLIP) offers a promising\nstarting point for CIL. However, CLIP makes decisions by matching visual\nembeddings to class names, overlooking the rich contextual information conveyed\nthrough language. For instance, the concept of ``cat'' can be decomposed into\nfeatures like tail, fur, and face for recognition. Besides, since the model is\ncontinually updated, these detailed features are overwritten in CIL, requiring\nexternal knowledge for compensation. In this paper, we introduce ExterNal\nknowledGe INjEction (ENGINE) for CLIP-based CIL. To enhance knowledge transfer\nfrom outside the dataset, we propose a dual-branch injection tuning framework\nthat encodes informative knowledge from both visual and textual modalities. The\nvisual branch is enhanced with data augmentation to enrich the visual features,\nwhile the textual branch leverages GPT-4 to rewrite discriminative descriptors.\nIn addition to this on-the-fly knowledge injection, we also implement\npost-tuning knowledge by re-ranking the prediction results during inference.\nWith the injected knowledge, the model can better capture informative features\nfor downstream tasks as data evolves. Extensive experiments demonstrate the\nstate-of-the-art performance of ENGINE. Code is available at:\nhttps://github.com/LAMDA-CL/ICCV25-ENGINE\n","authors":["Da-Wei Zhou","Kai-Wen Li","Jingyi Ning","Han-Jia Ye","Lijun Zhang","De-Chuan Zhan"],"pdf_url":"https://arxiv.org/pdf/2503.08510v2.pdf","comment":"Accepted to ICCV 2025. Code is available at:\n  https://github.com/LAMDA-CL/ICCV25-ENGINE"},{"id":"http://arxiv.org/abs/2507.18534v1","updated":"2025-07-24T16:01:34Z","published":"2025-07-24T16:01:34Z","title":"Elucidating the Design Space of Arbitrary-Noise-Based Diffusion Models","summary":"  EDM elucidates the unified design space of diffusion models, yet its fixed\nnoise patterns restricted to pure Gaussian noise, limit advancements in image\nrestoration. Our study indicates that forcibly injecting Gaussian noise\ncorrupts the degraded images, overextends the image transformation distance,\nand increases restoration complexity. To address this problem, our proposed EDA\nElucidates the Design space of Arbitrary-noise-based diffusion models.\nTheoretically, EDA expands the freedom of noise pattern while preserving the\noriginal module flexibility of EDM, with rigorous proof that increased noise\ncomplexity incurs no additional computational overhead during restoration. EDA\nis validated on three typical tasks: MRI bias field correction (global smooth\nnoise), CT metal artifact reduction (global sharp noise), and natural image\nshadow removal (local boundary-aware noise). With only 5 sampling steps, EDA\noutperforms most task-specific methods and achieves state-of-the-art\nperformance in bias field correction and shadow removal.\n","authors":["Xingyu Qiu","Mengying Yang","Xinghua Ma","Dong Liang","Yuzhen Li","Fanding Li","Gongning Luo","Wei Wang","Kuanquan Wang","Shuo Li"],"pdf_url":"https://arxiv.org/pdf/2507.18534v1.pdf","comment":"21 pages, 4 figures"},{"id":"http://arxiv.org/abs/2507.18533v1","updated":"2025-07-24T16:00:32Z","published":"2025-07-24T16:00:32Z","title":"C2G-KD: PCA-Constrained Generator for Data-Free Knowledge Distillation","summary":"  We introduce C2G-KD, a data-free knowledge distillation framework where a\nclass-conditional generator is trained to produce synthetic samples guided by a\nfrozen teacher model and geometric constraints derived from PCA. The generator\nnever observes real training data but instead learns to activate the teacher's\noutput through a combination of semantic and structural losses. By constraining\ngenerated samples to lie within class-specific PCA subspaces estimated from as\nfew as two real examples per class, we preserve topological consistency and\ndiversity. Experiments on MNIST show that even minimal class structure is\nsufficient to bootstrap useful synthetic training pipelines.\n","authors":["Magnus Bengtsson","Kenneth Östberg"],"pdf_url":"https://arxiv.org/pdf/2507.18533v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2506.09027v2","updated":"2025-07-24T15:55:00Z","published":"2025-06-10T17:53:29Z","title":"Diffuse and Disperse: Image Generation with Representation\n  Regularization","summary":"  The development of diffusion-based generative models over the past decade has\nlargely proceeded independently of progress in representation learning. These\ndiffusion models typically rely on regression-based objectives and generally\nlack explicit regularization. In this work, we propose \\textit{Dispersive\nLoss}, a simple plug-and-play regularizer that effectively improves\ndiffusion-based generative models. Our loss function encourages internal\nrepresentations to disperse in the hidden space, analogous to contrastive\nself-supervised learning, with the key distinction that it requires no positive\nsample pairs and therefore does not interfere with the sampling process used\nfor regression. Compared to the recent method of representation alignment\n(REPA), our approach is self-contained and minimalist, requiring no\npre-training, no additional parameters, and no external data. We evaluate\nDispersive Loss on the ImageNet dataset across a range of models and report\nconsistent improvements over widely used and strong baselines. We hope our work\nwill help bridge the gap between generative modeling and representation\nlearning.\n","authors":["Runqian Wang","Kaiming He"],"pdf_url":"https://arxiv.org/pdf/2506.09027v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02976v2","updated":"2025-07-24T15:50:13Z","published":"2025-06-30T21:10:19Z","title":"Are AI-Generated Fixes Secure? Analyzing LLM and Agent Patches on\n  SWE-bench","summary":"  Large Language Models (LLMs) and their agentic frameworks are increasingly\nadopted to automate software development tasks such as issue resolution and\nprogram repair. While prior work has identified security risks in LLM-generated\ncode, most evaluations have focused on synthetic or isolated settings, leaving\nopen questions about the security of these systems in real-world development\ncontexts. In this study, we present the first large-scale security analysis of\nLLM-generated patches using 20,000+ issues from the SWE-bench dataset. We\nevaluate patches produced by a standalone LLM (Llama 3.3) and compare them to\ndeveloper-written patches. We also assess the security of patches generated by\nthree top-performing agentic frameworks (OpenHands, AutoCodeRover, HoneyComb)\non a subset of our data. Finally, we analyze a wide range of code, issue, and\nproject-level factors to understand the conditions under which LLMs and agents\nare most likely to generate insecure code. Our findings reveal that the\nstandalone LLM introduces nearly 9x more new vulnerabilities than developers,\nwith many of these exhibiting unique patterns not found in developers' code.\nAgentic workflows also generate a significant number of vulnerabilities,\nparticularly when granting LLMs more autonomy, potentially increasing the\nlikelihood of misinterpreting project context or task requirements. We find\nthat vulnerabilities are more likely to occur in LLM patches associated with a\nhigher number of files, more lines of generated code, and GitHub issues that\nlack specific code snippets or information about the expected code behavior and\nsteps to reproduce. These results suggest that contextual factors play a\ncritical role in the security of the generated code and point toward the need\nfor proactive risk assessment methods that account for both code and\nissue-level information to complement existing vulnerability detection tools.\n","authors":["Amirali Sajadi","Kostadin Damevski","Preetha Chatterjee"],"pdf_url":"https://arxiv.org/pdf/2507.02976v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18523v1","updated":"2025-07-24T15:49:06Z","published":"2025-07-24T15:49:06Z","title":"The Moral Gap of Large Language Models","summary":"  Moral foundation detection is crucial for analyzing social discourse and\ndeveloping ethically-aligned AI systems. While large language models excel\nacross diverse tasks, their performance on specialized moral reasoning remains\nunclear.\n  This study provides the first comprehensive comparison between\nstate-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit\ndatasets using ROC, PR, and DET curve analysis.\n  Results reveal substantial performance gaps, with LLMs exhibiting high false\nnegative rates and systematic under-detection of moral content despite prompt\nengineering efforts. These findings demonstrate that task-specific fine-tuning\nremains superior to prompting for moral reasoning applications.\n","authors":["Maciej Skorski","Alina Landowska"],"pdf_url":"https://arxiv.org/pdf/2507.18523v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2309.03791v3","updated":"2025-07-24T15:47:29Z","published":"2023-09-07T15:41:45Z","title":"Optimal Transport Regularized Divergences: Application to Adversarial\n  Robustness","summary":"  We introduce a new class of optimal-transport-regularized divergences, $D^c$,\nconstructed via an infimal convolution between an information divergence, $D$,\nand an optimal-transport (OT) cost, $C$, and study their use in\ndistributionally robust optimization (DRO). In particular, we propose the\n$ARMOR_D$ methods as novel approaches to enhancing the adversarial robustness\nof deep learning models. These DRO-based methods are defined by minimizing the\nmaximum expected loss over a $D^c$-neighborhood of the empirical distribution\nof the training data. Viewed as a tool for constructing adversarial samples,\nour method allows samples to be both transported, according to the OT cost, and\nre-weighted, according to the information divergence; the addition of a\nprincipled and dynamical adversarial re-weighting on top of adversarial sample\ntransport is a key innovation of $ARMOR_D$. $ARMOR_D$ can be viewed as a\ngeneralization of the best-performing loss functions and OT costs in the\nadversarial training literature; we demonstrate this flexibility by using\n$ARMOR_D$ to augment the UDR, TRADES, and MART methods and obtain improved\nperformance on CIFAR-10 and CIFAR-100 image recognition. Specifically,\naugmenting with $ARMOR_D$ leads to 1.9\\% and 2.1\\% improvement against\nAutoAttack, a powerful ensemble of adversarial attacks, on CIFAR-10 and\nCIFAR-100 respectively. To foster reproducibility, we made the code accessible\nat https://github.com/star-ailab/ARMOR.\n","authors":["Jeremiah Birrell","Reza Ebrahimi"],"pdf_url":"https://arxiv.org/pdf/2309.03791v3.pdf","comment":"34 pages, 2 figures"},{"id":"http://arxiv.org/abs/2507.14679v2","updated":"2025-07-24T15:46:28Z","published":"2025-07-19T16:09:48Z","title":"GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character\n  Similarity Networks","summary":"  The exponential growth of spam text on the Internet necessitates robust\ndetection mechanisms to mitigate risks such as information leakage and social\ninstability. This work addresses two principal challenges: adversarial\nstrategies employed by spammers and the scarcity of labeled data. We propose a\nnovel spam-text detection framework GCC-Spam, which integrates three core\ninnovations. First, a character similarity network captures orthographic and\nphonetic features to counter character-obfuscation attacks and furthermore\nproduces sentence embeddings for downstream classification. Second, contrastive\nlearning enhances discriminability by optimizing the latent-space distance\nbetween spam and normal texts. Third, a Generative Adversarial Network (GAN)\ngenerates realistic pseudo-spam samples to alleviate data scarcity while\nimproving model robustness and classification accuracy. Extensive experiments\non real-world datasets demonstrate that our model outperforms baseline\napproaches, achieving higher detection rates with significantly fewer labeled\nexamples.\n","authors":["Zhijie Wang","Zixin Xu","Zhiyuan Pan"],"pdf_url":"https://arxiv.org/pdf/2507.14679v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20144v3","updated":"2025-07-24T15:45:59Z","published":"2025-02-27T14:35:47Z","title":"Robust sensitivity control in digital pathology via tile score\n  distribution matching","summary":"  Deploying digital pathology models across medical centers is challenging due\nto distribution shifts. Recent advances in domain generalization improve model\ntransferability in terms of aggregated performance measured by the Area Under\nCurve (AUC). However, clinical regulations often require to control the\ntransferability of other metrics, such as prescribed sensitivity levels. We\nintroduce a novel approach to control the sensitivity of whole slide image\n(WSI) classification models, based on optimal transport and Multiple Instance\nLearning (MIL). Validated across multiple cohorts and tasks, our method enables\nrobust sensitivity control with only a handful of calibration samples,\nproviding a practical solution for reliable deployment of computational\npathology systems.\n","authors":["Arthur Pignet","John Klein","Genevieve Robin","Antoine Olivier"],"pdf_url":"https://arxiv.org/pdf/2502.20144v3.pdf","comment":"Camera ready version. Accepted at MICCAI 2025"},{"id":"http://arxiv.org/abs/2507.18521v1","updated":"2025-07-24T15:45:26Z","published":"2025-07-24T15:45:26Z","title":"GLANCE: Graph Logic Attention Network with Cluster Enhancement for\n  Heterophilous Graph Representation Learning","summary":"  Graph Neural Networks (GNNs) have demonstrated significant success in\nlearning from graph-structured data but often struggle on heterophilous graphs,\nwhere connected nodes differ in features or class labels. This limitation\narises from indiscriminate neighbor aggregation and insufficient incorporation\nof higher-order structural patterns. To address these challenges, we propose\nGLANCE (Graph Logic Attention Network with Cluster Enhancement), a novel\nframework that integrates logic-guided reasoning, dynamic graph refinement, and\nadaptive clustering to enhance graph representation learning. GLANCE combines a\nlogic layer for interpretable and structured embeddings, multi-head\nattention-based edge pruning for denoising graph structures, and clustering\nmechanisms for capturing global patterns. Experimental results in benchmark\ndatasets, including Cornell, Texas, and Wisconsin, demonstrate that GLANCE\nachieves competitive performance, offering robust and interpretable solutions\nfor heterophilous graph scenarios. The proposed framework is lightweight,\nadaptable, and uniquely suited to the challenges of heterophilous graphs.\n","authors":["Zhongtian Sun","Anoushka Harit","Alexandra Cristea","Christl A. Donnelly","Pietro Liò"],"pdf_url":"https://arxiv.org/pdf/2507.18521v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18520v1","updated":"2025-07-24T15:45:23Z","published":"2025-07-24T15:45:23Z","title":"Euclidean Distance Deflation Under High-Dimensional Heteroskedastic\n  Noise","summary":"  Pairwise Euclidean distance calculation is a fundamental step in many machine\nlearning and data analysis algorithms. In real-world applications, however,\nthese distances are frequently distorted by heteroskedastic\nnoise$\\unicode{x2014}$a prevalent form of inhomogeneous corruption\ncharacterized by variable noise magnitudes across data observations. Such noise\ninflates the computed distances in a nontrivial way, leading to\nmisrepresentations of the underlying data geometry. In this work, we address\nthe tasks of estimating the noise magnitudes per observation and correcting the\npairwise Euclidean distances under heteroskedastic noise. Perhaps surprisingly,\nwe show that in general high-dimensional settings and without assuming prior\nknowledge on the clean data structure or noise distribution, both tasks can be\nperformed reliably, even when the noise levels vary considerably. Specifically,\nwe develop a principled, hyperparameter-free approach that jointly estimates\nthe noise magnitudes and corrects the distances. We provide theoretical\nguarantees for our approach, establishing probabilistic bounds on the\nestimation errors of both noise magnitudes and distances. These bounds,\nmeasured in the normalized $\\ell_1$ norm, converge to zero at polynomial rates\nas both feature dimension and dataset size increase. Experiments on synthetic\ndatasets demonstrate that our method accurately estimates distances in\nchallenging regimes, significantly improving the robustness of subsequent\ndistance-based computations. Notably, when applied to single-cell RNA\nsequencing data, our method yields noise magnitude estimates consistent with an\nestablished prototypical model, enabling accurate nearest neighbor\nidentification that is fundamental to many downstream analyses.\n","authors":["Keyi Li","Yuval Kluger","Boris Landa"],"pdf_url":"https://arxiv.org/pdf/2507.18520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18519v1","updated":"2025-07-24T15:42:22Z","published":"2025-07-24T15:42:22Z","title":"Revisiting Bisimulation Metric for Robust Representations in\n  Reinforcement Learning","summary":"  Bisimulation metric has long been regarded as an effective control-related\nrepresentation learning technique in various reinforcement learning tasks.\nHowever, in this paper, we identify two main issues with the conventional\nbisimulation metric: 1) an inability to represent certain distinctive\nscenarios, and 2) a reliance on predefined weights for differences in rewards\nand subsequent states during recursive updates. We find that the first issue\narises from an imprecise definition of the reward gap, whereas the second issue\nstems from overlooking the varying importance of reward difference and\nnext-state distinctions across different training stages and task settings. To\naddress these issues, by introducing a measure for state-action pairs, we\npropose a revised bisimulation metric that features a more precise definition\nof reward gap and novel update operators with adaptive coefficient. We also\noffer theoretical guarantees of convergence for our proposed metric and its\nimproved representation distinctiveness. In addition to our rigorous\ntheoretical analysis, we conduct extensive experiments on two representative\nbenchmarks, DeepMind Control and Meta-World, demonstrating the effectiveness of\nour approach.\n","authors":["Leiji Zhang","Zeyu Wang","Xin Li","Yao-Hui Li"],"pdf_url":"https://arxiv.org/pdf/2507.18519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20292v6","updated":"2025-07-24T15:38:22Z","published":"2025-02-27T17:17:43Z","title":"Visual Adaptive Prompting for Compositional Zero-Shot Learning","summary":"  Vision-Language Models (VLMs) have demonstrated impressive multimodal\ncapabilities in learning joint representations of visual and textual data,\nmaking them powerful tools for tasks such as Compositional Zero-Shot Learning\n(CZSL). CZSL requires models to generalize to novel combinations of visual\nprimitives--such as attributes and objects--that were not explicitly\nencountered during training. Recent works in prompting for CZSL have focused on\nmodifying inputs for the text encoder, often using static prompts that do not\nchange across varying visual contexts. However, these approaches struggle to\nfully capture varying visual contexts, as they focus on text adaptation rather\nthan leveraging visual features for compositional reasoning. To address this,\nwe propose a Visual Adaptive Prompting System (VAPS) that leverages a learnable\nvisual prompt repository and similarity-based retrieval mechanism within the\nframework of VLMs to bridge the gap between semantic and visual features. Our\nmethod introduces a dynamic visual prompt repository mechanism that selects the\nmost relevant attribute and object prompts based on the visual features of the\nimage. Our proposed system includes a visual prompt adapter that encourages the\nmodel to learn a more generalizable embedding space. Experiments on three CZSL\nbenchmarks, across both closed and open-world scenarios, demonstrate\nstate-of-the-art results.\n","authors":["Kyle Stein","Arash Mahyari","Guillermo Francia","Eman El-Sheikh"],"pdf_url":"https://arxiv.org/pdf/2502.20292v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10084v2","updated":"2025-07-24T15:37:18Z","published":"2025-07-14T09:11:33Z","title":"A Transfer Learning-Based Method for Water Body Segmentation in Remote\n  Sensing Imagery: A Case Study of the Zhada Tulin Area","summary":"  The Tibetan Plateau, known as the Asian Water Tower, faces significant water\nsecurity challenges due to its high sensitivity to climate change. Advancing\nEarth observation for sustainable water monitoring is thus essential for\nbuilding climate resilience in this region. This study proposes a two-stage\ntransfer learning strategy using the SegFormer model to overcome domain shift\nand data scarcit--key barriers in developing robust AI for climate-sensitive\napplications. After pre-training on a diverse source domain, our model was\nfine-tuned for the arid Zhada Tulin area. Experimental results show a\nsubstantial performance boost: the Intersection over Union (IoU) for water body\nsegmentation surged from 25.50% (direct transfer) to 64.84%. This AI-driven\naccuracy is crucial for disaster risk reduction, particularly in monitoring\nflash flood-prone systems. More importantly, the high-precision map reveals a\nhighly concentrated spatial distribution of water, with over 80% of the water\narea confined to less than 20% of the river channel length. This quantitative\nfinding provides crucial evidence for understanding hydrological processes and\ndesigning targeted water management and climate adaptation strategies. Our work\nthus demonstrates an effective technical solution for monitoring arid plateau\nregions and contributes to advancing AI-powered Earth observation for disaster\npreparedness in critical transboundary river headwaters.\n","authors":["Haonan Chen","Xin Tong"],"pdf_url":"https://arxiv.org/pdf/2507.10084v2.pdf","comment":"13 pages, 6 figures, 2 tables"},{"id":"http://arxiv.org/abs/2407.17226v6","updated":"2025-07-24T15:32:35Z","published":"2024-07-24T12:26:21Z","title":"Sublinear Regret for a Class of Continuous-Time Linear-Quadratic\n  Reinforcement Learning Problems","summary":"  We study reinforcement learning (RL) for a class of continuous-time\nlinear-quadratic (LQ) control problems for diffusions, where states are\nscalar-valued and running control rewards are absent but volatilities of the\nstate processes depend on both state and control variables. We apply a\nmodel-free approach that relies neither on knowledge of model parameters nor on\ntheir estimations, and devise an RL algorithm to learn the optimal policy\nparameter directly. Our main contributions include the introduction of an\nexploration schedule and a regret analysis of the proposed algorithm. We\nprovide the convergence rate of the policy parameter to the optimal one, and\nprove that the algorithm achieves a regret bound of $O(N^{\\frac{3}{4}})$ up to\na logarithmic factor, where $N$ is the number of learning episodes. We conduct\na simulation study to validate the theoretical results and demonstrate the\neffectiveness and reliability of the proposed algorithm. We also perform\nnumerical comparisons between our method and those of the recent model-based\nstochastic LQ RL studies adapted to the state- and control-dependent volatility\nsetting, demonstrating a better performance of the former in terms of regret\nbounds.\n","authors":["Yilie Huang","Yanwei Jia","Xun Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.17226v6.pdf","comment":"42 pages, 4 figures. Accepted for publication in SIAM Journal on\n  Control and Optimization (2025)"},{"id":"http://arxiv.org/abs/2506.22495v2","updated":"2025-07-24T15:31:13Z","published":"2025-06-25T03:25:49Z","title":"Masked Autoencoders that Feel the Heart: Unveiling Simplicity Bias for\n  ECG Analyses","summary":"  The diagnostic value of electrocardiogram (ECG) lies in its dynamic\ncharacteristics, ranging from rhythm fluctuations to subtle waveform\ndeformations that evolve across time and frequency domains. However, supervised\nECG models tend to overfit dominant and repetitive patterns, overlooking\nfine-grained but clinically critical cues, a phenomenon known as Simplicity\nBias (SB), where models favor easily learnable signals over subtle but\ninformative ones. In this work, we first empirically demonstrate the presence\nof SB in ECG analyses and its negative impact on diagnostic performance, while\nsimultaneously discovering that self-supervised learning (SSL) can alleviate\nit, providing a promising direction for tackling the bias. Following the SSL\nparadigm, we propose a novel method comprising two key components: 1)\nTemporal-Frequency aware Filters to capture temporal-frequency features\nreflecting the dynamic characteristics of ECG signals, and 2) building on this,\nMulti-Grained Prototype Reconstruction for coarse and fine representation\nlearning across dual domains, further mitigating SB. To advance SSL in ECG\nanalyses, we curate a large-scale multi-site ECG dataset with 1.53 million\nrecordings from over 300 clinical centers. Experiments on three downstream\ntasks across six ECG datasets demonstrate that our method effectively reduces\nSB and achieves state-of-the-art performance. Code and dataset will be released\npublicly.\n","authors":["He-Yang Xu","Hongxiang Gao","Yuwen Li","Xiu-Shen Wei","Chengyu Liu"],"pdf_url":"https://arxiv.org/pdf/2506.22495v2.pdf","comment":"there are factual errors"},{"id":"http://arxiv.org/abs/2506.19780v5","updated":"2025-07-24T15:23:54Z","published":"2025-06-24T16:47:17Z","title":"Multi-Preference Lambda-weighted Listwise DPO for Small-Scale Model\n  Alignment","summary":"  Large language models (LLMs) demonstrate strong generalization across a wide\nrange of language tasks, but often generate outputs that misalign with human\npreferences. Reinforcement Learning from Human Feedback (RLHF) addresses this\nby optimizing models toward human preferences using a learned reward function\nand reinforcement learning, yielding improved alignment but suffering from high\ncomputational cost and instability. Direct Preference Optimization (DPO)\nsimplifies the process by treating alignment as a classification task over\nbinary preference pairs, reducing training overhead while achieving competitive\nperformance. However, it assumes fixed, single-dimensional preferences and only\nsupports pairwise supervision.\n  To address these limitations, we propose Multi-Preference Lambda-weighted\nListwise DPO, which allows the model to learn from more detailed human feedback\nand flexibly balance multiple goals such as helpfulness, honesty, and fluency.\nOur method models full-ranked preference distributions rather than binary\ncomparisons, enabling more informative learning signals. The lambda vector\ncontrols the relative importance of different alignment goals, allowing the\nmodel to generalize across diverse human objectives. During inference, lambda\ncan be adjusted without retraining, providing controllable alignment behavior\nfor downstream use. We also introduce a learned scheduler that dynamically\nsamples performant lambda configurations to improve robustness.\n  Notably, our method requires only 20GB of GPU memory for training, making it\nsuitable for compute-constrained settings such as academic labs, educational\ntools, or on-device assistants. Experiments on 1B-2B scale models show that our\nmethod consistently outperforms standard DPO on alignment benchmarks while\nenabling efficient, controllable, and fine-grained adaptation suitable for\nreal-world deployment.\n","authors":["Yuhui Sun","Xiyao Wang","Zixi Li","Zhenlong Yuan","Jinman Zhao"],"pdf_url":"https://arxiv.org/pdf/2506.19780v5.pdf","comment":"12 pages, 12 figures, appendix included. To appear in Proceedings of\n  AAAI 2026. Code:\n  https://github.com/yuhui15/Multi-Preference-Lambda-weighted-DPO"},{"id":"http://arxiv.org/abs/2402.12118v2","updated":"2025-07-24T15:23:53Z","published":"2024-02-19T13:13:16Z","title":"DualXDA: Towards Sparse, Efficient and Explainable Data Attribution in\n  Large AI Models","summary":"  Deep learning models achieve remarkable performance, yet their\ndecision-making processes often remain opaque. In response, the field of\neXplainable Artificial Intelligence (XAI) has grown significantly over the last\ndecade, primarily focusing on feature attribution methods. Complementing this\nperspective, Data Attribution (DA) has emerged as a promising paradigm that\nshifts the focus from features to data provenance. However, existing DA\napproaches suffer from prohibitively high computational costs and memory\ndemands. Additionally, current attribution methods exhibit low sparsity,\nhindering the discovery of decisive patterns in the data. We introduce DualXDA,\na framework for sparse, efficient and explainable DA, comprised of two\ninterlinked approaches for Dual Data Attribution (DualDA) and eXplainable Data\nAttribution (XDA): With DualDA, we propose efficient and effective DA,\nleveraging Support Vector Machine theory to provide fast and naturally sparse\ndata attributions for AI predictions. We demonstrate that DualDA achieves high\nattribution quality, excels at solving a series of evaluated downstream tasks,\nwhile at the same time improving explanation time by a factor of up to\n4,100,000$\\times$ compared to the original Influence Functions method, and up\nto 11,000$\\times$ compared to the method's most efficient approximation from\nliterature. We further introduce XDA, a method for enhancing Data Attribution\nwith capabilities from feature attribution methods to explain why training\nsamples are relevant for the prediction of a test sample in terms of impactful\nfeatures. Taken together, our contributions in DualXDA ultimately point towards\na future of eXplainable AI applied at unprecedented scale, enabling\ntransparent, efficient and novel analysis of even the largest neural\narchitectures fostering a new generation of accountable AI systems. Code at\nhttps://github.com/gumityolcu/DualXDA.\n","authors":["Galip Ümit Yolcu","Moritz Weckbecker","Thomas Wiegand","Wojciech Samek","Sebastian Lapuschkin"],"pdf_url":"https://arxiv.org/pdf/2402.12118v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18504v1","updated":"2025-07-24T15:22:27Z","published":"2025-07-24T15:22:27Z","title":"Not All Features Deserve Attention: Graph-Guided Dependency Learning for\n  Tabular Data Generation with Language Models","summary":"  Large Language Models (LLMs) have shown strong potential for tabular data\ngeneration by modeling textualized feature-value pairs. However, tabular data\ninherently exhibits sparse feature-level dependencies, where many feature\ninteractions are structurally insignificant. This creates a fundamental\nmismatch as LLMs' self-attention mechanism inevitably distributes focus across\nall pairs, diluting attention on critical relationships, particularly in\ndatasets with complex dependencies or semantically ambiguous features. To\naddress this limitation, we propose GraDe (Graph-Guided Dependency Learning), a\nnovel method that explicitly integrates sparse dependency graphs into LLMs'\nattention mechanism. GraDe employs a lightweight dynamic graph learning module\nguided by externally extracted functional dependencies, prioritizing key\nfeature interactions while suppressing irrelevant ones. Our experiments across\ndiverse real-world datasets demonstrate that GraDe outperforms existing\nLLM-based approaches by up to 12% on complex datasets while achieving\ncompetitive results with state-of-the-art approaches in synthetic data quality.\nOur method is minimally intrusive yet effective, offering a practical solution\nfor structure-aware tabular data modeling with LLMs.\n","authors":["Zheyu Zhang","Shuo Yang","Bardh Prenkaj","Gjergji Kasneci"],"pdf_url":"https://arxiv.org/pdf/2507.18504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18915v2","updated":"2025-07-24T15:19:06Z","published":"2024-03-27T18:08:14Z","title":"PLOT-TAL: Prompt Learning with Optimal Transport for Few-Shot Temporal\n  Action Localization","summary":"  Few-shot temporal action localization (TAL) methods that adapt large models\nvia single-prompt tuning often fail to produce precise temporal boundaries.\nThis stems from the model learning a non-discriminative mean representation of\nan action from sparse data, which compromises generalization. We address this\nby proposing a new paradigm based on multi-prompt ensembles, where a set of\ndiverse, learnable prompts for each action is encouraged to specialize on\ncompositional sub-events. To enforce this specialization, we introduce\nPLOT-TAL, a framework that leverages Optimal Transport (OT) to find a globally\noptimal alignment between the prompt ensemble and the video's temporal\nfeatures. Our method establishes a new state-of-the-art on the challenging\nfew-shot benchmarks of THUMOS'14 and EPIC-Kitchens, without requiring complex\nmeta-learning. The significant performance gains, particularly at high IoU\nthresholds, validate our hypothesis and demonstrate the superiority of learning\ndistributed, compositional representations for precise temporal localization.\n","authors":["Edward Fish","Andrew Gilbert"],"pdf_url":"https://arxiv.org/pdf/2403.18915v2.pdf","comment":"Accepted to ICCVWS"},{"id":"http://arxiv.org/abs/2507.17311v2","updated":"2025-07-24T15:12:15Z","published":"2025-07-23T08:29:25Z","title":"EarthLink: A Self-Evolving AI Agent for Climate Science","summary":"  Modern Earth science is at an inflection point. The vast, fragmented, and\ncomplex nature of Earth system data, coupled with increasingly sophisticated\nanalytical demands, creates a significant bottleneck for rapid scientific\ndiscovery. Here we introduce EarthLink, the first AI agent designed as an\ninteractive copilot for Earth scientists. It automates the end-to-end research\nworkflow, from planning and code generation to multi-scenario analysis. Unlike\nstatic diagnostic tools, EarthLink can learn from user interaction,\ncontinuously refining its capabilities through a dynamic feedback loop. We\nvalidated its performance on a number of core scientific tasks of climate\nchange, ranging from model-observation comparisons to the diagnosis of complex\nphenomena. In a multi-expert evaluation, EarthLink produced scientifically\nsound analyses and demonstrated an analytical competency that was rated as\ncomparable to specific aspects of a human junior researcher's workflow.\nAdditionally, its transparent, auditable workflows and natural language\ninterface empower scientists to shift from laborious manual execution to\nstrategic oversight and hypothesis generation. EarthLink marks a pivotal step\ntowards an efficient, trustworthy, and collaborative paradigm for Earth system\nresearch in an era of accelerating global change. The system is accessible at\nour website https://earthlink.intern-ai.org.cn.\n","authors":["Zijie Guo","Jiong Wang","Xiaoyu Yue","Wangxu Wei","Zhe Jiang","Wanghan Xu","Ben Fei","Wenlong Zhang","Xinyu Gu","Lijing Cheng","Jing-Jia Luo","Chao Li","Yaqiang Wang","Tao Chen","Wanli Ouyang","Fenghua Ling","Lei Bai"],"pdf_url":"https://arxiv.org/pdf/2507.17311v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17813v2","updated":"2025-07-24T15:10:45Z","published":"2024-06-24T23:41:46Z","title":"Unsupervised Concept Drift Detection from Deep Learning Representations\n  in Real-time","summary":"  Concept drift is the phenomenon in which the underlying data distributions\nand statistical properties of a target domain change over time, leading to a\ndegradation in model performance. Consequently, production models require\ncontinuous drift detection monitoring. Most drift detection methods to date are\nsupervised, relying on ground-truth labels. However, they are inapplicable in\nmany real-world scenarios, as true labels are often unavailable. Although\nrecent efforts have proposed unsupervised drift detectors, many lack the\naccuracy required for reliable detection or are too computationally intensive\nfor real-time use in high-dimensional, large-scale production environments.\nMoreover, they often fail to characterize or explain drift effectively.\n  To address these limitations, we propose \\textsc{DriftLens}, an unsupervised\nframework for real-time concept drift detection and characterization. Designed\nfor deep learning classifiers handling unstructured data, \\textsc{DriftLens}\nleverages distribution distances in deep learning representations to enable\nefficient and accurate detection. Additionally, it characterizes drift by\nanalyzing and explaining its impact on each label. Our evaluation across\nclassifiers and data-types demonstrates that \\textsc{DriftLens} (i) outperforms\nprevious methods in detecting drift in 15/17 use cases; (ii) runs at least 5\ntimes faster; (iii) produces drift curves that align closely with actual drift\n(correlation $\\geq\\!0.85$); (iv) effectively identifies representative drift\nsamples as explanations.\n","authors":["Salvatore Greco","Bartolomeo Vacchetti","Daniele Apiletti","Tania Cerquitelli"],"pdf_url":"https://arxiv.org/pdf/2406.17813v2.pdf","comment":"Accepted at IEEE Transactions on Knowledge and Data Engineering\n  (TKDE)"},{"id":"http://arxiv.org/abs/2507.16761v2","updated":"2025-07-24T14:58:44Z","published":"2025-07-22T16:56:02Z","title":"Faithful, Interpretable Chest X-ray Diagnosis with Anti-Aliased B-cos\n  Networks","summary":"  Faithfulness and interpretability are essential for deploying deep neural\nnetworks (DNNs) in safety-critical domains such as medical imaging. B-cos\nnetworks offer a promising solution by replacing standard linear layers with a\nweight-input alignment mechanism, producing inherently interpretable,\nclass-specific explanations without post-hoc methods. While maintaining\ndiagnostic performance competitive with state-of-the-art DNNs, standard B-cos\nmodels suffer from severe aliasing artifacts in their explanation maps, making\nthem unsuitable for clinical use where clarity is essential. In this work, we\naddress these limitations by introducing anti-aliasing strategies using\nFLCPooling (FLC) and BlurPool (BP) to significantly improve explanation\nquality. Our experiments on chest X-ray datasets demonstrate that the modified\n$\\text{B-cos}_\\text{FLC}$ and $\\text{B-cos}_\\text{BP}$ preserve strong\npredictive performance while providing faithful and artifact-free explanations\nsuitable for clinical application in multi-class and multi-label settings. Code\navailable at: GitHub repository (url:\nhttps://github.com/mkleinma/B-cos-medical-paper).\n","authors":["Marcel Kleinmann","Shashank Agnihotri","Margret Keuper"],"pdf_url":"https://arxiv.org/pdf/2507.16761v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18464v1","updated":"2025-07-24T14:39:20Z","published":"2025-07-24T14:39:20Z","title":"DriftMoE: A Mixture of Experts Approach to Handle Concept Drifts","summary":"  Learning from non-stationary data streams subject to concept drift requires\nmodels that can adapt on-the-fly while remaining resource-efficient. Existing\nadaptive ensemble methods often rely on coarse-grained adaptation mechanisms or\nsimple voting schemes that fail to optimally leverage specialized knowledge.\nThis paper introduces DriftMoE, an online Mixture-of-Experts (MoE) architecture\nthat addresses these limitations through a novel co-training framework.\nDriftMoE features a compact neural router that is co-trained alongside a pool\nof incremental Hoeffding tree experts. The key innovation lies in a symbiotic\nlearning loop that enables expert specialization: the router selects the most\nsuitable expert for prediction, the relevant experts update incrementally with\nthe true label, and the router refines its parameters using a multi-hot\ncorrectness mask that reinforces every accurate expert. This feedback loop\nprovides the router with a clear training signal while accelerating expert\nspecialization. We evaluate DriftMoE's performance across nine state-of-the-art\ndata stream learning benchmarks spanning abrupt, gradual, and real-world drifts\ntesting two distinct configurations: one where experts specialize on data\nregimes (multi-class variant), and another where they focus on single-class\nspecialization (task-based variant). Our results demonstrate that DriftMoE\nachieves competitive results with state-of-the-art stream learning adaptive\nensembles, offering a principled and efficient approach to concept drift\nadaptation. All code, data pipelines, and reproducibility scripts are available\nin our public GitHub repository: https://github.com/miguel-ceadar/drift-moe.\n","authors":["Miguel Aspis","Sebastián A. Cajas Ordónez","Andrés L. Suárez-Cetrulo","Ricardo Simón Carbajo"],"pdf_url":"https://arxiv.org/pdf/2507.18464v1.pdf","comment":"Accepted at the SYNDAiTE@ECMLPKDD 2025 workshop"},{"id":"http://arxiv.org/abs/2507.18448v1","updated":"2025-07-24T14:33:13Z","published":"2025-07-24T14:33:13Z","title":"Restoring Rhythm: Punctuation Restoration Using Transformer Models for\n  Bangla, a Low-Resource Language","summary":"  Punctuation restoration enhances the readability of text and is critical for\npost-processing tasks in Automatic Speech Recognition (ASR), especially for\nlow-resource languages like Bangla. In this study, we explore the application\nof transformer-based models, specifically XLM-RoBERTa-large, to automatically\nrestore punctuation in unpunctuated Bangla text. We focus on predicting four\npunctuation marks: period, comma, question mark, and exclamation mark across\ndiverse text domains. To address the scarcity of annotated resources, we\nconstructed a large, varied training corpus and applied data augmentation\ntechniques. Our best-performing model, trained with an augmentation factor of\nalpha = 0.20%, achieves an accuracy of 97.1% on the News test set, 91.2% on the\nReference set, and 90.2% on the ASR set.\n  Results show strong generalization to reference and ASR transcripts,\ndemonstrating the model's effectiveness in real-world, noisy scenarios. This\nwork establishes a strong baseline for Bangla punctuation restoration and\ncontributes publicly available datasets and code to support future research in\nlow-resource NLP.\n","authors":["Md Obyedullahil Mamun","Md Adyelullahil Mamun","Arif Ahmad","Md. Imran Hossain Emu"],"pdf_url":"https://arxiv.org/pdf/2507.18448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.20268v2","updated":"2025-07-24T14:21:12Z","published":"2025-05-26T17:44:08Z","title":"Outcome-Based Online Reinforcement Learning: Algorithms and Fundamental\n  Limits","summary":"  Reinforcement learning with outcome-based feedback faces a fundamental\nchallenge: when rewards are only observed at trajectory endpoints, how do we\nassign credit to the right actions? This paper provides the first comprehensive\nanalysis of this problem in online RL with general function approximation. We\ndevelop a provably sample-efficient algorithm achieving $\\widetilde{O}({C_{\\rm\ncov} H^3}/{\\epsilon^2})$ sample complexity, where $C_{\\rm cov}$ is the\ncoverability coefficient of the underlying MDP. By leveraging general function\napproximation, our approach works effectively in large or infinite state spaces\nwhere tabular methods fail, requiring only that value functions and reward\nfunctions can be represented by appropriate function classes. Our results also\ncharacterize when outcome-based feedback is statistically separated from\nper-step rewards, revealing an unavoidable exponential separation for certain\nMDPs. For deterministic MDPs, we show how to eliminate the completeness\nassumption, dramatically simplifying the algorithm. We further extend our\napproach to preference-based feedback settings, proving that equivalent\nstatistical efficiency can be achieved even under more limited information.\nTogether, these results constitute a theoretical foundation for understanding\nthe statistical properties of outcome-based reinforcement learning.\n","authors":["Fan Chen","Zeyu Jia","Alexander Rakhlin","Tengyang Xie"],"pdf_url":"https://arxiv.org/pdf/2505.20268v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12358v4","updated":"2025-07-24T14:14:52Z","published":"2025-03-16T04:53:38Z","title":"IPCGRL: Language-Instructed Reinforcement Learning for Procedural Level\n  Generation","summary":"  Recent research has highlighted the significance of natural language in\nenhancing the controllability of generative models. While various efforts have\nbeen made to leverage natural language for content generation, research on deep\nreinforcement learning (DRL) agents utilizing text-based instructions for\nprocedural content generation remains limited. In this paper, we propose\nIPCGRL, an instruction-based procedural content generation method via\nreinforcement learning, which incorporates a sentence embedding model. IPCGRL\nfine-tunes task-specific embedding representations to effectively compress\ngame-level conditions. We evaluate IPCGRL in a two-dimensional level generation\ntask and compare its performance with a general-purpose embedding method. The\nresults indicate that IPCGRL achieves up to a 21.4% improvement in\ncontrollability and a 17.2% improvement in generalizability for unseen\ninstructions. Furthermore, the proposed method extends the modality of\nconditional input, enabling a more flexible and expressive interaction\nframework for procedural content generation.\n","authors":["In-Chang Baek","Sung-Hyun Kim","Seo-Young Lee","Dong-Hyeon Kim","Kyung-Joong Kim"],"pdf_url":"https://arxiv.org/pdf/2503.12358v4.pdf","comment":"9 pages, 9 figures, 3 tables, accepted to Conference on Games 2025"},{"id":"http://arxiv.org/abs/2507.18429v1","updated":"2025-07-24T14:08:33Z","published":"2025-07-24T14:08:33Z","title":"NLML-HPE: Head Pose Estimation with Limited Data via Manifold Learning","summary":"  Head pose estimation (HPE) plays a critical role in various computer vision\napplications such as human-computer interaction and facial recognition. In this\npaper, we propose a novel deep learning approach for head pose estimation with\nlimited training data via non-linear manifold learning called NLML-HPE. This\nmethod is based on the combination of tensor decomposition (i.e., Tucker\ndecomposition) and feed forward neural networks. Unlike traditional\nclassification-based approaches, our method formulates head pose estimation as\na regression problem, mapping input landmarks into a continuous representation\nof pose angles. To this end, our method uses tensor decomposition to split each\nEuler angle (yaw, pitch, roll) to separate subspaces and models each dimension\nof the underlying manifold as a cosine curve. We address two key challenges: 1.\nAlmost all HPE datasets suffer from incorrect and inaccurate pose annotations.\nHence, we generated a precise and consistent 2D head pose dataset for our\ntraining set by rotating 3D head models for a fixed set of poses and rendering\nthe corresponding 2D images. 2. We achieved real-time performance with limited\ntraining data as our method accurately captures the nature of rotation of an\nobject from facial landmarks. Once the underlying manifold for rotation around\neach axis is learned, the model is very fast in predicting unseen data. Our\ntraining and testing code is available online along with our trained models:\nhttps: //github.com/MahdiGhafoorian/NLML_HPE.\n","authors":["Mahdi Ghafourian","Federico M. Sukno"],"pdf_url":"https://arxiv.org/pdf/2507.18429v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21676v2","updated":"2025-07-24T14:04:20Z","published":"2025-03-27T16:43:45Z","title":"How do language models learn facts? Dynamics, curricula and\n  hallucinations","summary":"  Large language models accumulate vast knowledge during pre-training, yet the\ndynamics governing this acquisition remain poorly understood. This work\ninvestigates the learning dynamics of language models on a synthetic factual\nrecall task, uncovering three key findings: First, language models learn in\nthree phases, exhibiting a performance plateau before acquiring precise factual\nknowledge. Mechanistically, this plateau coincides with the formation of\nattention-based circuits that support recall. Second, the training data\ndistribution significantly impacts learning dynamics, as imbalanced\ndistributions lead to shorter plateaus. Finally, hallucinations emerge\nsimultaneously with knowledge, and integrating new knowledge into the model\nthrough fine-tuning is challenging, as it quickly corrupts its existing\nparametric memories. Our results emphasize the importance of data distribution\nin knowledge acquisition and suggest novel data scheduling strategies to\naccelerate neural network training.\n","authors":["Nicolas Zucchet","Jörg Bornschein","Stephanie Chan","Andrew Lampinen","Razvan Pascanu","Soham De"],"pdf_url":"https://arxiv.org/pdf/2503.21676v2.pdf","comment":"Accepted at the 2nd Conference on Language Modeling (2025)"},{"id":"http://arxiv.org/abs/2507.18423v1","updated":"2025-07-24T14:00:18Z","published":"2025-07-24T14:00:18Z","title":"Multi-Model Ensemble and Reservoir Computing for River Discharge\n  Prediction in Ungauged Basins","summary":"  Despite the critical need for accurate flood prediction and water management,\nmany regions lack sufficient river discharge observations, limiting the skill\nof rainfall-runoff analyses. Although numerous physically based and machine\nlearning models exist, achieving high accuracy, interpretability, and\ncomputational efficiency under data-scarce conditions remains a major\nchallenge. We address this challenge with a novel method, HYdrological\nPrediction with multi-model Ensemble and Reservoir computing (HYPER) that\nleverages multi-model ensemble and reservoir computing (RC). Our approach first\napplies Bayesian model averaging (BMA) to 43 \"uncalibrated\" catchment-based\nconceptual hydrological models. An RC model is then trained via linear\nregression to correct errors in the BMA output, a non-iterative process that\nensures high computational efficiency. For ungauged basins, we infer the\nrequired BMA and RC weights by linking them to catchment attributes from gauged\nbasins, creating a generalizable framework. We evaluated HYPER using data from\n87 river basins in Japan. In a data-rich scenario, HYPER (median Kling-Gupta\nEfficiency, KGE, of 0.56) performed comparably to a benchmark LSTM (KGE 0.55)\nbut required only 5% of its computational time. In a data-scarce scenario (23%\nof basins gauged), HYPER maintained robust performance (KGE 0.55) and lower\nuncertainty, whereas the LSTM's performance degraded significantly (KGE -0.04).\nThese results reveal that individual conceptual hydrological models do not\nnecessarily need to be calibrated when an effectively large ensemble is\nassembled and combined with machine-learning-based bias correction. HYPER\nprovides a robust, efficient, and generalizable solution for discharge\nprediction, particularly in ungauged basins, making it applicable to a wide\nrange of regions.\n","authors":["Mizuki Funato","Yohei Sawada"],"pdf_url":"https://arxiv.org/pdf/2507.18423v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06735v2","updated":"2025-07-24T13:57:08Z","published":"2025-07-09T10:48:00Z","title":"Residual Prior-driven Frequency-aware Network for Image Fusion","summary":"  Image fusion aims to integrate complementary information across modalities to\ngenerate high-quality fused images, thereby enhancing the performance of\nhigh-level vision tasks. While global spatial modeling mechanisms show\npromising results, constructing long-range feature dependencies in the spatial\ndomain incurs substantial computational costs. Additionally, the absence of\nground-truth exacerbates the difficulty of capturing complementary features\neffectively. To tackle these challenges, we propose a Residual Prior-driven\nFrequency-aware Network, termed as RPFNet. Specifically, RPFNet employs a\ndual-branch feature extraction framework: the Residual Prior Module (RPM)\nextracts modality-specific difference information from residual maps, thereby\nproviding complementary priors for fusion; the Frequency Domain Fusion Module\n(FDFM) achieves efficient global feature modeling and integration through\nfrequency-domain convolution. Additionally, the Cross Promotion Module (CPM)\nenhances the synergistic perception of local details and global structures\nthrough bidirectional feature interaction. During training, we incorporate an\nauxiliary decoder and saliency structure loss to strengthen the model's\nsensitivity to modality-specific differences. Furthermore, a combination of\nadaptive weight-based frequency contrastive loss and SSIM loss effectively\nconstrains the solution space, facilitating the joint capture of local details\nand global features while ensuring the retention of complementary information.\nExtensive experiments validate the fusion performance of RPFNet, which\neffectively integrates discriminative features, enhances texture details and\nsalient objects, and can effectively facilitate the deployment of the\nhigh-level vision task.\n","authors":["Guan Zheng","Xue Wang","Wenhua Qian","Peng Liu","Runzhuo Ma"],"pdf_url":"https://arxiv.org/pdf/2507.06735v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18417v1","updated":"2025-07-24T13:57:05Z","published":"2025-07-24T13:57:05Z","title":"FinDPO: Financial Sentiment Analysis for Algorithmic Trading through\n  Preference Optimization of LLMs","summary":"  Opinions expressed in online finance-related textual data are having an\nincreasingly profound impact on trading decisions and market movements. This\ntrend highlights the vital role of sentiment analysis as a tool for quantifying\nthe nature and strength of such opinions. With the rapid development of\nGenerative AI (GenAI), supervised fine-tuned (SFT) large language models (LLMs)\nhave become the de facto standard for financial sentiment analysis. However,\nthe SFT paradigm can lead to memorization of the training data and often fails\nto generalize to unseen samples. This is a critical limitation in financial\ndomains, where models must adapt to previously unobserved events and the\nnuanced, domain-specific language of finance. To this end, we introduce FinDPO,\nthe first finance-specific LLM framework based on post-training human\npreference alignment via Direct Preference Optimization (DPO). The proposed\nFinDPO achieves state-of-the-art performance on standard sentiment\nclassification benchmarks, outperforming existing supervised fine-tuned models\nby 11% on the average. Uniquely, the FinDPO framework enables the integration\nof a fine-tuned causal LLM into realistic portfolio strategies through a novel\n'logit-to-score' conversion, which transforms discrete sentiment predictions\ninto continuous, rankable sentiment scores (probabilities). In this way,\nsimulations demonstrate that FinDPO is the first sentiment-based approach to\nmaintain substantial positive returns of 67% annually and strong risk-adjusted\nperformance, as indicated by a Sharpe ratio of 2.0, even under realistic\ntransaction costs of 5 basis points (bps).\n","authors":["Giorgos Iacovides","Wuyang Zhou","Danilo Mandic"],"pdf_url":"https://arxiv.org/pdf/2507.18417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18405v1","updated":"2025-07-24T13:45:48Z","published":"2025-07-24T13:45:48Z","title":"Iwin Transformer: Hierarchical Vision Transformer using Interleaved\n  Windows","summary":"  We introduce Iwin Transformer, a novel position-embedding-free hierarchical\nvision transformer, which can be fine-tuned directly from low to high\nresolution, through the collaboration of innovative interleaved window\nattention and depthwise separable convolution. This approach uses attention to\nconnect distant tokens and applies convolution to link neighboring tokens,\nenabling global information exchange within a single module, overcoming Swin\nTransformer's limitation of requiring two consecutive blocks to approximate\nglobal attention. Extensive experiments on visual benchmarks demonstrate that\nIwin Transformer exhibits strong competitiveness in tasks such as image\nclassification (87.4 top-1 accuracy on ImageNet-1K), semantic segmentation and\nvideo action recognition. We also validate the effectiveness of the core\ncomponent in Iwin as a standalone module that can seamlessly replace the\nself-attention module in class-conditional image generation. The concepts and\nmethods introduced by the Iwin Transformer have the potential to inspire future\nresearch, like Iwin 3D Attention in video generation. The code and models are\navailable at https://github.com/cominder/Iwin-Transformer.\n","authors":["Simin Huo","Ning Li"],"pdf_url":"https://arxiv.org/pdf/2507.18405v1.pdf","comment":"14 pages, 10 figures, Submitted to IEEE Transactions on Pattern\n  Analysis and Machine Intelligence"},{"id":"http://arxiv.org/abs/2507.18392v1","updated":"2025-07-24T13:15:21Z","published":"2025-07-24T13:15:21Z","title":"CLEAR: Error Analysis via LLM-as-a-Judge Made Easy","summary":"  The evaluation of Large Language Models (LLMs) increasingly relies on other\nLLMs acting as judges. However, current evaluation paradigms typically yield a\nsingle score or ranking, answering which model is better but not why. While\nessential for benchmarking, these top-level scores obscure the specific,\nactionable reasons behind a model's performance. To bridge this gap, we\nintroduce CLEAR, an interactive, open-source package for LLM-based error\nanalysis. CLEAR first generates per-instance textual feedback, then it creates\na set of system-level error issues, and quantifies the prevalence of each\nidentified issue. Our package also provides users with an interactive dashboard\nthat allows for a comprehensive error analysis through aggregate\nvisualizations, applies interactive filters to isolate specific issues or score\nranges, and drills down to the individual instances that exemplify a particular\nbehavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks,\nand showcase its utility through a user case study.\n","authors":["Asaf Yehudai","Lilach Eden","Yotam Perlitz","Roy Bar-Haim","Michal Shmueli-Scheuer"],"pdf_url":"https://arxiv.org/pdf/2507.18392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18376v1","updated":"2025-07-24T12:52:32Z","published":"2025-07-24T12:52:32Z","title":"A Comprehensive Review of Diffusion Models in Smart Agriculture:\n  Progress, Applications, and Challenges","summary":"  With the global population growing and arable land resources becoming\nincreasingly scarce,smart agriculture and precision agriculture have emerged as\nkey directions for the future ofagricultural development.Artificial\nintelligence (AI) technologies, particularly deep learning models, have found\nwidespread applications in areas such as crop monitoring and pest detection. As\nan emerging generative model, diffusion models have shown significant promise\nin tasks like agricultural image processing, data augmentation, and remote\nsensing. Compared to traditional generative adversarial networks (GANs),\ndiffusion models offer superior training stability and generation quality,\neffectively addressing challenges such as limited agricultural data and\nimbalanced image samples. This paper reviews the latest advancements in the\napplication of diffusion models in agriculture, focusing on their potential in\ncrop pest and disease detection, remote sensing image enhancement, crop growth\nprediction, and agricultural resource management. Experimental results\ndemonstrate that diffusion models significantly improve model accuracy and\nrobustness in data augmentation, image generation, and denoising, especially in\ncomplex environments. Despite challenges related to computational efficiency\nand generalization capabilities, diffusion models are expected to play an\nincreasingly important role in smart and precision agriculture as technology\nadvances, providing substantial support for the sustainable development of\nglobal agriculture.\n","authors":["Xing Hua","Haodong Chen","Qianqian Duan","Danfeng Hong","Ruijiao Li","Huiliang Shang","Linghua Jiang","Haima Yang","Dawei Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.18376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18372v1","updated":"2025-07-24T12:49:41Z","published":"2025-07-24T12:49:41Z","title":"On Reconstructing Training Data From Bayesian Posteriors and Trained\n  Models","summary":"  Publicly releasing the specification of a model with its trained parameters\nmeans an adversary can attempt to reconstruct information about the training\ndata via training data reconstruction attacks, a major vulnerability of modern\nmachine learning methods. This paper makes three primary contributions:\nestablishing a mathematical framework to express the problem, characterising\nthe features of the training data that are vulnerable via a maximum mean\ndiscrepancy equivalance and outlining a score matching framework for\nreconstructing data in both Bayesian and non-Bayesian models, the former is a\nfirst in the literature.\n","authors":["George Wynne"],"pdf_url":"https://arxiv.org/pdf/2507.18372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18366v1","updated":"2025-07-24T12:46:40Z","published":"2025-07-24T12:46:40Z","title":"Efficient Uncertainty in LLMs through Evidential Knowledge Distillation","summary":"  Accurate uncertainty quantification remains a key challenge for standard\nLLMs, prompting the adoption of Bayesian and ensemble-based methods. However,\nsuch methods typically necessitate computationally expensive sampling,\ninvolving multiple forward passes to effectively estimate predictive\nuncertainty.\n  In this paper, we introduce a novel approach enabling efficient and effective\nuncertainty estimation in LLMs without sacrificing performance. Specifically,\nwe distill uncertainty-aware teacher models - originally requiring multiple\nforward passes - into compact student models sharing the same architecture but\nfine-tuned using Low-Rank Adaptation (LoRA). We compare two distinct\ndistillation strategies: one in which the student employs traditional\nsoftmax-based outputs, and another in which the student leverages\nDirichlet-distributed outputs to explicitly model epistemic uncertainty via\nevidential learning.\n  Empirical evaluations on classification datasets demonstrate that such\nstudents can achieve comparable or superior predictive and uncertainty\nquantification performance relative to their teacher models, while critically\nrequiring only a single forward pass. To our knowledge, this is the first\ndemonstration that immediate and robust uncertainty quantification can be\nachieved in LLMs through evidential distillation.\n","authors":["Lakshmana Sri Harsha Nemani","P. K. Srijith","Tomasz Kuśmierczyk"],"pdf_url":"https://arxiv.org/pdf/2507.18366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02987v3","updated":"2025-07-24T12:44:31Z","published":"2025-07-01T11:14:45Z","title":"Leveraging the Structure of Medical Data for Improved Representation\n  Learning","summary":"  Building generalizable medical AI systems requires pretraining strategies\nthat are data-efficient and domain-aware. Unlike internet-scale corpora,\nclinical datasets such as MIMIC-CXR offer limited image counts and scarce\nannotations, but exhibit rich internal structure through multi-view imaging. We\npropose a self-supervised framework that leverages the inherent structure of\nmedical datasets. Specifically, we treat paired chest X-rays (i.e., frontal and\nlateral views) as natural positive pairs, learning to reconstruct each view\nfrom sparse patches while aligning their latent embeddings. Our method requires\nno textual supervision and produces informative representations. Evaluated on\nMIMIC-CXR, we show strong performance compared to supervised objectives and\nbaselines being trained without leveraging structure. This work provides a\nlightweight, modality-agnostic blueprint for domain-specific pretraining where\ndata is structured but scarce\n","authors":["Andrea Agostini","Sonia Laguna","Alain Ryser","Samuel Ruiperez-Campillo","Moritz Vandenhirtz","Nicolas Deperrois","Farhad Nooralahzadeh","Michael Krauthammer","Thomas M. Sutter","Julia E. Vogt"],"pdf_url":"https://arxiv.org/pdf/2507.02987v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.16680v2","updated":"2025-07-24T12:38:41Z","published":"2025-07-22T15:16:18Z","title":"Latent Space Alignment for AI-Native MIMO Semantic Communications","summary":"  Semantic communications focus on prioritizing the understanding of the\nmeaning behind transmitted data and ensuring the successful completion of tasks\nthat motivate the exchange of information. However, when devices rely on\ndifferent languages, logic, or internal representations, semantic mismatches\nmay occur, potentially hindering mutual understanding. This paper introduces a\nnovel approach to addressing latent space misalignment in semantic\ncommunications, exploiting multiple-input multiple-output (MIMO)\ncommunications. Specifically, our method learns a MIMO precoder/decoder pair\nthat jointly performs latent space compression and semantic channel\nequalization, mitigating both semantic mismatches and physical channel\nimpairments. We explore two solutions: (i) a linear model, optimized by solving\na biconvex optimization problem via the alternating direction method of\nmultipliers (ADMM); (ii) a neural network-based model, which learns semantic\nMIMO precoder/decoder under transmission power budget and complexity\nconstraints. Numerical results demonstrate the effectiveness of the proposed\napproach in a goal-oriented semantic communication scenario, illustrating the\nmain trade-offs between accuracy, communication burden, and complexity of the\nsolutions.\n","authors":["Mario Edoardo Pandolfo","Simone Fiorellino","Emilio Calvanese Strinati","Paolo Di Lorenzo"],"pdf_url":"https://arxiv.org/pdf/2507.16680v2.pdf","comment":"Proc. of IEEE IJCNN 2025"},{"id":"http://arxiv.org/abs/2507.18352v1","updated":"2025-07-24T12:25:12Z","published":"2025-07-24T12:25:12Z","title":"Tiny is not small enough: High-quality, low-resource facial animation\n  models through hybrid knowledge distillation","summary":"  The training of high-quality, robust machine learning models for\nspeech-driven 3D facial animation requires a large, diverse dataset of\nhigh-quality audio-animation pairs. To overcome the lack of such a dataset,\nrecent work has introduced large pre-trained speech encoders that are robust to\nvariations in the input audio and, therefore, enable the facial animation model\nto generalize across speakers, audio quality, and languages. However, the\nresulting facial animation models are prohibitively large and lend themselves\nonly to offline inference on a dedicated machine. In this work, we explore\non-device, real-time facial animation models in the context of game\ndevelopment. We overcome the lack of large datasets by using hybrid knowledge\ndistillation with pseudo-labeling. Given a large audio dataset, we employ a\nhigh-performing teacher model to train very small student models. In contrast\nto the pre-trained speech encoders, our student models only consist of\nconvolutional and fully-connected layers, removing the need for attention\ncontext or recurrent updates. In our experiments, we demonstrate that we can\nreduce the memory footprint to up to 3.4 MB and required future audio context\nto up to 81 ms while maintaining high-quality animations. This paves the way\nfor on-device inference, an important step towards realistic, model-driven\ndigital characters.\n","authors":["Zhen Han","Mattias Teye","Derek Yadgaroff","Judith Bütepage"],"pdf_url":"https://arxiv.org/pdf/2507.18352v1.pdf","comment":"Accepted to ACM Transactions on Graphics 2025 (SIGGRAPH journal\n  track)"},{"id":"http://arxiv.org/abs/2507.18346v1","updated":"2025-07-24T12:19:25Z","published":"2025-07-24T12:19:25Z","title":"Low-rank adaptive physics-informed HyperDeepONets for solving\n  differential equations","summary":"  HyperDeepONets were introduced in Lee, Cho and Hwang [ICLR, 2023] as an\nalternative architecture for operator learning, in which a hypernetwork\ngenerates the weights for the trunk net of a DeepONet. While this improves\nexpressivity, it incurs high memory and computational costs due to the large\nnumber of output parameters required. In this work we introduce, in the\nphysics-informed machine learning setting, a variation, PI-LoRA-HyperDeepONets,\nwhich leverage low-rank adaptation (LoRA) to reduce complexity by decomposing\nthe hypernetwork's output layer weight matrix into two smaller low-rank\nmatrices. This reduces the number of trainable parameters while introducing an\nextra regularization of the trunk networks' weights. Through extensive\nexperiments on both ordinary and partial differential equations we show that\nPI-LoRA-HyperDeepONets achieve up to 70\\% reduction in parameters and\nconsistently outperform regular HyperDeepONets in terms of predictive accuracy\nand generalization.\n","authors":["Etienne Zeudong","Elsa Cardoso-Bihlo","Alex Bihlo"],"pdf_url":"https://arxiv.org/pdf/2507.18346v1.pdf","comment":"14 pages, 6 figures, 5 tables"},{"id":"http://arxiv.org/abs/2507.18333v1","updated":"2025-07-24T11:59:42Z","published":"2025-07-24T11:59:42Z","title":"Remembering the Markov Property in Cooperative MARL","summary":"  Cooperative multi-agent reinforcement learning (MARL) is typically formalised\nas a Decentralised Partially Observable Markov Decision Process (Dec-POMDP),\nwhere agents must reason about the environment and other agents' behaviour. In\npractice, current model-free MARL algorithms use simple recurrent function\napproximators to address the challenge of reasoning about others using partial\ninformation. In this position paper, we argue that the empirical success of\nthese methods is not due to effective Markov signal recovery, but rather to\nlearning simple conventions that bypass environment observations and memory.\nThrough a targeted case study, we show that co-adapting agents can learn\nbrittle conventions, which then fail when partnered with non-adaptive agents.\nCrucially, the same models can learn grounded policies when the task design\nnecessitates it, revealing that the issue is not a fundamental limitation of\nthe learning models but a failure of the benchmark design. Our analysis also\nsuggests that modern MARL environments may not adequately test the core\nassumptions of Dec-POMDPs. We therefore advocate for new cooperative\nenvironments built upon two core principles: (1) behaviours grounded in\nobservations and (2) memory-based reasoning about other agents, ensuring\nsuccess requires genuine skill rather than fragile, co-adapted agreements.\n","authors":["Kale-ab Abebe Tessera","Leonard Hinckeldey","Riccardo Zamboni","David Abel","Amos Storkey"],"pdf_url":"https://arxiv.org/pdf/2507.18333v1.pdf","comment":"RLC Finding the Frame Workshop Camera-Ready, 8 pages"},{"id":"http://arxiv.org/abs/2507.18332v1","updated":"2025-07-24T11:59:10Z","published":"2025-07-24T11:59:10Z","title":"Hierarchical Dimensionless Learning (Hi-π): A physics-data\n  hybrid-driven approach for discovering dimensionless parameter combinations","summary":"  Dimensional analysis provides a universal framework for reducing physical\ncomplexity and reveal inherent laws. However, its application to\nhigh-dimensional systems still generates redundant dimensionless parameters,\nmaking it challenging to establish physically meaningful descriptions. Here, we\nintroduce Hierarchical Dimensionless Learning (Hi-{\\pi}), a physics-data\nhybrid-driven method that combines dimensional analysis and symbolic regression\nto automatically discover key dimensionless parameter combination(s). We\napplied this method to classic examples in various research fields of fluid\nmechanics. For the Rayleigh-B\\'enard convection, this method accurately\nextracted two intrinsic dimensionless parameters: the Rayleigh number and the\nPrandtl number, validating its unified representation advantage across\nmultiscale data. For the viscous flows in a circular pipe, the method\nautomatically discovers two optimal dimensionless parameters: the Reynolds\nnumber and relative roughness, achieving a balance between accuracy and\ncomplexity. For the compressibility correction in subsonic flow, the method\neffectively extracts the classic compressibility correction formulation, while\ndemonstrating its capability to discover hierarchical structural expressions\nthrough optimal parameter transformations.\n","authors":["Mingkun Xia","Haitao Lin","Weiwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.18332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18330v1","updated":"2025-07-24T11:57:59Z","published":"2025-07-24T11:57:59Z","title":"GVCCS: A Dataset for Contrail Identification and Tracking on Visible\n  Whole Sky Camera Sequences","summary":"  Aviation's climate impact includes not only CO2 emissions but also\nsignificant non-CO2 effects, especially from contrails. These ice clouds can\nalter Earth's radiative balance, potentially rivaling the warming effect of\naviation CO2. Physics-based models provide useful estimates of contrail\nformation and climate impact, but their accuracy depends heavily on the quality\nof atmospheric input data and on assumptions used to represent complex\nprocesses like ice particle formation and humidity-driven persistence.\nObservational data from remote sensors, such as satellites and ground cameras,\ncould be used to validate and calibrate these models. However, existing\ndatasets don't explore all aspect of contrail dynamics and formation: they\ntypically lack temporal tracking, and do not attribute contrails to their\nsource flights. To address these limitations, we present the Ground Visible\nCamera Contrail Sequences (GVCCS), a new open data set of contrails recorded\nwith a ground-based all-sky camera in the visible range. Each contrail is\nindividually labeled and tracked over time, allowing a detailed analysis of its\nlifecycle. The dataset contains 122 video sequences (24,228 frames) and\nincludes flight identifiers for contrails that form above the camera. As\nreference, we also propose a unified deep learning framework for contrail\nanalysis using a panoptic segmentation model that performs semantic\nsegmentation (contrail pixel identification), instance segmentation (individual\ncontrail separation), and temporal tracking in a single architecture. By\nproviding high-quality, temporally resolved annotations and a benchmark for\nmodel evaluation, our work supports improved contrail monitoring and will\nfacilitate better calibration of physical models. This sets the groundwork for\nmore accurate climate impact understanding and assessments.\n","authors":["Gabriel Jarry","Ramon Dalmau","Philippe Very","Franck Ballerini","Stephania-Denisa Bocu"],"pdf_url":"https://arxiv.org/pdf/2507.18330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13101v3","updated":"2025-07-24T11:53:07Z","published":"2025-04-17T17:10:33Z","title":"Position: An Empirically Grounded Identifiability Theory Will Accelerate\n  Self-Supervised Learning Research","summary":"  Self-Supervised Learning (SSL) powers many current AI systems. As research\ninterest and investment grow, the SSL design space continues to expand. The\nPlatonic view of SSL, following the Platonic Representation Hypothesis (PRH),\nsuggests that despite different methods and engineering approaches, all\nrepresentations converge to the same Platonic ideal. However, this phenomenon\nlacks precise theoretical explanation. By synthesizing evidence from\nIdentifiability Theory (IT), we show that the PRH can emerge in SSL. However,\ncurrent IT cannot explain SSL's empirical success. To bridge the gap between\ntheory and practice, we propose expanding IT into what we term Singular\nIdentifiability Theory (SITh), a broader theoretical framework encompassing the\nentire SSL pipeline. SITh would allow deeper insights into the implicit data\nassumptions in SSL and advance the field towards learning more interpretable\nand generalizable representations. We highlight three critical directions for\nfuture research: 1) training dynamics and convergence properties of SSL; 2) the\nimpact of finite samples, batch size, and data diversity; and 3) the role of\ninductive biases in architecture, augmentations, initialization schemes, and\noptimizers.\n","authors":["Patrik Reizinger","Randall Balestriero","David Klindt","Wieland Brendel"],"pdf_url":"https://arxiv.org/pdf/2504.13101v3.pdf","comment":"ICML2025 camera ready"},{"id":"http://arxiv.org/abs/2507.18323v1","updated":"2025-07-24T11:49:46Z","published":"2025-07-24T11:49:46Z","title":"A Multi-Dataset Benchmark for Semi-Supervised Semantic Segmentation in\n  ECG Delineation","summary":"  Electrocardiogram (ECG) delineation, the segmentation of meaningful waveform\nfeatures, is critical for clinical diagnosis. Despite recent advances using\ndeep learning, progress has been limited by the scarcity of publicly available\nannotated datasets. Semi-supervised learning presents a promising solution by\nleveraging abundant unlabeled ECG data. In this study, we present the first\nsystematic benchmark for semi-supervised semantic segmentation (SemiSeg) in ECG\ndelineation. We curated and unified multiple public datasets, including\npreviously underused sources, to support robust and diverse evaluation. We\nadopted five representative SemiSeg algorithms from computer vision,\nimplemented them on two different architectures: the convolutional network and\nthe transformer, and evaluated them in two different settings: in-domain and\ncross-domain. Additionally, we propose ECG-specific training configurations and\naugmentation strategies and introduce a standardized evaluation framework. Our\nresults show that the transformer outperforms the convolutional network in\nsemi-supervised ECG delineation. We anticipate that our benchmark will serve as\na foundation for advancing semi-supervised ECG delineation methods and will\nfacilitate further research in this domain.\n","authors":["Minje Park","Jeonghwa Lim","Taehyung Yu","Sunghoon Joo"],"pdf_url":"https://arxiv.org/pdf/2507.18323v1.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2312.12102v3","updated":"2025-07-24T11:44:19Z","published":"2023-12-19T12:26:57Z","title":"I-CEE: Tailoring Explanations of Image Classification Models to User\n  Expertise","summary":"  Effectively explaining decisions of black-box machine learning models is\ncritical to responsible deployment of AI systems that rely on them. Recognizing\ntheir importance, the field of explainable AI (XAI) provides several techniques\nto generate these explanations. Yet, there is relatively little emphasis on the\nuser (the explainee) in this growing body of work and most XAI techniques\ngenerate \"one-size-fits-all\" explanations. To bridge this gap and achieve a\nstep closer towards human-centered XAI, we present I-CEE, a framework that\nprovides Image Classification Explanations tailored to User Expertise. Informed\nby existing work, I-CEE explains the decisions of image classification models\nby providing the user with an informative subset of training data (i.e.,\nexample images), corresponding local explanations, and model decisions.\nHowever, unlike prior work, I-CEE models the informativeness of the example\nimages to depend on user expertise, resulting in different examples for\ndifferent users. We posit that by tailoring the example set to user expertise,\nI-CEE can better facilitate users' understanding and simulatability of the\nmodel. To evaluate our approach, we conduct detailed experiments in both\nsimulation and with human participants (N = 100) on multiple datasets.\nExperiments with simulated users show that I-CEE improves users' ability to\naccurately predict the model's decisions (simulatability) compared to\nbaselines, providing promising preliminary results. Experiments with human\nparticipants demonstrate that our method significantly improves user\nsimulatability accuracy, highlighting the importance of human-centered XAI\n","authors":["Yao Rong","Peizhu Qian","Vaibhav Unhelkar","Enkelejda Kasneci"],"pdf_url":"https://arxiv.org/pdf/2312.12102v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18320v1","updated":"2025-07-24T11:43:46Z","published":"2025-07-24T11:43:46Z","title":"State of Health Estimation of Batteries Using a Time-Informed Dynamic\n  Sequence-Inverted Transformer","summary":"  The rapid adoption of battery-powered vehicles and energy storage systems\nover the past decade has made battery health monitoring increasingly critical.\nBatteries play a central role in the efficiency and safety of these systems,\nyet they inevitably degrade over time due to repeated charge-discharge cycles.\nThis degradation leads to reduced energy efficiency and potential overheating,\nposing significant safety concerns. Accurate estimation of a State of Health\n(SoH) of battery is therefore essential for ensuring operational reliability\nand safety. Several machine learning architectures, such as LSTMs,\ntransformers, and encoder-based models, have been proposed to estimate SoH from\ndischarge cycle data. However, these models struggle with the irregularities\ninherent in real-world measurements: discharge readings are often recorded at\nnon-uniform intervals, and the lengths of discharge cycles vary significantly.\nTo address this, most existing approaches extract features from the sequences\nrather than processing them in full, which introduces information loss and\ncompromises accuracy. To overcome these challenges, we propose a novel\narchitecture: Time-Informed Dynamic Sequence Inverted Transformer (TIDSIT).\nTIDSIT incorporates continuous time embeddings to effectively represent\nirregularly sampled data and utilizes padded sequences with temporal attention\nmechanisms to manage variable-length inputs without discarding sequence\ninformation. Experimental results on the NASA battery degradation dataset show\nthat TIDSIT significantly outperforms existing models, achieving over 50%\nreduction in prediction error and maintaining an SoH prediction error below\n0.58%. Furthermore, the architecture is generalizable and holds promise for\nbroader applications in health monitoring tasks involving irregular time-series\ndata.\n","authors":["Janak M. Patel","Milad Ramezankhani","Anirudh Deodhar","Dagnachew Birru"],"pdf_url":"https://arxiv.org/pdf/2507.18320v1.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2507.18313v1","updated":"2025-07-24T11:31:23Z","published":"2025-07-24T11:31:23Z","title":"Regression-aware Continual Learning for Android Malware Detection","summary":"  Malware evolves rapidly, forcing machine learning (ML)-based detectors to\nadapt continuously. With antivirus vendors processing hundreds of thousands of\nnew samples daily, datasets can grow to billions of examples, making full\nretraining impractical. Continual learning (CL) has emerged as a scalable\nalternative, enabling incremental updates without full data access while\nmitigating catastrophic forgetting. In this work, we analyze a critical yet\noverlooked issue in this context: security regression. Unlike forgetting, which\nmanifests as a general performance drop on previously seen data, security\nregression captures harmful prediction changes at the sample level, such as a\nmalware sample that was once correctly detected but evades detection after a\nmodel update. Although often overlooked, regressions pose serious risks in\nsecurity-critical applications, as the silent reintroduction of previously\ndetected threats in the system may undermine users' trust in the whole updating\nprocess. To address this issue, we formalize and quantify security regression\nin CL-based malware detectors and propose a regression-aware penalty to\nmitigate it. Specifically, we adapt Positive Congruent Training (PCT) to the CL\nsetting, preserving prior predictive behavior in a model-agnostic manner.\nExperiments on the ELSA, Tesseract, and AZ-Class datasets show that our method\neffectively reduces regression across different CL scenarios while maintaining\nstrong detection performance over time.\n","authors":["Daniele Ghiani","Daniele Angioni","Giorgio Piras","Angelo Sotgiu","Luca Minnei","Srishti Gupta","Maura Pintor","Fabio Roli","Battista Biggio"],"pdf_url":"https://arxiv.org/pdf/2507.18313v1.pdf","comment":"Submitted to IEEE Transactions on Information Forensics and Security"},{"id":"http://arxiv.org/abs/2504.10240v4","updated":"2025-07-24T11:31:03Z","published":"2025-04-14T14:02:09Z","title":"GNN-ACLP: Graph Neural Networks Based Analog Circuit Link Prediction","summary":"  Circuit link prediction identifying missing component connections from\nincomplete netlists is crucial in analog circuit design automation. However,\nexisting methods face three main challenges: 1) Insufficient use of topological\npatterns in circuit graphs reduces prediction accuracy; 2) Data scarcity due to\nthe complexity of annotations hinders model generalization; 3) Limited\nadaptability to various netlist formats. We propose GNN-ACLP, a graph neural\nnetworks (GNNs) based method featuring three innovations to tackle these\nchallenges. First, we introduce the SEAL (learning from Subgraphs, Embeddings,\nand Attributes for Link prediction) framework and achieve port-level accuracy\nin circuit link prediction. Second, we propose Netlist Babel Fish, a netlist\nformat conversion tool leveraging retrieval-augmented generation (RAG) with a\nlarge language model (LLM) to improve the compatibility of netlist formats.\nFinally, we construct SpiceNetlist, a comprehensive dataset that contains 775\nannotated circuits across 10 different component classes. Experiments\ndemonstrate accuracy improvements of 16.08% on SpiceNetlist, 11.38% on\nImage2Net, and 16.01% on Masala-CHAI compared to the baseline in intra-dataset\nevaluation, while maintaining accuracy from 92.05% to 99.07% in cross-dataset\nevaluation, exhibiting robust feature transfer capabilities.\n","authors":["Guanyuan Pan","Tiansheng Zhou","Bingtao Ma","Yaqi Wang","Jianxiang Zhao","Zhi Li","Yugui Lin","Pietro Lio","Shuai Wang"],"pdf_url":"https://arxiv.org/pdf/2504.10240v4.pdf","comment":"Code and data will be made available on request to the corresponding\n  author. V4 Update: Add Future Work; Improve Typesetting"},{"id":"http://arxiv.org/abs/2410.22074v2","updated":"2025-07-24T11:24:17Z","published":"2024-10-29T14:33:52Z","title":"Variational inference for pile-up removal at hadron colliders with\n  diffusion models","summary":"  In this paper, we present a novel method for pile-up removal of $pp$\ninteractions using variational inference with diffusion models, called vipr.\nInstead of using classification methods to identify which particles are from\nthe primary collision, a generative model is trained to predict the\nconstituents of the hard-scatter particle jets with pile-up removed. This\nresults in an estimate of the full posterior over hard-scatter jet\nconstituents, which has not yet been explored in the context of pile-up\nremoval, yielding a clear advantage over existing methods especially in the\npresence of imperfect detector efficiency. We evaluate the performance of vipr\nin a sample of jets from simulated $t\\bar{t}$ events overlain with pile-up\ncontamination. vipr outperforms softdrop and has comparable performance to\npuppiml in predicting the substructure of the hard-scatter jets over a wide\nrange of pile-up scenarios.\n","authors":["Malte Algren","Tobias Golling","Christopher Pollard","John Andrew Raine"],"pdf_url":"https://arxiv.org/pdf/2410.22074v2.pdf","comment":"19 pages, 13 figures"},{"id":"http://arxiv.org/abs/2507.17596v2","updated":"2025-07-24T11:04:42Z","published":"2025-07-23T15:28:23Z","title":"PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving","summary":"  While end-to-end autonomous driving models show promising results, their\npractical deployment is often hindered by large model sizes, a reliance on\nexpensive LiDAR sensors and computationally intensive BEV feature\nrepresentations. This limits their scalability, especially for mass-market\nvehicles equipped only with cameras. To address these challenges, we propose\nPRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving\narchitecture operates using only camera data, without explicit BEV\nrepresentation and forgoing the need for LiDAR. PRIX leverages a visual feature\nextractor coupled with a generative planning head to predict safe trajectories\nfrom raw pixel inputs directly. A core component of our architecture is the\nContext-aware Recalibration Transformer (CaRT), a novel module designed to\neffectively enhance multi-level visual features for more robust planning. We\ndemonstrate through comprehensive experiments that PRIX achieves\nstate-of-the-art performance on the NavSim and nuScenes benchmarks, matching\nthe capabilities of larger, multimodal diffusion planners while being\nsignificantly more efficient in terms of inference speed and model size, making\nit a practical solution for real-world deployment. Our work is open-source and\nthe code will be at https://maxiuw.github.io/prix.\n","authors":["Maciej K. Wozniak","Lianhang Liu","Yixi Cai","Patric Jensfelt"],"pdf_url":"https://arxiv.org/pdf/2507.17596v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2507.18297v1","updated":"2025-07-24T11:02:13Z","published":"2025-07-24T11:02:13Z","title":"Self-Supervised Coarsening of Unstructured Grid with Automatic\n  Differentiation","summary":"  Due to the high computational load of modern numerical simulation, there is a\ndemand for approaches that would reduce the size of discrete problems while\nkeeping the accuracy reasonable. In this work, we present an original algorithm\nto coarsen an unstructured grid based on the concepts of differentiable\nphysics. We achieve this by employing k-means clustering, autodifferentiation\nand stochastic minimization algorithms. We demonstrate performance of the\ndesigned algorithm on two PDEs: a linear parabolic equation which governs\nslightly compressible fluid flow in porous media and the wave equation. Our\nresults show that in the considered scenarios, we reduced the number of grid\npoints up to 10 times while preserving the modeled variable dynamics in the\npoints of interest. The proposed approach can be applied to the simulation of\nan arbitrary system described by evolutionary partial differential equations.\n","authors":["Sergei Shumilin","Alexander Ryabov","Nikolay Yavich","Evgeny Burnaev","Vladimir Vanovskiy"],"pdf_url":"https://arxiv.org/pdf/2507.18297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18293v1","updated":"2025-07-24T10:57:20Z","published":"2025-07-24T10:57:20Z","title":"Leveraging Data Augmentation and Siamese Learning for Predictive Process\n  Monitoring","summary":"  Predictive Process Monitoring (PPM) enables forecasting future events or\noutcomes of ongoing business process instances based on event logs. However,\ndeep learning PPM approaches are often limited by the low variability and small\nsize of real-world event logs. To address this, we introduce SiamSA-PPM, a\nnovel self-supervised learning framework that combines Siamese learning with\nStatistical Augmentation for Predictive Process Monitoring. It employs three\nnovel statistically grounded transformation methods that leverage control-flow\nsemantics and frequent behavioral patterns to generate realistic, semantically\nvalid new trace variants. These augmented views are used within a Siamese\nlearning setup to learn generalizable representations of process prefixes\nwithout the need for labeled supervision. Extensive experiments on real-life\nevent logs demonstrate that SiamSA-PPM achieves competitive or superior\nperformance compared to the SOTA in both next activity and final outcome\nprediction tasks. Our results further show that statistical augmentation\nsignificantly outperforms random transformations and improves variability in\nthe data, highlighting SiamSA-PPM as a promising direction for training data\nenrichment in process prediction.\n","authors":["Sjoerd van Straten","Alessandro Padella","Marwan Hassani"],"pdf_url":"https://arxiv.org/pdf/2507.18293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07769v2","updated":"2025-07-24T10:44:28Z","published":"2025-07-10T13:54:38Z","title":"BEAVER: Building Environments with Assessable Variation for Evaluating\n  Multi-Objective Reinforcement Learning","summary":"  Recent years have seen significant advancements in designing reinforcement\nlearning (RL)-based agents for building energy management. While individual\nsuccess is observed in simulated or controlled environments, the scalability of\nRL approaches in terms of efficiency and generalization across building\ndynamics and operational scenarios remains an open question. In this work, we\nformally characterize the generalization space for the cross-environment,\nmulti-objective building energy management task, and formulate the\nmulti-objective contextual RL problem. Such a formulation helps understand the\nchallenges of transferring learned policies across varied operational contexts\nsuch as climate and heat convection dynamics under multiple control objectives\nsuch as comfort level and energy consumption. We provide a principled way to\nparameterize such contextual information in realistic building RL environments,\nand construct a novel benchmark to facilitate the evaluation of generalizable\nRL algorithms in practical building control tasks. Our results show that\nexisting multi-objective RL methods are capable of achieving reasonable\ntrade-offs between conflicting objectives. However, their performance degrades\nunder certain environment variations, underscoring the importance of\nincorporating dynamics-dependent contextual information into the policy\nlearning process.\n","authors":["Ruohong Liu","Jack Umenberger","Yize Chen"],"pdf_url":"https://arxiv.org/pdf/2507.07769v2.pdf","comment":"Accepted at the Workshop on Computational Optimization of Buildings\n  (ICML CO-BUILD), 42nd International Conference on Machine Learning (ICML\n  2025), Vancouver, Canada"},{"id":"http://arxiv.org/abs/2507.18262v1","updated":"2025-07-24T10:07:31Z","published":"2025-07-24T10:07:31Z","title":"ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic\n  Grounding for Generalizable Robotic Manipulation","summary":"  Semantics-driven 3D spatial constraints align highlevel semantic\nrepresentations with low-level action spaces, facilitating the unification of\ntask understanding and execution in robotic manipulation. The synergistic\nreasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation\nModels (VFMs) enables cross-modal 3D spatial constraint construction.\nNevertheless, existing methods have three key limitations: (1) coarse semantic\ngranularity in constraint modeling, (2) lack of real-time closed-loop planning,\n(3) compromised robustness in semantically diverse environments. To address\nthese challenges, we propose ReSem3D, a unified manipulation framework for\nsemantically diverse environments, leveraging the synergy between VFMs and\nMLLMs to achieve fine-grained visual grounding and dynamically constructs\nhierarchical 3D spatial constraints for real-time manipulation. Specifically,\nthe framework is driven by hierarchical recursive reasoning in MLLMs, which\ninteract with VFMs to automatically construct 3D spatial constraints from\nnatural language instructions and RGB-D observations in two stages: part-level\nextraction and region-level refinement. Subsequently, these constraints are\nencoded as real-time optimization objectives in joint space, enabling reactive\nbehavior to dynamic disturbances. Extensive simulation and real-world\nexperiments are conducted in semantically rich household and sparse chemical\nlab environments. The results demonstrate that ReSem3D performs diverse\nmanipulation tasks under zero-shot conditions, exhibiting strong adaptability\nand generalization. Code and videos at https://resem3d.github.io.\n","authors":["Chenyu Su","Weiwei Shang","Chen Qian","Fei Zhang","Shuang Cong"],"pdf_url":"https://arxiv.org/pdf/2507.18262v1.pdf","comment":"12 pages,9 figures"},{"id":"http://arxiv.org/abs/2507.16548v2","updated":"2025-07-24T09:56:46Z","published":"2025-07-22T12:57:25Z","title":"Alternative Loss Function in Evaluation of Transformer Models","summary":"  The proper design and architecture of testing machine learning models,\nespecially in their application to quantitative finance problems, is crucial.\nThe most important aspect of this process is selecting an adequate loss\nfunction for training, validation, estimation purposes, and hyperparameter\ntuning. Therefore, in this research, through empirical experiments on equity\nand cryptocurrency assets, we apply the Mean Absolute Directional Loss (MADL)\nfunction, which is more adequate for optimizing forecast-generating models used\nin algorithmic investment strategies. The MADL function results are compared\nbetween Transformer and LSTM models, and we show that in almost every case,\nTransformer results are significantly better than those obtained with LSTM.\n","authors":["Jakub Michańków","Paweł Sakowski","Robert Ślepaczuk"],"pdf_url":"https://arxiv.org/pdf/2507.16548v2.pdf","comment":"12 pages, fixed grammar, typos and minor error in tables"},{"id":"http://arxiv.org/abs/2506.16297v3","updated":"2025-07-24T09:52:06Z","published":"2025-06-19T13:17:30Z","title":"SyncMapV2: Robust and Adaptive Unsupervised Segmentation","summary":"  Human vision excels at segmenting visual cues without the need for explicit\ntraining, and it remains remarkably robust even as noise severity increases. In\ncontrast, existing AI algorithms struggle to maintain accuracy under similar\nconditions. Here, we present SyncMapV2, the first to solve unsupervised\nsegmentation with state-of-the-art robustness. SyncMapV2 exhibits a minimal\ndrop in mIoU, only 0.01%, under digital corruption, compared to a 23.8% drop\nobserved in SOTA methods. This superior performance extends across various\ntypes of corruption: noise (7.3% vs. 37.7%), weather (7.5% vs. 33.8%), and blur\n(7.0% vs. 29.5%). Notably, SyncMapV2 accomplishes this without any robust\ntraining, supervision, or loss functions. It is based on a learning paradigm\nthat uses self-organizing dynamical equations combined with concepts from\nrandom networks. Moreover, unlike conventional methods that require\nre-initialization for each new input, SyncMapV2 adapts online, mimicking the\ncontinuous adaptability of human vision. Thus, we go beyond the accurate and\nrobust results, and present the first algorithm that can do all the above\nonline, adapting to input rather than re-initializing. In adaptability tests,\nSyncMapV2 demonstrates near-zero performance degradation, which motivates and\nfosters a new generation of robust and adaptive intelligence in the near\nfuture.\n","authors":["Heng Zhang","Zikang Wan","Danilo Vasconcellos Vargas"],"pdf_url":"https://arxiv.org/pdf/2506.16297v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18242v1","updated":"2025-07-24T09:30:37Z","published":"2025-07-24T09:30:37Z","title":"Boosting Revisited: Benchmarking and Advancing LP-Based Ensemble Methods","summary":"  Despite their theoretical appeal, totally corrective boosting methods based\non linear programming have received limited empirical attention. In this paper,\nwe conduct the first large-scale experimental study of six LP-based boosting\nformulations, including two novel methods, NM-Boost and QRLP-Boost, across 20\ndiverse datasets. We evaluate the use of both heuristic and optimal base\nlearners within these formulations, and analyze not only accuracy, but also\nensemble sparsity, margin distribution, anytime performance, and hyperparameter\nsensitivity. We show that totally corrective methods can outperform or match\nstate-of-the-art heuristics like XGBoost and LightGBM when using shallow trees,\nwhile producing significantly sparser ensembles. We further show that these\nmethods can thin pre-trained ensembles without sacrificing performance, and we\nhighlight both the strengths and limitations of using optimal decision trees in\nthis context.\n","authors":["Fabian Akkerman","Julien Ferry","Christian Artigues","Emmanuel Hebrard","Thibaut Vidal"],"pdf_url":"https://arxiv.org/pdf/2507.18242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04151v2","updated":"2025-07-24T09:25:16Z","published":"2025-03-06T07:01:08Z","title":"Robust Multi-View Learning via Representation Fusion of Sample-Level\n  Attention and Alignment of Simulated Perturbation","summary":"  Recently, multi-view learning (MVL) has garnered significant attention due to\nits ability to fuse discriminative information from multiple views. However,\nreal-world multi-view datasets are often heterogeneous and imperfect, which\nusually causes MVL methods designed for specific combinations of views to lack\napplication potential and limits their effectiveness. To address this issue, we\npropose a novel robust MVL method (namely RML) with simultaneous representation\nfusion and alignment. Specifically, we introduce a simple yet effective\nmulti-view transformer fusion network where we transform heterogeneous\nmulti-view data into homogeneous word embeddings, and then integrate multiple\nviews by the sample-level attention mechanism to obtain a fused representation.\nFurthermore, we propose a simulated perturbation based multi-view contrastive\nlearning framework that dynamically generates the noise and unusable\nperturbations for simulating imperfect data conditions. The simulated noisy and\nunusable data obtain two distinct fused representations, and we utilize\ncontrastive learning to align them for learning discriminative and robust\nrepresentations. Our RML is self-supervised and can also be applied for\ndownstream tasks as a regularization. In experiments, we employ it in\nmulti-view unsupervised clustering, noise-label classification, and as a\nplug-and-play module for cross-modal hashing retrieval. Extensive comparison\nexperiments and ablation studies validate RML's effectiveness. Code is\navailable at https://github.com/SubmissionsIn/RML.\n","authors":["Jie Xu","Na Zhao","Gang Niu","Masashi Sugiyama","Xiaofeng Zhu"],"pdf_url":"https://arxiv.org/pdf/2503.04151v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.16068v2","updated":"2025-07-24T09:25:12Z","published":"2025-07-21T21:09:15Z","title":"Compositional Coordination for Multi-Robot Teams with Large Language\n  Models","summary":"  Multi-robot coordination has traditionally relied on a mission-specific and\nexpert-driven pipeline, where natural language mission descriptions are\nmanually translated by domain experts into mathematical formulation, algorithm\ndesign, and executable code. This conventional process is labor-intensive,\ninaccessible to non-experts, and inflexible to changes in mission requirements.\nHere, we propose LAN2CB (Language to Collective Behavior), a novel framework\nthat leverages large language models (LLMs) to streamline and generalize the\nmulti-robot coordination pipeline. LAN2CB transforms natural language (NL)\nmission descriptions into executable Python code for multi-robot systems\nthrough two core modules: (1) Mission Analysis, which parses mission\ndescriptions into behavior trees, and (2) Code Generation, which leverages the\nbehavior tree and a structured knowledge base to generate robot control code.\nWe further introduce a dataset of natural language mission descriptions to\nsupport development and benchmarking. Experiments in both simulation and\nreal-world environments demonstrate that LAN2CB enables robust and flexible\nmulti-robot coordination from natural language, significantly reducing manual\nengineering effort and supporting broad generalization across diverse mission\ntypes. Website: https://sites.google.com/view/lan-cb\n","authors":["Zhehui Huang","Guangyao Shi","Yuwei Wu","Vijay Kumar","Gaurav S. Sukhatme"],"pdf_url":"https://arxiv.org/pdf/2507.16068v2.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2506.11790v2","updated":"2025-07-24T09:17:21Z","published":"2025-06-13T13:52:32Z","title":"Why Do Class-Dependent Evaluation Effects Occur with Time Series Feature\n  Attributions? A Synthetic Data Investigation","summary":"  Evaluating feature attribution methods represents a critical challenge in\nexplainable AI (XAI), as researchers typically rely on perturbation-based\nmetrics when ground truth is unavailable. However, recent work reveals that\nthese evaluation metrics can show different performance across predicted\nclasses within the same dataset. These \"class-dependent evaluation effects\"\nraise questions about whether perturbation analysis reliably measures\nattribution quality, with direct implications for XAI method development and\nevaluation trustworthiness. We investigate under which conditions these\nclass-dependent effects arise by conducting controlled experiments with\nsynthetic time series data where ground truth feature locations are known. We\nsystematically vary feature types and class contrasts across binary\nclassification tasks, then compare perturbation-based degradation scores with\nground truth-based precision-recall metrics using multiple attribution methods.\nOur experiments demonstrate that class-dependent effects emerge with both\nevaluation approaches, even in simple scenarios with temporally localized\nfeatures, triggered by basic variations in feature amplitude or temporal extent\nbetween classes. Most critically, we find that perturbation-based and ground\ntruth metrics frequently yield contradictory assessments of attribution quality\nacross classes, with weak correlations between evaluation approaches. These\nfindings suggest that researchers should interpret perturbation-based metrics\nwith care, as they may not always align with whether attributions correctly\nidentify discriminating features. By showing this disconnect, our work points\ntoward reconsidering what attribution evaluation actually measures and\ndeveloping more rigorous evaluation methods that capture multiple dimensions of\nattribution quality.\n","authors":["Gregor Baer","Isel Grau","Chao Zhang","Pieter Van Gorp"],"pdf_url":"https://arxiv.org/pdf/2506.11790v2.pdf","comment":"Accepted at TempXAI Workshop @ ECML-PKDD 2025 (Explainable AI for\n  Time Series and Data Streams)"},{"id":"http://arxiv.org/abs/2507.18220v1","updated":"2025-07-24T09:15:26Z","published":"2025-07-24T09:15:26Z","title":"Sparse identification of nonlinear dynamics with library optimization\n  mechanism: Recursive long-term prediction perspective","summary":"  The sparse identification of nonlinear dynamics (SINDy) approach can discover\nthe governing equations of dynamical systems based on measurement data, where\nthe dynamical model is identified as the sparse linear combination of the given\nbasis functions. A major challenge in SINDy is the design of a library, which\nis a set of candidate basis functions, as the appropriate library is not\ntrivial for many dynamical systems. To overcome this difficulty, this study\nproposes SINDy with library optimization mechanism (SINDy-LOM), which is a\ncombination of the sparse regression technique and the novel learning strategy\nof the library. In the proposed approach, the basis functions are parametrized.\nThe SINDy-LOM approach involves a two-layer optimization architecture: the\ninner-layer, in which the data-driven model is extracted as the sparse linear\ncombination of the candidate basis functions, and the outer-layer, in which the\nbasis functions are optimized from the viewpoint of the recursive long-term\n(RLT) prediction accuracy; thus, the library design is reformulated as the\noptimization of the parametrized basis functions. The resulting SINDy-LOM model\nhas good interpretability and usability, as the proposed approach yields the\nparsimonious model. The library optimization mechanism significantly reduces\nuser burden. The RLT perspective improves the reliability of the resulting\nmodel compared with the traditional SINDy approach that can only ensure the\none-step-ahead prediction accuracy. The validity of the proposed approach is\ndemonstrated by applying it to a diesel engine airpath system, which is a\nwell-known complex industrial system.\n","authors":["Ansei Yonezawa","Heisei Yonezawa","Shuichi Yahagi","Itsuro Kajiwara","Shinya Kijimoto","Hikaru Taniuchi","Kentaro Murakami"],"pdf_url":"https://arxiv.org/pdf/2507.18220v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2507.18219v1","updated":"2025-07-24T09:15:07Z","published":"2025-07-24T09:15:07Z","title":"FedSA-GCL: A Semi-Asynchronous Federated Graph Learning Framework with\n  Personalized Aggregation and Cluster-Aware Broadcasting","summary":"  Federated Graph Learning (FGL) is a distributed learning paradigm that\nenables collaborative training over large-scale subgraphs located on multiple\nlocal systems. However, most existing FGL approaches rely on synchronous\ncommunication, which leads to inefficiencies and is often impractical in\nreal-world deployments. Meanwhile, current asynchronous federated learning\n(AFL) methods are primarily designed for conventional tasks such as image\nclassification and natural language processing, without accounting for the\nunique topological properties of graph data. Directly applying these methods to\ngraph learning can possibly result in semantic drift and representational\ninconsistency in the global model. To address these challenges, we propose\nFedSA-GCL, a semi-asynchronous federated framework that leverages both\ninter-client label distribution divergence and graph topological\ncharacteristics through a novel ClusterCast mechanism for efficient training.\nWe evaluate FedSA-GCL on multiple real-world graph datasets using the Louvain\nand Metis split algorithms, and compare it against 9 baselines. Extensive\nexperiments demonstrate that our method achieves strong robustness and\noutstanding efficiency, outperforming the baselines by an average of 2.92% with\nthe Louvain and by 3.4% with the Metis.\n","authors":["Zhongzheng Yuan","Lianshuai Guo","Xunkai Li","Yinlin Zhu","Wenyu Wang","Meixia Qu"],"pdf_url":"https://arxiv.org/pdf/2507.18219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02418v3","updated":"2025-07-24T09:06:37Z","published":"2024-03-04T19:12:13Z","title":"The Role of the Time-Dependent Hessian in High-Dimensional Optimization","summary":"  Gradient descent is commonly used to find minima in rough landscapes,\nparticularly in recent machine learning applications. However, a theoretical\nunderstanding of why good solutions are found remains elusive, especially in\nstrongly non-convex and high-dimensional settings. Here, we focus on the phase\nretrieval problem as a typical example, which has received a lot of attention\nrecently in theoretical machine learning. We analyze the Hessian during\ngradient descent, identify a dynamical transition in its spectral properties,\nand relate it to the ability of escaping rough regions in the loss landscape.\nWhen the signal-to-noise ratio (SNR) is large enough, an informative negative\ndirection exists in the Hessian at the beginning of the descent, i.e in the\ninitial condition. While descending, a BBP transition in the spectrum takes\nplace in finite time: the direction is lost, and the dynamics is trapped in a\nrugged region filled with marginally stable bad minima. Surprisingly, for\nfinite system sizes, this window of negative curvature allows the system to\nrecover the signal well before the theoretical SNR found for infinite sizes,\nemphasizing the central role of initialization and early-time dynamics for\nefficiently navigating rough landscapes.\n","authors":["Tony Bonnaire","Giulio Biroli","Chiara Cammarota"],"pdf_url":"https://arxiv.org/pdf/2403.02418v3.pdf","comment":"32 pages"},{"id":"http://arxiv.org/abs/2507.18196v1","updated":"2025-07-24T08:54:17Z","published":"2025-07-24T08:54:17Z","title":"Goal-based Trajectory Prediction for improved Cross-Dataset\n  Generalization","summary":"  To achieve full autonomous driving, a good understanding of the surrounding\nenvironment is necessary. Especially predicting the future states of other\ntraffic participants imposes a non-trivial challenge. Current SotA-models\nalready show promising results when trained on real datasets (e.g. Argoverse2,\nNuScenes). Problems arise when these models are deployed to new/unseen areas.\nTypically, performance drops significantly, indicating that the models lack\ngeneralization. In this work, we introduce a new Graph Neural Network (GNN)\nthat utilizes a heterogeneous graph consisting of traffic participants and\nvectorized road network. Latter, is used to classify goals, i.e. endpoints of\nthe predicted trajectories, in a multi-staged approach, leading to a better\ngeneralization to unseen scenarios. We show the effectiveness of the goal\nselection process via cross-dataset evaluation, i.e. training on Argoverse2 and\nevaluating on NuScenes.\n","authors":["Daniel Grimm","Ahmed Abouelazm","J. Marius Zöllner"],"pdf_url":"https://arxiv.org/pdf/2507.18196v1.pdf","comment":"Accepted on IEEE ITSC 2025"},{"id":"http://arxiv.org/abs/2505.05086v2","updated":"2025-07-24T08:52:22Z","published":"2025-05-08T09:34:15Z","title":"Beyond Low-rank Decomposition: A Shortcut Approach for Efficient\n  On-Device Learning","summary":"  On-device learning has emerged as a promising direction for AI development,\nparticularly because of its potential to reduce latency issues and mitigate\nprivacy risks associated with device-server communication, while improving\nenergy efficiency. Despite these advantages, significant memory and\ncomputational constraints still represent major challenges for its deployment.\nDrawing on previous studies on low-rank decomposition methods that address\nactivation memory bottlenecks in backpropagation, we propose a novel shortcut\napproach as an alternative. Our analysis and experiments demonstrate that our\nmethod can reduce activation memory usage, even up to $120.09\\times$ compared\nto vanilla training, while also reducing overall training FLOPs up to\n$1.86\\times$ when evaluated on traditional benchmarks.\n","authors":["Le-Trung Nguyen","Ael Quelennec","Van-Tam Nguyen","Enzo Tartaglione"],"pdf_url":"https://arxiv.org/pdf/2505.05086v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15610v4","updated":"2025-07-24T08:48:10Z","published":"2025-02-21T17:31:22Z","title":"A general language model for peptide identification","summary":"  Accurate identification of bioactive peptides (BPs) and protein\npost-translational modifications (PTMs) is essential for understanding protein\nfunction and advancing therapeutic discovery. However, most computational\nmethods remain limited in their generalizability across diverse peptide\nfunctions. Here, we present PDeepPP, a unified deep learning framework that\nintegrates pretrained protein language models with a hybrid\ntransformer-convolutional architecture, enabling robust identification across\ndiverse peptide classes and PTM sites. We curated comprehensive benchmark\ndatasets and implemented strategies to address data imbalance, allowing PDeepPP\nto systematically extract both global and local sequence features. Through\nextensive analyses-including dimensionality reduction and comparison\nstudies-PDeepPP demonstrates strong, interpretable peptide representations and\nachieves state-of-the-art performance in 25 of the 33 biological identification\ntasks. Notably, PDeepPP attains high accuracy in antimicrobial (0.9726) and\nphosphorylation site (0.9984) identification, with 99.5% specificity in\nglycosylation site prediction and substantial reduction in false negatives in\nantimalarial tasks. By enabling large-scale, accurate peptide analysis, PDeepPP\nsupports biomedical research and the discovery of novel therapeutic targets for\ndisease treatment. All code, datasets, and pretrained models are publicly\navailable via GitHub:https://github.com/fondress/PDeepPP and Hugging\nFace:https://huggingface.co/fondress/PDeppPP.\n","authors":["Jixiu Zhai","Tianchi Lu","Haitian Zhong","Ziyang Xu","Yuhuan Liu","Shengrui Xu","Jingwan Wang","Dan Huang"],"pdf_url":"https://arxiv.org/pdf/2502.15610v4.pdf","comment":"24 pages, 9 figures, 4 tables, submitted to arXiv"},{"id":"http://arxiv.org/abs/2507.18183v1","updated":"2025-07-24T08:29:21Z","published":"2025-07-24T08:29:21Z","title":"ChronoSelect: Robust Learning with Noisy Labels via Dynamics Temporal\n  Memory","summary":"  Training deep neural networks on real-world datasets is often hampered by the\npresence of noisy labels, which can be memorized by over-parameterized models,\nleading to significant degradation in generalization performance. While\nexisting methods for learning with noisy labels (LNL) have made considerable\nprogress, they fundamentally suffer from static snapshot evaluations and fail\nto leverage the rich temporal dynamics of learning evolution. In this paper, we\npropose ChronoSelect (chrono denoting its temporal nature), a novel framework\nfeaturing an innovative four-stage memory architecture that compresses\nprediction history into compact temporal distributions. Our unique sliding\nupdate mechanism with controlled decay maintains only four dynamic memory units\nper sample, progressively emphasizing recent patterns while retaining essential\nhistorical knowledge. This enables precise three-way sample partitioning into\nclean, boundary, and noisy subsets through temporal trajectory analysis and\ndual-branch consistency. Theoretical guarantees prove the mechanism's\nconvergence and stability under noisy conditions. Extensive experiments\ndemonstrate ChronoSelect's state-of-the-art performance across synthetic and\nreal-world benchmarks.\n","authors":["Jianchao Wang","Qingfeng Li","Pengcheng Zheng","Xiaorong Pu","Yazhou Ren"],"pdf_url":"https://arxiv.org/pdf/2507.18183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.17723v2","updated":"2025-07-24T08:03:09Z","published":"2025-04-24T16:36:19Z","title":"Statistical Runtime Verification for LLMs via Robustness Estimation","summary":"  Adversarial robustness verification is essential for ensuring the safe\ndeployment of Large Language Models (LLMs) in runtime-critical applications.\nHowever, formal verification techniques remain computationally infeasible for\nmodern LLMs due to their exponential runtime and white-box access requirements.\nThis paper presents a case study adapting and extending the RoMA statistical\nverification framework to assess its feasibility as an online runtime\nrobustness monitor for LLMs in black-box deployment settings. Our adaptation of\nRoMA analyzes confidence score distributions under semantic perturbations to\nprovide quantitative robustness assessments with statistically validated\nbounds. Our empirical validation against formal verification baselines\ndemonstrates that RoMA achieves comparable accuracy (within 1\\% deviation), and\nreduces verification times from hours to minutes. We evaluate this framework\nacross semantic, categorial, and orthographic perturbation domains. Our results\ndemonstrate RoMA's effectiveness for robustness monitoring in operational LLM\ndeployments. These findings point to RoMA as a potentially scalable alternative\nwhen formal methods are infeasible, with promising implications for runtime\nverification in LLM-based systems.\n","authors":["Natan Levy","Adiel Ashrov","Guy Katz"],"pdf_url":"https://arxiv.org/pdf/2504.17723v2.pdf","comment":"20 pages, 4 figures"},{"id":"http://arxiv.org/abs/2507.14516v2","updated":"2025-07-24T07:48:25Z","published":"2025-07-19T07:32:00Z","title":"SDSC:A Structure-Aware Metric for Semantic Signal Representation\n  Learning","summary":"  We propose the Signal Dice Similarity Coefficient (SDSC), a structure-aware\nmetric function for time series self-supervised representation learning. Most\nSelf-Supervised Learning (SSL) methods for signals commonly adopt\ndistance-based objectives such as mean squared error (MSE), which are sensitive\nto amplitude, invariant to waveform polarity, and unbounded in scale. These\nproperties hinder semantic alignment and reduce interpretability. SDSC\naddresses this by quantifying structural agreement between temporal signals\nbased on the intersection of signed amplitudes, derived from the Dice\nSimilarity Coefficient (DSC).Although SDSC is defined as a structure-aware\nmetric, it can be used as a loss by subtracting from 1 and applying a\ndifferentiable approximation of the Heaviside function for gradient-based\noptimization. A hybrid loss formulation is also proposed to combine SDSC with\nMSE, improving stability and preserving amplitude where necessary. Experiments\non forecasting and classification benchmarks demonstrate that SDSC-based\npre-training achieves comparable or improved performance over MSE, particularly\nin in-domain and low-resource scenarios. The results suggest that structural\nfidelity in signal representations enhances the semantic representation\nquality, supporting the consideration of structure-aware metrics as viable\nalternatives to conventional distance-based methods.\n","authors":["Jeyoung Lee","Hochul Kang"],"pdf_url":"https://arxiv.org/pdf/2507.14516v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18155v1","updated":"2025-07-24T07:41:40Z","published":"2025-07-24T07:41:40Z","title":"GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar","summary":"  Despite recent progress in 3D head avatar generation, balancing identity\npreservation, i.e., reconstruction, with novel poses and expressions, i.e.,\nanimation, remains a challenge. Existing methods struggle to adapt Gaussians to\nvarying geometrical deviations across facial regions, resulting in suboptimal\nquality. To address this, we propose GeoAvatar, a framework for adaptive\ngeometrical Gaussian Splatting. GeoAvatar leverages Adaptive Pre-allocation\nStage (APS), an unsupervised method that segments Gaussians into rigid and\nflexible sets for adaptive offset regularization. Then, based on mouth anatomy\nand dynamics, we introduce a novel mouth structure and the part-wise\ndeformation strategy to enhance the animation fidelity of the mouth. Finally,\nwe propose a regularization loss for precise rigging between Gaussians and 3DMM\nfaces. Moreover, we release DynamicFace, a video dataset with highly expressive\nfacial motions. Extensive experiments show the superiority of GeoAvatar\ncompared to state-of-the-art methods in reconstruction and novel animation\nscenarios.\n","authors":["SeungJun Moon","Hah Min Lew","Seungeun Lee","Ji-Su Kang","Gyeong-Moon Park"],"pdf_url":"https://arxiv.org/pdf/2507.18155v1.pdf","comment":"ICCV 2025, Project page: https://hahminlew.github.io/geoavatar/"},{"id":"http://arxiv.org/abs/2507.18153v1","updated":"2025-07-24T07:39:07Z","published":"2025-07-24T07:39:07Z","title":"When Noisy Labels Meet Class Imbalance on Graphs: A Graph Augmentation\n  Method with LLM and Pseudo Label","summary":"  Class-imbalanced graph node classification is a practical yet underexplored\nresearch problem. Although recent studies have attempted to address this issue,\nthey typically assume clean and reliable labels when processing\nclass-imbalanced graphs. This assumption often violates the nature of\nreal-world graphs, where labels frequently contain noise. Given this gap, this\npaper systematically investigates robust node classification for\nclass-imbalanced graphs with noisy labels. We propose GraphALP, a novel Graph\nAugmentation framework based on Large language models (LLMs) and\nPseudo-labeling techniques. Specifically, we design an LLM-based oversampling\nmethod to generate synthetic minority nodes, producing label-accurate minority\nnodes to alleviate class imbalance. Based on the class-balanced graphs, we\ndevelop a dynamically weighted pseudo-labeling method to obtain high-confidence\npseudo labels to reduce label noise ratio. Additionally, we implement a\nsecondary LLM-guided oversampling mechanism to mitigate potential class\ndistribution skew caused by pseudo labels. Experimental results show that\nGraphALP achieves superior performance over state-of-the-art methods on\nclass-imbalanced graphs with noisy labels.\n","authors":["Riting Xia","Rucong Wang","Yulin Liu","Anchen Li","Xueyan Liu","Yan Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.18153v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05345v2","updated":"2025-07-24T07:03:53Z","published":"2024-09-09T06:03:23Z","title":"Robust Non-adaptive Group Testing under Errors in Group Membership\n  Specifications","summary":"  Given $p$ samples, each of which may or may not be defective, group testing\n(GT) aims to determine their defect status by performing tests on $n < p$\n`groups', where a group is formed by mixing a subset of the $p$ samples.\nAssuming that the number of defective samples is very small compared to $p$, GT\nalgorithms have provided excellent recovery of the status of all $p$ samples\nwith even a small number of groups. Most existing methods, however, assume that\nthe group memberships are accurately specified. This assumption may not always\nbe true in all applications, due to various resource constraints. Such errors\ncould occur, eg, when a technician, preparing the groups in a laboratory,\nunknowingly mixes together an incorrect subset of samples as compared to what\nwas specified. We develop a new GT method, the Debiased Robust Lasso Test\nMethod (DRLT), that handles such group membership specification errors. The\nproposed DRLT method is based on an approach to debias, or reduce the inherent\nbias in, estimates produced by Lasso, a popular and effective sparse regression\ntechnique. We also provide theoretical upper bounds on the reconstruction error\nproduced by our estimator. Our approach is then combined with two carefully\ndesigned hypothesis tests respectively for (i) the identification of defective\nsamples in the presence of errors in group membership specifications, and (ii)\nthe identification of groups with erroneous membership specifications. The DRLT\napproach extends the literature on bias mitigation of statistical estimators\nsuch as the LASSO, to handle the important case when some of the measurements\ncontain outliers, due to factors such as group membership specification errors.\nWe present numerical results which show that our approach outperforms several\nbaselines and robust regression techniques for identification of defective\nsamples as well as erroneously specified groups.\n","authors":["Shuvayan Banerjee","Radhendushka Srivastava","James Saunderson","Ajit Rajwade"],"pdf_url":"https://arxiv.org/pdf/2409.05345v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18139v1","updated":"2025-07-24T07:01:52Z","published":"2025-07-24T07:01:52Z","title":"Neuromorphic Computing for Embodied Intelligence in Autonomous Systems:\n  Current Trends, Challenges, and Future Directions","summary":"  The growing need for intelligent, adaptive, and energy-efficient autonomous\nsystems across fields such as robotics, mobile agents (e.g., UAVs), and\nself-driving vehicles is driving interest in neuromorphic computing. By drawing\ninspiration from biological neural systems, neuromorphic approaches offer\npromising pathways to enhance the perception, decision-making, and\nresponsiveness of autonomous platforms. This paper surveys recent progress in\nneuromorphic algorithms, specialized hardware, and cross-layer optimization\nstrategies, with a focus on their deployment in real-world autonomous\nscenarios. Special attention is given to event-based dynamic vision sensors and\ntheir role in enabling fast, efficient perception. The discussion highlights\nnew methods that improve energy efficiency, robustness, adaptability, and\nreliability through the integration of spiking neural networks into autonomous\nsystem architectures. We integrate perspectives from machine learning,\nrobotics, neuroscience, and neuromorphic engineering to offer a comprehensive\nview of the state of the field. Finally, emerging trends and open challenges\nare explored, particularly in the areas of real-time decision-making, continual\nlearning, and the development of secure, resilient autonomous systems.\n","authors":["Alberto Marchisio","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2507.18139v1.pdf","comment":"To appear at the 31st IEEE International Symposium on On-Line Testing\n  and Robust System Design (IOLTS), Ischia, Italy, July 2025"},{"id":"http://arxiv.org/abs/2507.09305v3","updated":"2025-07-24T06:51:41Z","published":"2025-07-12T14:46:42Z","title":"DAA*: Deep Angular A Star for Image-based Path Planning","summary":"  Path smoothness is often overlooked in path imitation learning from expert\ndemonstrations. In this paper, we introduce a novel learning method, termed\ndeep angular A* (DAA*), by incorporating the proposed path angular freedom\n(PAF) into A* to improve path similarity through adaptive path smoothness. The\nPAF aims to explore the effect of move angles on path node expansion by finding\nthe trade-off between their minimum and maximum values, allowing for high\nadaptiveness for imitation learning. DAA* improves path optimality by closely\naligning with the reference path through joint optimization of path shortening\nand smoothing, which correspond to heuristic distance and PAF, respectively.\nThroughout comprehensive evaluations on 7 datasets, including 4 maze datasets,\n2 video-game datasets, and a real-world drone-view dataset containing 2\nscenarios, we demonstrate remarkable improvements of our DAA* over neural A* in\npath similarity between the predicted and reference paths with a shorter path\nlength when the shortest path is plausible, improving by 9.0% SPR, 6.9% ASIM,\nand 3.9% PSIM. Furthermore, when jointly learning pathfinding with both path\nloss and path probability map loss, DAA* significantly outperforms the\nstate-of-the-art TransPath by 6.3% SPR, 6.0% PSIM, and 3.7% ASIM. We also\ndiscuss the minor trade-off between path optimality and search efficiency where\napplicable. Our code and model weights are available at\nhttps://github.com/zwxu064/DAAStar.git.\n","authors":["Zhiwei Xu"],"pdf_url":"https://arxiv.org/pdf/2507.09305v3.pdf","comment":"International Conference on Computer Vision (ICCV), 2025"},{"id":"http://arxiv.org/abs/2507.17348v2","updated":"2025-07-24T06:38:39Z","published":"2025-07-23T09:28:52Z","title":"TOC-UCO: a comprehensive repository of tabular ordinal classification\n  datasets","summary":"  An ordinal classification (OC) problem corresponds to a special type of\nclassification characterised by the presence of a natural order relationship\namong the classes. This type of problem can be found in a number of real-world\napplications, motivating the design and development of many ordinal\nmethodologies over the last years. However, it is important to highlight that\nthe development of the OC field suffers from one main disadvantage: the lack of\na comprehensive set of datasets on which novel approaches to the literature\nhave to be benchmarked. In order to approach this objective, this manuscript\nfrom the University of C\\'ordoba (UCO), which have previous experience on the\nOC field, provides the literature with a publicly available repository of\ntabular data for a robust validation of novel OC approaches, namely TOC-UCO\n(Tabular Ordinal Classification repository of the UCO). Specifically, this\nrepository includes a set of $46$ tabular ordinal datasets, preprocessed under\na common framework and ensured to have a reasonable number of patterns and an\nappropriate class distribution. We also provide the sources and preprocessing\nsteps of each dataset, along with details on how to benchmark a novel approach\nusing the TOC-UCO repository. For this, indices for $30$ different randomised\ntrain-test partitions are provided to facilitate the reproducibility of the\nexperiments.\n","authors":["Rafael Ayllón-Gavilán","David Guijo-Rubio","Antonio Manuel Gómez-Orellana","Francisco Bérchez-Moreno","Víctor Manuel Vargas-Yun","Pedro A. Gutiérrez"],"pdf_url":"https://arxiv.org/pdf/2507.17348v2.pdf","comment":"25 single column pages, 5 figures, 7 tables"},{"id":"http://arxiv.org/abs/2507.18122v1","updated":"2025-07-24T06:17:39Z","published":"2025-07-24T06:17:39Z","title":"Maximizing Prefix-Confidence at Test-Time Efficiently Improves\n  Mathematical Reasoning","summary":"  Recent work has shown that language models can self-improve by maximizing\ntheir own confidence in their predictions, without relying on external\nverifiers or reward signals. In this work, we study the test-time scaling of\nlanguage models for mathematical reasoning tasks, where the model's own\nconfidence is used to select the most promising attempts. Surprisingly, we find\nthat we can achieve significant performance gains by continuing only the most\npromising attempt, selected by the model's prefix-confidence. We systematically\nevaluate prefix-confidence scaling on five mathematical reasoning datasets: the\nschool-level GSM8K and MATH500, and the competition-level AMC23, AIME24, and\nAIME25. We find that prefix-confidence scaling with prefixes of only 32 tokens\nachieves a better accuracy-compute trade-off than majority voting. Moreover,\nprefix-confidence scaling appears less susceptible than BoN to length biases.\nFinally, we also evaluate test-time training with prefix-confidence and find\nthat, while outperforming the base model, it does not improve over\nprefix-confidence scaling.\n","authors":["Matthias Otth","Jonas Hübotter","Ido Hakimi","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2507.18122v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11936v3","updated":"2025-07-24T06:15:29Z","published":"2025-07-16T06:03:08Z","title":"A Survey of Deep Learning for Geometry Problem Solving","summary":"  Geometry problem solving is a key area of mathematical reasoning, which is\nwidely involved in many important fields such as education, mathematical\nability assessment of artificial intelligence, and multimodal ability\nassessment. In recent years, the rapid development of deep learning technology,\nespecially the rise of multimodal large language models, has triggered a\nwidespread research boom. This paper provides a survey of the applications of\ndeep learning in geometry problem solving, including (i) a comprehensive\nsummary of the relevant tasks in geometry problem solving; (ii) a thorough\nreview of related deep learning methods; (iii) a detailed analysis of\nevaluation metrics and methods; and (iv) a critical discussion of the current\nchallenges and future directions that can be explored. Our goal is to provide a\ncomprehensive and practical reference of deep learning for geometry problem\nsolving to promote further developments in this field. We create a continuously\nupdated list of papers on GitHub: https://github.com/majianz/dl4gps.\n","authors":["Jianzhe Ma","Wenxuan Wang","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2507.11936v3.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2506.03590v3","updated":"2025-07-24T06:09:00Z","published":"2025-06-04T05:44:03Z","title":"VCDiag: Classifying Erroneous Waveforms for Failure Triage Acceleration","summary":"  Failure triage in design functional verification is critical but\ntime-intensive, relying on manual specification reviews, log inspections, and\nwaveform analyses. While machine learning (ML) has improved areas like stimulus\ngeneration and coverage closure, its application to RTL-level simulation\nfailure triage, particularly for large designs, remains limited. VCDiag offers\nan efficient, adaptable approach using VCD data to classify failing waveforms\nand pinpoint likely failure locations. In the largest experiment, VCDiag\nachieves over 94% accuracy in identifying the top three most likely modules.\nThe framework introduces a novel signal selection and statistical compression\napproach, achieving over 120x reduction in raw data size while preserving\nfeatures essential for classification. It can also be integrated into diverse\nVerilog/SystemVerilog designs and testbenches.\n","authors":["Minh Luu","Surya Jasper","Khoi Le","Evan Pan","Michael Quinn","Aakash Tyagi","Jiang Hu"],"pdf_url":"https://arxiv.org/pdf/2506.03590v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.16901v4","updated":"2025-07-24T06:08:33Z","published":"2023-05-26T13:14:05Z","title":"Generalizing Adam to Manifolds for Efficiently Training Transformers","summary":"  One of the primary reasons behind the success of neural networks has been the\nemergence of an array of new, highly-successful optimizers, perhaps most\nimportantly the Adam optimizer. It is widely used for training neural networks,\nyet notoriously hard to interpret. Lacking a clear physical intuition, Adam is\ndifficult to generalize to manifolds. Some attempts have been made to directly\napply parts of the Adam algorithm to manifolds or to find an underlying\nstructure, but a full generalization has remained elusive.\n  In this work a new approach is presented that leverages the special structure\nof the manifolds which are relevant for optimization of neural networks, such\nas the Stiefel manifold, the symplectic Stiefel manifold and the Grassmann\nmanifold: all of these are homogeneous spaces and as such admit a global\ntangent space representation - a common vector space (Lie subspace) in which\nall tangent spaces can easily be represented.\n  This global tangent space representation is used to perform all of the steps\nin the Adam optimizer and we are able to fully generalize the optimizer to\nmanifolds without a projection step. The resulting algorithm is then applied to\ntrain a transformer for which orthogonality constraints are enforced up to\nmachine precision and we observe significant speed-ups in the training process.\n","authors":["Benedikt Brantner"],"pdf_url":"https://arxiv.org/pdf/2305.16901v4.pdf","comment":"32 pages, 6 figures (some of which contain subfigures), presented at\n  Enumath2023 and Enumath2025"},{"id":"http://arxiv.org/abs/2507.18118v1","updated":"2025-07-24T06:05:56Z","published":"2025-07-24T06:05:56Z","title":"A Two-armed Bandit Framework for A/B Testing","summary":"  A/B testing is widely used in modern technology companies for policy\nevaluation and product deployment, with the goal of comparing the outcomes\nunder a newly-developed policy against a standard control. Various causal\ninference and reinforcement learning methods developed in the literature are\napplicable to A/B testing. This paper introduces a two-armed bandit framework\ndesigned to improve the power of existing approaches. The proposed procedure\nconsists of three main steps: (i) employing doubly robust estimation to\ngenerate pseudo-outcomes, (ii) utilizing a two-armed bandit framework to\nconstruct the test statistic, and (iii) applying a permutation-based method to\ncompute the $p$-value. We demonstrate the efficacy of the proposed method\nthrough asymptotic theories, numerical experiments and real-world data from a\nridesharing company, showing its superior performance in comparison to existing\nmethods.\n","authors":["Jinjuan Wang","Qianglin Wen","Yu Zhang","Xiaodong Yan","Chengchun Shi"],"pdf_url":"https://arxiv.org/pdf/2507.18118v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.16182v2","updated":"2025-07-24T06:01:17Z","published":"2025-07-22T02:53:13Z","title":"The Impact of Pseudo-Science in Financial Loans Risk Prediction","summary":"  We study the societal impact of pseudo-scientific assumptions for predicting\nthe behavior of people in a straightforward application of machine learning to\nrisk prediction in financial lending. This use case also exemplifies the impact\nof survival bias in loan return prediction. We analyze the models in terms of\ntheir accuracy and social cost, showing that the socially optimal model may not\nimply a significant accuracy loss for this downstream task. Our results are\nverified for commonly used learning methods and datasets. Our findings also\nshow that there is a natural dynamic when training models that suffer survival\nbias where accuracy slightly deteriorates, and whose recall and precision\nimproves with time. These results act as an illusion, leading the observer to\nbelieve that the system is getting better, when in fact the model is suffering\nfrom increasingly more unfairness and survival bias.\n","authors":["Bruno Scarone","Ricardo Baeza-Yates"],"pdf_url":"https://arxiv.org/pdf/2507.16182v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10610v3","updated":"2025-07-24T06:00:26Z","published":"2024-08-20T07:42:42Z","title":"On the Approximation of Stationary Processes using the ARMA Model","summary":"  We revisit an old problem related to Autoregressive Moving Average (ARMA)\nmodels, on quantifying and bounding the approximation error between a true\nstationary process $X_t$ and an ARMA model $Y_t$. We take the transfer function\nrepresentation of an ARMA model and show that the associated $L^{\\infty}$ norm\nprovides a valid alternate norm that controls the $L^2$ norm and has structural\nproperties comparable to the cepstral norm. We show that a certain subspace of\nstationary processes, which includes ARMA models, forms a Banach algebra under\nthe $L^{\\infty}$ norm that respects the group structure of $H^{\\infty}$\ntransfer functions. The natural definition of invertibility in this algebra is\nconsistent with the original definition of ARMA invertibility, and generalizes\nbetter to non-ARMA processes than Wiener's $\\ell^1$ condition. Finally, we\ncalculate some explicit approximation bounds in the simpler context of\ncontinuous transfer functions, and critique some heuristic ideas on Pad\\'e\napproximations and parsimonious models.\n","authors":["Anand Ganesh","Babhrubahan Bose","Anand Rajagopalan"],"pdf_url":"https://arxiv.org/pdf/2408.10610v3.pdf","comment":"16 pages, 1 figure"},{"id":"http://arxiv.org/abs/2507.18115v1","updated":"2025-07-24T05:56:25Z","published":"2025-07-24T05:56:25Z","title":"Agentic AI framework for End-to-End Medical Data Inference","summary":"  Building and deploying machine learning solutions in healthcare remains\nexpensive and labor-intensive due to fragmented preprocessing workflows, model\ncompatibility issues, and stringent data privacy constraints. In this work, we\nintroduce an Agentic AI framework that automates the entire clinical data\npipeline, from ingestion to inference, through a system of modular,\ntask-specific agents. These agents handle both structured and unstructured\ndata, enabling automatic feature selection, model selection, and preprocessing\nrecommendation without manual intervention. We evaluate the system on publicly\navailable datasets from geriatrics, palliative care, and colonoscopy imaging.\nFor example, in the case of structured data (anxiety data) and unstructured\ndata (colonoscopy polyps data), the pipeline begins with file-type detection by\nthe Ingestion Identifier Agent, followed by the Data Anonymizer Agent ensuring\nprivacy compliance, where we first identify the data type and then anonymize\nit. The Feature Extraction Agent identifies features using an embedding-based\napproach for tabular data, extracting all column names, and a multi-stage\nMedGemma-based approach for image data, which infers modality and disease name.\nThese features guide the Model-Data Feature Matcher Agent in selecting the\nbest-fit model from a curated repository. The Preprocessing Recommender Agent\nand Preprocessing Implementor Agent then apply tailored preprocessing based on\ndata type and model requirements. Finally, the ``Model Inference Agent\" runs\nthe selected model on the uploaded data and generates interpretable outputs\nusing tools like SHAP, LIME, and DETR attention maps. By automating these\nhigh-friction stages of the ML lifecycle, the proposed framework reduces the\nneed for repeated expert intervention, offering a scalable, cost-efficient\npathway for operationalizing AI in clinical environments.\n","authors":["Soorya Ram Shimgekar","Shayan Vassef","Abhay Goyal","Navin Kumar","Koustuv Saha"],"pdf_url":"https://arxiv.org/pdf/2507.18115v1.pdf","comment":"10 pages, 5 figures, 2 tables, BIBM conference"},{"id":"http://arxiv.org/abs/2507.18114v1","updated":"2025-07-24T05:55:28Z","published":"2025-07-24T05:55:28Z","title":"Nonconvex Optimization Framework for Group-Sparse Feedback\n  Linear-Quadratic Optimal Control I: Penalty Approach","summary":"  This paper develops a unified nonconvex optimization framework for the design\nof group-sparse feedback controllers in infinite-horizon linear-quadratic (LQ)\nproblems. We address two prominent extensions of the classical LQ problem: the\ndistributed LQ problem with fixed communication topology (DFT-LQ) and the\nsparse feedback LQ problem (SF-LQ), both of which are motivated by the need for\nscalable and structure-aware control in large-scale systems. Unlike existing\napproaches that rely on convex relaxations or are limited to block-diagonal\nstructures, we directly formulate the controller synthesis as a\nfinite-dimensional nonconvex optimization problem with group $\\ell_0$-norm\nregularization, capturing general sparsity patterns. We establish a connection\nbetween DFT-LQ and SF-LQ problems, showing that both can be addressed within\nour unified framework. Furthermore, we propose a penalty-based proximal\nalternating linearized minimization (PALM) algorithm and provide a rigorous\nconvergence analysis under mild assumptions, overcoming the lack of coercivity\nin the objective function. The proposed method admits efficient solvers for all\nsubproblems and guarantees global convergence to critical points. Our results\nfill a key gap in the literature by enabling the direct design of group-sparse\nfeedback gains with theoretical guarantees, without resorting to convex\nsurrogates or restrictive structural assumptions.\n","authors":["Lechen Feng","Xun Li","Yuan-Hua Ni"],"pdf_url":"https://arxiv.org/pdf/2507.18114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18113v1","updated":"2025-07-24T05:52:06Z","published":"2025-07-24T05:52:06Z","title":"Policy Disruption in Reinforcement Learning:Adversarial Attack with\n  Large Language Models and Critical State Identification","summary":"  Reinforcement learning (RL) has achieved remarkable success in fields like\nrobotics and autonomous driving, but adversarial attacks designed to mislead RL\nsystems remain challenging. Existing approaches often rely on modifying the\nenvironment or policy, limiting their practicality. This paper proposes an\nadversarial attack method in which existing agents in the environment guide the\ntarget policy to output suboptimal actions without altering the environment. We\npropose a reward iteration optimization framework that leverages large language\nmodels (LLMs) to generate adversarial rewards explicitly tailored to the\nvulnerabilities of the target agent, thereby enhancing the effectiveness of\ninducing the target agent toward suboptimal decision-making. Additionally, a\ncritical state identification algorithm is designed to pinpoint the target\nagent's most vulnerable states, where suboptimal behavior from the victim leads\nto significant degradation in overall performance. Experimental results in\ndiverse environments demonstrate the superiority of our method over existing\napproaches.\n","authors":["Junyong Jiang","Buwei Tian","Chenxing Xu","Songze Li","Lu Dong"],"pdf_url":"https://arxiv.org/pdf/2507.18113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18111v1","updated":"2025-07-24T05:45:41Z","published":"2025-07-24T05:45:41Z","title":"Percentile-Based Deep Reinforcement Learning and Reward Based\n  Personalization For Delay Aware RAN Slicing in O-RAN","summary":"  In this paper, we tackle the challenge of radio access network (RAN) slicing\nwithin an open RAN (O-RAN) architecture. Our focus centers on a network that\nincludes multiple mobile virtual network operators (MVNOs) competing for\nphysical resource blocks (PRBs) with the goal of meeting probabilistic delay\nupper bound constraints for their clients while minimizing PRB utilization.\nInitially, we derive a reward function based on the law of large numbers (LLN),\nthen implement practical modifications to adapt it for real-world experimental\nscenarios. We then propose our solution, the Percentile-based Delay-Aware Deep\nReinforcement Learning (PDA-DRL), which demonstrates its superiority over\nseveral baselines, including DRL models optimized for average delay\nconstraints, by achieving a 38\\% reduction in resultant average delay.\nFurthermore, we delve into the issue of model weight sharing among multiple\nMVNOs to develop a robust personalized model. We introduce a reward-based\npersonalization method where each agent prioritizes other agents' model weights\nbased on their performance. This technique surpasses traditional aggregation\nmethods, such as federated averaging, and strategies reliant on traffic\npatterns and model weight distance similarities.\n","authors":["Peyman Tehrani","Anas Alsoliman"],"pdf_url":"https://arxiv.org/pdf/2507.18111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18103v1","updated":"2025-07-24T05:29:18Z","published":"2025-07-24T05:29:18Z","title":"A New Pair of GloVes","summary":"  This report documents, describes, and evaluates new 2024 English GloVe\n(Global Vectors for Word Representation) models. While the original GloVe\nmodels built in 2014 have been widely used and found useful, languages and the\nworld continue to evolve and we thought that current usage could benefit from\nupdated models. Moreover, the 2014 models were not carefully documented as to\nthe exact data versions and preprocessing that were used, and we rectify this\nby documenting these new models. We trained two sets of word embeddings using\nWikipedia, Gigaword, and a subset of Dolma. Evaluation through vocabulary\ncomparison, direct testing, and NER tasks shows that the 2024 vectors\nincorporate new culturally and linguistically relevant words, perform\ncomparably on structural tasks like analogy and similarity, and demonstrate\nimproved performance on recent, temporally dependent NER datasets such as\nnon-Western newswire data.\n","authors":["Riley Carlson","John Bauer","Christopher D. Manning"],"pdf_url":"https://arxiv.org/pdf/2507.18103v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18099v1","updated":"2025-07-24T05:23:02Z","published":"2025-07-24T05:23:02Z","title":"Comparison of Segmentation Methods in Remote Sensing for Land Use Land\n  Cover","summary":"  Land Use Land Cover (LULC) mapping is essential for urban and resource\nplanning, and is one of the key elements in developing smart and sustainable\ncities.This study evaluates advanced LULC mapping techniques, focusing on\nLook-Up Table (LUT)-based Atmospheric Correction applied to Cartosat\nMultispectral (MX) sensor images, followed by supervised and semi-supervised\nlearning models for LULC prediction. We explore DeeplabV3+ and Cross-Pseudo\nSupervision (CPS). The CPS model is further refined with dynamic weighting,\nenhancing pseudo-label reliability during training. This comprehensive approach\nanalyses the accuracy and utility of LULC mapping techniques for various urban\nplanning applications. A case study of Hyderabad, India, illustrates\nsignificant land use changes due to rapid urbanization. By analyzing Cartosat\nMX images over time, we highlight shifts such as urban sprawl, shrinking green\nspaces, and expanding industrial areas. This demonstrates the practical utility\nof these techniques for urban planners and policymakers.\n","authors":["Naman Srivastava","Joel D Joy","Yash Dixit","Swarup E","Rakshit Ramesh"],"pdf_url":"https://arxiv.org/pdf/2507.18099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18098v1","updated":"2025-07-24T05:19:07Z","published":"2025-07-24T05:19:07Z","title":"Learning from Hard Labels with Additional Supervision on\n  Non-Hard-Labeled Classes","summary":"  In scenarios where training data is limited due to observation costs or data\nscarcity, enriching the label information associated with each instance becomes\ncrucial for building high-accuracy classification models. In such contexts, it\nis often feasible to obtain not only hard labels but also {\\it additional\nsupervision}, such as the confidences for the hard labels. This setting\nnaturally raises fundamental questions: {\\it What kinds of additional\nsupervision are intrinsically beneficial?} And {\\it how do they contribute to\nimproved generalization performance?} To address these questions, we propose a\ntheoretical framework that treats both hard labels and additional supervision\nas probability distributions, and constructs soft labels through their affine\ncombination. Our theoretical analysis reveals that the essential component of\nadditional supervision is not the confidence score of the assigned hard label,\nbut rather the information of the distribution over the non-hard-labeled\nclasses. Moreover, we demonstrate that the additional supervision and the\nmixing coefficient contribute to the refinement of soft labels in complementary\nroles. Intuitively, in the probability simplex, the additional supervision\ndetermines the direction in which the deterministic distribution representing\nthe hard label should be adjusted toward the true label distribution, while the\nmixing coefficient controls the step size along that direction. Through\ngeneralization error analysis, we theoretically characterize how the additional\nsupervision and its mixing coefficient affect both the convergence rate and\nasymptotic value of the error bound. Finally, we experimentally demonstrate\nthat, based on our theory, designing additional supervision can lead to\nimproved classification accuracy, even when utilized in a simple manner.\n","authors":["Kosuke Sugiyama","Masato Uchida"],"pdf_url":"https://arxiv.org/pdf/2507.18098v1.pdf","comment":"32 pages, 11 figures"},{"id":"http://arxiv.org/abs/2507.15205v2","updated":"2025-07-24T05:15:18Z","published":"2025-07-21T03:12:54Z","title":"Long-Short Distance Graph Neural Networks and Improved Curriculum\n  Learning for Emotion Recognition in Conversation","summary":"  Emotion Recognition in Conversation (ERC) is a practical and challenging\ntask. This paper proposes a novel multimodal approach, the Long-Short Distance\nGraph Neural Network (LSDGNN). Based on the Directed Acyclic Graph (DAG), it\nconstructs a long-distance graph neural network and a short-distance graph\nneural network to obtain multimodal features of distant and nearby utterances,\nrespectively. To ensure that long- and short-distance features are as distinct\nas possible in representation while enabling mutual influence between the two\nmodules, we employ a Differential Regularizer and incorporate a BiAffine Module\nto facilitate feature interaction. In addition, we propose an Improved\nCurriculum Learning (ICL) to address the challenge of data imbalance. By\ncomputing the similarity between different emotions to emphasize the shifts in\nsimilar emotions, we design a \"weighted emotional shift\" metric and develop a\ndifficulty measurer, enabling a training process that prioritizes learning easy\nsamples before harder ones. Experimental results on the IEMOCAP and MELD\ndatasets demonstrate that our model outperforms existing benchmarks.\n","authors":["Xinran Li","Xiujuan Xu","Jiaqi Qiao"],"pdf_url":"https://arxiv.org/pdf/2507.15205v2.pdf","comment":"Accepted by the 28th European Conference on Artificial Intelligence\n  (ECAI 2025)"},{"id":"http://arxiv.org/abs/2506.15690v3","updated":"2025-07-24T05:08:02Z","published":"2025-05-26T22:10:52Z","title":"LLM Web Dynamics: Tracing Model Collapse in a Network of LLMs","summary":"  The increasing use of synthetic data from the public Internet has enhanced\ndata usage efficiency in large language model (LLM) training. However, the\npotential threat of model collapse remains insufficiently explored. Existing\nstudies primarily examine model collapse in a single model setting or rely\nsolely on statistical surrogates. In this work, we introduce LLM Web Dynamics\n(LWD), an efficient framework for investigating model collapse at the network\nlevel. By simulating the Internet with a retrieval-augmented generation (RAG)\ndatabase, we analyze the convergence pattern of model outputs. Furthermore, we\nprovide theoretical guarantees for this convergence by drawing an analogy to\ninteracting Gaussian Mixture Models.\n","authors":["Tianyu Wang","Akira Horiguchi","Lingyou Pang","Carey E. Priebe"],"pdf_url":"https://arxiv.org/pdf/2506.15690v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.12312v4","updated":"2025-07-24T05:01:33Z","published":"2024-05-20T18:14:33Z","title":"A Principled Approach for Data Bias Mitigation","summary":"  The widespread use of machine learning and data-driven algorithms for\ndecision making has been steadily increasing over many years. \\emph{Bias} in\nthe data can adversely affect this decision-making. We present a new mitigation\nstrategy to address data bias. Our methods are explainable and come with\nmathematical guarantees of correctness. They can take advantage of new work on\ntable discovery to find new tuples that can be added to a dataset to create\nreal datasets that are unbiased or less biased. Our framework covers data with\nnon-binary labels and with multiple sensitive attributes. Hence, we are able to\nmeasure and mitigate bias that does not appear over a single attribute (or\nfeature), but only intersectionally, when considering a combination of\nattributes. We evaluate our techniques on publicly available datasets and\nprovide a theoretical analysis of our results, highlighting novel insights into\ndata bias.\n","authors":["Bruno Scarone","Alfredo Viola","Renée J. Miller","Ricardo Baeza-Yates"],"pdf_url":"https://arxiv.org/pdf/2405.12312v4.pdf","comment":"Accepted to AIES 2025"},{"id":"http://arxiv.org/abs/2506.16685v2","updated":"2025-07-24T04:54:15Z","published":"2025-06-20T01:57:47Z","title":"Compliant Residual DAgger: Improving Real-World Contact-Rich\n  Manipulation with Human Corrections","summary":"  We address key challenges in Dataset Aggregation (DAgger) for real-world\ncontact-rich manipulation: how to collect informative human correction data and\nhow to effectively update policies with this new data. We introduce Compliant\nResidual DAgger (CR-DAgger), which contains two novel components: 1) a\nCompliant Intervention Interface that leverages compliance control, allowing\nhumans to provide gentle, accurate delta action corrections without\ninterrupting the ongoing robot policy execution; and 2) a Compliant Residual\nPolicy formulation that learns from human corrections while incorporating force\nfeedback and force control. Our system significantly enhances performance on\nprecise contact-rich manipulation tasks using minimal correction data,\nimproving base policy success rates by over 50\\% on two challenging tasks (book\nflipping and belt assembly) while outperforming both retraining-from-scratch\nand finetuning approaches. Through extensive real-world experiments, we provide\npractical guidance for implementing effective DAgger in real-world robot\nlearning tasks. Result videos are available at:\nhttps://compliant-residual-dagger.github.io/\n","authors":["Xiaomeng Xu","Yifan Hou","Zeyi Liu","Shuran Song"],"pdf_url":"https://arxiv.org/pdf/2506.16685v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04379v2","updated":"2025-07-24T04:36:04Z","published":"2024-02-06T20:35:28Z","title":"Fine-Tuned Language Models Generate Stable Inorganic Materials as Text","summary":"  We propose fine-tuning large language models for generation of stable\nmaterials. While unorthodox, fine-tuning large language models on text-encoded\natomistic data is simple to implement yet reliable, with around 90% of sampled\nstructures obeying physical constraints on atom positions and charges. Using\nenergy above hull calculations from both learned ML potentials and\ngold-standard DFT calculations, we show that our strongest model (fine-tuned\nLLaMA-2 70B) can generate materials predicted to be metastable at about twice\nthe rate (49% vs 28%) of CDVAE, a competing diffusion model. Because of text\nprompting's inherent flexibility, our models can simultaneously be used for\nunconditional generation of stable material, infilling of partial structures\nand text-conditional generation. Finally, we show that language models' ability\nto capture key symmetries of crystal structures improves with model scale,\nsuggesting that the biases of pretrained LLMs are surprisingly well-suited for\natomistic data.\n","authors":["Nate Gruver","Anuroop Sriram","Andrea Madotto","Andrew Gordon Wilson","C. Lawrence Zitnick","Zachary Ulissi"],"pdf_url":"https://arxiv.org/pdf/2402.04379v2.pdf","comment":"ICLR 2024. Code available at:\n  https://github.com/facebookresearch/crystal-llm"},{"id":"http://arxiv.org/abs/2308.01358v2","updated":"2025-07-24T04:23:22Z","published":"2023-08-02T18:02:00Z","title":"Compressed and distributed least-squares regression: convergence rates\n  with applications to Federated Learning","summary":"  In this paper, we investigate the impact of compression on stochastic\ngradient algorithms for machine learning, a technique widely used in\ndistributed and federated learning. We underline differences in terms of\nconvergence rates between several unbiased compression operators, that all\nsatisfy the same condition on their variance, thus going beyond the classical\nworst-case analysis. To do so, we focus on the case of least-squares regression\n(LSR) and analyze a general stochastic approximation algorithm for minimizing\nquadratic functions relying on a random field. We consider weak assumptions on\nthe random field, tailored to the analysis (specifically, expected H\\\"older\nregularity), and on the noise covariance, enabling the analysis of various\nrandomizing mechanisms, including compression. We then extend our results to\nthe case of federated learning.\n  More formally, we highlight the impact on the convergence of the covariance\n$\\mathfrak{C}_{\\mathrm{ania}}$ of the additive noise induced by the algorithm.\nWe demonstrate despite the non-regularity of the stochastic field, that the\nlimit variance term scales with $\\mathrm{Tr}(\\mathfrak{C}_{\\mathrm{ania}}\nH^{-1})/K$ (where $H$ is the Hessian of the optimization problem and $K$ the\nnumber of iterations) generalizing the rate for the vanilla LSR case where it\nis $\\sigma^2 \\mathrm{Tr}(H H^{-1}) / K = \\sigma^2 d / K$ (Bach and Moulines,\n2013). Then, we analyze the dependency of $\\mathfrak{C}_{\\mathrm{ania}}$ on the\ncompression strategy and ultimately its impact on convergence, first in the\ncentralized case, then in two heterogeneous FL frameworks.\n","authors":["Constantin Philippenko","Aymeric Dieuleveut"],"pdf_url":"https://arxiv.org/pdf/2308.01358v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06764v2","updated":"2025-07-24T03:58:47Z","published":"2025-02-10T18:44:25Z","title":"History-Guided Video Diffusion","summary":"  Classifier-free guidance (CFG) is a key technique for improving conditional\ngeneration in diffusion models, enabling more accurate control while enhancing\nsample quality. It is natural to extend this technique to video diffusion,\nwhich generates video conditioned on a variable number of context frames,\ncollectively referred to as history. However, we find two key challenges to\nguiding with variable-length history: architectures that only support\nfixed-size conditioning, and the empirical observation that CFG-style history\ndropout performs poorly. To address this, we propose the Diffusion Forcing\nTransformer (DFoT), a video diffusion architecture and theoretically grounded\ntraining objective that jointly enable conditioning on a flexible number of\nhistory frames. We then introduce History Guidance, a family of guidance\nmethods uniquely enabled by DFoT. We show that its simplest form, vanilla\nhistory guidance, already significantly improves video generation quality and\ntemporal consistency. A more advanced method, history guidance across time and\nfrequency further enhances motion dynamics, enables compositional\ngeneralization to out-of-distribution history, and can stably roll out\nextremely long videos. Project website: https://boyuan.space/history-guidance\n","authors":["Kiwhan Song","Boyuan Chen","Max Simchowitz","Yilun Du","Russ Tedrake","Vincent Sitzmann"],"pdf_url":"https://arxiv.org/pdf/2502.06764v2.pdf","comment":"ICML 2025. Project website: https://boyuan.space/history-guidance"},{"id":"http://arxiv.org/abs/2507.18073v1","updated":"2025-07-24T03:55:19Z","published":"2025-07-24T03:55:19Z","title":"Squeeze10-LLM: Squeezing LLMs' Weights by 10 Times via a Staged\n  Mixed-Precision Quantization Method","summary":"  Deploying large language models (LLMs) is challenging due to their massive\nparameters and high computational costs. Ultra low-bit quantization can\nsignificantly reduce storage and accelerate inference, but extreme compression\n(i.e., mean bit-width <= 2) often leads to severe performance degradation. To\naddress this, we propose Squeeze10-LLM, effectively \"squeezing\" 16-bit LLMs'\nweights by 10 times. Specifically, Squeeze10-LLM is a staged mixed-precision\npost-training quantization (PTQ) framework and achieves an average of 1.6 bits\nper weight by quantizing 80% of the weights to 1 bit and 20% to 4 bits. We\nintroduce Squeeze10LLM with two key innovations: Post-Binarization Activation\nRobustness (PBAR) and Full Information Activation Supervision (FIAS). PBAR is a\nrefined weight significance metric that accounts for the impact of quantization\non activations, improving accuracy in low-bit settings. FIAS is a strategy that\npreserves full activation information during quantization to mitigate\ncumulative error propagation across layers. Experiments on LLaMA and LLaMA2\nshow that Squeeze10-LLM achieves state-of-the-art performance for sub-2bit\nweight-only quantization, improving average accuracy from 43% to 56% on six\nzero-shot classification tasks--a significant boost over existing PTQ methods.\nOur code will be released upon publication.\n","authors":["Qingcheng Zhu","Yangyang Ren","Linlin Yang","Mingbao Lin","Yanjing Li","Sheng Xu","Zichao Feng","Haodong Zhu","Yuguang Yang","Juan Zhang","Runqi Wang","Baochang Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.18073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18072v1","updated":"2025-07-24T03:55:04Z","published":"2025-07-24T03:55:04Z","title":"C-AAE: Compressively Anonymizing Autoencoders for Privacy-Preserving\n  Activity Recognition in Healthcare Sensor Streams","summary":"  Wearable accelerometers and gyroscopes encode fine-grained behavioural\nsignatures that can be exploited to re-identify users, making privacy\nprotection essential for healthcare applications. We introduce C-AAE, a\ncompressive anonymizing autoencoder that marries an Anonymizing AutoEncoder\n(AAE) with Adaptive Differential Pulse-Code Modulation (ADPCM). The AAE first\nprojects raw sensor windows into a latent space that retains activity-relevant\nfeatures while suppressing identity cues. ADPCM then differentially encodes\nthis latent stream, further masking residual identity information and shrinking\nthe bitrate. Experiments on the MotionSense and PAMAP2 datasets show that C-AAE\ncuts user re-identification F1 scores by 10-15 percentage points relative to\nAAE alone, while keeping activity-recognition F1 within 5 percentage points of\nthe unprotected baseline. ADPCM also reduces data volume by roughly 75 %,\neasing transmission and storage overheads. These results demonstrate that C-AAE\noffers a practical route to balancing privacy and utility in continuous,\nsensor-based activity recognition for healthcare.\n","authors":["Ryusei Fujimoto","Yugo Nakamura","Yutaka Arakawa"],"pdf_url":"https://arxiv.org/pdf/2507.18072v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18071v1","updated":"2025-07-24T03:50:32Z","published":"2025-07-24T03:50:32Z","title":"Group Sequence Policy Optimization","summary":"  This paper introduces Group Sequence Policy Optimization (GSPO), our stable,\nefficient, and performant reinforcement learning algorithm for training large\nlanguage models. Unlike previous algorithms that adopt token-level importance\nratios, GSPO defines the importance ratio based on sequence likelihood and\nperforms sequence-level clipping, rewarding, and optimization. We demonstrate\nthat GSPO achieves superior training efficiency and performance compared to the\nGRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and\nhas the potential for simplifying the design of RL infrastructure. These merits\nof GSPO have contributed to the remarkable improvements in the latest Qwen3\nmodels.\n","authors":["Chujie Zheng","Shixuan Liu","Mingze Li","Xiong-Hui Chen","Bowen Yu","Chang Gao","Kai Dang","Yuqiong Liu","Rui Men","An Yang","Jingren Zhou","Junyang Lin"],"pdf_url":"https://arxiv.org/pdf/2507.18071v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.01144v5","updated":"2025-07-24T03:46:03Z","published":"2025-01-02T08:57:00Z","title":"BlockDialect: Block-wise Fine-grained Mixed Format Quantization for\n  Energy-Efficient LLM Inference","summary":"  The rapidly increasing size of large language models (LLMs) presents\nsignificant challenges in memory usage and computational costs. Quantizing both\nweights and activations can address these issues, with hardware-supported\nfine-grained scaling emerging as a promising solution to mitigate outliers.\nHowever, existing methods struggle to capture nuanced block data distributions.\nWe propose BlockDialect, a block-wise fine-grained mixed format technique that\nassigns a per-block optimal number format from a formatbook for better data\nrepresentation. Additionally, we introduce DialectFP4, a formatbook of FP4\nvariants (akin to dialects) that adapt to diverse data distributions. To\nleverage this efficiently, we propose a two-stage approach for online\nDialectFP4 activation quantization. Importantly, DialectFP4 ensures energy\nefficiency by selecting representable values as scaled integers compatible with\nlow-precision integer arithmetic. BlockDialect achieves 10.78% (7.48%) accuracy\ngain on the LLaMA3-8B (LLaMA2-7B) model compared to MXFP4 format with lower bit\nusage per data, while being only 5.45% (2.69%) below full precision even when\nquantizing full-path matrix multiplication. Focusing on how to represent over\nhow to scale, our work presents a promising path for energy-efficient LLM\ninference.\n","authors":["Wonsuk Jang","Thierry Tambe"],"pdf_url":"https://arxiv.org/pdf/2501.01144v5.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2507.18067v1","updated":"2025-07-24T03:42:06Z","published":"2025-07-24T03:42:06Z","title":"Multiscale Neural PDE Surrogates for Prediction and Downscaling:\n  Application to Ocean Currents","summary":"  Accurate modeling of physical systems governed by partial differential\nequations is a central challenge in scientific computing. In oceanography,\nhigh-resolution current data are critical for coastal management, environmental\nmonitoring, and maritime safety. However, available satellite products, such as\nCopernicus data for sea water velocity at ~0.08 degrees spatial resolution and\nglobal ocean models, often lack the spatial granularity required for detailed\nlocal analyses. In this work, we (a) introduce a supervised deep learning\nframework based on neural operators for solving PDEs and providing arbitrary\nresolution solutions, and (b) propose downscaling models with an application to\nCopernicus ocean current data. Additionally, our method can model surrogate\nPDEs and predict solutions at arbitrary resolution, regardless of the input\nresolution. We evaluated our model on real-world Copernicus ocean current data\nand synthetic Navier-Stokes simulation datasets.\n","authors":["Abdessamad El-Kabid","Loubna Benabbou","Redouane Lguensat","Alex Hernández-García"],"pdf_url":"https://arxiv.org/pdf/2507.18067v1.pdf","comment":"Workshop @ ICML2025"},{"id":"http://arxiv.org/abs/2503.06079v2","updated":"2025-07-24T03:38:00Z","published":"2025-03-08T06:01:10Z","title":"Fixing the Pitfalls of Probabilistic Time-Series Forecasting Evaluation\n  by Kernel Quadrature","summary":"  Despite the significance of probabilistic time-series forecasting models,\ntheir evaluation metrics often involve intractable integrations. The most\nwidely used metric, the continuous ranked probability score (CRPS), is a\nstrictly proper scoring function; however, its computation requires\napproximation. We found that popular CRPS estimators--specifically, the\nquantile-based estimator implemented in the widely used GluonTS library and the\nprobability-weighted moment approximation--both exhibit inherent estimation\nbiases. These biases lead to crude approximations, resulting in improper\nrankings of forecasting model performance when CRPS values are close. To\naddress this issue, we introduced a kernel quadrature approach that leverages\nan unbiased CRPS estimator and employs cubature construction for scalable\ncomputation. Empirically, our approach consistently outperforms the two widely\nused CRPS estimators.\n","authors":["Masaki Adachi","Masahiro Fujisawa","Michael A Osborne"],"pdf_url":"https://arxiv.org/pdf/2503.06079v2.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2212.10678v4","updated":"2025-07-24T03:20:40Z","published":"2022-12-20T22:41:24Z","title":"Causally Testing Gender Bias in LLMs: A Case Study on Occupational Bias","summary":"  Generated texts from large language models (LLMs) have been shown to exhibit\na variety of harmful, human-like biases against various demographics. These\nfindings motivate research efforts aiming to understand and measure such\neffects. This paper introduces a causal formulation for bias measurement in\ngenerative language models. Based on this theoretical foundation, we outline a\nlist of desiderata for designing robust bias benchmarks. We then propose a\nbenchmark called OccuGender, with a bias-measuring procedure to investigate\noccupational gender bias. We test several state-of-the-art open-source LLMs on\nOccuGender, including Llama, Mistral, and their instruction-tuned versions. The\nresults show that these models exhibit substantial occupational gender bias.\nLastly, we discuss prompting strategies for bias mitigation and an extension of\nour causal formulation to illustrate the generalizability of our framework. Our\ncode and data https://github.com/chenyuen0103/gender-bias.\n","authors":["Yuen Chen","Vethavikashini Chithrra Raghuram","Justus Mattern","Rada Mihalcea","Zhijing Jin"],"pdf_url":"https://arxiv.org/pdf/2212.10678v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14445v2","updated":"2025-07-24T03:19:19Z","published":"2024-04-20T08:08:28Z","title":"A Multi-Faceted Evaluation Framework for Assessing Synthetic Data\n  Generated by Large Language Models","summary":"  The rapid advancements in generative AI and large language models (LLMs) have\nopened up new avenues for producing synthetic data, particularly in the realm\nof structured tabular formats, such as product reviews. Despite the potential\nbenefits, concerns regarding privacy leakage have surfaced, especially when\npersonal information is utilized in the training datasets. In addition, there\nis an absence of a comprehensive evaluation framework capable of quantitatively\nmeasuring the quality of the generated synthetic data and their utility for\ndownstream tasks. In response to this gap, we introduce SynEval, an open-source\nevaluation framework designed to assess the fidelity, utility, and privacy\npreservation of synthetically generated tabular data via a suite of diverse\nevaluation metrics. We validate the efficacy of our proposed framework -\nSynEval - by applying it to synthetic product review data generated by three\nstate-of-the-art LLMs: ChatGPT, Claude, and Llama. Our experimental findings\nilluminate the trade-offs between various evaluation metrics in the context of\nsynthetic data generation. Furthermore, SynEval stands as a critical instrument\nfor researchers and practitioners engaged with synthetic tabular data,,\nempowering them to judiciously determine the suitability of the generated data\nfor their specific applications, with an emphasis on upholding user privacy.\n","authors":["Yefeng Yuan","Yuhong Liu","Liang Cheng"],"pdf_url":"https://arxiv.org/pdf/2404.14445v2.pdf","comment":"10 pages, 1 figure, 4 tables"},{"id":"http://arxiv.org/abs/2507.18055v1","updated":"2025-07-24T03:12:16Z","published":"2025-07-24T03:12:16Z","title":"Privacy-Preserving Synthetic Review Generation with Diverse Writing\n  Styles Using LLMs","summary":"  The increasing use of synthetic data generated by Large Language Models\n(LLMs) presents both opportunities and challenges in data-driven applications.\nWhile synthetic data provides a cost-effective, scalable alternative to\nreal-world data to facilitate model training, its diversity and privacy risks\nremain underexplored. Focusing on text-based synthetic data, we propose a\ncomprehensive set of metrics to quantitatively assess the diversity (i.e.,\nlinguistic expression, sentiment, and user perspective), and privacy (i.e.,\nre-identification risk and stylistic outliers) of synthetic datasets generated\nby several state-of-the-art LLMs. Experiment results reveal significant\nlimitations in LLMs' capabilities in generating diverse and privacy-preserving\nsynthetic data. Guided by the evaluation results, a prompt-based approach is\nproposed to enhance the diversity of synthetic reviews while preserving\nreviewer privacy.\n","authors":["Tevin Atwal","Chan Nam Tieu","Yefeng Yuan","Zhan Shi","Yuhong Liu","Liang Cheng"],"pdf_url":"https://arxiv.org/pdf/2507.18055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.06021v2","updated":"2025-07-24T02:14:16Z","published":"2025-06-06T12:12:02Z","title":"Unisoma: A Unified Transformer-based Solver for Multi-Solid Systems","summary":"  Multi-solid systems are foundational to a wide range of real-world\napplications, yet modeling their complex interactions remains challenging.\nExisting deep learning methods predominantly rely on implicit modeling, where\nthe factors influencing solid deformation are not explicitly represented but\nare instead indirectly learned. However, as the number of solids increases,\nthese methods struggle to accurately capture intricate physical interactions.\nIn this paper, we introduce a novel explicit modeling paradigm that\nincorporates factors influencing solid deformation through structured modules.\nSpecifically, we present Unisoma, a unified and flexible Transformer-based\nmodel capable of handling variable numbers of solids. Unisoma directly captures\nphysical interactions using contact modules and adaptive interaction allocation\nmechanism, and learns the deformation through a triplet relationship. Compared\nto implicit modeling techniques, explicit modeling is more well-suited for\nmulti-solid systems with diverse coupling patterns, as it enables detailed\ntreatment of each solid while preventing information blending and confusion.\nExperimentally, Unisoma achieves consistent state-of-the-art performance across\nseven well-established datasets and two complex multi-solid tasks. Code is\navaiable at https://github.com/therontau0054/Unisoma.\n","authors":["Shilong Tao","Zhe Feng","Haonan Sun","Zhanxing Zhu","Yunhuai Liu"],"pdf_url":"https://arxiv.org/pdf/2506.06021v2.pdf","comment":"Proceedings of the 42nd International Conference on Machine Learning"},{"id":"http://arxiv.org/abs/2507.18031v1","updated":"2025-07-24T02:04:58Z","published":"2025-07-24T02:04:58Z","title":"ViGText: Deepfake Image Detection with Vision-Language Model\n  Explanations and Graph Neural Networks","summary":"  The rapid rise of deepfake technology, which produces realistic but\nfraudulent digital content, threatens the authenticity of media. Traditional\ndeepfake detection approaches often struggle with sophisticated, customized\ndeepfakes, especially in terms of generalization and robustness against\nmalicious attacks. This paper introduces ViGText, a novel approach that\nintegrates images with Vision Large Language Model (VLLM) Text explanations\nwithin a Graph-based framework to improve deepfake detection. The novelty of\nViGText lies in its integration of detailed explanations with visual data, as\nit provides a more context-aware analysis than captions, which often lack\nspecificity and fail to reveal subtle inconsistencies. ViGText systematically\ndivides images into patches, constructs image and text graphs, and integrates\nthem for analysis using Graph Neural Networks (GNNs) to identify deepfakes.\nThrough the use of multi-level feature extraction across spatial and frequency\ndomains, ViGText captures details that enhance its robustness and accuracy to\ndetect sophisticated deepfakes. Extensive experiments demonstrate that ViGText\nsignificantly enhances generalization and achieves a notable performance boost\nwhen it detects user-customized deepfakes. Specifically, average F1 scores rise\nfrom 72.45% to 98.32% under generalization evaluation, and reflects the model's\nsuperior ability to generalize to unseen, fine-tuned variations of stable\ndiffusion models. As for robustness, ViGText achieves an increase of 11.1% in\nrecall compared to other deepfake detection approaches. When facing targeted\nattacks that exploit its graph-based architecture, ViGText limits\nclassification performance degradation to less than 4%. ViGText uses detailed\nvisual and textual analysis to set a new standard for detecting deepfakes,\nhelping ensure media authenticity and information integrity.\n","authors":["Ahmad ALBarqawi","Mahmoud Nazzal","Issa Khalil","Abdallah Khreishah","NhatHai Phan"],"pdf_url":"https://arxiv.org/pdf/2507.18031v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15087v2","updated":"2025-07-24T01:49:32Z","published":"2024-09-23T15:01:09Z","title":"AI Workflow, External Validation, and Development in Eye Disease\n  Diagnosis","summary":"  Timely disease diagnosis is challenging due to increasing disease burdens and\nlimited clinician availability. AI shows promise in diagnosis accuracy but\nfaces real-world application issues due to insufficient validation in clinical\nworkflows and diverse populations. This study addresses gaps in medical AI\ndownstream accountability through a case study on age-related macular\ndegeneration (AMD) diagnosis and severity classification. We designed and\nimplemented an AI-assisted diagnostic workflow for AMD, comparing diagnostic\nperformance with and without AI assistance among 24 clinicians from 12\ninstitutions with real patient data sampled from the Age-Related Eye Disease\nStudy (AREDS). Additionally, we demonstrated continual enhancement of an\nexisting AI model by incorporating approximately 40,000 additional medical\nimages (named AREDS2 dataset). The improved model was then systematically\nevaluated using both AREDS and AREDS2 test sets, as well as an external test\nset from Singapore. AI assistance markedly enhanced diagnostic accuracy and\nclassification for 23 out of 24 clinicians, with the average F1-score\nincreasing by 20% from 37.71 (Manual) to 45.52 (Manual + AI) (P-value <\n0.0001), achieving an improvement of over 50% in some cases. In terms of\nefficiency, AI assistance reduced diagnostic times for 17 out of the 19\nclinicians tracked, with time savings of up to 40%. Furthermore, a model\nequipped with continual learning showed robust performance across three\nindependent datasets, recording a 29% increase in accuracy, and elevating the\nF1-score from 42 to 54 in the Singapore population.\n","authors":["Qingyu Chen","Tiarnan D L Keenan","Elvira Agron","Alexis Allot","Emily Guan","Bryant Duong","Amr Elsawy","Benjamin Hou","Cancan Xue","Sanjeeb Bhandari","Geoffrey Broadhead","Chantal Cousineau-Krieger","Ellen Davis","William G Gensheimer","David Grasic","Seema Gupta","Luis Haddock","Eleni Konstantinou","Tania Lamba","Michele Maiberger","Dimosthenis Mantopoulos","Mitul C Mehta","Ayman G Nahri","Mutaz AL-Nawaflh","Arnold Oshinsky","Brittany E Powell","Boonkit Purt","Soo Shin","Hillary Stiefel","Alisa T Thavikulwat","Keith James Wroblewski","Tham Yih Chung","Chui Ming Gemmy Cheung","Ching-Yu Cheng","Emily Y Chew","Michelle R. Hribar","Michael F. Chiang","Zhiyong Lu"],"pdf_url":"https://arxiv.org/pdf/2409.15087v2.pdf","comment":"Published in JAMA Network Open,\n  doi:10.1001/jamanetworkopen.2025.17204"},{"id":"http://arxiv.org/abs/2507.18022v1","updated":"2025-07-24T01:47:34Z","published":"2025-07-24T01:47:34Z","title":"Does visualization help AI understand data?","summary":"  Charts and graphs help people analyze data, but can they also be useful to AI\nsystems? To investigate this question, we perform a series of experiments with\ntwo commercial vision-language models: GPT 4.1 and Claude 3.5. Across three\nrepresentative analysis tasks, the two systems describe synthetic datasets more\nprecisely and accurately when raw data is accompanied by a scatterplot,\nespecially as datasets grow in complexity. Comparison with two baselines --\nproviding a blank chart and a chart with mismatched data -- shows that the\nimproved performance is due to the content of the charts. Our results are\ninitial evidence that AI systems, like humans, can benefit from visualization.\n","authors":["Victoria R. Li","Johnathan Sun","Martin Wattenberg"],"pdf_url":"https://arxiv.org/pdf/2507.18022v1.pdf","comment":"5 pages, 6 figures"},{"id":"http://arxiv.org/abs/2507.18021v1","updated":"2025-07-24T01:31:49Z","published":"2025-07-24T01:31:49Z","title":"Zeroth-order log-concave sampling","summary":"  We study the zeroth-order query complexity of log-concave sampling,\nspecifically uniform sampling from convex bodies using membership oracles. We\npropose a simple variant of the proximal sampler that achieves the query\ncomplexity with matched R\\'enyi orders between the initial warmness and output\nguarantee. Specifically, for any $\\varepsilon>0$ and $q\\geq2$, the sampler,\ninitialized at $\\pi_{0}$, outputs a sample whose law is $\\varepsilon$-close in\n$q$-R\\'enyi divergence to $\\pi$, the uniform distribution over a convex body in\n$\\mathbb{R}^{d}$, using\n$\\widetilde{O}(qM_{q}^{q/(q-1)}d^{2}\\,\\lVert\\operatorname{cov}\\pi\\rVert\\log\\frac{1}{\\varepsilon})$\nmembership queries, where\n$M_{q}=\\lVert\\text{d}\\pi_{0}/\\text{d}\\pi\\rVert_{L^{q}(\\pi)}$.\n  We further introduce a simple annealing scheme that produces a warm start in\n$q$-R\\'enyi divergence (i.e., $M_{q}=O(1)$) using\n$\\widetilde{O}(qd^{2}R^{3/2}\\,\\lVert\\operatorname{cov}\\pi\\rVert^{1/4})$\nqueries, where $R^{2}=\\mathbb{E}_{\\pi}[|\\cdot|^{2}]$. This interpolates between\nknown complexities for warm-start generation in total variation and\nR\\'enyi-infinity divergence. To relay a R\\'enyi warmness across the annealing\nscheme, we establish hypercontractivity under simultaneous heat flow and\ntranslate it into an improved mixing guarantee for the proximal sampler under a\nlogarithmic Sobolev inequality. These results extend naturally to general\nlog-concave distributions accessible via evaluation oracles, incurring\nadditional quadratic queries.\n","authors":["Yunbum Kook"],"pdf_url":"https://arxiv.org/pdf/2507.18021v1.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2507.18014v1","updated":"2025-07-24T01:09:25Z","published":"2025-07-24T01:09:25Z","title":"Predictive Scaling Laws for Efficient GRPO Training of Large Reasoning\n  Models","summary":"  Fine-tuning large language models (LLMs) for reasoning tasks using\nreinforcement learning methods like Group Relative Policy Optimization (GRPO)\nis computationally expensive. To address this, we propose a predictive\nframework that models training dynamics and helps optimize resource usage.\nThrough experiments on Llama and Qwen models (3B 8B), we derive an empirical\nscaling law based on model size, initial performance, and training progress.\nThis law predicts reward trajectories and identifies three consistent training\nphases: slow start, rapid improvement, and plateau. We find that training\nbeyond certain number of an epoch offers little gain, suggesting earlier\nstopping can significantly reduce compute without sacrificing performance. Our\napproach generalizes across model types, providing a practical guide for\nefficient GRPO-based fine-tuning.\n","authors":["Datta Nimmaturi","Vaishnavi Bhargava","Rajat Ghosh","Johnu George","Debojyoti Dutta"],"pdf_url":"https://arxiv.org/pdf/2507.18014v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16315v3","updated":"2025-07-24T00:51:58Z","published":"2025-03-20T16:38:16Z","title":"Active Learning For Repairable Hardware Systems With Partial Coverage","summary":"  Identifying the optimal diagnostic test and hardware system instance to infer\nreliability characteristics using field data is challenging, especially when\nconstrained by fixed budgets and minimal maintenance cycles. Active Learning\n(AL) has shown promise for parameter inference with limited data and budget\nconstraints in machine learning/deep learning tasks. However, AL for\nreliability model parameter inference remains underexplored for repairable\nhardware systems. It requires specialized AL Acquisition Functions (AFs) that\nconsider hardware aging and the fact that a hardware system consists of\nmultiple sub-systems, which may undergo only partial testing during a given\ndiagnostic test. To address these challenges, we propose a relaxed Mixed\nInteger Semidefinite Program (MISDP) AL AF that incorporates Diagnostic\nCoverage (DC), Fisher Information Matrices (FIMs), and diagnostic testing\nbudgets. Furthermore, we design empirical-based simulation experiments focusing\non two diagnostic testing scenarios: (1) partial tests of a hardware system\nwith overlapping subsystem coverage, and (2) partial tests where one diagnostic\ntest fully subsumes the subsystem coverage of another. We evaluate our proposed\napproach against the most widely used AL AF in the literature (entropy), as\nwell as several intuitive AL AFs tailored for reliability model parameter\ninference. Our proposed AF ranked best on average among the alternative AFs\nacross 6,000 experimental configurations, with respect to Area Under the Curve\n(AUC) of the Absolute Total Expected Event Error (ATEER) and Mean Squared Error\n(MSE) curves, with statistical significance calculated at a 0.05 alpha level\nusing a Friedman hypothesis test.\n","authors":["Michael Potter","Beyza Kalkanlı","Deniz Erdoğmuş","Michael Everett"],"pdf_url":"https://arxiv.org/pdf/2503.16315v3.pdf","comment":"Submitted to IEEE Access - Reliability Society"},{"id":"http://arxiv.org/abs/2503.18273v2","updated":"2025-07-24T00:22:47Z","published":"2025-03-24T01:41:24Z","title":"Analyzing Islamophobic Discourse Using Semi-Coded Terms and LLMs","summary":"  In recent years, Islamophobia has gained significant traction across Western\nsocieties, fueled by the rise of digital communication networks. This paper\nperforms a large-scale analysis of specialized, semi-coded Islamophobic terms\nsuch as (muzrat, pislam, mudslime, mohammedan, muzzies) floated on extremist\nsocial platforms, i.e., 4Chan, Gab, Telegram, etc. Many of these terms appear\nlexically neutral or ambiguous outside of specific contexts, making them\ndifficult for both human moderators and automated systems to reliably identify\nas hate speech. First, we use Large Language Models (LLMs) to show their\nability to understand these terms. Second, Google Perspective API suggests that\nIslamophobic posts tend to receive higher toxicity scores than other categories\nof hate speech like Antisemitism. Finally, we use BERT topic modeling approach\nto extract different topics and Islamophobic discourse on these social\nplatforms. Our findings indicate that LLMs understand these Out-Of-Vocabulary\n(OOV) slurs; however, further improvements in moderation strategies and\nalgorithmic detection are necessary to address such discourse effectively. Our\ntopic modeling also indicates that Islamophobic text is found across various\npolitical, conspiratorial, and far-right movements and is particularly directed\nagainst Muslim immigrants. Taken altogether, we performed one of the first\nstudies on Islamophobic semi-coded terms and shed a global light on\nIslamophobia.\n","authors":["Raza Ul Mustafa","Roi Dupart","Gabrielle Smith","Noman Ashraf","Nathalie Japkowicz"],"pdf_url":"https://arxiv.org/pdf/2503.18273v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12127v4","updated":"2025-07-24T00:06:46Z","published":"2024-11-18T23:41:27Z","title":"Fine-Grained Uncertainty Quantification via Collisions","summary":"  We propose a new and intuitive metric for aleatoric uncertainty\nquantification (UQ), the prevalence of class collisions defined as the same\ninput being observed in different classes. We use the rate of class collisions\nto define the collision matrix, a novel and uniquely fine-grained measure of\nuncertainty. For a classification problem involving $K$ classes, the $K\\times\nK$ collision matrix $S$ measures the inherent difficulty in distinguishing\nbetween each pair of classes. We discuss several applications of the collision\nmatrix, establish its fundamental mathematical properties, as well as show its\nrelationship with existing UQ methods, including the Bayes error rate (BER). We\nalso address the new problem of estimating the collision matrix using one-hot\nlabeled data by proposing a series of innovative techniques to estimate $S$.\nFirst, we learn a pair-wise contrastive model which accepts two inputs and\ndetermines if they belong to the same class. We then show that this contrastive\nmodel (which is PAC learnable) can be used to estimate the Gramian matrix of\n$S$, defined as $G=S^TS$. Finally, we show that under reasonable assumptions,\n$G$ can be used to uniquely recover $S$, a new result on non-negative matrices\nwhich could be of independent interest. With a method to estimate $S$\nestablished, we demonstrate how this estimate of $S$, in conjunction with the\ncontrastive model, can be used to estimate the posterior class portability\ndistribution of any point. Experimental results are also presented to validate\nour methods of estimating the collision matrix and class posterior\ndistributions on several datasets.\n","authors":["Jesse Friedbaum","Sudarshan Adiga","Ravi Tandon"],"pdf_url":"https://arxiv.org/pdf/2411.12127v4.pdf","comment":null}]},"2025-07-23T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2507.17990v1","updated":"2025-07-23T23:46:57Z","published":"2025-07-23T23:46:57Z","title":"Rapid Modeling Architecture for Lightweight Simulator to Accelerate and\n  Improve Decision Making for Industrial Systems","summary":"  Designing industrial systems, such as building, improving, and automating\ndistribution centers and manufacturing plants, involves critical\ndecision-making with limited information in the early phases. The lack of\ninformation leads to less accurate designs of the systems, which are often\ndifficult to resolve later. It is effective to use simulators to model the\ndesigned system and find out the issues early. However, the modeling time\nrequired by conventional simulators is too long to allow for rapid model\ncreation to meet decision-making demands. In this paper, we propose a Rapid\nModeling Architecture (RMA) for a lightweight industrial simulator that\nmitigates the modeling burden while maintaining the essential details in order\nto accelerate and improve decision-making. We have prototyped a simulator based\non the RMA and applied it to the actual factory layout design problem. We also\ncompared the modeling time of our simulator to that of an existing simulator,\nand as a result, our simulator achieved a 78.3% reduction in modeling time\ncompared to conventional simulators.\n","authors":["Takumi Kato","Zhi Li Hu"],"pdf_url":"https://arxiv.org/pdf/2507.17990v1.pdf","comment":"8 pages, 13 figures. Manuscript accepted at the 2025 IEEE 21st\n  International Conference on Automation Science and Engineering (CASE 2025)"},{"id":"http://arxiv.org/abs/2507.17943v1","updated":"2025-07-23T21:22:41Z","published":"2025-07-23T21:22:41Z","title":"Automated Brake Onset Detection in Naturalistic Driving Data","summary":"  Response timing measures play a crucial role in the assessment of automated\ndriving systems (ADS) in collision avoidance scenarios, including but not\nlimited to establishing human benchmarks and comparing ADS to human driver\nresponse performance. For example, measuring the response time (of a human\ndriver or ADS) to a conflict requires the determination of a stimulus onset and\na response onset. In existing studies, response onset relies on manual\nannotation or vehicle control signals such as accelerator and brake pedal\nmovements. These methods are not applicable when analyzing large scale data\nwhere vehicle control signals are not available. This holds in particular for\nthe rapidly expanding sets of ADS log data where the behavior of surrounding\nroad users is observed via onboard sensors. To advance evaluation techniques\nfor ADS and enable measuring response timing when vehicle control signals are\nnot available, we developed a simple and efficient algorithm, based on a\npiecewise linear acceleration model, to automatically estimate brake onset that\ncan be applied to any type of driving data that includes vehicle longitudinal\ntime series data. We also proposed a manual annotation method to identify brake\nonset and used it as ground truth for validation. R2 was used as a confidence\nmetric to measure the accuracy of the algorithm, and its classification\nperformance was analyzed using naturalistic collision avoidance data of both\nADS and humans, where our method was validated against human manual annotation.\nAlthough our algorithm is subject to certain limitations, it is efficient,\ngeneralizable, applicable to any road user and scenario types, and is highly\nconfigurable.\n","authors":["Shu-Yuan Liu","Johan Engström","Gustav Markkula"],"pdf_url":"https://arxiv.org/pdf/2507.17943v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17859v1","updated":"2025-07-23T18:32:01Z","published":"2025-07-23T18:32:01Z","title":"FishDet-M: A Unified Large-Scale Benchmark for Robust Fish Detection and\n  CLIP-Guided Model Selection in Diverse Aquatic Visual Domains","summary":"  Accurate fish detection in underwater imagery is essential for ecological\nmonitoring, aquaculture automation, and robotic perception. However, practical\ndeployment remains limited by fragmented datasets, heterogeneous imaging\nconditions, and inconsistent evaluation protocols. To address these gaps, we\npresent \\textit{FishDet-M}, the largest unified benchmark for fish detection,\ncomprising 13 publicly available datasets spanning diverse aquatic environments\nincluding marine, brackish, occluded, and aquarium scenes. All data are\nharmonized using COCO-style annotations with both bounding boxes and\nsegmentation masks, enabling consistent and scalable cross-domain evaluation.\nWe systematically benchmark 28 contemporary object detection models, covering\nthe YOLOv8 to YOLOv12 series, R-CNN based detectors, and DETR based models.\nEvaluations are conducted using standard metrics including mAP, mAP@50, and\nmAP@75, along with scale-specific analyses (AP$_S$, AP$_M$, AP$_L$) and\ninference profiling in terms of latency and parameter count. The results\nhighlight the varying detection performance across models trained on FishDet-M,\nas well as the trade-off between accuracy and efficiency across models of\ndifferent architectures. To support adaptive deployment, we introduce a\nCLIP-based model selection framework that leverages vision-language alignment\nto dynamically identify the most semantically appropriate detector for each\ninput image. This zero-shot selection strategy achieves high performance\nwithout requiring ensemble computation, offering a scalable solution for\nreal-time applications. FishDet-M establishes a standardized and reproducible\nplatform for evaluating object detection in complex aquatic scenes. All\ndatasets, pretrained models, and evaluation tools are publicly available to\nfacilitate future research in underwater computer vision and intelligent marine\nsystems.\n","authors":["Muayad Abujabal","Lyes Saad Saoud","Irfan Hussain"],"pdf_url":"https://arxiv.org/pdf/2507.17859v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17856v1","updated":"2025-07-23T18:26:18Z","published":"2025-07-23T18:26:18Z","title":"A Step-by-step Guide on Nonlinear Model Predictive Control for Safe\n  Mobile Robot Navigation","summary":"  Designing a Model Predictive Control (MPC) scheme that enables a mobile robot\nto safely navigate through an obstacle-filled environment is a complicated yet\nessential task in robotics. In this technical report, safety refers to ensuring\nthat the robot respects state and input constraints while avoiding collisions\nwith obstacles despite the presence of disturbances and measurement noise. This\nreport offers a step-by-step approach to implementing Nonlinear Model\nPredictive Control (NMPC) schemes addressing these safety requirements.\nNumerous books and survey papers provide comprehensive overviews of linear MPC\n(LMPC) \\cite{bemporad2007robust,kouvaritakis2016model}, NMPC\n\\cite{rawlings2017model,allgower2004nonlinear,mayne2014model,grune2017nonlinear,saltik2018outlook},\nand their applications in various domains, including robotics\n\\cite{nascimento2018nonholonomic,nguyen2021model,shi2021advanced,wei2022mpc}.\nThis report does not aim to replicate those exhaustive reviews. Instead, it\nfocuses specifically on NMPC as a foundation for safe mobile robot navigation.\nThe goal is to provide a practical and accessible path from theoretical\nconcepts to mathematical proofs and implementation, emphasizing safety and\nperformance guarantees. It is intended for researchers, robotics engineers, and\npractitioners seeking to bridge the gap between theoretical NMPC formulations\nand real-world robotic applications.\n  This report is not necessarily meant to remain fixed over time. If someone\nfinds an error in the presented theory, please reach out via the given email\naddresses. We are happy to update the document if necessary.\n","authors":["Dennis Benders","Laura Ferranti","Johannes Köhler"],"pdf_url":"https://arxiv.org/pdf/2507.17856v1.pdf","comment":"51 pages, 3 figures"},{"id":"http://arxiv.org/abs/2507.17846v1","updated":"2025-07-23T18:13:41Z","published":"2025-07-23T18:13:41Z","title":"PinchBot: Long-Horizon Deformable Manipulation with Guided Diffusion\n  Policy","summary":"  Pottery creation is a complicated art form that requires dexterous, precise\nand delicate actions to slowly morph a block of clay to a meaningful, and often\nuseful 3D goal shape. In this work, we aim to create a robotic system that can\ncreate simple pottery goals with only pinch-based actions. This pinch pottery\ntask allows us to explore the challenges of a highly multi-modal and\nlong-horizon deformable manipulation task. To this end, we present PinchBot, a\ngoal-conditioned diffusion policy model that when combined with pre-trained 3D\npoint cloud embeddings, task progress prediction and collision-constrained\naction projection, is able to successfully create a variety of simple pottery\ngoals. For experimental videos and access to the demonstration dataset, please\nvisit our project website:\nhttps://sites.google.com/andrew.cmu.edu/pinchbot/home.\n","authors":["Alison Bartsch","Arvind Car","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2507.17846v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.20424v2","updated":"2025-07-23T17:39:54Z","published":"2025-05-26T18:17:07Z","title":"Robot Operation of Home Appliances by Reading User Manuals","summary":"  Operating home appliances, among the most common tools in every household, is\na critical capability for assistive home robots. This paper presents ApBot, a\nrobot system that operates novel household appliances by \"reading\" their user\nmanuals. ApBot faces multiple challenges: (i) infer goal-conditioned partial\npolicies from their unstructured, textual descriptions in a user manual\ndocument, (ii) ground the policies to the appliance in the physical world, and\n(iii) execute the policies reliably over potentially many steps, despite\ncompounding errors. To tackle these challenges, ApBot constructs a structured,\nsymbolic model of an appliance from its manual, with the help of a large\nvision-language model (VLM). It grounds the symbolic actions visually to\ncontrol panel elements. Finally, ApBot closes the loop by updating the model\nbased on visual feedback. Our experiments show that across a wide range of\nsimulated and real-world appliances, ApBot achieves consistent and\nstatistically significant improvements in task success rate, compared with\nstate-of-the-art large VLMs used directly as control policies. These results\nsuggest that a structured internal representations plays an important role in\nrobust robot operation of home appliances, especially, complex ones.\n","authors":["Jian Zhang","Hanbo Zhang","Anxing Xiao","David Hsu"],"pdf_url":"https://arxiv.org/pdf/2505.20424v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.19664v5","updated":"2025-07-23T17:31:03Z","published":"2024-04-30T15:57:41Z","title":"Towards Generalist Robot Learning from Internet Video: A Survey","summary":"  Scaling deep learning to massive and diverse internet data has driven\nremarkable breakthroughs in domains such as video generation and natural\nlanguage processing. Robot learning, however, has thus far failed to replicate\nthis success and remains constrained by a scarcity of available data. Learning\nfrom videos (LfV) methods aim to address this data bottleneck by augmenting\ntraditional robot data with large-scale internet video. This video data\nprovides foundational information regarding physical dynamics, behaviours, and\ntasks, and can be highly informative for general-purpose robots.\n  This survey systematically examines the emerging field of LfV. We first\noutline essential concepts, including detailing fundamental LfV challenges such\nas distribution shift and missing action labels in video data. Next, we\ncomprehensively review current methods for extracting knowledge from\nlarge-scale internet video, overcoming LfV challenges, and improving robot\nlearning through video-informed training. The survey concludes with a critical\ndiscussion of future opportunities. Here, we emphasize the need for scalable\nfoundation model approaches that can leverage the full range of available\ninternet video and enhance the learning of robot policies and dynamics models.\nOverall, the survey aims to inform and catalyse future LfV research, driving\nprogress towards general-purpose robots.\n","authors":["Robert McCarthy","Daniel C. H. Tan","Dominik Schmidt","Fernando Acero","Nathan Herr","Yilun Du","Thomas G. Thuruthel","Zhibin Li"],"pdf_url":"https://arxiv.org/pdf/2404.19664v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.21427v2","updated":"2025-07-23T17:30:42Z","published":"2025-06-26T16:09:53Z","title":"Flow-Based Single-Step Completion for Efficient and Expressive Policy\n  Learning","summary":"  Generative models such as diffusion and flow-matching offer expressive\npolicies for offline reinforcement learning (RL) by capturing rich, multimodal\naction distributions, but their iterative sampling introduces high inference\ncosts and training instability due to gradient propagation across sampling\nsteps. We propose the \\textit{Single-Step Completion Policy} (SSCP), a\ngenerative policy trained with an augmented flow-matching objective to predict\ndirect completion vectors from intermediate flow samples, enabling accurate,\none-shot action generation. In an off-policy actor-critic framework, SSCP\ncombines the expressiveness of generative models with the training and\ninference efficiency of unimodal policies, without requiring long\nbackpropagation chains. Our method scales effectively to offline,\noffline-to-online, and online RL settings, offering substantial gains in speed\nand adaptability over diffusion-based baselines. We further extend SSCP to\ngoal-conditioned RL, enabling flat policies to exploit subgoal structures\nwithout explicit hierarchical inference. SSCP achieves strong results across\nstandard offline RL and behavior cloning benchmarks, positioning it as a\nversatile, expressive, and efficient framework for deep RL and sequential\ndecision-making.\n","authors":["Prajwal Koirala","Cody Fleming"],"pdf_url":"https://arxiv.org/pdf/2506.21427v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17679v1","updated":"2025-07-23T16:42:12Z","published":"2025-07-23T16:42:12Z","title":"Safety Assurance for Quadrotor Kinodynamic Motion Planning","summary":"  Autonomous drones have gained considerable attention for applications in\nreal-world scenarios, such as search and rescue, inspection, and delivery. As\ntheir use becomes ever more pervasive in civilian applications, failure to\nensure safe operation can lead to physical damage to the system, environmental\npollution, and even loss of human life. Recent work has demonstrated that\nmotion planning techniques effectively generate a collision-free trajectory\nduring navigation. However, these methods, while creating the motion plans, do\nnot inherently consider the safe operational region of the system, leading to\npotential safety constraints violation during deployment. In this paper, we\npropose a method that leverages run time safety assurance in a kinodynamic\nmotion planning scheme to satisfy the system's operational constraints. First,\nwe use a sampling-based geometric planner to determine a high-level\ncollision-free path within a user-defined space. Second, we design a low-level\nsafety assurance filter to provide safety guarantees to the control input of a\nLinear Quadratic Regulator (LQR) designed with the purpose of trajectory\ntracking. We demonstrate our proposed approach in a restricted 3D simulation\nenvironment using a model of the Crazyflie 2.0 drone.\n","authors":["Theodoros Tavoulareas","Marzia Cescon"],"pdf_url":"https://arxiv.org/pdf/2507.17679v1.pdf","comment":"Accepted for publication at 2025 Modeling, Estimation and Control\n  Conference (MECC)"},{"id":"http://arxiv.org/abs/2507.17665v1","updated":"2025-07-23T16:29:57Z","published":"2025-07-23T16:29:57Z","title":"Perspective-Invariant 3D Object Detection","summary":"  With the rise of robotics, LiDAR-based 3D object detection has garnered\nsignificant attention in both academia and industry. However, existing datasets\nand methods predominantly focus on vehicle-mounted platforms, leaving other\nautonomous platforms underexplored. To bridge this gap, we introduce Pi3DET,\nthe first benchmark featuring LiDAR data and 3D bounding box annotations\ncollected from multiple platforms: vehicle, quadruped, and drone, thereby\nfacilitating research in 3D object detection for non-vehicle platforms as well\nas cross-platform 3D detection. Based on Pi3DET, we propose a novel\ncross-platform adaptation framework that transfers knowledge from the\nwell-studied vehicle platform to other platforms. This framework achieves\nperspective-invariant 3D detection through robust alignment at both geometric\nand feature levels. Additionally, we establish a benchmark to evaluate the\nresilience and robustness of current 3D detectors in cross-platform scenarios,\nproviding valuable insights for developing adaptive 3D perception systems.\nExtensive experiments validate the effectiveness of our approach on challenging\ncross-platform tasks, demonstrating substantial gains over existing adaptation\nmethods. We hope this work paves the way for generalizable and unified 3D\nperception systems across diverse and complex environments. Our Pi3DET dataset,\ncross-platform benchmark suite, and annotation toolkit have been made publicly\navailable.\n","authors":["Ao Liang","Lingdong Kong","Dongyue Lu","Youquan Liu","Jian Fang","Huaici Zhao","Wei Tsang Ooi"],"pdf_url":"https://arxiv.org/pdf/2507.17665v1.pdf","comment":"ICCV 2025; 46 pages, 18 figures, 22 tables; Project Page at\n  https://pi3det.github.io"},{"id":"http://arxiv.org/abs/2507.17664v1","updated":"2025-07-23T16:29:52Z","published":"2025-07-23T16:29:52Z","title":"Talk2Event: Grounded Understanding of Dynamic Scenes from Event Cameras","summary":"  Event cameras offer microsecond-level latency and robustness to motion blur,\nmaking them ideal for understanding dynamic environments. Yet, connecting these\nasynchronous streams to human language remains an open challenge. We introduce\nTalk2Event, the first large-scale benchmark for language-driven object\ngrounding in event-based perception. Built from real-world driving data, we\nprovide over 30,000 validated referring expressions, each enriched with four\ngrounding attributes -- appearance, status, relation to viewer, and relation to\nother objects -- bridging spatial, temporal, and relational reasoning. To fully\nexploit these cues, we propose EventRefer, an attribute-aware grounding\nframework that dynamically fuses multi-attribute representations through a\nMixture of Event-Attribute Experts (MoEE). Our method adapts to different\nmodalities and scene dynamics, achieving consistent gains over state-of-the-art\nbaselines in event-only, frame-only, and event-frame fusion settings. We hope\nour dataset and approach will establish a foundation for advancing multimodal,\ntemporally-aware, and language-driven perception in real-world robotics and\nautonomy.\n","authors":["Lingdong Kong","Dongyue Lu","Ao Liang","Rong Li","Yuhao Dong","Tianshuai Hu","Lai Xing Ng","Wei Tsang Ooi","Benoit R. Cottereau"],"pdf_url":"https://arxiv.org/pdf/2507.17664v1.pdf","comment":"Preprint; 42 pages, 17 figures, 16 tables; Project Page at\n  https://talk2event.github.io"},{"id":"http://arxiv.org/abs/2507.17661v1","updated":"2025-07-23T16:29:45Z","published":"2025-07-23T16:29:45Z","title":"Monocular Semantic Scene Completion via Masked Recurrent Networks","summary":"  Monocular Semantic Scene Completion (MSSC) aims to predict the voxel-wise\noccupancy and semantic category from a single-view RGB image. Existing methods\nadopt a single-stage framework that aims to simultaneously achieve visible\nregion segmentation and occluded region hallucination, while also being\naffected by inaccurate depth estimation. Such methods often achieve suboptimal\nperformance, especially in complex scenes. We propose a novel two-stage\nframework that decomposes MSSC into coarse MSSC followed by the Masked\nRecurrent Network. Specifically, we propose the Masked Sparse Gated Recurrent\nUnit (MS-GRU) which concentrates on the occupied regions by the proposed mask\nupdating mechanism, and a sparse GRU design is proposed to reduce the\ncomputation cost. Additionally, we propose the distance attention projection to\nreduce projection errors by assigning different attention scores according to\nthe distance to the observed surface. Experimental results demonstrate that our\nproposed unified framework, MonoMRN, effectively supports both indoor and\noutdoor scenes and achieves state-of-the-art performance on the NYUv2 and\nSemanticKITTI datasets. Furthermore, we conduct robustness analysis under\nvarious disturbances, highlighting the role of the Masked Recurrent Network in\nenhancing the model's resilience to such challenges. The source code is\npublicly available.\n","authors":["Xuzhi Wang","Xinran Wu","Song Wang","Lingdong Kong","Ziping Zhao"],"pdf_url":"https://arxiv.org/pdf/2507.17661v1.pdf","comment":"ICCV 2025; 15 pages, 10 figures, 6 tables; Code at\n  https://github.com/alanWXZ/MonoMRN"},{"id":"http://arxiv.org/abs/2507.17649v1","updated":"2025-07-23T16:14:08Z","published":"2025-07-23T16:14:08Z","title":"Event Detection for Active Lower Limb Prosthesis","summary":"  Accurate event detection is key to the successful design of semi-passive and\npowered prosthetics. Kinematically, the natural knee is complex, with\ntranslation and rotation components that have a substantial impact on gait\ncharacteristics. When simplified to a pin joint, some of this behaviour is\nlost. This study investigates the role of cruciate ligament stretch in event\ndetection. A bicondylar knee design was used, constrained by analogues of the\nanterior and posterior cruciate ligaments. This offers the ability to\ncharacterize knee kinematics by the stretch of the ligaments. The ligament\nstretch was recorded using LVDTs parallel to the ligaments of the Russell knee\non a bent knee crutch. Which was used to capture data on a treadmill at 3\nspeeds. This study finds speed dependence within the stretch of the cruciate\nligaments, prominently around 5\\% and 80\\% of the gait cycle for the posterior\nand anterior. The cycle profile remains consistent with speed; therefore, other\nstatic events such as the turning point feature at around 90\\% and 95\\% of the\ncycle, for the posterior and anterior, respectively, could be used as a\npredictive precursor for initial contact. Likewise at 90\\% and 95\\%, another\npair of turning points that in this case could be used to predict foot flat.\nThis concludes that the use of a bicondylar knee design could improve the\ndetection of events during the gait cycle, and therefore could increase the\naccuracy of subsequent controllers for powered prosthetics.\n","authors":["J. D. Clark","P. Ellison"],"pdf_url":"https://arxiv.org/pdf/2507.17649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.03038v3","updated":"2025-07-23T16:02:07Z","published":"2025-04-03T21:32:32Z","title":"How to Adapt Control Barrier Functions? A Learning-Based Approach with\n  Applications to a VTOL Quadplane","summary":"  In this paper, we present a novel theoretical framework for online adaptation\nof Control Barrier Function (CBF) parameters, i.e., of the class K functions\nincluded in the CBF condition, under input constraints. We introduce the\nconcept of locally validated CBF parameters, which are adapted online to\nguarantee finite-horizon safety, based on conditions derived from Nagumo's\ntheorem and tangent cone analysis. To identify these parameters online, we\nintegrate a learning-based approach with an uncertainty-aware verification\nprocess that account for both epistemic and aleatoric uncertainties inherent in\nneural network predictions. Our method is demonstrated on a VTOL quadplane\nmodel during challenging transition and landing maneuvers, showcasing enhanced\nperformance while maintaining safety.\n","authors":["Taekyung Kim","Randal W. Beard","Dimitra Panagou"],"pdf_url":"https://arxiv.org/pdf/2504.03038v3.pdf","comment":"2025 IEEE Conference on Decision and Control (CDC). Project page:\n  https://www.taekyung.me/how-to-adapt-cbf"},{"id":"http://arxiv.org/abs/2411.00107v2","updated":"2025-07-23T15:24:16Z","published":"2024-10-31T18:02:30Z","title":"First, Learn What You Don't Know: Active Information Gathering for\n  Driving at the Limits of Handling","summary":"  Combining data-driven models that adapt online and model predictive control\n(MPC) has enabled effective control of nonlinear systems. However, when\ndeployed on unstable systems, online adaptation may not be fast enough to\nensure reliable simultaneous learning and control. For example, a controller on\na vehicle executing highly dynamic maneuvers--such as drifting to avoid an\nobstacle--may push the vehicle's tires to their friction limits, destabilizing\nthe vehicle and allowing modeling errors to quickly compound and cause a loss\nof control. To address this challenge, we present an active information\ngathering framework for identifying vehicle dynamics as quickly as possible. We\npropose an expressive vehicle dynamics model that leverages Bayesian last-layer\nmeta-learning to enable rapid online adaptation. The model's uncertainty\nestimates are used to guide informative data collection and quickly improve the\nmodel prior to deployment. Dynamic drifting experiments on a Toyota Supra show\nthat (i) the framework enables reliable control of a vehicle at the edge of\nstability, (ii) online adaptation alone may not suffice for zero-shot control\nand can lead to undesirable transient errors or spin-outs, and (iii) active\ndata collection helps achieve reliable performance.\n","authors":["Alexander Davydov","Franck Djeumou","Marcus Greiff","Makoto Suminaka","Michael Thompson","John Subosits","Thomas Lew"],"pdf_url":"https://arxiv.org/pdf/2411.00107v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17585v1","updated":"2025-07-23T15:20:31Z","published":"2025-07-23T15:20:31Z","title":"From Scan to Action: Leveraging Realistic Scans for Embodied Scene\n  Understanding","summary":"  Real-world 3D scene-level scans offer realism and can enable better\nreal-world generalizability for downstream applications. However, challenges\nsuch as data volume, diverse annotation formats, and tool compatibility limit\ntheir use. This paper demonstrates a methodology to effectively leverage these\nscans and their annotations. We propose a unified annotation integration using\nUSD, with application-specific USD flavors. We identify challenges in utilizing\nholistic real-world scan datasets and present mitigation strategies. The\nefficacy of our approach is demonstrated through two downstream applications:\nLLM-based scene editing, enabling effective LLM understanding and adaptation of\nthe data (80% success), and robotic simulation, achieving an 87% success rate\nin policy learning.\n","authors":["Anna-Maria Halacheva","Jan-Nico Zaech","Sombit Dey","Luc Van Gool","Danda Pani Paudel"],"pdf_url":"https://arxiv.org/pdf/2507.17585v1.pdf","comment":"Accepted at the OpenSUN3D Workshop, CVPR 2025. This workshop paper is\n  not included in the official CVPR proceedings"},{"id":"http://arxiv.org/abs/2507.17572v1","updated":"2025-07-23T15:03:40Z","published":"2025-07-23T15:03:40Z","title":"KernelSOS for Global Sampling-Based Optimal Control and Estimation via\n  Semidefinite Programming","summary":"  Global optimization has gained attraction over the past decades, thanks to\nthe development of both theoretical foundations and efficient numerical\nroutines to cope with optimization problems of various complexities. Among\nrecent methods, Kernel Sum of Squares (KernelSOS) appears as a powerful\nframework, leveraging the potential of sum of squares methods from the\npolynomial optimization community with the expressivity of kernel methods\nwidely used in machine learning. This paper applies the kernel sum of squares\nframework for solving control and estimation problems, which exhibit poor local\nminima. We demonstrate that KernelSOS performs well on a selection of problems\nfrom both domains. In particular, we show that KernelSOS is competitive with\nother sum of squares approaches on estimation problems, while being applicable\nto non-polynomial and non-parametric formulations. The sample-based nature of\nKernelSOS allows us to apply it to trajectory optimization problems with an\nintegrated simulator treated as a black box, both as a standalone method and as\na powerful initialization method for local solvers, facilitating the discovery\nof better solutions.\n","authors":["Antoine Groudiev","Fabian Schramm","Éloïse Berthier","Justin Carpentier","Frederike Dümbgen"],"pdf_url":"https://arxiv.org/pdf/2507.17572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17561v1","updated":"2025-07-23T14:51:55Z","published":"2025-07-23T14:51:55Z","title":"Robot-mediated physical Human-Human Interaction in Neurorehabilitation:\n  a position paper","summary":"  Neurorehabilitation conventionally relies on the interaction between a\npatient and a physical therapist. Robotic systems can improve and enrich the\nphysical feedback provided to patients after neurological injury, but they\nunder-utilize the adaptability and clinical expertise of trained therapists. In\nthis position paper, we advocate for a novel approach that integrates the\ntherapist's clinical expertise and nuanced decision-making with the strength,\naccuracy, and repeatability of robotics: Robot-mediated physical Human-Human\nInteraction. This framework, which enables two individuals to physically\ninteract through robotic devices, has been studied across diverse research\ngroups and has recently emerged as a promising link between conventional manual\ntherapy and rehabilitation robotics, harmonizing the strengths of both\napproaches. This paper presents the rationale of a multidisciplinary\nteam-including engineers, doctors, and physical therapists-for conducting\nresearch that utilizes: a unified taxonomy to describe robot-mediated\nrehabilitation, a framework of interaction based on social psychology, and a\ntechnological approach that makes robotic systems seamless facilitators of\nnatural human-human interaction.\n","authors":["Lorenzo Vianello","Matthew Short","Julia Manczurowsky","Emek Barış Küçüktabak","Francesco Di Tommaso","Alessia Noccaro","Laura Bandini","Shoshana Clark","Alaina Fiorenza","Francesca Lunardini","Alberto Canton","Marta Gandolla","Alessandra L. G. Pedrocchi","Emilia Ambrosini","Manuel Murie-Fernandez","Carmen B. Roman","Jesus Tornero","Natacha Leon","Andrew Sawers","Jim Patton","Domenico Formica","Nevio Luigi Tagliamonte","Georg Rauter","Kilian Baur","Fabian Just","Christopher J. Hasson","Vesna D. Novak","Jose L. Pons"],"pdf_url":"https://arxiv.org/pdf/2507.17561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17531v1","updated":"2025-07-23T14:10:48Z","published":"2025-07-23T14:10:48Z","title":"When and Where Localization Fails: An Analysis of the Iterative Closest\n  Point in Evolving Environment","summary":"  Robust relocalization in dynamic outdoor environments remains a key challenge\nfor autonomous systems relying on 3D lidar. While long-term localization has\nbeen widely studied, short-term environmental changes, occurring over days or\nweeks, remain underexplored despite their practical significance. To address\nthis gap, we present a highresolution, short-term multi-temporal dataset\ncollected weekly from February to April 2025 across natural and semi-urban\nsettings. Each session includes high-density point cloud maps, 360 deg\npanoramic images, and trajectory data. Projected lidar scans, derived from the\npoint cloud maps and modeled with sensor-accurate occlusions, are used to\nevaluate alignment accuracy against the ground truth using two Iterative\nClosest Point (ICP) variants: Point-to-Point and Point-to-Plane. Results show\nthat Point-to-Plane offers significantly more stable and accurate registration,\nparticularly in areas with sparse features or dense vegetation. This study\nprovides a structured dataset for evaluating short-term localization\nrobustness, a reproducible framework for analyzing scan-to-map alignment under\nnoise, and a comparative evaluation of ICP performance in evolving outdoor\nenvironments. Our analysis underscores how local geometry and environmental\nvariability affect localization success, offering insights for designing more\nresilient robotic systems.\n","authors":["Abdel-Raouf Dannaoui","Johann Laconte","Christophe Debain","Francois Pomerleau","Paul Checchin"],"pdf_url":"https://arxiv.org/pdf/2507.17531v1.pdf","comment":"7 pages, 7 figures, proceedings in European Conference on Mobile\n  Robots (ECMR) 2025"},{"id":"http://arxiv.org/abs/2507.17530v1","updated":"2025-07-23T14:07:56Z","published":"2025-07-23T14:07:56Z","title":"Generalized Advantage Estimation for Distributional Policy Gradients","summary":"  Generalized Advantage Estimation (GAE) has been used to mitigate the\ncomputational complexity of reinforcement learning (RL) by employing an\nexponentially weighted estimation of the advantage function to reduce the\nvariance in policy gradient estimates. Despite its effectiveness, GAE is not\ndesigned to handle value distributions integral to distributional RL, which can\ncapture the inherent stochasticity in systems and is hence more robust to\nsystem noises. To address this gap, we propose a novel approach that utilizes\nthe optimal transport theory to introduce a Wasserstein-like directional\nmetric, which measures both the distance and the directional discrepancies\nbetween probability distributions. Using the exponentially weighted estimation,\nwe leverage this Wasserstein-like directional metric to derive distributional\nGAE (DGAE). Similar to traditional GAE, our proposed DGAE provides a\nlow-variance advantage estimate with controlled bias, making it well-suited for\npolicy gradient algorithms that rely on advantage estimation for policy\nupdates. We integrated DGAE into three different policy gradient methods.\nAlgorithms were evaluated across various OpenAI Gym environments and compared\nwith the baselines with traditional GAE to assess the performance.\n","authors":["Shahil Shaik","Jonathon M. Smereka","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2507.17530v1.pdf","comment":"6 pages, 3 figures, published at ACC 2025 Conference"},{"id":"http://arxiv.org/abs/2507.17520v1","updated":"2025-07-23T13:57:06Z","published":"2025-07-23T13:57:06Z","title":"InstructVLA: Vision-Language-Action Instruction Tuning from\n  Understanding to Manipulation","summary":"  To operate effectively in the real world, robots must integrate multimodal\nreasoning with precise action generation. However, existing\nvision-language-action (VLA) models often sacrifice one for the other, narrow\ntheir abilities to task-specific manipulation data, and suffer catastrophic\nforgetting of pre-trained vision-language capabilities. To bridge this gap, we\nintroduce InstructVLA, an end-to-end VLA model that preserves the flexible\nreasoning of large vision-language models (VLMs) while delivering leading\nmanipulation performance. InstructVLA introduces a novel training paradigm,\nVision-Language-Action Instruction Tuning (VLA-IT), which employs multimodal\ntraining with mixture-of-experts adaptation to jointly optimize textual\nreasoning and action generation on both standard VLM corpora and a curated\n650K-sample VLA-IT dataset. On in-domain SimplerEnv tasks, InstructVLA achieves\n30.5% improvement over SpatialVLA. To evaluate generalization, we introduce\nSimplerEnv-Instruct, an 80-task benchmark requiring closed-loop control and\nhigh-level instruction understanding, where it outperforms a fine-tuned OpenVLA\nby 92% and an action expert aided by GPT-4o by 29%. Additionally, InstructVLA\nsurpasses baseline VLMs on multimodal tasks and exhibits inference-time scaling\nby leveraging textual reasoning to boost manipulation performance in both\nsimulated and real-world settings. These results demonstrate InstructVLA's\npotential for bridging intuitive and steerable human-robot interaction with\nefficient policy learning.\n","authors":["Shuai Yang","Hao Li","Yilun Chen","Bin Wang","Yang Tian","Tai Wang","Hanqing Wang","Feng Zhao","Yiyi Liao","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2507.17520v1.pdf","comment":"38 pages"},{"id":"http://arxiv.org/abs/2505.07983v2","updated":"2025-07-23T13:42:48Z","published":"2025-05-12T18:29:35Z","title":"Virtual Holonomic Constraints in Motion Planning: Revisiting Feasibility\n  and Limitations","summary":"  This paper addresses the feasibility of virtual holonomic constraints (VHCs)\nin the context of motion planning for underactuated mechanical systems with a\nsingle degree of underactuation. While existing literature has established a\nwidely accepted definition of VHC, we argue that this definition is overly\nrestrictive and excludes a broad class of admissible trajectories from\nconsideration. To illustrate this point, we analyze a periodic motion of the\nPlanar Vertical Take-Off and Landing (PVTOL) aircraft that satisfies all\nstandard motion planning requirements, including orbital stabilizability.\nHowever, for this solution -- as well as for a broad class of similar ones --\nthere exists no VHC that satisfies the conventional definition. We further\nprovide a formal proof demonstrating that the conditions imposed by this\ndefinition necessarily fail for a broad class of trajectories of mechanical\nsystems. These findings call for a reconsideration of the current definition of\nVHCs, with the potential to significantly broaden their applicability in motion\nplanning.\n","authors":["Maksim Surov"],"pdf_url":"https://arxiv.org/pdf/2505.07983v2.pdf","comment":"17 pages, 3 figure"},{"id":"http://arxiv.org/abs/2407.18661v2","updated":"2025-07-23T13:33:38Z","published":"2024-07-26T10:53:57Z","title":"Optimizing Design and Control Methods for Using Collaborative Robots in\n  Upper-Limb Rehabilitation","summary":"  In this paper, we address the development of a robotic rehabilitation system\nfor the upper limbs based on collaborative end-effector solutions. The use of\ncommercial collaborative robots offers significant advantages for this task, as\nthey are optimized from an engineering perspective and ensure safe physical\ninteraction with humans. However, they also come with noticeable drawbacks,\nsuch as the limited range of sizes available on the market and the standard\ncontrol modes, which are primarily oriented towards industrial or service\napplications. To address these limitations, we propose an optimization-based\ndesign method to fully exploit the capability of the cobot in performing\nrehabilitation tasks. Additionally, we introduce a novel control architecture\nbased on an admittance-type Virtual Fixture method, which constrains the motion\nof the robot along a prescribed path. This approach allows for an intuitive\ndefinition of the task to be performed via Programming by Demonstration and\nenables the system to operate both passively and actively. In passive mode, the\nsystem supports the patient during task execution with additional force, while\nin active mode, it opposes the motion with a braking force. Experimental\nresults demonstrate the effectiveness of the proposed method.\n","authors":["Dario Onfiani","Marco Caramaschi","Luigi Biagiotti","Fabio Pini"],"pdf_url":"https://arxiv.org/pdf/2407.18661v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17455v1","updated":"2025-07-23T12:23:03Z","published":"2025-07-23T12:23:03Z","title":"VLM-Guided Visual Place Recognition for Planet-Scale Geo-Localization","summary":"  Geo-localization from a single image at planet scale (essentially an advanced\nor extreme version of the kidnapped robot problem) is a fundamental and\nchallenging task in applications such as navigation, autonomous driving and\ndisaster response due to the vast diversity of locations, environmental\nconditions, and scene variations. Traditional retrieval-based methods for\ngeo-localization struggle with scalability and perceptual aliasing, while\nclassification-based approaches lack generalization and require extensive\ntraining data. Recent advances in vision-language models (VLMs) offer a\npromising alternative by leveraging contextual understanding and reasoning.\nHowever, while VLMs achieve high accuracy, they are often prone to\nhallucinations and lack interpretability, making them unreliable as standalone\nsolutions. In this work, we propose a novel hybrid geo-localization framework\nthat combines the strengths of VLMs with retrieval-based visual place\nrecognition (VPR) methods. Our approach first leverages a VLM to generate a\nprior, effectively guiding and constraining the retrieval search space. We then\nemploy a retrieval step, followed by a re-ranking mechanism that selects the\nmost geographically plausible matches based on feature similarity and proximity\nto the initially estimated coordinates. We evaluate our approach on multiple\ngeo-localization benchmarks and show that it consistently outperforms prior\nstate-of-the-art methods, particularly at street (up to 4.51%) and city level\n(up to 13.52%). Our results demonstrate that VLM-generated geographic priors in\ncombination with VPR lead to scalable, robust, and accurate geo-localization\nsystems.\n","authors":["Sania Waheed","Na Min An","Michael Milford","Sarvapali D. Ramchurn","Shoaib Ehsan"],"pdf_url":"https://arxiv.org/pdf/2507.17455v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17445v1","updated":"2025-07-23T12:07:21Z","published":"2025-07-23T12:07:21Z","title":"IndoorBEV: Joint Detection and Footprint Completion of Objects via\n  Mask-based Prediction in Indoor Scenarios for Bird's-Eye View Perception","summary":"  Detecting diverse objects within complex indoor 3D point clouds presents\nsignificant challenges for robotic perception, particularly with varied object\nshapes, clutter, and the co-existence of static and dynamic elements where\ntraditional bounding box methods falter. To address these limitations, we\npropose IndoorBEV, a novel mask-based Bird's-Eye View (BEV) method for indoor\nmobile robots.\n  In a BEV method, a 3D scene is projected into a 2D BEV grid which handles\nnaturally occlusions and provides a consistent top-down view aiding to\ndistinguish static obstacles from dynamic agents. The obtained 2D BEV results\nis directly usable to downstream robotic tasks like navigation, motion\nprediction, and planning. Our architecture utilizes an axis compact encoder and\na window-based backbone to extract rich spatial features from this BEV map. A\nquery-based decoder head then employs learned object queries to concurrently\npredict object classes and instance masks in the BEV space. This mask-centric\nformulation effectively captures the footprint of both static and dynamic\nobjects regardless of their shape, offering a robust alternative to bounding\nbox regression. We demonstrate the effectiveness of IndoorBEV on a custom\nindoor dataset featuring diverse object classes including static objects\n  and dynamic elements like robots and miscellaneous items, showcasing its\npotential for robust indoor scene understanding.\n","authors":["Haichuan Li","Changda Tian","Panos Trahanias","Tomi Westerlund"],"pdf_url":"https://arxiv.org/pdf/2507.17445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17401v1","updated":"2025-07-23T10:57:59Z","published":"2025-07-23T10:57:59Z","title":"The Wilhelm Tell Dataset of Affordance Demonstrations","summary":"  Affordances - i.e. possibilities for action that an environment or objects in\nit provide - are important for robots operating in human environments to\nperceive. Existing approaches train such capabilities on annotated static\nimages or shapes. This work presents a novel dataset for affordance learning of\ncommon household tasks. Unlike previous approaches, our dataset consists of\nvideo sequences demonstrating the tasks from first- and third-person\nperspectives, along with metadata about the affordances that are manifested in\nthe task, and is aimed towards training perception systems to recognize\naffordance manifestations. The demonstrations were collected from several\nparticipants and in total record about seven hours of human activity. The\nvariety of task performances also allows studying preparatory maneuvers that\npeople may perform for a task, such as how they arrange their task space, which\nis also relevant for collaborative service robots.\n","authors":["Rachel Ringe","Mihai Pomarlan","Nikolaos Tsiogkas","Stefano De Giorgis","Maria Hedblom","Rainer Malaka"],"pdf_url":"https://arxiv.org/pdf/2507.17401v1.pdf","comment":"\\c{opyright} 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2507.17383v1","updated":"2025-07-23T10:26:10Z","published":"2025-07-23T10:26:10Z","title":"Confidence Calibration in Vision-Language-Action Models","summary":"  Trustworthy robot behavior requires not only high levels of task success but\nalso that the robot can reliably quantify how likely it is to succeed. To this\nend, we present the first systematic study of confidence calibration in\nvision-language-action (VLA) foundation models, which map visual observations\nand natural-language instructions to low-level robot motor commands. We begin\nwith extensive benchmarking to understand the critical relationship between\ntask success and calibration error across multiple datasets and VLA variants,\nfinding that task performance and calibration are not in tension. Next, we\nintroduce prompt ensembles for VLAs, a lightweight, Bayesian-inspired algorithm\nthat averages confidence across paraphrased instructions and consistently\nimproves calibration. We further analyze calibration over the task time\nhorizon, showing that confidence is often most reliable after making some\nprogress, suggesting natural points for risk-aware intervention. Finally, we\nreveal differential miscalibration across action dimensions and propose\naction-wise Platt scaling, a method to recalibrate each action dimension\nindependently to produce better confidence estimates. Our aim in this study is\nto begin to develop the tools and conceptual understanding necessary to render\nVLAs both highly performant and highly trustworthy via reliable uncertainty\nquantification.\n","authors":["Thomas P Zollo","Richard Zemel"],"pdf_url":"https://arxiv.org/pdf/2507.17383v1.pdf","comment":"34 pages, 19 figures"},{"id":"http://arxiv.org/abs/2507.17379v1","updated":"2025-07-23T10:23:15Z","published":"2025-07-23T10:23:15Z","title":"Language-Conditioned Open-Vocabulary Mobile Manipulation with Pretrained\n  Models","summary":"  Open-vocabulary mobile manipulation (OVMM) that involves the handling of\nnovel and unseen objects across different workspaces remains a significant\nchallenge for real-world robotic applications. In this paper, we propose a\nnovel Language-conditioned Open-Vocabulary Mobile Manipulation framework, named\nLOVMM, incorporating the large language model (LLM) and vision-language model\n(VLM) to tackle various mobile manipulation tasks in household environments.\nOur approach is capable of solving various OVMM tasks with free-form natural\nlanguage instructions (e.g. \"toss the food boxes on the office room desk to the\ntrash bin in the corner\", and \"pack the bottles from the bed to the box in the\nguestroom\"). Extensive experiments simulated in complex household environments\nshow strong zero-shot generalization and multi-task learning abilities of\nLOVMM. Moreover, our approach can also generalize to multiple tabletop\nmanipulation tasks and achieve better success rates compared to other\nstate-of-the-art methods.\n","authors":["Shen Tan","Dong Zhou","Xiangyu Shao","Junqiao Wang","Guanghui Sun"],"pdf_url":"https://arxiv.org/pdf/2507.17379v1.pdf","comment":"IJCAI 2025"},{"id":"http://arxiv.org/abs/2507.17376v1","updated":"2025-07-23T10:19:22Z","published":"2025-07-23T10:19:22Z","title":"An Exploratory Study on Human-Robot Interaction using Semantics-based\n  Situational Awareness","summary":"  In this paper, we investigate the impact of high-level semantics (evaluation\nof the environment) on Human-Robot Teams (HRT) and Human-Robot Interaction\n(HRI) in the context of mobile robot deployments. Although semantics has been\nwidely researched in AI, how high-level semantics can benefit the HRT paradigm\nis underexplored, often fuzzy, and intractable. We applied a semantics-based\nframework that could reveal different indicators of the environment (i.e. how\nmuch semantic information exists) in a mock-up disaster response mission. In\nsuch missions, semantics are crucial as the HRT should handle complex\nsituations and respond quickly with correct decisions, where humans might have\na high workload and stress. Especially when human operators need to shift their\nattention between robots and other tasks, they will struggle to build\nSituational Awareness (SA) quickly. The experiment suggests that the presented\nsemantics: 1) alleviate the perceived workload of human operators; 2) increase\nthe operator's trust in the SA; and 3) help to reduce the reaction time in\nswitching the level of autonomy when needed. Additionally, we find that\nparticipants with higher trust in the system are encouraged by high-level\nsemantics to use teleoperation mode more.\n","authors":["Tianshu Ruan","Aniketh Ramesh","Rustam Stolkin","Manolis Chiou"],"pdf_url":"https://arxiv.org/pdf/2507.17376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17338v1","updated":"2025-07-23T09:08:21Z","published":"2025-07-23T09:08:21Z","title":"Mobile Manipulation with Active Inference for Long-Horizon Rearrangement\n  Tasks","summary":"  Despite growing interest in active inference for robotic control, its\napplication to complex, long-horizon tasks remains untested. We address this\ngap by introducing a fully hierarchical active inference architecture for\ngoal-directed behavior in realistic robotic settings. Our model combines a\nhigh-level active inference model that selects among discrete skills realized\nvia a whole-body active inference controller. This unified approach enables\nflexible skill composition, online adaptability, and recovery from task\nfailures without requiring offline training. Evaluated on the Habitat Benchmark\nfor mobile manipulation, our method outperforms state-of-the-art baselines\nacross the three long-horizon tasks, demonstrating for the first time that\nactive inference can scale to the complexity of modern robotics benchmarks.\n","authors":["Corrado Pezzato","Ozan Çatal","Toon Van de Maele","Riddhi J. Pitliya","Tim Verbelen"],"pdf_url":"https://arxiv.org/pdf/2507.17338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.22404v2","updated":"2025-07-23T09:07:16Z","published":"2025-05-28T14:34:32Z","title":"Efficient Precision-Scalable Hardware for Microscaling (MX) Processing\n  in Robotics Learning","summary":"  Autonomous robots require efficient on-device learning to adapt to new\nenvironments without cloud dependency. For this edge training, Microscaling\n(MX) data types offer a promising solution by combining integer and\nfloating-point representations with shared exponents, reducing energy\nconsumption while maintaining accuracy. However, the state-of-the-art\ncontinuous learning processor, namely Dacapo, faces limitations with its\nMXINT-only support and inefficient vector-based grouping during\nbackpropagation. In this paper, we present, to the best of our knowledge, the\nfirst work that addresses these limitations with two key innovations: (1) a\nprecision-scalable arithmetic unit that supports all six MX data types by\nexploiting sub-word parallelism and unified integer and floating-point\nprocessing; and (2) support for square shared exponent groups to enable\nefficient weight handling during backpropagation, removing storage redundancy\nand quantization overhead. We evaluate our design against Dacapo under\niso-peak-throughput on four robotics workloads in TSMC 16nm FinFET technology\nat 400MHz, reaching a 51% lower memory footprint, and 4x higher effective\ntraining throughput, while achieving comparable energy efficiency, enabling\nefficient robotics continual learning at the edge.\n","authors":["Stef Cuyckens","Xiaoling Yi","Nitish Satya Murthy","Chao Fang","Marian Verhelst"],"pdf_url":"https://arxiv.org/pdf/2505.22404v2.pdf","comment":"To appear in 2025 IEEE/ACM International Symposium on Low Power\n  Electronics and Design (ISLPED 2025)"},{"id":"http://arxiv.org/abs/2506.09859v2","updated":"2025-07-23T08:56:08Z","published":"2025-06-11T15:31:25Z","title":"Hierarchical Learning-Enhanced MPC for Safe Crowd Navigation with\n  Heterogeneous Constraints","summary":"  In this paper, we propose a novel hierarchical framework for robot navigation\nin dynamic environments with heterogeneous constraints. Our approach leverages\na graph neural network trained via reinforcement learning (RL) to efficiently\nestimate the robot's cost-to-go, formulated as local goal recommendations. A\nspatio-temporal path-searching module, which accounts for kinematic\nconstraints, is then employed to generate a reference trajectory to facilitate\nsolving the non-convex optimization problem used for explicit constraint\nenforcement. More importantly, we introduce an incremental action-masking\nmechanism and a privileged learning strategy, enabling end-to-end training of\nthe proposed planner. Both simulation and real-world experiments demonstrate\nthat the proposed method effectively addresses local planning in complex\ndynamic environments, achieving state-of-the-art (SOTA) performance. Compared\nwith existing learning-optimization hybrid methods, our approach eliminates the\ndependency on high-fidelity simulation environments, offering significant\nadvantages in computational efficiency and training scalability. The code will\nbe released as open-source upon acceptance of the paper.\n","authors":["Huajian Liu","Yixuan Feng","Wei Dong","Kunpeng Fan","Chao Wang","Yongzhuo Gao"],"pdf_url":"https://arxiv.org/pdf/2506.09859v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17317v1","updated":"2025-07-23T08:31:35Z","published":"2025-07-23T08:31:35Z","title":"HuNavSim 2.0","summary":"  This work presents a new iteration of the Human Navigation Simulator\n(HuNavSim), a novel open-source tool for the simulation of different\nhuman-agent navigation behaviors in scenarios with mobile robots. The tool,\nprogrammed under the ROS 2 framework, can be used together with different\nwell-known robotics simulators such as Gazebo or NVidia Isaac Sim. The main\ngoal is to facilitate the development and evaluation of human-aware robot\nnavigation systems in simulation. In this new version, several features have\nbeen improved and new ones added, such as the extended set of actions and\nconditions that can be combined in Behavior Trees to compound complex and\nrealistic human behaviors.\n","authors":["Miguel Escudero-Jiménez","Noé Pérez-Higueras","Andrés Martínez-Silva","Fernando Caballero","Luis Merino"],"pdf_url":"https://arxiv.org/pdf/2507.17317v1.pdf","comment":"Preprint submitted to the 8th Iberian Robotics Conference (ROBOT\n  2025)"},{"id":"http://arxiv.org/abs/2507.14456v3","updated":"2025-07-23T08:26:59Z","published":"2025-07-19T03:04:28Z","title":"GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for\n  End-to-End Autonomous Driving","summary":"  End-to-end autonomous driving requires adaptive and robust handling of\ncomplex and diverse traffic environments. However, prevalent single-mode\nplanning methods attempt to learn an overall policy while struggling to acquire\ndiversified driving skills to handle diverse scenarios. Therefore, this paper\nproposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework\nfeaturing a Global Expert, a Scene-Adaptive Experts Group, and equipped with a\nDual-aware Router. Specifically, the Global Expert is trained on the overall\ndataset, possessing robust performance. The Scene-Adaptive Experts are trained\non corresponding scene subsets, achieving adaptive performance. The Dual-aware\nRouter simultaneously considers scenario-level features and routing uncertainty\nto dynamically activate expert modules. Through the effective coupling of the\nGlobal Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,\nGEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS\noutperforms existing methods in the Bench2Drive closed-loop benchmark and\nachieves state-of-the-art performance in Driving Score and Success Rate, even\nwith only monocular vision input. Furthermore, ablation studies demonstrate\nsignificant improvements over the original single-expert baseline: 7.67% in\nDriving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The\ncode will be available at https://github.com/newbrains1/GEMINUS.\n","authors":["Chi Wan","Yixin Cui","Jiatong Du","Shuo Yang","Yulong Bai","Yanjun Huang"],"pdf_url":"https://arxiv.org/pdf/2507.14456v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17294v1","updated":"2025-07-23T07:54:10Z","published":"2025-07-23T07:54:10Z","title":"VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level\n  Tactile Feedback","summary":"  Tactile feedback is generally recognized to be crucial for effective\ninteraction with the physical world. However, state-of-the-art\nVision-Language-Action (VLA) models lack the ability to interpret and use\ntactile signals, limiting their effectiveness in contact-rich tasks.\nIncorporating tactile feedback into these systems is challenging due to the\nabsence of large multi-modal datasets. We present VLA-Touch, an approach that\nenhances generalist robot policies with tactile sensing \\emph{without\nfine-tuning} the base VLA. Our method introduces two key innovations: (1) a\npipeline that leverages a pretrained tactile-language model that provides\nsemantic tactile feedback for high-level task planning, and (2) a\ndiffusion-based controller that refines VLA-generated actions with tactile\nsignals for contact-rich manipulation. Through real-world experiments, we\ndemonstrate that our dual-level integration of tactile feedback improves task\nplanning efficiency while enhancing execution precision. Code is open-sourced\nat \\href{https://github.com/jxbi1010/VLA-Touch}{this URL}.\n","authors":["Jianxin Bi","Kevin Yuchen Ma","Ce Hao","Mike Zheng Shou","Harold Soh"],"pdf_url":"https://arxiv.org/pdf/2507.17294v1.pdf","comment":"19 pages, 5 figures"},{"id":"http://arxiv.org/abs/2507.17275v1","updated":"2025-07-23T07:25:04Z","published":"2025-07-23T07:25:04Z","title":"Prolonging Tool Life: Learning Skillful Use of General-purpose Tools\n  through Lifespan-guided Reinforcement Learning","summary":"  In inaccessible environments with uncertain task demands, robots often rely\non general-purpose tools that lack predefined usage strategies. These tools are\nnot tailored for particular operations, making their longevity highly sensitive\nto how they are used. This creates a fundamental challenge: how can a robot\nlearn a tool-use policy that both completes the task and prolongs the tool's\nlifespan? In this work, we address this challenge by introducing a\nreinforcement learning (RL) framework that incorporates tool lifespan as a\nfactor during policy optimization. Our framework leverages Finite Element\nAnalysis (FEA) and Miner's Rule to estimate Remaining Useful Life (RUL) based\non accumulated stress, and integrates the RUL into the RL reward to guide\npolicy learning toward lifespan-guided behavior. To handle the fact that RUL\ncan only be estimated after task execution, we introduce an Adaptive Reward\nNormalization (ARN) mechanism that dynamically adjusts reward scaling based on\nestimated RULs, ensuring stable learning signals. We validate our method across\nsimulated and real-world tool use tasks, including Object-Moving and\nDoor-Opening with multiple general-purpose tools. The learned policies\nconsistently prolong tool lifespan (up to 8.01x in simulation) and transfer\neffectively to real-world settings, demonstrating the practical value of\nlearning lifespan-guided tool use strategies.\n","authors":["Po-Yen Wu","Cheng-Yu Kuo","Yuki Kadokawa","Takamitsu Matsubara"],"pdf_url":"https://arxiv.org/pdf/2507.17275v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2505.01709v3","updated":"2025-07-23T07:22:26Z","published":"2025-05-03T06:17:18Z","title":"RoBridge: A Hierarchical Architecture Bridging Cognition and Execution\n  for General Robotic Manipulation","summary":"  Operating robots in open-ended scenarios with diverse tasks is a crucial\nresearch and application direction in robotics. While recent progress in\nnatural language processing and large multimodal models has enhanced robots'\nability to understand complex instructions, robot manipulation still faces the\nprocedural skill dilemma and the declarative skill dilemma in open\nenvironments. Existing methods often compromise cognitive and executive\ncapabilities. To address these challenges, in this paper, we propose RoBridge,\na hierarchical intelligent architecture for general robotic manipulation. It\nconsists of a high-level cognitive planner (HCP) based on a large-scale\npre-trained vision-language model (VLM), an invariant operable representation\n(IOR) serving as a symbolic bridge, and a generalist embodied agent (GEA).\nRoBridge maintains the declarative skill of VLM and unleashes the procedural\nskill of reinforcement learning, effectively bridging the gap between cognition\nand execution. RoBridge demonstrates significant performance improvements over\nexisting baselines, achieving a 75% success rate on new tasks and an 83%\naverage success rate in sim-to-real generalization using only five real-world\ndata samples per task. This work represents a significant step towards\nintegrating cognitive reasoning with physical execution in robotic systems,\noffering a new paradigm for general robotic manipulation.\n","authors":["Kaidong Zhang","Rongtao Xu","Pengzhen Ren","Junfan Lin","Hefeng Wu","Liang Lin","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2505.01709v3.pdf","comment":"project page: https://abliao.github.io/RoBridge/"},{"id":"http://arxiv.org/abs/2507.17253v1","updated":"2025-07-23T06:49:56Z","published":"2025-07-23T06:49:56Z","title":"Optimizing Delivery Logistics: Enhancing Speed and Safety with Drone\n  Technology","summary":"  The increasing demand for fast and cost effective last mile delivery\nsolutions has catalyzed significant advancements in drone based logistics. This\nresearch describes the development of an AI integrated drone delivery system,\nfocusing on route optimization, object detection, secure package handling, and\nreal time tracking. The proposed system leverages YOLOv4 Tiny for object\ndetection, the NEO 6M GPS module for navigation, and the A7670 SIM module for\nreal time communication. A comparative analysis of lightweight AI models and\nhardware components is conducted to determine the optimal configuration for\nreal time UAV based delivery. Key challenges including battery efficiency,\nregulatory compliance, and security considerations are addressed through the\nintegration of machine learning techniques, IoT devices, and encryption\nprotocols. Preliminary studies demonstrate improvement in delivery time\ncompared to conventional ground based logistics, along with high accuracy\nrecipient authentication through facial recognition. The study also discusses\nethical implications and societal acceptance of drone deliveries, ensuring\ncompliance with FAA, EASA and DGCA regulatory standards. Note: This paper\npresents the architecture, design, and preliminary simulation results of the\nproposed system. Experimental results, simulation benchmarks, and deployment\nstatistics are currently being acquired. A comprehensive analysis will be\nincluded in the extended version of this work.\n","authors":["Maharshi Shastri","Ujjval Shrivastav"],"pdf_url":"https://arxiv.org/pdf/2507.17253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17220v1","updated":"2025-07-23T05:34:20Z","published":"2025-07-23T05:34:20Z","title":"PIG-Nav: Key Insights for Pretrained Image Goal Navigation Models","summary":"  Recent studies have explored pretrained (foundation) models for vision-based\nrobotic navigation, aiming to achieve generalizable navigation and positive\ntransfer across diverse environments while enhancing zero-shot performance in\nunseen settings. In this work, we introduce PIG-Nav (Pretrained Image-Goal\nNavigation), a new approach that further investigates pretraining strategies\nfor vision-based navigation models and contributes in two key areas.\nModel-wise, we identify two critical design choices that consistently improve\nthe performance of pretrained navigation models: (1) integrating an\nearly-fusion network structure to combine visual observations and goal images\nvia appropriately pretrained Vision Transformer (ViT) image encoder, and (2)\nintroducing suitable auxiliary tasks to enhance global navigation\nrepresentation learning, thus further improving navigation performance.\nDataset-wise, we propose a novel data preprocessing pipeline for efficiently\nlabeling large-scale game video datasets for navigation model training. We\ndemonstrate that augmenting existing open navigation datasets with diverse\ngameplay videos improves model performance. Our model achieves an average\nimprovement of 22.6% in zero-shot settings and a 37.5% improvement in\nfine-tuning settings over existing visual navigation foundation models in two\ncomplex simulated environments and one real-world environment. These results\nadvance the state-of-the-art in pretrained image-goal navigation models.\nNotably, our model maintains competitive performance while requiring\nsignificantly less fine-tuning data, highlighting its potential for real-world\ndeployment with minimal labeled supervision.\n","authors":["Jiansong Wan","Chengming Zhou","Jinkua Liu","Xiangge Huang","Xiaoyu Chen","Xiaohan Yi","Qisen Yang","Baiting Zhu","Xin-Qiang Cai","Lixing Liu","Rushuai Yang","Chuheng Zhang","Sherif Abdelfattah","Hayong Shin","Pushi Zhang","Li Zhao","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2507.17220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.08979v2","updated":"2025-07-23T05:32:38Z","published":"2025-06-10T16:48:27Z","title":"Rethinking Range-View LiDAR Segmentation in Adverse Weather","summary":"  LiDAR segmentation has emerged as an important task to enrich scene\nperception and understanding. Range-view-based methods have gained popularity\ndue to their high computational efficiency and compatibility with real-time\ndeployment. However, their generalized performance under adverse weather\nconditions remains underexplored, limiting their reliability in real-world\nenvironments. In this work, we identify and analyze the unique challenges that\naffect the generalization of range-view LiDAR segmentation in severe weather.\nTo address these challenges, we propose a modular and lightweight framework\nthat enhances robustness without altering the core architecture of existing\nmodels. Our method reformulates the initial stem block of standard range-view\nnetworks into two branches to process geometric attributes and reflectance\nintensity separately. Specifically, a Geometric Abnormality Suppression (GAS)\nmodule reduces the influence of weather-induced spatial noise, and a\nReflectance Distortion Calibration (RDC) module corrects reflectance\ndistortions through memory-guided adaptive instance normalization. The\nprocessed features are then fused and passed to the original segmentation\npipeline. Extensive experiments on different benchmarks and baseline models\ndemonstrate that our approach significantly improves generalization to adverse\nweather with minimal inference overhead, offering a practical and effective\nsolution for real-world LiDAR segmentation.\n","authors":["Longyu Yang","Lu Zhang","Jun Liu","Yap-Peng Tan","Heng Tao Shen","Xiaofeng Zhu","Ping Hu"],"pdf_url":"https://arxiv.org/pdf/2506.08979v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07493v2","updated":"2025-07-23T05:31:07Z","published":"2024-12-10T13:18:45Z","title":"Onto-LLM-TAMP: Knowledge-oriented Task and Motion Planning using Large\n  Language Models","summary":"  Performing complex manipulation tasks in dynamic environments requires\nefficient Task and Motion Planning (TAMP) approaches that combine high-level\nsymbolic plans with low-level motion control. Advances in Large Language Models\n(LLMs), such as GPT-4, are transforming task planning by offering natural\nlanguage as an intuitive and flexible way to describe tasks, generate symbolic\nplans, and reason. However, the effectiveness of LLM-based TAMP approaches is\nlimited due to static and template-based prompting, which limits adaptability\nto dynamic environments and complex task contexts. To address these\nlimitations, this work proposes a novel Onto-LLM-TAMP framework that employs\nknowledge-based reasoning to refine and expand user prompts with\ntask-contextual reasoning and knowledge-based environment state descriptions.\nIntegrating domain-specific knowledge into the prompt ensures semantically\naccurate and context-aware task plans. The proposed framework demonstrates its\neffectiveness by resolving semantic errors in symbolic plan generation, such as\nmaintaining logical temporal goal ordering in scenarios involving hierarchical\nobject placement. The proposed framework is validated through both simulation\nand real-world scenarios, demonstrating significant improvements over the\nbaseline approach in terms of adaptability to dynamic environments and the\ngeneration of semantically correct task plans.\n","authors":["Muhayy Ud Din","Jan Rosell","Waseem Akram","Isiah Zaplana","Maximo A Roa","Irfan Hussain"],"pdf_url":"https://arxiv.org/pdf/2412.07493v2.pdf","comment":"Submitted to knowledge based systems"},{"id":"http://arxiv.org/abs/2507.17210v1","updated":"2025-07-23T05:06:34Z","published":"2025-07-23T05:06:34Z","title":"FAST-Calib: LiDAR-Camera Extrinsic Calibration in One Second","summary":"  This paper proposes FAST-Calib, a fast and user-friendly LiDAR-camera\nextrinsic calibration tool based on a custom-made 3D target. FAST-Calib\nsupports both mechanical and solid-state LiDARs by leveraging an efficient and\nreliable edge extraction algorithm that is agnostic to LiDAR scan patterns. It\nalso compensates for edge dilation artifacts caused by LiDAR spot spread\nthrough ellipse fitting, and supports joint optimization across multiple\nscenes. We validate FAST-Calib on three LiDAR models (Ouster, Avia, and\nMid360), each paired with a wide-angle camera. Experimental results demonstrate\nsuperior accuracy and robustness compared to existing methods. With\npoint-to-point registration errors consistently below 6.5mm and total\nprocessing time under 0.7s, FAST-Calib provides an efficient, accurate, and\ntarget-based automatic calibration pipeline. We have open-sourced our code and\ndataset on GitHub to benefit the robotics community.\n","authors":["Chunran Zheng","Fu Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.17210v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12122v2","updated":"2025-07-23T04:56:04Z","published":"2025-03-15T13:03:20Z","title":"ICCO: Learning an Instruction-conditioned Coordinator for\n  Language-guided Task-aligned Multi-robot Control","summary":"  Recent advances in Large Language Models (LLMs) have permitted the\ndevelopment of language-guided multi-robot systems, which allow robots to\nexecute tasks based on natural language instructions. However, achieving\neffective coordination in distributed multi-agent environments remains\nchallenging due to (1) misalignment between instructions and task requirements\nand (2) inconsistency in robot behaviors when they independently interpret\nambiguous instructions. To address these challenges, we propose\nInstruction-Conditioned Coordinator (ICCO), a Multi-Agent Reinforcement\nLearning (MARL) framework designed to enhance coordination in language-guided\nmulti-robot systems. ICCO consists of a Coordinator agent and multiple Local\nAgents, where the Coordinator generates Task-Aligned and Consistent\nInstructions (TACI) by integrating language instructions with environmental\nstates, ensuring task alignment and behavioral consistency. The Coordinator and\nLocal Agents are jointly trained to optimize a reward function that balances\ntask efficiency and instruction following. A Consistency Enhancement Term is\nadded to the learning objective to maximize mutual information between\ninstructions and robot behaviors, further improving coordination. Simulation\nand real-world experiments validate the effectiveness of ICCO in achieving\nlanguage-guided task-aligned multi-robot control. The demonstration can be\nfound at https://yanoyoshiki.github.io/ICCO/.\n","authors":["Yoshiki Yano","Kazuki Shibata","Maarten Kokshoorn","Takamitsu Matsubara"],"pdf_url":"https://arxiv.org/pdf/2503.12122v2.pdf","comment":"8 pages, 9 figures, to be published in the 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems"},{"id":"http://arxiv.org/abs/2507.17163v1","updated":"2025-07-23T03:09:22Z","published":"2025-07-23T03:09:22Z","title":"Reconfigurable Tendon-Driven Robots: Eliminating Inter-segmental\n  Coupling via Independently Lockable Joints","summary":"  With a slender redundant body, the tendon-driven robot (TDR) has a large\nworkspace and great maneuverability while working in complex environments. TDR\ncomprises multiple independently controlled robot segments, each with a set of\ndriving tendons. While increasing the number of robot segments enhances\ndexterity and expands the workspace, this structural expansion also introduces\nintensified inter-segmental coupling. Therefore, achieving precise TDR control\nrequires more complex models and additional motors. This paper presents a\nreconfigurable tendon-driven robot (RTR) equipped with innovative lockable\njoints. Each joint's state (locked/free) can be individually controlled through\na pair of antagonistic tendons, and its structure eliminates the need for a\ncontinuous power supply to maintain the state. Operators can selectively\nactuate the targeted robot segments, and this scheme fundamentally eliminates\nthe inter-segmental coupling, thereby avoiding the requirement for complex\ncoordinated control between segments. The workspace of RTR has been simulated\nand compared with traditional TDRs' workspace, and RTR's advantages are further\nrevealed. The kinematics and statics models of the RTR have been derived and\nvalidation experiments have been conducted. Demonstrations have been performed\nusing a seven-joint RTR prototype to show its reconfigurability and moving\nability in complex environments with an actuator pack comprising only six\nmotors.\n","authors":["Botao Lin","Shuang Song","Jiaole Wang"],"pdf_url":"https://arxiv.org/pdf/2507.17163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17152v1","updated":"2025-07-23T02:35:04Z","published":"2025-07-23T02:35:04Z","title":"JAM: Keypoint-Guided Joint Prediction after Classification-Aware\n  Marginal Proposal for Multi-Agent Interaction","summary":"  Predicting the future motion of road participants is a critical task in\nautonomous driving. In this work, we address the challenge of low-quality\ngeneration of low-probability modes in multi-agent joint prediction. To tackle\nthis issue, we propose a two-stage multi-agent interactive prediction framework\nnamed \\textit{keypoint-guided joint prediction after classification-aware\nmarginal proposal} (JAM). The first stage is modeled as a marginal prediction\nprocess, which classifies queries by trajectory type to encourage the model to\nlearn all categories of trajectories, providing comprehensive mode information\nfor the joint prediction module. The second stage is modeled as a joint\nprediction process, which takes the scene context and the marginal proposals\nfrom the first stage as inputs to learn the final joint distribution. We\nexplicitly introduce key waypoints to guide the joint prediction module in\nbetter capturing and leveraging the critical information from the initial\npredicted trajectories. We conduct extensive experiments on the real-world\nWaymo Open Motion Dataset interactive prediction benchmark. The results show\nthat our approach achieves competitive performance. In particular, in the\nframework comparison experiments, the proposed JAM outperforms other prediction\nframeworks and achieves state-of-the-art performance in interactive trajectory\nprediction. The code is available at https://github.com/LinFunster/JAM to\nfacilitate future research.\n","authors":["Fangze Lin","Ying He","Fei Yu","Hong Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.17152v1.pdf","comment":"IROS 2025 Accepted"},{"id":"http://arxiv.org/abs/2507.17144v1","updated":"2025-07-23T02:25:03Z","published":"2025-07-23T02:25:03Z","title":"Falconry-like palm landing by a flapping-wing drone based on the human\n  gesture interaction and distance-aware flight planning","summary":"  Flapping-wing drones have attracted significant attention due to their\nbiomimetic flight. They are considered more human-friendly due to their\ncharacteristics such as low noise and flexible wings, making them suitable for\nhuman-drone interactions. However, few studies have explored the practical\ninteraction between humans and flapping-wing drones. On establishing a physical\ninteraction system with flapping-wing drones, we can acquire inspirations from\nfalconers who guide birds of prey to land on their arms. This interaction\ninterprets the human body as a dynamic landing platform, which can be utilized\nin various scenarios such as crowded or spatially constrained environments.\nThus, in this study, we propose a falconry-like interaction system in which a\nflapping-wing drone performs a palm landing motion on a human hand. To achieve\na safe approach toward humans, we design a trajectory planning method that\nconsiders both physical and psychological factors of the human safety such as\nthe drone's velocity and distance from the user. We use a commercial flapping\nplatform with our implemented motion planning and conduct experiments to\nevaluate the palm landing performance and safety. The results demonstrate that\nour approach enables safe and smooth hand landing interactions. To the best of\nour knowledge, it is the first time to achieve a contact-based interaction\nbetween flapping-wing drones and humans.\n","authors":["Kazuki Numazato","Keiichiro Kan","Masaki Kitagawa","Yunong Li","Johannes Kubel","Moju Zhao"],"pdf_url":"https://arxiv.org/pdf/2507.17144v1.pdf","comment":"8 pages, 14 figures"},{"id":"http://arxiv.org/abs/2507.17141v1","updated":"2025-07-23T02:23:41Z","published":"2025-07-23T02:23:41Z","title":"Towards Human-level Intelligence via Human-like Whole-Body Manipulation","summary":"  Building general-purpose intelligent robots has long been a fundamental goal\nof robotics. A promising approach is to mirror the evolutionary trajectory of\nhumans: learning through continuous interaction with the environment, with\nearly progress driven by the imitation of human behaviors. Achieving this goal\npresents three core challenges: (1) designing safe robotic hardware with\nhuman-level physical capabilities; (2) developing an intuitive and scalable\nwhole-body teleoperation interface for data collection; and (3) creating\nalgorithms capable of learning whole-body visuomotor policies from human\ndemonstrations. To address these challenges in a unified framework, we propose\nAstribot Suite, a robot learning suite for whole-body manipulation aimed at\ngeneral daily tasks across diverse environments. We demonstrate the\neffectiveness of our system on a wide range of activities that require\nwhole-body coordination, extensive reachability, human-level dexterity, and\nagility. Our results show that Astribot's cohesive integration of embodiment,\nteleoperation interface, and learning pipeline marks a significant step towards\nreal-world, general-purpose whole-body robotic manipulation, laying the\ngroundwork for the next generation of intelligent robots.\n","authors":["Guang Gao","Jianan Wang","Jinbo Zuo","Junnan Jiang","Jingfan Zhang","Xianwen Zeng","Yuejiang Zhu","Lianyang Ma","Ke Chen","Minhua Sheng","Ruirui Zhang","Zhaohui An"],"pdf_url":"https://arxiv.org/pdf/2507.17141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17140v1","updated":"2025-07-23T02:23:34Z","published":"2025-07-23T02:23:34Z","title":"Multi-Objective Trajectory Planning for a Robotic Arm in Curtain Wall\n  Installation","summary":"  In the context of labor shortages and rising costs, construction robots are\nregarded as the key to revolutionizing traditional construction methods and\nimproving efficiency and quality in the construction industry. In order to\nensure that construction robots can perform tasks efficiently and accurately in\ncomplex construction environments, traditional single-objective trajectory\noptimization methods are difficult to meet the complex requirements of the\nchanging construction environment. Therefore, we propose a multi-objective\ntrajectory optimization for the robotic arm used in the curtain wall\ninstallation. First, we design a robotic arm for curtain wall installation,\nintegrating serial, parallel, and folding arm elements, while considering its\nphysical properties and motion characteristics. In addition, this paper\nproposes an NSGA-III-FO algorithm (NSGA-III with Focused Operator, NSGA-III-FO)\nthat incorporates a focus operator screening mechanism to accelerate the\nconvergence of the algorithm towards the Pareto front, thereby effectively\nbalancing the multi-objective constraints of construction robots. The proposed\nalgorithm is tested against NSGA-III, MOEA/D, and MSOPS-II in ten consecutive\ntrials on the DTLZ3 and WFG3 test functions, showing significantly better\nconvergence efficiency than the other algorithms. Finally, we conduct two sets\nof experiments on the designed robotic arm platform, which confirm the\nefficiency and practicality of the NSGA-III-FO algorithm in solving\nmulti-objective trajectory planning problems for curtain wall installation\ntasks.\n","authors":["Xiao Liu","Yunxiao Cheng","Weijun Wang","Tianlun Huang","Zhiyong Wang","Wei Feng"],"pdf_url":"https://arxiv.org/pdf/2507.17140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17136v1","updated":"2025-07-23T02:17:27Z","published":"2025-07-23T02:17:27Z","title":"Dynamic Parameter Identification of a Curtain Wall Installation Robotic\n  Arm","summary":"  In the construction industry, traditional methods fail to meet the modern\ndemands for efficiency and quality. The curtain wall installation is a critical\ncomponent of construction projects. We design a hydraulically driven robotic\narm for curtain wall installation and a dynamic parameter identification\nmethod. We establish a Denavit-Hartenberg (D-H) model based on measured robotic\narm structural parameters and integrate hydraulic cylinder dynamics to\nconstruct a composite parametric system driven by a Stribeck friction model. By\ndesigning high-signal-to-noise ratio displacement excitation signals for\nhydraulic cylinders and combining Fourier series to construct optimal\nexcitation trajectories that satisfy joint constraints, this method effectively\nexcites the characteristics of each parameter in the minimal parameter set of\nthe dynamic model of the robotic arm. On this basis, a hierarchical progressive\nparameter identification strategy is proposed: least squares estimation is\nemployed to separately identify and jointly calibrate the dynamic parameters of\nboth the hydraulic cylinder and the robotic arm, yielding Stribeck model curves\nfor each joint. Experimental validation on a robotic arm platform demonstrates\nresidual standard deviations below 0.4 Nm between theoretical and measured\njoint torques, confirming high-precision dynamic parameter identification for\nthe hydraulic-driven curtain wall installation robotic arm. This significantly\ncontributes to enhancing the intelligence level of curtain wall installation\noperations.\n","authors":["Xiao Liu","Yunxiao Cheng","Weijun Wang","Tianlun Huang","Wei Feng"],"pdf_url":"https://arxiv.org/pdf/2507.17136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17132v1","updated":"2025-07-23T02:12:35Z","published":"2025-07-23T02:12:35Z","title":"Dynamic Modeling and Dimensional Optimization of Legged Mechanisms for\n  Construction Robot","summary":"  With the rapid development of the construction industry, issues such as harsh\nworking environments, high-intensity and high-risk tasks, and labor shortages\nhave become increasingly prominent. This drives higher demands for construction\nrobots in terms of low energy consumption, high mobility, and high load\ncapacity. This paper focuses on the design and optimization of leg structures\nfor construction robots, aiming to improve their dynamic performance, reduce\nenergy consumption, and enhance load-bearing capabilities. Firstly, based on\nthe leg configuration of ants in nature, we design a structure for the robot's\nleg. Secondly, we propose a novel structural optimization method. Using the\nLagrangian approach, a dynamic model of the leg was established. Combining the\ndynamic model with the leg's motion trajectory, we formulated multiple dynamic\nevaluation metrics and conducted a comprehensive optimization study on the\ngeometric parameters of each leg segment. The results show that the optimized\nleg structure reduces peak joint torques and energy consumption by over 20%.\nFinally, dynamic simulation experiments were conducted using ADAMS. The results\ndemonstrate a significant reduction in the driving power of each joint after\noptimization, validating the effectiveness and rationality of the proposed\nstrategy. This study provides a theoretical foundation and technical support\nfor the design of heavy-load, high-performance construction robots.\n","authors":["Xiao Liu","Xianlong Yang","Weijun Wang","Wei Feng"],"pdf_url":"https://arxiv.org/pdf/2507.17132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17130v1","updated":"2025-07-23T02:11:12Z","published":"2025-07-23T02:11:12Z","title":"MARSCalib: Multi-robot, Automatic, Robust, Spherical Target-based\n  Extrinsic Calibration in Field and Extraterrestrial Environments","summary":"  This paper presents a novel spherical target-based LiDAR-camera extrinsic\ncalibration method designed for outdoor environments with multi-robot systems,\nconsidering both target and sensor corruption. The method extracts the 2D\nellipse center from the image and the 3D sphere center from the pointcloud,\nwhich are then paired to compute the transformation matrix. Specifically, the\nimage is first decomposed using the Segment Anything Model (SAM). Then, a novel\nalgorithm extracts an ellipse from a potentially corrupted sphere, and the\nextracted center of ellipse is corrected for errors caused by the perspective\nprojection model. For the LiDAR pointcloud, points on the sphere tend to be\nhighly noisy due to the absence of flat regions. To accurately extract the\nsphere from these noisy measurements, we apply a hierarchical weighted sum to\nthe accumulated pointcloud. Through experiments, we demonstrated that the\nsphere can be robustly detected even under both types of corruption,\noutperforming other targets. We evaluated our method using three different\ntypes of LiDARs (spinning, solid-state, and non-repetitive) with cameras\npositioned in three different locations. Furthermore, we validated the\nrobustness of our method to target corruption by experimenting with spheres\nsubjected to various types of degradation. These experiments were conducted in\nboth a planetary test and a field environment. Our code is available at\nhttps://github.com/sparolab/MARSCalib.\n","authors":["Seokhwan Jeong","Hogyun Kim","Younggun Cho"],"pdf_url":"https://arxiv.org/pdf/2507.17130v1.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2507.17089v1","updated":"2025-07-23T00:09:36Z","published":"2025-07-23T00:09:36Z","title":"IONext: Unlocking the Next Era of Inertial Odometry","summary":"  Researchers have increasingly adopted Transformer-based models for inertial\nodometry. While Transformers excel at modeling long-range dependencies, their\nlimited sensitivity to local, fine-grained motion variations and lack of\ninherent inductive biases often hinder localization accuracy and\ngeneralization. Recent studies have shown that incorporating large-kernel\nconvolutions and Transformer-inspired architectural designs into CNN can\neffectively expand the receptive field, thereby improving global motion\nperception. Motivated by these insights, we propose a novel CNN-based module\ncalled the Dual-wing Adaptive Dynamic Mixer (DADM), which adaptively captures\nboth global motion patterns and local, fine-grained motion features from\ndynamic inputs. This module dynamically generates selective weights based on\nthe input, enabling efficient multi-scale feature aggregation. To further\nimprove temporal modeling, we introduce the Spatio-Temporal Gating Unit (STGU),\nwhich selectively extracts representative and task-relevant motion features in\nthe temporal domain. This unit addresses the limitations of temporal modeling\nobserved in existing CNN approaches. Built upon DADM and STGU, we present a new\nCNN-based inertial odometry backbone, named Next Era of Inertial Odometry\n(IONext). Extensive experiments on six public datasets demonstrate that IONext\nconsistently outperforms state-of-the-art (SOTA) Transformer- and CNN-based\nmethods. For instance, on the RNIN dataset, IONext reduces the average ATE by\n10% and the average RTE by 12% compared to the representative model iMOT.\n","authors":["Shanshan Zhang","Siyue Wang","Tianshui Wen","Qi Zhang","Ziheng Zhou","Lingxiang Zheng","Yu Yang"],"pdf_url":"https://arxiv.org/pdf/2507.17089v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2507.17987v1","updated":"2025-07-23T23:35:11Z","published":"2025-07-23T23:35:11Z","title":"Bearded Dragon Activity Recognition Pipeline: An AI-Based Approach to\n  Behavioural Monitoring","summary":"  Traditional monitoring of bearded dragon (Pogona Viticeps) behaviour is\ntime-consuming and prone to errors. This project introduces an automated system\nfor real-time video analysis, using You Only Look Once (YOLO) object detection\nmodels to identify two key behaviours: basking and hunting. We trained five\nYOLO variants (v5, v7, v8, v11, v12) on a custom, publicly available dataset of\n1200 images, encompassing bearded dragons (600), heating lamps (500), and\ncrickets (100). YOLOv8s was selected as the optimal model due to its superior\nbalance of accuracy (mAP@0.5:0.95 = 0.855) and speed. The system processes\nvideo footage by extracting per-frame object coordinates, applying temporal\ninterpolation for continuity, and using rule-based logic to classify specific\nbehaviours. Basking detection proved reliable. However, hunting detection was\nless accurate, primarily due to weak cricket detection (mAP@0.5 = 0.392).\nFuture improvements will focus on enhancing cricket detection through expanded\ndatasets or specialised small-object detectors. This automated system offers a\nscalable solution for monitoring reptile behaviour in controlled environments,\nsignificantly improving research efficiency and data quality.\n","authors":["Arsen Yermukan","Pedro Machado","Feliciano Domingos","Isibor Kennedy Ihianle","Jordan J. Bird","Stefano S. K. Kaburu","Samantha J. Ward"],"pdf_url":"https://arxiv.org/pdf/2507.17987v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.06121v8","updated":"2025-07-23T23:32:46Z","published":"2025-04-08T15:13:01Z","title":"A Robust Real-Time Lane Detection Method with Fog-Enhanced Feature\n  Fusion for Foggy Conditions","summary":"  Lane detection is a critical component of Advanced Driver Assistance Systems\n(ADAS). Existing lane detection algorithms generally perform well under\nfavorable weather conditions. However, their performance degrades significantly\nin adverse conditions, such as fog, which increases the risk of traffic\naccidents. This challenge is compounded by the lack of specialized datasets and\nmethods designed for foggy environments. To address this, we introduce the\nFoggyLane dataset, captured in real-world foggy scenarios, and synthesize two\nadditional datasets, FoggyCULane and FoggyTusimple, from existing popular lane\ndetection datasets. Furthermore, we propose a robust Fog-Enhanced Network for\nlane detection, incorporating a Global Feature Fusion Module (GFFM) to capture\nglobal relationships in foggy images, a Kernel Feature Fusion Module (KFFM) to\nmodel the structural and positional relationships of lane instances, and a\nLow-level Edge Enhanced Module (LEEM) to address missing edge details in foggy\nconditions. Comprehensive experiments demonstrate that our method achieves\nstate-of-the-art performance, with F1-scores of 95.04 on FoggyLane, 79.85 on\nFoggyCULane, and 96.95 on FoggyTusimple. Additionally, with TensorRT\nacceleration, the method reaches a processing speed of 38.4 FPS on the NVIDIA\nJetson AGX Orin, confirming its real-time capabilities and robustness in foggy\nenvironments.\n","authors":["Ronghui Zhang","Yuhang Ma","Tengfei Li","Ziyu Lin","Yueying Wu","Junzhou Chen","Lin Zhang","Jia Hu","Tony Z. Qiu","Konghui Guo"],"pdf_url":"https://arxiv.org/pdf/2504.06121v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17971v1","updated":"2025-07-23T22:37:26Z","published":"2025-07-23T22:37:26Z","title":"Benchmarking of Deep Learning Methods for Generic MRI\n  Multi-OrganAbdominal Segmentation","summary":"  Recent advances in deep learning have led to robust automated tools for\nsegmentation of abdominal computed tomography (CT). Meanwhile, segmentation of\nmagnetic resonance imaging (MRI) is substantially more challenging due to the\ninherent signal variability and the increased effort required for annotating\ntraining datasets. Hence, existing approaches are trained on limited sets of\nMRI sequences, which might limit their generalizability. To characterize the\nlandscape of MRI abdominal segmentation tools, we present here a comprehensive\nbenchmarking of the three state-of-the-art and open-source models:\nMRSegmentator, MRISegmentator-Abdomen, and TotalSegmentator MRI. Since these\nmodels are trained using labor-intensive manual annotation cycles, we also\nintroduce and evaluate ABDSynth, a SynthSeg-based model purely trained on\nwidely available CT segmentations (no real images). More generally, we assess\naccuracy and generalizability by leveraging three public datasets (not seen by\nany of the evaluated methods during their training), which span all major\nmanufacturers, five MRI sequences, as well as a variety of subject conditions,\nvoxel resolutions, and fields-of-view. Our results reveal that MRSegmentator\nachieves the best performance and is most generalizable. In contrast, ABDSynth\nyields slightly less accurate results, but its relaxed requirements in training\ndata make it an alternative when the annotation budget is limited. The\nevaluation code and datasets are given for future benchmarking at\nhttps://github.com/deepakri201/AbdoBench, along with inference code and weights\nfor ABDSynth.\n","authors":["Deepa Krishnaswamy","Cosmin Ciausu","Steve Pieper","Ron Kikinis","Benjamin Billot","Andrey Fedorov"],"pdf_url":"https://arxiv.org/pdf/2507.17971v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08513v2","updated":"2025-07-23T22:34:55Z","published":"2025-07-11T12:00:10Z","title":"Advancing Multimodal LLMs by Large-Scale 3D Visual Instruction Dataset\n  Generation","summary":"  Multimodal Large Language Models (MLLMs) struggle with accurately capturing\ncamera-object relations, especially for object orientation, camera viewpoint,\nand camera shots. This stems from the fact that existing MLLMs are trained on\nimages with limited diverse camera-object relations and corresponding textual\ndescriptions. To address this, we propose a synthetic generation pipeline to\ncreate large-scale 3D visual instruction datasets. Our framework takes 3D\nassets as input and uses rendering and diffusion-based image generation models\nto create photorealistic images preserving precise camera-object relations.\nAdditionally, large language models (LLMs) are used to generate text prompts\nfor guiding visual instruction tuning and controlling image generation. We\ncreate Ultimate3D, a dataset of 240K VQAs with precise camera-object\nannotations, and corresponding benchmark. MLLMs fine-tuned on our proposed\ndataset outperform commercial models by a large margin, achieving an average\naccuracy improvement of 33.4% on camera-object relation recognition tasks. Our\ncode, dataset, and benchmark will contribute to broad MLLM applications.\n","authors":["Liu He","Xiao Zeng","Yizhi Song","Albert Y. C. Chen","Lu Xia","Shashwat Verma","Sankalp Dayal","Min Sun","Cheng-Hao Kuo","Daniel Aliaga"],"pdf_url":"https://arxiv.org/pdf/2507.08513v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17963v1","updated":"2025-07-23T22:09:38Z","published":"2025-07-23T22:09:38Z","title":"Zero-Shot Dynamic Concept Personalization with Grid-Based LoRA","summary":"  Recent advances in text-to-video generation have enabled high-quality\nsynthesis from text and image prompts. While the personalization of dynamic\nconcepts, which capture subject-specific appearance and motion from a single\nvideo, is now feasible, most existing methods require per-instance fine-tuning,\nlimiting scalability. We introduce a fully zero-shot framework for dynamic\nconcept personalization in text-to-video models. Our method leverages\nstructured 2x2 video grids that spatially organize input and output pairs,\nenabling the training of lightweight Grid-LoRA adapters for editing and\ncomposition within these grids. At inference, a dedicated Grid Fill module\ncompletes partially observed layouts, producing temporally coherent and\nidentity preserving outputs. Once trained, the entire system operates in a\nsingle forward pass, generalizing to previously unseen dynamic concepts without\nany test-time optimization. Extensive experiments demonstrate high-quality and\nconsistent results across a wide range of subjects beyond trained concepts and\nediting scenarios.\n","authors":["Rameen Abdal","Or Patashnik","Ekaterina Deyneka","Hao Chen","Aliaksandr Siarohin","Sergey Tulyakov","Daniel Cohen-Or","Kfir Aberman"],"pdf_url":"https://arxiv.org/pdf/2507.17963v1.pdf","comment":"Project Page and Video :\n  https://snap-research.github.io/zero-shot-dynamic-concepts/"},{"id":"http://arxiv.org/abs/2507.17959v1","updated":"2025-07-23T22:03:29Z","published":"2025-07-23T22:03:29Z","title":"OPEN: A Benchmark Dataset and Baseline for Older Adult Patient\n  Engagement Recognition in Virtual Rehabilitation Learning Environments","summary":"  Engagement in virtual learning is essential for participant satisfaction,\nperformance, and adherence, particularly in online education and virtual\nrehabilitation, where interactive communication plays a key role. Yet,\naccurately measuring engagement in virtual group settings remains a challenge.\nThere is increasing interest in using artificial intelligence (AI) for\nlarge-scale, real-world, automated engagement recognition. While engagement has\nbeen widely studied in younger academic populations, research and datasets\nfocused on older adults in virtual and telehealth learning settings remain\nlimited. Existing methods often neglect contextual relevance and the\nlongitudinal nature of engagement across sessions. This paper introduces OPEN\n(Older adult Patient ENgagement), a novel dataset supporting AI-driven\nengagement recognition. It was collected from eleven older adults participating\nin weekly virtual group learning sessions over six weeks as part of cardiac\nrehabilitation, producing over 35 hours of data, making it the largest dataset\nof its kind. To protect privacy, raw video is withheld; instead, the released\ndata include facial, hand, and body joint landmarks, along with affective and\nbehavioral features extracted from video. Annotations include binary engagement\nstates, affective and behavioral labels, and context-type indicators, such as\nwhether the instructor addressed the group or an individual. The dataset offers\nversions with 5-, 10-, 30-second, and variable-length samples. To demonstrate\nutility, multiple machine learning and deep learning models were trained,\nachieving engagement recognition accuracy of up to 81 percent. OPEN provides a\nscalable foundation for personalized engagement modeling in aging populations\nand contributes to broader engagement recognition research.\n","authors":["Ali Abedi","Sadaf Safa","Tracey J. F. Colella","Shehroz S. Khan"],"pdf_url":"https://arxiv.org/pdf/2507.17959v1.pdf","comment":"14 pages, 3 figures, 7 tables"},{"id":"http://arxiv.org/abs/2507.17958v1","updated":"2025-07-23T22:02:56Z","published":"2025-07-23T22:02:56Z","title":"VIBE: Video-Input Brain Encoder for fMRI Response Modeling","summary":"  We present VIBE, a two-stage Transformer that fuses multi-modal video, audio,\nand text features to predict fMRI activity. Representations from open-source\nmodels (Qwen2.5, BEATs, Whisper, SlowFast, V-JEPA) are merged by a\nmodality-fusion transformer and temporally decoded by a prediction transformer\nwith rotary embeddings. Trained on 65 hours of movie data from the CNeuroMod\ndataset and ensembled across 20 seeds, VIBE attains mean parcel-wise Pearson\ncorrelations of 32.25 on in-distribution Friends S07 and 21.25 on six\nout-of-distribution films. An earlier iteration of the same architecture\nobtained 0.3198 and 0.2096, respectively, winning Phase-1 and placing second\noverall in the Algonauts 2025 Challenge.\n","authors":["Daniel Carlstrom Schad","Shrey Dixit","Janis Keck","Viktor Studenyak","Aleksandr Shpilevoi","Andrej Bicanski"],"pdf_url":"https://arxiv.org/pdf/2507.17958v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17957v1","updated":"2025-07-23T22:02:17Z","published":"2025-07-23T22:02:17Z","title":"AFRDA: Attentive Feature Refinement for Domain Adaptive Semantic\n  Segmentation","summary":"  In Unsupervised Domain Adaptive Semantic Segmentation (UDA-SS), a model is\ntrained on labeled source domain data (e.g., synthetic images) and adapted to\nan unlabeled target domain (e.g., real-world images) without access to target\nannotations. Existing UDA-SS methods often struggle to balance fine-grained\nlocal details with global contextual information, leading to segmentation\nerrors in complex regions. To address this, we introduce the Adaptive Feature\nRefinement (AFR) module, which enhances segmentation accuracy by refining\nhighresolution features using semantic priors from low-resolution logits. AFR\nalso integrates high-frequency components, which capture fine-grained\nstructures and provide crucial boundary information, improving object\ndelineation. Additionally, AFR adaptively balances local and global information\nthrough uncertaintydriven attention, reducing misclassifications. Its\nlightweight design allows seamless integration into HRDA-based UDA methods,\nleading to state-of-the-art segmentation performance. Our approach improves\nexisting UDA-SS methods by 1.05% mIoU on GTA V --> Cityscapes and 1.04% mIoU on\nSynthia-->Cityscapes. The implementation of our framework is available at:\nhttps://github.com/Masrur02/AFRDA\n","authors":["Md. Al-Masrur Khan","Durgakant Pushp","Lantao Liu"],"pdf_url":"https://arxiv.org/pdf/2507.17957v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05407v3","updated":"2025-07-23T20:26:17Z","published":"2023-12-08T23:43:17Z","title":"ODES: Domain Adaptation with Expert Guidance for Online Medical Image\n  Segmentation","summary":"  Unsupervised domain adaptive segmentation typically relies on self-training\nusing pseudo labels predicted by a pre-trained network on an unlabeled target\ndataset. However, the noisy nature of such pseudo-labels presents a major\nbottleneck in adapting a network to the distribution shift between source and\ntarget datasets. This challenge is exaggerated when the network encounters an\nincoming data stream in online fashion, where the network is constrained to\nadapt to incoming streams of target domain data in exactly one round of forward\nand backward passes. In this scenario, relying solely on inaccurate\npseudo-labels can lead to low-quality segmentation, which is detrimental to\nmedical image analysis where accuracy and precision are of utmost priority. We\nhypothesize that a small amount of pixel-level annotation obtained from an\nexpert can address this problem, thereby enhancing the performance of domain\nadaptation of online streaming data, even in the absence of dedicated training\ndata. We call our method ODES: Domain Adaptation with Expert Guidance for\nOnline Medical Image Segmentation that adapts to each incoming data batch in an\nonline setup, incorporating feedback from an expert through active learning.\nThrough active learning, the most informative pixels in each image can be\nselected for expert annotation. However, the acquisition of pixel-level\nannotations across all images in a batch often leads to redundant information\nwhile increasing temporal overhead in online learning. To reduce the annotation\nacquisition time and make the adaptation process more online-friendly, we\nfurther propose a novel image-pruning strategy that selects the most useful\nsubset of images from the current batch for active learning. Our proposed\napproach outperforms existing online adaptation approaches and produces\ncompetitive results compared to offline domain adaptive active learning\nmethods.\n","authors":["Md Shazid Islam","Sayak Nag","Arindam Dutta","Miraj Ahmed","Fahim Faisal Niloy","Amit K. Roy-Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2312.05407v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17911v1","updated":"2025-07-23T20:21:29Z","published":"2025-07-23T20:21:29Z","title":"Hierarchical Diffusion Framework for Pseudo-Healthy Brain MRI Inpainting\n  with Enhanced 3D Consistency","summary":"  Pseudo-healthy image inpainting is an essential preprocessing step for\nanalyzing pathological brain MRI scans. Most current inpainting methods favor\nslice-wise 2D models for their high in-plane fidelity, but their independence\nacross slices produces discontinuities in the volume. Fully 3D models alleviate\nthis issue, but their high model capacity demands extensive training data for\nreliable, high-fidelity synthesis -- often impractical in medical settings. We\naddress these limitations with a hierarchical diffusion framework by replacing\ndirect 3D modeling with two perpendicular coarse-to-fine 2D stages. An axial\ndiffusion model first yields a coarse, globally consistent inpainting; a\ncoronal diffusion model then refines anatomical details. By combining\nperpendicular spatial views with adaptive resampling, our method balances data\nefficiency and volumetric consistency. Our experiments show our approach\noutperforms state-of-the-art baselines in both realism and volumetric\nconsistency, making it a promising solution for pseudo-healthy image\ninpainting. Code is available at\nhttps://github.com/dou0000/3dMRI-Consistent-Inpaint.\n","authors":["Dou Hoon Kwark","Shirui Luo","Xiyue Zhu","Yudu Li","Zhi-Pei Liang","Volodymyr Kindratenko"],"pdf_url":"https://arxiv.org/pdf/2507.17911v1.pdf","comment":"11 pages, 2 figures"},{"id":"http://arxiv.org/abs/2507.17897v1","updated":"2025-07-23T19:48:27Z","published":"2025-07-23T19:48:27Z","title":"Multimodal Recurrent Ensembles for Predicting Brain Responses to\n  Naturalistic Movies (Algonauts 2025)","summary":"  Accurately predicting distributed cortical responses to naturalistic stimuli\nrequires models that integrate visual, auditory and semantic information over\ntime. We present a hierarchical multimodal recurrent ensemble that maps\npretrained video, audio, and language embeddings to fMRI time series recorded\nwhile four subjects watched almost 80 hours of movies provided by the Algonauts\n2025 challenge. Modality-specific bidirectional RNNs encode temporal dynamics;\ntheir hidden states are fused and passed to a second recurrent layer, and\nlightweight subject-specific heads output responses for 1000 cortical parcels.\nTraining relies on a composite MSE-correlation loss and a curriculum that\ngradually shifts emphasis from early sensory to late association regions.\nAveraging 100 model variants further boosts robustness. The resulting system\nranked third on the competition leaderboard, achieving an overall Pearson r =\n0.2094 and the highest single-parcel peak score (mean r = 0.63) among all\nparticipants, with particularly strong gains for the most challenging subject\n(Subject 5). The approach establishes a simple, extensible baseline for future\nmultimodal brain-encoding benchmarks.\n","authors":["Semih Eren","Deniz Kucukahmetler","Nico Scherf"],"pdf_url":"https://arxiv.org/pdf/2507.17897v1.pdf","comment":"8 pages, 2 figures, 1 table. Invited report, CCN 2025 Algonauts\n  Project session (3rd-place team). Code:\n  https://github.com/erensemih/Algonauts2025_ModalityRNN"},{"id":"http://arxiv.org/abs/2507.17892v1","updated":"2025-07-23T19:41:49Z","published":"2025-07-23T19:41:49Z","title":"DiNAT-IR: Exploring Dilated Neighborhood Attention for High-Quality\n  Image Restoration","summary":"  Transformers, with their self-attention mechanisms for modeling long-range\ndependencies, have become a dominant paradigm in image restoration tasks.\nHowever, the high computational cost of self-attention limits scalability to\nhigh-resolution images, making efficiency-quality trade-offs a key research\nfocus. To address this, Restormer employs channel-wise self-attention, which\ncomputes attention across channels instead of spatial dimensions. While\neffective, this approach may overlook localized artifacts that are crucial for\nhigh-quality image restoration. To bridge this gap, we explore Dilated\nNeighborhood Attention (DiNA) as a promising alternative, inspired by its\nsuccess in high-level vision tasks. DiNA balances global context and local\nprecision by integrating sliding-window attention with mixed dilation factors,\neffectively expanding the receptive field without excessive overhead. However,\nour preliminary experiments indicate that directly applying this global-local\ndesign to the classic deblurring task hinders accurate visual restoration,\nprimarily due to the constrained global context understanding within local\nattention. To address this, we introduce a channel-aware module that\ncomplements local attention, effectively integrating global context without\nsacrificing pixel-level precision. The proposed DiNAT-IR, a Transformer-based\narchitecture specifically designed for image restoration, achieves competitive\nresults across multiple benchmarks, offering a high-quality solution for\ndiverse low-level computer vision problems.\n","authors":["Hanzhou Liu","Binghan Li","Chengkai Liu","Mi Lu"],"pdf_url":"https://arxiv.org/pdf/2507.17892v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13180v3","updated":"2025-07-23T19:22:35Z","published":"2025-04-17T17:59:56Z","title":"PerceptionLM: Open-Access Data and Models for Detailed Visual\n  Understanding","summary":"  Vision-language models are integral to computer vision research, yet many\nhigh-performing models remain closed-source, obscuring their data, design and\ntraining recipe. The research community has responded by using distillation\nfrom black-box models to label training data, achieving strong benchmark\nresults, at the cost of measurable scientific progress. However, without\nknowing the details of the teacher model and its data sources, scientific\nprogress remains difficult to measure. In this paper, we study building a\nPerception Language Model (PLM) in a fully open and reproducible framework for\ntransparent research in image and video understanding. We analyze standard\ntraining pipelines without distillation from proprietary models and explore\nlarge-scale synthetic data to identify critical data gaps, particularly in\ndetailed video understanding. To bridge these gaps, we release 2.8M\nhuman-labeled instances of fine-grained video question-answer pairs and\nspatio-temporally grounded video captions. Additionally, we introduce\nPLM-VideoBench, a suite for evaluating challenging video understanding tasks\nfocusing on the ability to reason about \"what\", \"where\", \"when\", and \"how\" of a\nvideo. We make our work fully reproducible by providing data, training recipes,\ncode & models. https://github.com/facebookresearch/perception_models\n","authors":["Jang Hyun Cho","Andrea Madotto","Effrosyni Mavroudi","Triantafyllos Afouras","Tushar Nagarajan","Muhammad Maaz","Yale Song","Tengyu Ma","Shuming Hu","Suyog Jain","Miguel Martin","Huiyu Wang","Hanoona Rasheed","Peize Sun","Po-Yao Huang","Daniel Bolya","Nikhila Ravi","Shashank Jain","Tammy Stark","Shane Moon","Babak Damavandi","Vivian Lee","Andrew Westbury","Salman Khan","Philipp Krähenbühl","Piotr Dollár","Lorenzo Torresani","Kristen Grauman","Christoph Feichtenhofer"],"pdf_url":"https://arxiv.org/pdf/2504.13180v3.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2507.17869v1","updated":"2025-07-23T18:53:23Z","published":"2025-07-23T18:53:23Z","title":"Integrating Feature Selection and Machine Learning for Nitrogen\n  Assessment in Grapevine Leaves using In-Field Hyperspectral Imaging","summary":"  Nitrogen (N) is one of the most crucial nutrients in vineyards, affecting\nplant growth and subsequent products such as wine and juice. Because soil N has\nhigh spatial and temporal variability, it is desirable to accurately estimate\nthe N concentration of grapevine leaves and manage fertilization at the\nindividual plant level to optimally meet plant needs. In this study, we used\nin-field hyperspectral images with wavelengths ranging from $400 to 1000nm of\nfour different grapevine cultivars collected from distinct vineyards and over\ntwo growth stages during two growing seasons to develop models for predicting N\nconcentration at the leaf-level and canopy-level. After image processing, two\nfeature selection methods were employed to identify the optimal set of spectral\nbands that were responsive to leaf N concentrations. The selected spectral\nbands were used to train and test two different Machine Learning (ML) models,\nGradient Boosting and XGBoost, for predicting nitrogen concentrations. The\ncomparison of selected bands for both leaf-level and canopy-level datasets\nshowed that most of the spectral regions identified by the feature selection\nmethods were across both methods and the dataset types (leaf- and canopy-level\ndatasets), particularly in the key regions, 500-525nm, 650-690nm, 750-800nm,\nand 900-950nm. These findings indicated the robustness of these spectral\nregions for predicting nitrogen content. The results for N prediction\ndemonstrated that the ML model achieved an R square of 0.49 for canopy-level\ndata and an R square of 0.57 for leaf-level data, despite using different sets\nof selected spectral bands for each analysis level. The study demonstrated the\npotential of using in-field hyperspectral imaging and the use of spectral data\nin integrated feature selection and ML techniques to monitor N status in\nvineyards.\n","authors":["Atif Bilal Asad","Achyut Paudel","Safal Kshetri","Chenchen Kang","Salik Ram Khanal","Nataliya Shcherbatyuk","Pierre Davadant","R. Paul Schreiner","Santosh Kalauni","Manoj Karkee","Markus Keller"],"pdf_url":"https://arxiv.org/pdf/2507.17869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.03170v2","updated":"2025-07-23T18:41:23Z","published":"2025-05-28T18:52:40Z","title":"PALADIN : Robust Neural Fingerprinting for Text-to-Image Diffusion\n  Models","summary":"  The risk of misusing text-to-image generative models for malicious uses,\nespecially due to the open-source development of such models, has become a\nserious concern. As a risk mitigation strategy, attributing generative models\nwith neural fingerprinting is emerging as a popular technique. There has been a\nplethora of recent work that aim for addressing neural fingerprinting. A\ntrade-off between the attribution accuracy and generation quality of such\nmodels has been studied extensively. None of the existing methods yet achieved\n100% attribution accuracy. However, any model with less than cent percent\naccuracy is practically non-deployable. In this work, we propose an accurate\nmethod to incorporate neural fingerprinting for text-to-image diffusion models\nleveraging the concepts of cyclic error correcting codes from the literature of\ncoding theory.\n","authors":["Murthy L","Subarna Tripathi"],"pdf_url":"https://arxiv.org/pdf/2506.03170v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.22929v2","updated":"2025-07-23T18:38:58Z","published":"2025-03-29T01:22:50Z","title":"Unsupervised Feature Disentanglement and Augmentation Network for\n  One-class Face Anti-spoofing","summary":"  Face anti-spoofing (FAS) techniques aim to enhance the security of facial\nidentity authentication by distinguishing authentic live faces from deceptive\nattempts. While two-class FAS methods risk overfitting to training attacks to\nachieve better performance, one-class FAS approaches handle unseen attacks well\nbut are less robust to domain information entangled within the liveness\nfeatures. To address this, we propose an Unsupervised Feature Disentanglement\nand Augmentation Network (\\textbf{UFDANet}), a one-class FAS technique that\nenhances generalizability by augmenting face images via disentangled features.\nThe \\textbf{UFDANet} employs a novel unsupervised feature disentangling method\nto separate the liveness and domain features, facilitating discriminative\nfeature learning. It integrates an out-of-distribution liveness feature\naugmentation scheme to synthesize new liveness features of unseen spoof\nclasses, which deviate from the live class, thus enhancing the representability\nand discriminability of liveness features. Additionally, \\textbf{UFDANet}\nincorporates a domain feature augmentation routine to synthesize unseen domain\nfeatures, thereby achieving better generalizability. Extensive experiments\ndemonstrate that the proposed \\textbf{UFDANet} outperforms previous one-class\nFAS methods and achieves comparable performance to state-of-the-art two-class\nFAS methods.\n","authors":["Pei-Kai Huang","Jun-Xiong Chong","Ming-Tsung Hsu","Fang-Yu Hsu","Yi-Ting Lin","Kai-Heng Chien","Hao-Chiang Shao","Chiou-Ting Hsu"],"pdf_url":"https://arxiv.org/pdf/2503.22929v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17860v1","updated":"2025-07-23T18:33:27Z","published":"2025-07-23T18:33:27Z","title":"Towards Facilitated Fairness Assessment of AI-based Skin Lesion\n  Classifiers Through GenAI-based Image Synthesis","summary":"  Recent advancements in Deep Learning and its application on the edge hold\ngreat potential for the revolution of routine screenings for skin cancers like\nMelanoma. Along with the anticipated benefits of this technology, potential\ndangers arise from unforseen and inherent biases. Thus, assessing and improving\nthe fairness of such systems is of utmost importance. A key challenge in\nfairness assessment is to ensure that the evaluation dataset is sufficiently\nrepresentative of different Personal Identifiable Information (PII) (sex, age,\nand race) and other minority groups. Against the backdrop of this challenge,\nthis study leverages the state-of-the-art Generative AI (GenAI) LightningDiT\nmodel to assess the fairness of publicly available melanoma classifiers. The\nresults suggest that fairness assessment using highly realistic synthetic data\nis a promising direction. Yet, our findings indicate that verifying fairness\nbecomes difficult when the melanoma-detection model used for evaluation is\ntrained on data that differ from the dataset underpinning the synthetic images.\nNonetheless, we propose that our approach offers a valuable new avenue for\nemploying synthetic data to gauge and enhance fairness in medical-imaging GenAI\nsystems.\n","authors":["Ko Watanabe. Stanislav Frolov. Adriano Lucieri. Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2507.17860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17859v1","updated":"2025-07-23T18:32:01Z","published":"2025-07-23T18:32:01Z","title":"FishDet-M: A Unified Large-Scale Benchmark for Robust Fish Detection and\n  CLIP-Guided Model Selection in Diverse Aquatic Visual Domains","summary":"  Accurate fish detection in underwater imagery is essential for ecological\nmonitoring, aquaculture automation, and robotic perception. However, practical\ndeployment remains limited by fragmented datasets, heterogeneous imaging\nconditions, and inconsistent evaluation protocols. To address these gaps, we\npresent \\textit{FishDet-M}, the largest unified benchmark for fish detection,\ncomprising 13 publicly available datasets spanning diverse aquatic environments\nincluding marine, brackish, occluded, and aquarium scenes. All data are\nharmonized using COCO-style annotations with both bounding boxes and\nsegmentation masks, enabling consistent and scalable cross-domain evaluation.\nWe systematically benchmark 28 contemporary object detection models, covering\nthe YOLOv8 to YOLOv12 series, R-CNN based detectors, and DETR based models.\nEvaluations are conducted using standard metrics including mAP, mAP@50, and\nmAP@75, along with scale-specific analyses (AP$_S$, AP$_M$, AP$_L$) and\ninference profiling in terms of latency and parameter count. The results\nhighlight the varying detection performance across models trained on FishDet-M,\nas well as the trade-off between accuracy and efficiency across models of\ndifferent architectures. To support adaptive deployment, we introduce a\nCLIP-based model selection framework that leverages vision-language alignment\nto dynamically identify the most semantically appropriate detector for each\ninput image. This zero-shot selection strategy achieves high performance\nwithout requiring ensemble computation, offering a scalable solution for\nreal-time applications. FishDet-M establishes a standardized and reproducible\nplatform for evaluating object detection in complex aquatic scenes. All\ndatasets, pretrained models, and evaluation tools are publicly available to\nfacilitate future research in underwater computer vision and intelligent marine\nsystems.\n","authors":["Muayad Abujabal","Lyes Saad Saoud","Irfan Hussain"],"pdf_url":"https://arxiv.org/pdf/2507.17859v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16943v3","updated":"2025-07-23T18:22:22Z","published":"2025-02-24T08:11:29Z","title":"MAD-AD: Masked Diffusion for Unsupervised Brain Anomaly Detection","summary":"  Unsupervised anomaly detection in brain images is crucial for identifying\ninjuries and pathologies without access to labels. However, the accurate\nlocalization of anomalies in medical images remains challenging due to the\ninherent complexity and variability of brain structures and the scarcity of\nannotated abnormal data. To address this challenge, we propose a novel approach\nthat incorporates masking within diffusion models, leveraging their generative\ncapabilities to learn robust representations of normal brain anatomy. During\ntraining, our model processes only normal brain MRI scans and performs a\nforward diffusion process in the latent space that adds noise to the features\nof randomly-selected patches. Following a dual objective, the model learns to\nidentify which patches are noisy and recover their original features. This\nstrategy ensures that the model captures intricate patterns of normal brain\nstructures while isolating potential anomalies as noise in the latent space. At\ninference, the model identifies noisy patches corresponding to anomalies and\ngenerates a normal counterpart for these patches by applying a reverse\ndiffusion process. Our method surpasses existing unsupervised anomaly detection\ntechniques, demonstrating superior performance in generating accurate normal\ncounterparts and localizing anomalies. The code is available at\nhhttps://github.com/farzad-bz/MAD-AD.\n","authors":["Farzad Beizaee","Gregory Lodygensky","Christian Desrosiers","Jose Dolz"],"pdf_url":"https://arxiv.org/pdf/2502.16943v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17853v1","updated":"2025-07-23T18:20:46Z","published":"2025-07-23T18:20:46Z","title":"Detail++: Training-Free Detail Enhancer for Text-to-Image Diffusion\n  Models","summary":"  Recent advances in text-to-image (T2I) generation have led to impressive\nvisual results. However, these models still face significant challenges when\nhandling complex prompt, particularly those involving multiple subjects with\ndistinct attributes. Inspired by the human drawing process, which first\noutlines the composition and then incrementally adds details, we propose\nDetail++, a training-free framework that introduces a novel Progressive Detail\nInjection (PDI) strategy to address this limitation. Specifically, we decompose\na complex prompt into a sequence of simplified sub-prompts, guiding the\ngeneration process in stages. This staged generation leverages the inherent\nlayout-controlling capacity of self-attention to first ensure global\ncomposition, followed by precise refinement. To achieve accurate binding\nbetween attributes and corresponding subjects, we exploit cross-attention\nmechanisms and further introduce a Centroid Alignment Loss at test time to\nreduce binding noise and enhance attribute consistency. Extensive experiments\non T2I-CompBench and a newly constructed style composition benchmark\ndemonstrate that Detail++ significantly outperforms existing methods,\nparticularly in scenarios involving multiple objects and complex stylistic\nconditions.\n","authors":["Lifeng Chen","Jiner Wang","Zihao Pan","Beier Zhu","Xiaofeng Yang","Chi Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.17853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.01256v2","updated":"2025-07-23T18:17:16Z","published":"2023-03-02T13:36:28Z","title":"Choosing Public Datasets for Private Machine Learning via Gradient\n  Subspace Distance","summary":"  Differentially private stochastic gradient descent privatizes model training\nby injecting noise into each iteration, where the noise magnitude increases\nwith the number of model parameters. Recent works suggest that we can reduce\nthe noise by leveraging public data for private machine learning, by projecting\ngradients onto a subspace prescribed by the public data. However, given a\nchoice of public datasets, it is not a priori clear which one may be most\nappropriate for the private task. We give an algorithm for selecting a public\ndataset by measuring a low-dimensional subspace distance between gradients of\nthe public and private examples. We provide theoretical analysis demonstrating\nthat the excess risk scales with this subspace distance. This distance is easy\nto compute and robust to modifications in the setting. Empirical evaluation\nshows that trained model accuracy is monotone in this distance.\n","authors":["Xin Gu","Gautam Kamath","Zhiwei Steven Wu"],"pdf_url":"https://arxiv.org/pdf/2303.01256v2.pdf","comment":"Accepted to SaTML 2025"},{"id":"http://arxiv.org/abs/2507.17844v1","updated":"2025-07-23T18:11:39Z","published":"2025-07-23T18:11:39Z","title":"SV3.3B: A Sports Video Understanding Model for Action Recognition","summary":"  This paper addresses the challenge of automated sports video analysis, which\nhas traditionally been limited by computationally intensive models requiring\nserver-side processing and lacking fine-grained understanding of athletic\nmovements. Current approaches struggle to capture the nuanced biomechanical\ntransitions essential for meaningful sports analysis, often missing critical\nphases like preparation, execution, and follow-through that occur within\nseconds. To address these limitations, we introduce SV3.3B, a lightweight 3.3B\nparameter video understanding model that combines novel temporal motion\ndifference sampling with self-supervised learning for efficient on-device\ndeployment. Our approach employs a DWT-VGG16-LDA based keyframe extraction\nmechanism that intelligently identifies the 16 most representative frames from\nsports sequences, followed by a V-DWT-JEPA2 encoder pretrained through\nmask-denoising objectives and an LLM decoder fine-tuned for sports action\ndescription generation. Evaluated on a subset of the NSVA basketball dataset,\nSV3.3B achieves superior performance across both traditional text generation\nmetrics and sports-specific evaluation criteria, outperforming larger\nclosed-source models including GPT-4o variants while maintaining significantly\nlower computational requirements. Our model demonstrates exceptional capability\nin generating technically detailed and analytically rich sports descriptions,\nachieving 29.2% improvement over GPT-4o in ground truth validation metrics,\nwith substantial improvements in information density, action complexity, and\nmeasurement precision metrics essential for comprehensive athletic analysis.\nModel Available at https://huggingface.co/sportsvision/SV3.3B.\n","authors":["Sai Varun Kodathala","Yashwanth Reddy Vutukoori","Rakesh Vunnam"],"pdf_url":"https://arxiv.org/pdf/2507.17844v1.pdf","comment":"8 pages, 6 figures, 4 tables. Submitted to AIxSET 2025"},{"id":"http://arxiv.org/abs/2507.17748v1","updated":"2025-07-23T17:59:02Z","published":"2025-07-23T17:59:02Z","title":"Large Learning Rates Simultaneously Achieve Robustness to Spurious\n  Correlations and Compressibility","summary":"  Robustness and resource-efficiency are two highly desirable properties for\nmodern machine learning models. However, achieving them jointly remains a\nchallenge. In this paper, we position high learning rates as a facilitator for\nsimultaneously achieving robustness to spurious correlations and network\ncompressibility. We demonstrate that large learning rates also produce\ndesirable representation properties such as invariant feature utilization,\nclass separation, and activation sparsity. Importantly, our findings indicate\nthat large learning rates compare favorably to other hyperparameters and\nregularization methods, in consistently satisfying these properties in tandem.\nIn addition to demonstrating the positive effect of large learning rates across\ndiverse spurious correlation datasets, models, and optimizers, we also present\nstrong evidence that the previously documented success of large learning rates\nin standard classification tasks is likely due to its effect on addressing\nhidden/rare spurious correlations in the training dataset.\n","authors":["Melih Barsbey","Lucas Prieto","Stefanos Zafeiriou","Tolga Birdal"],"pdf_url":"https://arxiv.org/pdf/2507.17748v1.pdf","comment":"Accepted at ICCV 2025, 23 pages"},{"id":"http://arxiv.org/abs/2507.17745v1","updated":"2025-07-23T17:57:16Z","published":"2025-07-23T17:57:16Z","title":"Ultra3D: Efficient and High-Fidelity 3D Generation with Part Attention","summary":"  Recent advances in sparse voxel representations have significantly improved\nthe quality of 3D content generation, enabling high-resolution modeling with\nfine-grained geometry. However, existing frameworks suffer from severe\ncomputational inefficiencies due to the quadratic complexity of attention\nmechanisms in their two-stage diffusion pipelines. In this work, we propose\nUltra3D, an efficient 3D generation framework that significantly accelerates\nsparse voxel modeling without compromising quality. Our method leverages the\ncompact VecSet representation to efficiently generate a coarse object layout in\nthe first stage, reducing token count and accelerating voxel coordinate\nprediction. To refine per-voxel latent features in the second stage, we\nintroduce Part Attention, a geometry-aware localized attention mechanism that\nrestricts attention computation within semantically consistent part regions.\nThis design preserves structural continuity while avoiding unnecessary global\nattention, achieving up to 6.7x speed-up in latent generation. To support this\nmechanism, we construct a scalable part annotation pipeline that converts raw\nmeshes into part-labeled sparse voxels. Extensive experiments demonstrate that\nUltra3D supports high-resolution 3D generation at 1024 resolution and achieves\nstate-of-the-art performance in both visual fidelity and user preference.\n","authors":["Yiwen Chen","Zhihao Li","Yikai Wang","Hu Zhang","Qin Li","Chi Zhang","Guosheng Lin"],"pdf_url":"https://arxiv.org/pdf/2507.17745v1.pdf","comment":"Project Page: https://buaacyw.github.io/ultra3d/"},{"id":"http://arxiv.org/abs/2507.17744v1","updated":"2025-07-23T17:57:09Z","published":"2025-07-23T17:57:09Z","title":"Yume: An Interactive World Generation Model","summary":"  Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/.\n","authors":["Xiaofeng Mao","Shaoheng Lin","Zhen Li","Chuanhao Li","Wenshuo Peng","Tong He","Jiangmiao Pang","Mingmin Chi","Yu Qiao","Kaipeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.17744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17729v1","updated":"2025-07-23T17:43:35Z","published":"2025-07-23T17:43:35Z","title":"A Comprehensive Evaluation Framework for the Study of the Effects of\n  Facial Filters on Face Recognition Accuracy","summary":"  Facial filters are now commonplace for social media users around the world.\nPrevious work has demonstrated that facial filters can negatively impact\nautomated face recognition performance. However, these studies focus on small\nnumbers of hand-picked filters in particular styles. In order to more\neffectively incorporate the wide ranges of filters present on various social\nmedia applications, we introduce a framework that allows for larger-scale study\nof the impact of facial filters on automated recognition. This framework\nincludes a controlled dataset of face images, a principled filter selection\nprocess that selects a representative range of filters for experimentation, and\na set of experiments to evaluate the filters' impact on recognition. We\ndemonstrate our framework with a case study of filters from the American\napplications Instagram and Snapchat and the Chinese applications Meitu and Pitu\nto uncover cross-cultural differences. Finally, we show how the filtering\neffect in a face embedding space can easily be detected and restored to improve\nface recognition performance.\n","authors":["Kagan Ozturk","Louisa Conwill","Jacob Gutierrez","Kevin Bowyer","Walter J. Scheirer"],"pdf_url":"https://arxiv.org/pdf/2507.17729v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17801v1","updated":"2025-07-23T17:42:13Z","published":"2025-07-23T17:42:13Z","title":"Lumina-mGPT 2.0: Stand-Alone AutoRegressive Image Modeling","summary":"  We present Lumina-mGPT 2.0, a stand-alone, decoder-only autoregressive model\nthat revisits and revitalizes the autoregressive paradigm for high-quality\nimage generation and beyond. Unlike existing approaches that rely on pretrained\ncomponents or hybrid architectures, Lumina-mGPT 2.0 is trained entirely from\nscratch, enabling unrestricted architectural design and licensing freedom. It\nachieves generation quality on par with state-of-the-art diffusion models such\nas DALL-E 3 and SANA, while preserving the inherent flexibility and\ncompositionality of autoregressive modeling. Our unified tokenization scheme\nallows the model to seamlessly handle a wide spectrum of tasks-including\nsubject-driven generation, image editing, controllable synthesis, and dense\nprediction-within a single generative framework. To further boost usability, we\nincorporate efficient decoding strategies like inference-time scaling and\nspeculative Jacobi sampling to improve quality and speed, respectively.\nExtensive evaluations on standard text-to-image benchmarks (e.g., GenEval, DPG)\ndemonstrate that Lumina-mGPT 2.0 not only matches but in some cases surpasses\ndiffusion-based models. Moreover, we confirm its multi-task capabilities on the\nGraph200K benchmark, with the native Lumina-mGPT 2.0 performing exceptionally\nwell. These results position Lumina-mGPT 2.0 as a strong, flexible foundation\nmodel for unified multimodal generation. We have released our training details,\ncode, and models at https://github.com/Alpha-VLLM/Lumina-mGPT-2.0.\n","authors":["Yi Xin","Juncheng Yan","Qi Qin","Zhen Li","Dongyang Liu","Shicheng Li","Victor Shea-Jay Huang","Yupeng Zhou","Renrui Zhang","Le Zhuo","Tiancheng Han","Xiaoqing Sun","Siqi Luo","Mengmeng Wang","Bin Fu","Yuewen Cao","Hongsheng Li","Guangtao Zhai","Xiaohong Liu","Yu Qiao","Peng Gao"],"pdf_url":"https://arxiv.org/pdf/2507.17801v1.pdf","comment":"Tech Report, 23 pages, 11 figures, 7 tables"},{"id":"http://arxiv.org/abs/2507.17725v1","updated":"2025-07-23T17:35:48Z","published":"2025-07-23T17:35:48Z","title":"On the Interaction of Compressibility and Adversarial Robustness","summary":"  Modern neural networks are expected to simultaneously satisfy a host of\ndesirable properties: accurate fitting to training data, generalization to\nunseen inputs, parameter and computational efficiency, and robustness to\nadversarial perturbations. While compressibility and robustness have each been\nstudied extensively, a unified understanding of their interaction still remains\nelusive. In this work, we develop a principled framework to analyze how\ndifferent forms of compressibility - such as neuron-level sparsity and spectral\ncompressibility - affect adversarial robustness. We show that these forms of\ncompression can induce a small number of highly sensitive directions in the\nrepresentation space, which adversaries can exploit to construct effective\nperturbations. Our analysis yields a simple yet instructive robustness bound,\nrevealing how neuron and spectral compressibility impact $L_\\infty$ and $L_2$\nrobustness via their effects on the learned representations. Crucially, the\nvulnerabilities we identify arise irrespective of how compression is achieved -\nwhether via regularization, architectural bias, or implicit learning dynamics.\nThrough empirical evaluations across synthetic and realistic tasks, we confirm\nour theoretical predictions, and further demonstrate that these vulnerabilities\npersist under adversarial training and transfer learning, and contribute to the\nemergence of universal adversarial perturbations. Our findings show a\nfundamental tension between structured compressibility and robustness, and\nsuggest new pathways for designing models that are both efficient and secure.\n","authors":["Melih Barsbey","Antônio H. Ribeiro","Umut Şimşekli","Tolga Birdal"],"pdf_url":"https://arxiv.org/pdf/2507.17725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17722v1","updated":"2025-07-23T17:32:17Z","published":"2025-07-23T17:32:17Z","title":"BetterCheck: Towards Safeguarding VLMs for Automotive Perception Systems","summary":"  Large language models (LLMs) are growingly extended to process multimodal\ndata such as text and video simultaneously. Their remarkable performance in\nunderstanding what is shown in images is surpassing specialized neural networks\n(NNs) such as Yolo that is supporting only a well-formed but very limited\nvocabulary, ie., objects that they are able to detect. When being\nnon-restricted, LLMs and in particular state-of-the-art vision language models\n(VLMs) show impressive performance to describe even complex traffic situations.\nThis is making them potentially suitable components for automotive perception\nsystems to support the understanding of complex traffic situations or edge case\nsituation. However, LLMs and VLMs are prone to hallucination, which mean to\neither potentially not seeing traffic agents such as vulnerable road users who\nare present in a situation, or to seeing traffic agents who are not there in\nreality. While the latter is unwanted making an ADAS or autonomous driving\nsystems (ADS) to unnecessarily slow down, the former could lead to disastrous\ndecisions from an ADS. In our work, we are systematically assessing the\nperformance of 3 state-of-the-art VLMs on a diverse subset of traffic\nsituations sampled from the Waymo Open Dataset to support safety guardrails for\ncapturing such hallucinations in VLM-supported perception systems. We observe\nthat both, proprietary and open VLMs exhibit remarkable image understanding\ncapabilities even paying thorough attention to fine details sometimes difficult\nto spot for us humans. However, they are also still prone to making up elements\nin their descriptions to date requiring hallucination detection strategies such\nas BetterCheck that we propose in our work.\n","authors":["Malsha Ashani Mahawatta Dona","Beatriz Cabrero-Daniel","Yinan Yu","Christian Berger"],"pdf_url":"https://arxiv.org/pdf/2507.17722v1.pdf","comment":"Accepted in The IEEE International Conference on Intelligent\n  Transportation Systems (ITSC)2025"},{"id":"http://arxiv.org/abs/2505.18079v3","updated":"2025-07-23T17:26:05Z","published":"2025-05-23T16:37:36Z","title":"Deep Video Discovery: Agentic Search with Tool Use for Long-form Video\n  Understanding","summary":"  Long-form video understanding presents significant challenges due to\nextensive temporal-spatial complexity and the difficulty of question answering\nunder such extended contexts. While Large Language Models (LLMs) have\ndemonstrated considerable advancements in video analysis capabilities and long\ncontext handling, they continue to exhibit limitations when processing\ninformation-dense hour-long videos. To overcome such limitations, we propose\nthe Deep Video Discovery agent to leverage an agentic search strategy over\nsegmented video clips. Different from previous video agents manually designing\na rigid workflow, our approach emphasizes the autonomous nature of agents. By\nproviding a set of search-centric tools on multi-granular video database, our\nDVD agent leverages the advanced reasoning capability of LLM to plan on its\ncurrent observation state, strategically selects tools, formulates appropriate\nparameters for actions, and iteratively refines its internal reasoning in light\nof the gathered information. We perform comprehensive evaluation on multiple\nlong video understanding benchmarks that demonstrates the advantage of the\nentire system design. Our DVD agent achieves SOTA performance, significantly\nsurpassing prior works by a large margin on the challenging LVBench dataset.\nComprehensive ablation studies and in-depth tool analyses are also provided,\nyielding insights to further advance intelligent agents tailored for long-form\nvideo understanding tasks. The code has been released in\nhttps://github.com/microsoft/DeepVideoDiscovery.\n","authors":["Xiaoyi Zhang","Zhaoyang Jia","Zongyu Guo","Jiahao Li","Bin Li","Houqiang Li","Yan Lu"],"pdf_url":"https://arxiv.org/pdf/2505.18079v3.pdf","comment":"V3 draft. Under review"},{"id":"http://arxiv.org/abs/2507.17692v1","updated":"2025-07-23T16:57:43Z","published":"2025-07-23T16:57:43Z","title":"Joint Asymmetric Loss for Learning with Noisy Labels","summary":"  Learning with noisy labels is a crucial task for training accurate deep\nneural networks. To mitigate label noise, prior studies have proposed various\nrobust loss functions, particularly symmetric losses. Nevertheless, symmetric\nlosses usually suffer from the underfitting issue due to the overly strict\nconstraint. To address this problem, the Active Passive Loss (APL) jointly\noptimizes an active and a passive loss to mutually enhance the overall fitting\nability. Within APL, symmetric losses have been successfully extended, yielding\nadvanced robust loss functions. Despite these advancements, emerging\ntheoretical analyses indicate that asymmetric losses, a new class of robust\nloss functions, possess superior properties compared to symmetric losses.\nHowever, existing asymmetric losses are not compatible with advanced\noptimization frameworks such as APL, limiting their potential and\napplicability. Motivated by this theoretical gap and the prospect of asymmetric\nlosses, we extend the asymmetric loss to the more complex passive loss scenario\nand propose the Asymetric Mean Square Error (AMSE), a novel asymmetric loss. We\nrigorously establish the necessary and sufficient condition under which AMSE\nsatisfies the asymmetric condition. By substituting the traditional symmetric\npassive loss in APL with our proposed AMSE, we introduce a novel robust loss\nframework termed Joint Asymmetric Loss (JAL). Extensive experiments demonstrate\nthe effectiveness of our method in mitigating label noise. Code available at:\nhttps://github.com/cswjl/joint-asymmetric-loss\n","authors":["Jialiang Wang","Xianming Liu","Xiong Zhou","Gangfeng Hu","Deming Zhai","Junjun Jiang","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2507.17692v1.pdf","comment":"Accepted by ICCV 2025"},{"id":"http://arxiv.org/abs/2507.17682v1","updated":"2025-07-23T16:44:22Z","published":"2025-07-23T16:44:22Z","title":"Audio-Vision Contrastive Learning for Phonological Class Recognition","summary":"  Accurate classification of articulatory-phonological features plays a vital\nrole in understanding human speech production and developing robust speech\ntechnologies, particularly in clinical contexts where targeted phonemic\nanalysis and therapy can improve disease diagnosis accuracy and personalized\nrehabilitation. In this work, we propose a multimodal deep learning framework\nthat combines real-time magnetic resonance imaging (rtMRI) and speech signals\nto classify three key articulatory dimensions: manner of articulation, place of\narticulation, and voicing. We perform classification on 15 phonological classes\nderived from the aforementioned articulatory dimensions and evaluate the system\nwith four audio/vision configurations: unimodal rtMRI, unimodal audio signals,\nmultimodal middle fusion, and contrastive learning-based audio-vision fusion.\nExperimental results on the USC-TIMIT dataset show that our contrastive\nlearning-based approach achieves state-of-the-art performance, with an average\nF1-score of 0.81, representing an absolute increase of 0.23 over the unimodal\nbaseline. The results confirm the effectiveness of contrastive representation\nlearning for multimodal articulatory analysis. Our code and processed dataset\nwill be made publicly available at\nhttps://github.com/DaE-plz/AC_Contrastive_Phonology to support future research.\n","authors":["Daiqi Liu","Tomás Arias-Vergara","Jana Hutter","Andreas Maier","Paula Andrea Pérez-Toro"],"pdf_url":"https://arxiv.org/pdf/2507.17682v1.pdf","comment":"conference to TSD 2025"},{"id":"http://arxiv.org/abs/2507.17678v1","updated":"2025-07-23T16:40:43Z","published":"2025-07-23T16:40:43Z","title":"MCM: Mamba-based Cardiac Motion Tracking using Sequential Images in MRI","summary":"  Myocardial motion tracking is important for assessing cardiac function and\ndiagnosing cardiovascular diseases, for which cine cardiac magnetic resonance\n(CMR) has been established as the gold standard imaging modality. Many existing\nmethods learn motion from single image pairs consisting of a reference frame\nand a randomly selected target frame from the cardiac cycle. However, these\nmethods overlook the continuous nature of cardiac motion and often yield\ninconsistent and non-smooth motion estimations. In this work, we propose a\nnovel Mamba-based cardiac motion tracking network (MCM) that explicitly\nincorporates target image sequence from the cardiac cycle to achieve smooth and\ntemporally consistent motion tracking. By developing a bi-directional Mamba\nblock equipped with a bi-directional scanning mechanism, our method facilitates\nthe estimation of plausible deformation fields. With our proposed motion\ndecoder that integrates motion information from frames adjacent to the target\nframe, our method further enhances temporal coherence. Moreover, by taking\nadvantage of Mamba's structured state-space formulation, the proposed method\nlearns the continuous dynamics of the myocardium from sequential images without\nincreasing computational complexity. We evaluate the proposed method on two\npublic datasets. The experimental results demonstrate that the proposed method\nquantitatively and qualitatively outperforms both conventional and\nstate-of-the-art learning-based cardiac motion tracking methods. The code is\navailable at https://github.com/yjh-0104/MCM.\n","authors":["Jiahui Yin","Xinxing Cheng","Jinming Duan","Yan Pang","Declan O'Regan","Hadrien Reynaud","Qingjie Meng"],"pdf_url":"https://arxiv.org/pdf/2507.17678v1.pdf","comment":"Medical Image Computing and Computer-Assisted Intervention (MICCAI),\n  Reconstruction and Imaging Motion Estimation Workshop (RIME), 2025"},{"id":"http://arxiv.org/abs/2507.17800v1","updated":"2025-07-23T16:35:25Z","published":"2025-07-23T16:35:25Z","title":"Improving Multislice Electron Ptychography with a Generative Prior","summary":"  Multislice electron ptychography (MEP) is an inverse imaging technique that\ncomputationally reconstructs the highest-resolution images of atomic crystal\nstructures from diffraction patterns. Available algorithms often solve this\ninverse problem iteratively but are both time consuming and produce suboptimal\nsolutions due to their ill-posed nature. We develop MEP-Diffusion, a diffusion\nmodel trained on a large database of crystal structures specifically for MEP to\naugment existing iterative solvers. MEP-Diffusion is easily integrated as a\ngenerative prior into existing reconstruction methods via Diffusion Posterior\nSampling (DPS). We find that this hybrid approach greatly enhances the quality\nof the reconstructed 3D volumes, achieving a 90.50% improvement in SSIM over\nexisting methods.\n","authors":["Christian K. Belardi","Chia-Hao Lee","Yingheng Wang","Justin Lovelace","Kilian Q. Weinberger","David A. Muller","Carla P. Gomes"],"pdf_url":"https://arxiv.org/pdf/2507.17800v1.pdf","comment":"16 pages, 10 figures, 5 tables"},{"id":"http://arxiv.org/abs/2507.17665v1","updated":"2025-07-23T16:29:57Z","published":"2025-07-23T16:29:57Z","title":"Perspective-Invariant 3D Object Detection","summary":"  With the rise of robotics, LiDAR-based 3D object detection has garnered\nsignificant attention in both academia and industry. However, existing datasets\nand methods predominantly focus on vehicle-mounted platforms, leaving other\nautonomous platforms underexplored. To bridge this gap, we introduce Pi3DET,\nthe first benchmark featuring LiDAR data and 3D bounding box annotations\ncollected from multiple platforms: vehicle, quadruped, and drone, thereby\nfacilitating research in 3D object detection for non-vehicle platforms as well\nas cross-platform 3D detection. Based on Pi3DET, we propose a novel\ncross-platform adaptation framework that transfers knowledge from the\nwell-studied vehicle platform to other platforms. This framework achieves\nperspective-invariant 3D detection through robust alignment at both geometric\nand feature levels. Additionally, we establish a benchmark to evaluate the\nresilience and robustness of current 3D detectors in cross-platform scenarios,\nproviding valuable insights for developing adaptive 3D perception systems.\nExtensive experiments validate the effectiveness of our approach on challenging\ncross-platform tasks, demonstrating substantial gains over existing adaptation\nmethods. We hope this work paves the way for generalizable and unified 3D\nperception systems across diverse and complex environments. Our Pi3DET dataset,\ncross-platform benchmark suite, and annotation toolkit have been made publicly\navailable.\n","authors":["Ao Liang","Lingdong Kong","Dongyue Lu","Youquan Liu","Jian Fang","Huaici Zhao","Wei Tsang Ooi"],"pdf_url":"https://arxiv.org/pdf/2507.17665v1.pdf","comment":"ICCV 2025; 46 pages, 18 figures, 22 tables; Project Page at\n  https://pi3det.github.io"},{"id":"http://arxiv.org/abs/2507.17664v1","updated":"2025-07-23T16:29:52Z","published":"2025-07-23T16:29:52Z","title":"Talk2Event: Grounded Understanding of Dynamic Scenes from Event Cameras","summary":"  Event cameras offer microsecond-level latency and robustness to motion blur,\nmaking them ideal for understanding dynamic environments. Yet, connecting these\nasynchronous streams to human language remains an open challenge. We introduce\nTalk2Event, the first large-scale benchmark for language-driven object\ngrounding in event-based perception. Built from real-world driving data, we\nprovide over 30,000 validated referring expressions, each enriched with four\ngrounding attributes -- appearance, status, relation to viewer, and relation to\nother objects -- bridging spatial, temporal, and relational reasoning. To fully\nexploit these cues, we propose EventRefer, an attribute-aware grounding\nframework that dynamically fuses multi-attribute representations through a\nMixture of Event-Attribute Experts (MoEE). Our method adapts to different\nmodalities and scene dynamics, achieving consistent gains over state-of-the-art\nbaselines in event-only, frame-only, and event-frame fusion settings. We hope\nour dataset and approach will establish a foundation for advancing multimodal,\ntemporally-aware, and language-driven perception in real-world robotics and\nautonomy.\n","authors":["Lingdong Kong","Dongyue Lu","Ao Liang","Rong Li","Yuhao Dong","Tianshuai Hu","Lai Xing Ng","Wei Tsang Ooi","Benoit R. Cottereau"],"pdf_url":"https://arxiv.org/pdf/2507.17664v1.pdf","comment":"Preprint; 42 pages, 17 figures, 16 tables; Project Page at\n  https://talk2event.github.io"},{"id":"http://arxiv.org/abs/2507.17662v1","updated":"2025-07-23T16:29:46Z","published":"2025-07-23T16:29:46Z","title":"Mammo-Mamba: A Hybrid State-Space and Transformer Architecture with\n  Sequential Mixture of Experts for Multi-View Mammography","summary":"  Breast cancer (BC) remains one of the leading causes of cancer-related\nmortality among women, despite recent advances in Computer-Aided Diagnosis\n(CAD) systems. Accurate and efficient interpretation of multi-view mammograms\nis essential for early detection, driving a surge of interest in Artificial\nIntelligence (AI)-powered CAD models. While state-of-the-art multi-view\nmammogram classification models are largely based on Transformer architectures,\ntheir computational complexity scales quadratically with the number of image\npatches, highlighting the need for more efficient alternatives. To address this\nchallenge, we propose Mammo-Mamba, a novel framework that integrates Selective\nState-Space Models (SSMs), transformer-based attention, and expert-driven\nfeature refinement into a unified architecture. Mammo-Mamba extends the\nMambaVision backbone by introducing the Sequential Mixture of Experts (SeqMoE)\nmechanism through its customized SecMamba block. The SecMamba is a modified\nMambaVision block that enhances representation learning in high-resolution\nmammographic images by enabling content-adaptive feature refinement. These\nblocks are integrated into the deeper stages of MambaVision, allowing the model\nto progressively adjust feature emphasis through dynamic expert gating,\neffectively mitigating the limitations of traditional Transformer models.\nEvaluated on the CBIS-DDSM benchmark dataset, Mammo-Mamba achieves superior\nclassification performance across all key metrics while maintaining\ncomputational efficiency.\n","authors":["Farnoush Bayatmakou","Reza Taleei","Nicole Simone","Arash Mohammadi"],"pdf_url":"https://arxiv.org/pdf/2507.17662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17661v1","updated":"2025-07-23T16:29:45Z","published":"2025-07-23T16:29:45Z","title":"Monocular Semantic Scene Completion via Masked Recurrent Networks","summary":"  Monocular Semantic Scene Completion (MSSC) aims to predict the voxel-wise\noccupancy and semantic category from a single-view RGB image. Existing methods\nadopt a single-stage framework that aims to simultaneously achieve visible\nregion segmentation and occluded region hallucination, while also being\naffected by inaccurate depth estimation. Such methods often achieve suboptimal\nperformance, especially in complex scenes. We propose a novel two-stage\nframework that decomposes MSSC into coarse MSSC followed by the Masked\nRecurrent Network. Specifically, we propose the Masked Sparse Gated Recurrent\nUnit (MS-GRU) which concentrates on the occupied regions by the proposed mask\nupdating mechanism, and a sparse GRU design is proposed to reduce the\ncomputation cost. Additionally, we propose the distance attention projection to\nreduce projection errors by assigning different attention scores according to\nthe distance to the observed surface. Experimental results demonstrate that our\nproposed unified framework, MonoMRN, effectively supports both indoor and\noutdoor scenes and achieves state-of-the-art performance on the NYUv2 and\nSemanticKITTI datasets. Furthermore, we conduct robustness analysis under\nvarious disturbances, highlighting the role of the Masked Recurrent Network in\nenhancing the model's resilience to such challenges. The source code is\npublicly available.\n","authors":["Xuzhi Wang","Xinran Wu","Song Wang","Lingdong Kong","Ziping Zhao"],"pdf_url":"https://arxiv.org/pdf/2507.17661v1.pdf","comment":"ICCV 2025; 15 pages, 10 figures, 6 tables; Code at\n  https://github.com/alanWXZ/MonoMRN"},{"id":"http://arxiv.org/abs/2507.17659v1","updated":"2025-07-23T16:24:57Z","published":"2025-07-23T16:24:57Z","title":"See the Forest and the Trees: A Synergistic Reasoning Framework for\n  Knowledge-Based Visual Question Answering","summary":"  Multimodal Large Language Models (MLLMs) have pushed the frontiers of\nKnowledge-Based Visual Question Answering (KBVQA), yet their reasoning is\nfundamentally bottlenecked by a reliance on uni-dimensional evidence. This\n\"seeing only the trees, but not the forest\" approach prevents robust,\nmulti-faceted understanding. Inspired by the principle of seeing both the\nforest and trees, we propose Synergos-VQA, a novel synergistic reasoning\nframework. At its core, Synergos-VQA concurrently generates and fuses three\ncomplementary evidence streams at inference time: (1) Holistic Evidence to\nperceive the entire scene (the \"forest\"), (2) Structural Evidence from a\nprototype-driven module to identify key objects (the \"trees\"), and (3) Causal\nEvidence from a counterfactual probe to ensure the reasoning is robustly\ngrounded. By synergistically fusing this multi-faceted evidence, our framework\nachieves a more comprehensive and reliable reasoning process. Extensive\nexperiments show that Synergos-VQA decisively establishes a new\nstate-of-the-art on three challenging benchmarks, including OK-VQA and A-OKVQA.\nFurthermore, our approach demonstrates strong plug-and-play capabilities,\nsignificantly boosting various open-source MLLMs and proving that superior\nmethodological design can outperform sheer model scale.\n","authors":["Junjie Wang","Yunhan Tang","Yijie Wang","Zhihao Yuan","Huan Wang","Yangfan He","Bin Li"],"pdf_url":"https://arxiv.org/pdf/2507.17659v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17657v1","updated":"2025-07-23T16:20:47Z","published":"2025-07-23T16:20:47Z","title":"Attention (as Discrete-Time Markov) Chains","summary":"  We introduce a new interpretation of the attention matrix as a discrete-time\nMarkov chain. Our interpretation sheds light on common operations involving\nattention scores such as selection, summation, and averaging in a unified\nframework. It further extends them by considering indirect attention,\npropagated through the Markov chain, as opposed to previous studies that only\nmodel immediate effects. Our main observation is that tokens corresponding to\nsemantically similar regions form a set of metastable states, where the\nattention clusters, while noisy attention scores tend to disperse. Metastable\nstates and their prevalence can be easily computed through simple matrix\nmultiplication and eigenanalysis, respectively. Using these lightweight tools,\nwe demonstrate state-of-the-art zero-shot segmentation. Lastly, we define\nTokenRank -- the steady state vector of the Markov chain, which measures global\ntoken importance. We demonstrate that using it brings improvements in\nunconditional image generation. We believe our framework offers a fresh view of\nhow tokens are being attended in modern visual transformers.\n","authors":["Yotam Erel","Olaf Dünkel","Rishabh Dabral","Vladislav Golyanik","Christian Theobalt","Amit H. Bermano"],"pdf_url":"https://arxiv.org/pdf/2507.17657v1.pdf","comment":"Project page: https://yoterel.github.io/attention_chains/"},{"id":"http://arxiv.org/abs/2507.17651v1","updated":"2025-07-23T16:15:48Z","published":"2025-07-23T16:15:48Z","title":"CNS-Bench: Benchmarking Image Classifier Robustness Under Continuous\n  Nuisance Shifts","summary":"  An important challenge when using computer vision models in the real world is\nto evaluate their performance in potential out-of-distribution (OOD) scenarios.\nWhile simple synthetic corruptions are commonly applied to test OOD robustness,\nthey often fail to capture nuisance shifts that occur in the real world.\nRecently, diffusion models have been applied to generate realistic images for\nbenchmarking, but they are restricted to binary nuisance shifts. In this work,\nwe introduce CNS-Bench, a Continuous Nuisance Shift Benchmark to quantify OOD\nrobustness of image classifiers for continuous and realistic generative\nnuisance shifts. CNS-Bench allows generating a wide range of individual\nnuisance shifts in continuous severities by applying LoRA adapters to diffusion\nmodels. To address failure cases, we propose a filtering mechanism that\noutperforms previous methods, thereby enabling reliable benchmarking with\ngenerative models. With the proposed benchmark, we perform a large-scale study\nto evaluate the robustness of more than 40 classifiers under various nuisance\nshifts. Through carefully designed comparisons and analyses, we find that model\nrankings can change for varying shifts and shift scales, which cannot be\ncaptured when applying common binary shifts. Additionally, we show that\nevaluating the model performance on a continuous scale allows the\nidentification of model failure points, providing a more nuanced understanding\nof model robustness. Project page including code and data:\nhttps://genintel.github.io/CNS.\n","authors":["Olaf Dünkel","Artur Jesslen","Jiahao Xie","Christian Theobalt","Christian Rupprecht","Adam Kortylewski"],"pdf_url":"https://arxiv.org/pdf/2507.17651v1.pdf","comment":"ICCV 2025. Project page: https://genintel.github.io/CNS"},{"id":"http://arxiv.org/abs/2501.13926v2","updated":"2025-07-23T16:09:10Z","published":"2025-01-23T18:59:43Z","title":"Can We Generate Images with CoT? Let's Verify and Reinforce Image\n  Generation Step by Step","summary":"  Chain-of-Thought (CoT) reasoning has been extensively explored in large\nmodels to tackle complex understanding tasks. However, it still remains an open\nquestion whether such strategies can be applied to verifying and reinforcing\nimage generation scenarios. In this paper, we provide the first comprehensive\ninvestigation of the potential of CoT reasoning to enhance autoregressive image\ngeneration. We focus on three techniques: scaling test-time computation for\nverification, aligning model preferences with Direct Preference Optimization\n(DPO), and integrating these techniques for complementary effects. Our results\ndemonstrate that these approaches can be effectively adapted and combined to\nsignificantly improve image generation performance. Furthermore, given the\npivotal role of reward models in our findings, we propose the Potential\nAssessment Reward Model (PARM) and PARM++, specialized for autoregressive image\ngeneration. PARM adaptively assesses each generation step through a potential\nassessment approach, merging the strengths of existing reward models, and\nPARM++ further introduces a reflection mechanism to self-correct the generated\nunsatisfactory image, which is the first to incorporate reflection in\nautoregressive image generation. Using our investigated reasoning strategies,\nwe enhance a baseline model, Show-o, to achieve superior results, with a\nsignificant +24% improvement on the GenEval benchmark, surpassing Stable\nDiffusion 3 by +15%. We hope our study provides unique insights and paves a new\npath for integrating CoT reasoning with autoregressive image generation. Code\nand models are released at https://github.com/ZiyuGuo99/Image-Generation-CoT\n","authors":["Ziyu Guo","Renrui Zhang","Chengzhuo Tong","Zhizheng Zhao","Rui Huang","Haoquan Zhang","Manyuan Zhang","Jiaming Liu","Shanghang Zhang","Peng Gao","Hongsheng Li","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2501.13926v2.pdf","comment":"Journal Version. Code and models are released at\n  https://github.com/ZiyuGuo99/Image-Generation-CoT"},{"id":"http://arxiv.org/abs/2507.17640v1","updated":"2025-07-23T16:04:45Z","published":"2025-07-23T16:04:45Z","title":"The Early Bird Identifies the Worm: You Can't Beat a Head Start in\n  Long-Term Body Re-ID (ECHO-BID)","summary":"  Person identification in unconstrained viewing environments presents\nsignificant challenges due to variations in distance, viewpoint, imaging\nconditions, and clothing. We introduce $\\textbf{E}$va $\\textbf{C}$lothes-Change\nfrom $\\textbf{H}$idden $\\textbf{O}$bjects - $\\textbf{B}$ody\n$\\textbf{ID}$entification (ECHO-BID), a class of long-term re-id models built\non object-pretrained EVA-02 Large backbones. We compare ECHO-BID to 9 other\nmodels that vary systematically in backbone architecture, model size, scale of\nobject classification pretraining, and transfer learning protocol. Models were\nevaluated on benchmark datasets across constrained, unconstrained, and occluded\nsettings. ECHO-BID, with transfer learning on the most challenging\nclothes-change data, achieved state-of-the-art results on long-term re-id --\nsubstantially outperforming other methods. ECHO-BID also surpassed other\nmethods by a wide margin in occluded viewing scenarios. A combination of\nincreased model size and Masked Image Modeling during pretraining underlie\nECHO-BID's strong performance on long-term re-id. Notably, a smaller, but more\nchallenging transfer learning dataset, generalized better across datasets than\na larger, less challenging one. However, the larger dataset with an additional\nfine-tuning step proved best on the most difficult data. Selecting the correct\npretrained backbone architecture and transfer learning protocols can drive\nsubstantial gains in long-term re-id performance.\n","authors":["Thomas M. Metz","Matthew Q. Hill","Alice J. O'Toole"],"pdf_url":"https://arxiv.org/pdf/2507.17640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17617v1","updated":"2025-07-23T15:48:16Z","published":"2025-07-23T15:48:16Z","title":"Reusing Attention for One-stage Lane Topology Understanding","summary":"  Understanding lane toplogy relationships accurately is critical for safe\nautonomous driving. However, existing two-stage methods suffer from\ninefficiencies due to error propagations and increased computational overheads.\nTo address these challenges, we propose a one-stage architecture that\nsimultaneously predicts traffic elements, lane centerlines and topology\nrelationship, improving both the accuracy and inference speed of lane topology\nunderstanding for autonomous driving. Our key innovation lies in reusing\nintermediate attention resources within distinct transformer decoders. This\napproach effectively leverages the inherent relational knowledge within the\nelement detection module to enable the modeling of topology relationships among\ntraffic elements and lanes without requiring additional computationally\nexpensive graph networks. Furthermore, we are the first to demonstrate that\nknowledge can be distilled from models that utilize standard definition (SD)\nmaps to those operates without using SD maps, enabling superior performance\neven in the absence of SD maps. Extensive experiments on the OpenLane-V2\ndataset show that our approach outperforms baseline methods in both accuracy\nand efficiency, achieving superior results in lane detection, traffic element\nidentification, and topology reasoning. Our code is available at\nhttps://github.com/Yang-Li-2000/one-stage.git.\n","authors":["Yang Li","Zongzheng Zhang","Xuchong Qiu","Xinrun Li","Ziming Liu","Leichen Wang","Ruikai Li","Zhenxin Zhu","Huan-ang Gao","Xiaojian Lin","Zhiyong Cui","Hang Zhao","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2507.17617v1.pdf","comment":"Accepted to IROS 2025, Project Page:\n  https://github.com/Yang-Li-2000/one-stage.git"},{"id":"http://arxiv.org/abs/2507.17616v1","updated":"2025-07-23T15:47:34Z","published":"2025-07-23T15:47:34Z","title":"Vision Transformer attention alignment with human visual perception in\n  aesthetic object evaluation","summary":"  Visual attention mechanisms play a crucial role in human perception and\naesthetic evaluation. Recent advances in Vision Transformers (ViTs) have\ndemonstrated remarkable capabilities in computer vision tasks, yet their\nalignment with human visual attention patterns remains underexplored,\nparticularly in aesthetic contexts. This study investigates the correlation\nbetween human visual attention and ViT attention mechanisms when evaluating\nhandcrafted objects. We conducted an eye-tracking experiment with 30\nparticipants (9 female, 21 male, mean age 24.6 years) who viewed 20 artisanal\nobjects comprising basketry bags and ginger jars. Using a Pupil Labs\neye-tracker, we recorded gaze patterns and generated heat maps representing\nhuman visual attention. Simultaneously, we analyzed the same objects using a\npre-trained ViT model with DINO (Self-DIstillation with NO Labels), extracting\nattention maps from each of the 12 attention heads. We compared human and ViT\nattention distributions using Kullback-Leibler divergence across varying\nGaussian parameters (sigma=0.1 to 3.0). Statistical analysis revealed optimal\ncorrelation at sigma=2.4 +-0.03, with attention head #12 showing the strongest\nalignment with human visual patterns. Significant differences were found\nbetween attention heads, with heads #7 and #9 demonstrating the greatest\ndivergence from human attention (p< 0.05, Tukey HSD test). Results indicate\nthat while ViTs exhibit more global attention patterns compared to human focal\nattention, certain attention heads can approximate human visual behavior,\nparticularly for specific object features like buckles in basketry items. These\nfindings suggest potential applications of ViT attention mechanisms in product\ndesign and aesthetic evaluation, while highlighting fundamental differences in\nattention strategies between human perception and current AI models.\n","authors":["Miguel Carrasco","César González-Martín","José Aranda","Luis Oliveros"],"pdf_url":"https://arxiv.org/pdf/2507.17616v1.pdf","comment":"25 pages, 15 figures"},{"id":"http://arxiv.org/abs/2507.17613v1","updated":"2025-07-23T15:46:09Z","published":"2025-07-23T15:46:09Z","title":"InvRGB+L: Inverse Rendering of Complex Scenes with Unified Color and\n  LiDAR Reflectance Modeling","summary":"  We present InvRGB+L, a novel inverse rendering model that reconstructs large,\nrelightable, and dynamic scenes from a single RGB+LiDAR sequence. Conventional\ninverse graphics methods rely primarily on RGB observations and use LiDAR\nmainly for geometric information, often resulting in suboptimal material\nestimates due to visible light interference. We find that LiDAR's intensity\nvalues-captured with active illumination in a different spectral range-offer\ncomplementary cues for robust material estimation under variable lighting.\nInspired by this, InvRGB+L leverages LiDAR intensity cues to overcome\nchallenges inherent in RGB-centric inverse graphics through two key\ninnovations: (1) a novel physics-based LiDAR shading model and (2) RGB-LiDAR\nmaterial consistency losses. The model produces novel-view RGB and LiDAR\nrenderings of urban and indoor scenes and supports relighting, night\nsimulations, and dynamic object insertions, achieving results that surpass\ncurrent state-of-the-art methods in both scene-level urban inverse rendering\nand LiDAR simulation.\n","authors":["Xiaoxue Chen","Bhargav Chandaka","Chih-Hao Lin","Ya-Qin Zhang","David Forsyth","Hao Zhao","Shenlong Wang"],"pdf_url":"https://arxiv.org/pdf/2507.17613v1.pdf","comment":"Accepted to ICCV 2025"},{"id":"http://arxiv.org/abs/2506.15610v2","updated":"2025-07-23T15:38:09Z","published":"2025-06-18T16:40:05Z","title":"BoxFusion: Reconstruction-Free Open-Vocabulary 3D Object Detection via\n  Real-Time Multi-View Box Fusion","summary":"  Open-vocabulary 3D object detection has gained significant interest due to\nits critical applications in autonomous driving and embodied AI. Existing\ndetection methods, whether offline or online, typically rely on dense point\ncloud reconstruction, which imposes substantial computational overhead and\nmemory constraints, hindering real-time deployment in downstream tasks. To\naddress this, we propose a novel reconstruction-free online framework tailored\nfor memory-efficient and real-time 3D detection. Specifically, given streaming\nposed RGB-D video input, we leverage Cubify Anything as a pre-trained visual\nfoundation model (VFM) for single-view 3D object detection by bounding boxes,\ncoupled with CLIP to capture open-vocabulary semantics of detected objects. To\nfuse all detected bounding boxes across different views into a unified one, we\nemploy an association module for correspondences of multi-views and an\noptimization module to fuse the 3D bounding boxes of the same instance\npredicted in multi-views. The association module utilizes 3D Non-Maximum\nSuppression (NMS) and a box correspondence matching module, while the\noptimization module uses an IoU-guided efficient random optimization technique\nbased on particle filtering to enforce multi-view consistency of the 3D\nbounding boxes while minimizing computational complexity. Extensive experiments\non ScanNetV2 and CA-1M datasets demonstrate that our method achieves\nstate-of-the-art performance among online methods. Benefiting from this novel\nreconstruction-free paradigm for 3D object detection, our method exhibits great\ngeneralization abilities in various scenarios, enabling real-time perception\neven in environments exceeding 1000 square meters.\n","authors":["Yuqing Lan","Chenyang Zhu","Zhirui Gao","Jiazhao Zhang","Yihan Cao","Renjiao Yi","Yijie Wang","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2506.15610v2.pdf","comment":"Project page: https://lanlan96.github.io/BoxFusion/"},{"id":"http://arxiv.org/abs/2507.17597v1","updated":"2025-07-23T15:28:57Z","published":"2025-07-23T15:28:57Z","title":"Explainable AI for Collaborative Assessment of 2D/3D Registration\n  Quality","summary":"  As surgery embraces digital transformation--integrating sophisticated\nimaging, advanced algorithms, and robotics to support and automate complex\nsub-tasks--human judgment of system correctness remains a vital safeguard for\npatient safety. This shift introduces new \"operator-type\" roles tasked with\nverifying complex algorithmic outputs, particularly at critical junctures of\nthe procedure, such as the intermediary check before drilling or implant\nplacement. A prime example is 2D/3D registration, a key enabler of image-based\nsurgical navigation that aligns intraoperative 2D images with preoperative 3D\ndata. Although registration algorithms have advanced significantly, they\noccasionally yield inaccurate results. Because even small misalignments can\nlead to revision surgery or irreversible surgical errors, there is a critical\nneed for robust quality assurance. Current visualization-based strategies alone\nhave been found insufficient to enable humans to reliably detect 2D/3D\nregistration misalignments. In response, we propose the first artificial\nintelligence (AI) framework trained specifically for 2D/3D registration quality\nverification, augmented by explainability features that clarify the model's\ndecision-making. Our explainable AI (XAI) approach aims to enhance informed\ndecision-making for human operators by providing a second opinion together with\na rationale behind it. Through algorithm-centric and human-centered\nevaluations, we systematically compare four conditions: AI-only, human-only,\nhuman-AI, and human-XAI. Our findings reveal that while explainability features\nmodestly improve user trust and willingness to override AI errors, they do not\nexceed the standalone AI in aggregate performance. Nevertheless, future work\nextending both the algorithmic design and the human-XAI collaboration elements\nholds promise for more robust quality assurance of 2D/3D registration.\n","authors":["Sue Min Cho","Alexander Do","Russell H. Taylor","Mathias Unberath"],"pdf_url":"https://arxiv.org/pdf/2507.17597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17594v1","updated":"2025-07-23T15:27:09Z","published":"2025-07-23T15:27:09Z","title":"RemixFusion: Residual-based Mixed Representation for Large-scale Online\n  RGB-D Reconstruction","summary":"  The introduction of the neural implicit representation has notably propelled\nthe advancement of online dense reconstruction techniques. Compared to\ntraditional explicit representations, such as TSDF, it improves the mapping\ncompleteness and memory efficiency. However, the lack of reconstruction details\nand the time-consuming learning of neural representations hinder the widespread\napplication of neural-based methods to large-scale online reconstruction. We\nintroduce RemixFusion, a novel residual-based mixed representation for scene\nreconstruction and camera pose estimation dedicated to high-quality and\nlarge-scale online RGB-D reconstruction. In particular, we propose a\nresidual-based map representation comprised of an explicit coarse TSDF grid and\nan implicit neural module that produces residuals representing fine-grained\ndetails to be added to the coarse grid. Such mixed representation allows for\ndetail-rich reconstruction with bounded time and memory budget, contrasting\nwith the overly-smoothed results by the purely implicit representations, thus\npaving the way for high-quality camera tracking. Furthermore, we extend the\nresidual-based representation to handle multi-frame joint pose optimization via\nbundle adjustment (BA). In contrast to the existing methods, which optimize\nposes directly, we opt to optimize pose changes. Combined with a novel\ntechnique for adaptive gradient amplification, our method attains better\noptimization convergence and global optimality. Furthermore, we adopt a local\nmoving volume to factorize the mixed scene representation with a\ndivide-and-conquer design to facilitate efficient online learning in our\nresidual-based framework. Extensive experiments demonstrate that our method\nsurpasses all state-of-the-art ones, including those based either on explicit\nor implicit representations, in terms of the accuracy of both mapping and\ntracking on large-scale scenes.\n","authors":["Yuqing Lan","Chenyang Zhu","Shuaifeng Zhi","Jiazhao Zhang","Zhoufeng Wang","Renjiao Yi","Yijie Wang","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2507.17594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17588v1","updated":"2025-07-23T15:22:51Z","published":"2025-07-23T15:22:51Z","title":"Dual-branch Prompting for Multimodal Machine Translation","summary":"  Multimodal Machine Translation (MMT) typically enhances text-only translation\nby incorporating aligned visual features. Despite the remarkable progress,\nstate-of-the-art MMT approaches often rely on paired image-text inputs at\ninference and are sensitive to irrelevant visual noise, which limits their\nrobustness and practical applicability. To address these issues, we propose\nD2P-MMT, a diffusion-based dual-branch prompting framework for robust\nvision-guided translation. Specifically, D2P-MMT requires only the source text\nand a reconstructed image generated by a pre-trained diffusion model, which\nnaturally filters out distracting visual details while preserving semantic\ncues. During training, the model jointly learns from both authentic and\nreconstructed images using a dual-branch prompting strategy, encouraging rich\ncross-modal interactions. To bridge the modality gap and mitigate\ntraining-inference discrepancies, we introduce a distributional alignment loss\nthat enforces consistency between the output distributions of the two branches.\nExtensive experiments on the Multi30K dataset demonstrate that D2P-MMT achieves\nsuperior translation performance compared to existing state-of-the-art\napproaches.\n","authors":["Jie Wang","Zhendong Yang","Liansong Zong","Xiaobo Zhang","Dexian Wang","Ji Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.17588v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17585v1","updated":"2025-07-23T15:20:31Z","published":"2025-07-23T15:20:31Z","title":"From Scan to Action: Leveraging Realistic Scans for Embodied Scene\n  Understanding","summary":"  Real-world 3D scene-level scans offer realism and can enable better\nreal-world generalizability for downstream applications. However, challenges\nsuch as data volume, diverse annotation formats, and tool compatibility limit\ntheir use. This paper demonstrates a methodology to effectively leverage these\nscans and their annotations. We propose a unified annotation integration using\nUSD, with application-specific USD flavors. We identify challenges in utilizing\nholistic real-world scan datasets and present mitigation strategies. The\nefficacy of our approach is demonstrated through two downstream applications:\nLLM-based scene editing, enabling effective LLM understanding and adaptation of\nthe data (80% success), and robotic simulation, achieving an 87% success rate\nin policy learning.\n","authors":["Anna-Maria Halacheva","Jan-Nico Zaech","Sombit Dey","Luc Van Gool","Danda Pani Paudel"],"pdf_url":"https://arxiv.org/pdf/2507.17585v1.pdf","comment":"Accepted at the OpenSUN3D Workshop, CVPR 2025. This workshop paper is\n  not included in the official CVPR proceedings"},{"id":"http://arxiv.org/abs/2507.17577v1","updated":"2025-07-23T15:11:25Z","published":"2025-07-23T15:11:25Z","title":"Boosting Ray Search Procedure of Hard-label Attacks with Transfer-based\n  Priors","summary":"  One of the most practical and challenging types of black-box adversarial\nattacks is the hard-label attack, where only the top-1 predicted label is\navailable. One effective approach is to search for the optimal ray direction\nfrom the benign image that minimizes the $\\ell_p$-norm distance to the\nadversarial region. The unique advantage of this approach is that it transforms\nthe hard-label attack into a continuous optimization problem. The objective\nfunction value is the ray's radius, which can be obtained via binary search at\na high query cost. Existing methods use a \"sign trick\" in gradient estimation\nto reduce the number of queries. In this paper, we theoretically analyze the\nquality of this gradient estimation and propose a novel prior-guided approach\nto improve ray search efficiency both theoretically and empirically.\nSpecifically, we utilize the transfer-based priors from surrogate models, and\nour gradient estimators appropriately integrate them by approximating the\nprojection of the true gradient onto the subspace spanned by these priors and\nrandom directions, in a query-efficient manner. We theoretically derive the\nexpected cosine similarities between the obtained gradient estimators and the\ntrue gradient, and demonstrate the improvement achieved by incorporating\npriors. Extensive experiments on the ImageNet and CIFAR-10 datasets show that\nour approach significantly outperforms 11 state-of-the-art methods in terms of\nquery efficiency.\n","authors":["Chen Ma","Xinjie Xu","Shuyu Cheng","Qi Xuan"],"pdf_url":"https://arxiv.org/pdf/2507.17577v1.pdf","comment":"Published at ICLR 2025 (Spotlight paper)"},{"id":"http://arxiv.org/abs/2505.10016v2","updated":"2025-07-23T15:01:56Z","published":"2025-05-15T06:58:45Z","title":"Application of YOLOv8 in monocular downward multiple Car Target\n  detection","summary":"  Autonomous driving technology is progressively transforming traditional car\ndriving methods, marking a significant milestone in modern transportation.\nObject detection serves as a cornerstone of autonomous systems, playing a vital\nrole in enhancing driving safety, enabling autonomous functionality, improving\ntraffic efficiency, and facilitating effective emergency responses. However,\ncurrent technologies such as radar for environmental perception, cameras for\nroad perception, and vehicle sensor networks face notable challenges, including\nhigh costs, vulnerability to weather and lighting conditions, and limited\nresolution.To address these limitations, this paper presents an improved\nautonomous target detection network based on YOLOv8. By integrating structural\nreparameterization technology, a bidirectional pyramid structure network model,\nand a novel detection pipeline into the YOLOv8 framework, the proposed approach\nachieves highly efficient and precise detection of multi-scale, small, and\nremote objects. Experimental results demonstrate that the enhanced model can\neffectively detect both large and small objects with a detection accuracy of\n65%, showcasing significant advancements over traditional methods.This improved\nmodel holds substantial potential for real-world applications and is\nwell-suited for autonomous driving competitions, such as the Formula Student\nAutonomous China (FSAC), particularly excelling in scenarios involving\nsingle-target and small-object detection.\n","authors":["Shijie Lyu"],"pdf_url":"https://arxiv.org/pdf/2505.10016v2.pdf","comment":"This submission included authors who did not consent to the\n  submission. The paper is being withdrawn until authorship issues are resolved"},{"id":"http://arxiv.org/abs/2505.10027v2","updated":"2025-07-23T15:01:44Z","published":"2025-05-15T07:17:03Z","title":"ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model\n  Super-Resolution Reconstruction","summary":"  With the rapid advancement of remote sensing technology, super-resolution\nimage reconstruction is of great research and practical significance. Existing\ndeep learning methods have made progress but still face limitations in handling\ncomplex scenes and preserving image details. This paper proposes a\nreinforcement learning-based latent diffusion model (LDM) fine-tuning method\nfor remote sensing image super-resolution. The method constructs a\nreinforcement learning environment with states, actions, and rewards,\noptimizing decision objectives through proximal policy optimization (PPO)\nduring the reverse denoising process of the LDM model. Experiments on the\nRESISC45 dataset show significant improvements over the baseline model in PSNR,\nSSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11,\nand LPIPS reducing by 0.06-0.10, particularly in structured and complex natural\nscenes. The results demonstrate the method's effectiveness in enhancing\nsuper-resolution quality and adaptability across scenes.\n","authors":["Shijie Lyu"],"pdf_url":"https://arxiv.org/pdf/2505.10027v2.pdf","comment":"This submission included authors who did not consent to the\n  submission. The paper is being withdrawn until authorship issues are resolved"},{"id":"http://arxiv.org/abs/2312.03584v2","updated":"2025-07-23T14:47:48Z","published":"2023-12-06T16:19:51Z","title":"Context Diffusion: In-Context Aware Image Generation","summary":"  We propose Context Diffusion, a diffusion-based framework that enables image\ngeneration models to learn from visual examples presented in context. Recent\nwork tackles such in-context learning for image generation, where a query image\nis provided alongside context examples and text prompts. However, the quality\nand context fidelity of the generated images deteriorate when the prompt is not\npresent, demonstrating that these models cannot truly learn from the visual\ncontext. To address this, we propose a novel framework that separates the\nencoding of the visual context and the preservation of the desired image\nlayout. This results in the ability to learn from the visual context and\nprompts, but also from either of them. Furthermore, we enable our model to\nhandle few-shot settings, to effectively address diverse in-context learning\nscenarios. Our experiments and human evaluation demonstrate that Context\nDiffusion excels in both in-domain and out-of-domain tasks, resulting in an\noverall enhancement in image quality and context fidelity compared to\ncounterpart models.\n","authors":["Ivona Najdenkoska","Animesh Sinha","Abhimanyu Dubey","Dhruv Mahajan","Vignesh Ramanathan","Filip Radenovic"],"pdf_url":"https://arxiv.org/pdf/2312.03584v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17554v1","updated":"2025-07-23T14:43:22Z","published":"2025-07-23T14:43:22Z","title":"An h-space Based Adversarial Attack for Protection Against Few-shot\n  Personalization","summary":"  The versatility of diffusion models in generating customized images from few\nsamples raises significant privacy concerns, particularly regarding\nunauthorized modifications of private content. This concerning issue has\nrenewed the efforts in developing protection mechanisms based on adversarial\nattacks, which generate effective perturbations to poison diffusion models. Our\nwork is motivated by the observation that these models exhibit a high degree of\nabstraction within their semantic latent space (`h-space'), which encodes\ncritical high-level features for generating coherent and meaningful content. In\nthis paper, we propose a novel anti-customization approach, called HAAD\n(h-space based Adversarial Attack for Diffusion models), that leverages\nadversarial attacks to craft perturbations based on the h-space that can\nefficiently degrade the image generation process. Building upon HAAD, we\nfurther introduce a more efficient variant, HAAD-KV, that constructs\nperturbations solely based on the KV parameters of the h-space. This strategy\noffers a stronger protection, that is computationally less expensive. Despite\ntheir simplicity, our methods outperform state-of-the-art adversarial attacks,\nhighlighting their effectiveness.\n","authors":["Xide Xu","Sandesh Kamath","Muhammad Atif Butt","Bogdan Raducanu"],"pdf_url":"https://arxiv.org/pdf/2507.17554v1.pdf","comment":"32 pages, 15 figures. Accepted by ACM Multimedia 2025"},{"id":"http://arxiv.org/abs/2506.05367v2","updated":"2025-07-23T14:25:59Z","published":"2025-05-27T22:40:35Z","title":"Text2Stereo: Repurposing Stable Diffusion for Stereo Generation with\n  Consistency Rewards","summary":"  In this paper, we propose a novel diffusion-based approach to generate stereo\nimages given a text prompt. Since stereo image datasets with large baselines\nare scarce, training a diffusion model from scratch is not feasible. Therefore,\nwe propose leveraging the strong priors learned by Stable Diffusion and\nfine-tuning it on stereo image datasets to adapt it to the task of stereo\ngeneration. To improve stereo consistency and text-to-image alignment, we\nfurther tune the model using prompt alignment and our proposed stereo\nconsistency reward functions. Comprehensive experiments demonstrate the\nsuperiority of our approach in generating high-quality stereo images across\ndiverse scenarios, outperforming existing methods.\n","authors":["Aakash Garg","Libing Zeng","Andrii Tsarov","Nima Khademi Kalantari"],"pdf_url":"https://arxiv.org/pdf/2506.05367v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17539v1","updated":"2025-07-23T14:19:30Z","published":"2025-07-23T14:19:30Z","title":"Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration\n  Through Clinical Cognitive Chain Reasoning","summary":"  Multimodal large language models (MLLMs) demonstrate significant potential in\nthe field of medical diagnosis. However, they face critical challenges in\nspecialized domains such as ophthalmology, particularly the fragmentation of\nannotation granularity and inconsistencies in clinical reasoning logic, which\nhinder precise cross-modal understanding. This paper introduces FundusExpert,\nan ophthalmology-specific MLLM with integrated positioning-diagnosis reasoning\ncapabilities, along with FundusGen, a dataset constructed through the\nintelligent Fundus-Engine system. Fundus-Engine automates localization and\nleverages MLLM-based semantic expansion to integrate global disease\nclassification, local object detection, and fine-grained feature analysis\nwithin a single fundus image. Additionally, by constructing a clinically\naligned cognitive chain, it guides the model to generate interpretable\nreasoning paths. FundusExpert, fine-tuned with instruction data from FundusGen,\nachieves the best performance in ophthalmic question-answering tasks,\nsurpassing the average accuracy of the 40B MedRegA by 26.6%. It also excels in\nzero-shot report generation tasks, achieving a clinical consistency of 77.0%,\nsignificantly outperforming GPT-4o's 47.6%. Furthermore, we reveal a scaling\nlaw between data quality and model capability ($L \\propto N^{0.068}$),\ndemonstrating that the cognitive alignment annotations in FundusGen enhance\ndata utilization efficiency. By integrating region-level localization with\ndiagnostic reasoning chains, our work develops a scalable, clinically-aligned\nMLLM and explores a pathway toward bridging the visual-language gap in specific\nMLLMs. Our project can be found at https://github.com/MeteorElf/FundusExpert.\n","authors":["Xinyao Liu","Diping Song"],"pdf_url":"https://arxiv.org/pdf/2507.17539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17533v1","updated":"2025-07-23T14:13:14Z","published":"2025-07-23T14:13:14Z","title":"Multi-modal Multi-task Pre-training for Improved Point Cloud\n  Understanding","summary":"  Recent advances in multi-modal pre-training methods have shown promising\neffectiveness in learning 3D representations by aligning multi-modal features\nbetween 3D shapes and their corresponding 2D counterparts. However, existing\nmulti-modal pre-training frameworks primarily rely on a single pre-training\ntask to gather multi-modal data in 3D applications. This limitation prevents\nthe models from obtaining the abundant information provided by other relevant\ntasks, which can hinder their performance in downstream tasks, particularly in\ncomplex and diverse domains. In order to tackle this issue, we propose MMPT, a\nMulti-modal Multi-task Pre-training framework designed to enhance point cloud\nunderstanding. Specifically, three pre-training tasks are devised: (i)\nToken-level reconstruction (TLR) aims to recover masked point tokens, endowing\nthe model with representative learning abilities. (ii) Point-level\nreconstruction (PLR) is integrated to predict the masked point positions\ndirectly, and the reconstructed point cloud can be considered as a transformed\npoint cloud used in the subsequent task. (iii) Multi-modal contrastive learning\n(MCL) combines feature correspondences within and across modalities, thus\nassembling a rich learning signal from both 3D point cloud and 2D image\nmodalities in a self-supervised manner. Moreover, this framework operates\nwithout requiring any 3D annotations, making it scalable for use with large\ndatasets. The trained encoder can be effectively transferred to various\ndownstream tasks. To demonstrate its effectiveness, we evaluated its\nperformance compared to state-of-the-art methods in various discriminant and\ngenerative applications under widely-used benchmarks.\n","authors":["Liwen Liu","Weidong Yang","Lipeng Ma","Ben Fei"],"pdf_url":"https://arxiv.org/pdf/2507.17533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17522v1","updated":"2025-07-23T14:03:54Z","published":"2025-07-23T14:03:54Z","title":"STQE: Spatial-Temporal Quality Enhancement for G-PCC Compressed Dynamic\n  Point Clouds","summary":"  Very few studies have addressed quality enhancement for compressed dynamic\npoint clouds. In particular, the effective exploitation of spatial-temporal\ncorrelations between point cloud frames remains largely unexplored. Addressing\nthis gap, we propose a spatial-temporal attribute quality enhancement (STQE)\nnetwork that exploits both spatial and temporal correlations to improve the\nvisual quality of G-PCC compressed dynamic point clouds. Our contributions\ninclude a recoloring-based motion compensation module that remaps reference\nattribute information to the current frame geometry to achieve precise\ninter-frame geometric alignment, a channel-aware temporal attention module that\ndynamically highlights relevant regions across bidirectional reference frames,\na Gaussian-guided neighborhood feature aggregation module that efficiently\ncaptures spatial dependencies between geometry and color attributes, and a\njoint loss function based on the Pearson correlation coefficient, designed to\nalleviate over-smoothing effects typical of point-wise mean squared error\noptimization. When applied to the latest G-PCC test model, STQE achieved\nimprovements of 0.855 dB, 0.682 dB, and 0.828 dB in delta PSNR, with\nBj{\\o}ntegaard Delta rate (BD-rate) reductions of -25.2%, -31.6%, and -32.5%\nfor the Luma, Cb, and Cr components, respectively.\n","authors":["Tian Guo","Hui Yuan","Xiaolong Mao","Shiqi Jiang","Raouf Hamzaoui","Sam Kwong"],"pdf_url":"https://arxiv.org/pdf/2507.17522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17520v1","updated":"2025-07-23T13:57:06Z","published":"2025-07-23T13:57:06Z","title":"InstructVLA: Vision-Language-Action Instruction Tuning from\n  Understanding to Manipulation","summary":"  To operate effectively in the real world, robots must integrate multimodal\nreasoning with precise action generation. However, existing\nvision-language-action (VLA) models often sacrifice one for the other, narrow\ntheir abilities to task-specific manipulation data, and suffer catastrophic\nforgetting of pre-trained vision-language capabilities. To bridge this gap, we\nintroduce InstructVLA, an end-to-end VLA model that preserves the flexible\nreasoning of large vision-language models (VLMs) while delivering leading\nmanipulation performance. InstructVLA introduces a novel training paradigm,\nVision-Language-Action Instruction Tuning (VLA-IT), which employs multimodal\ntraining with mixture-of-experts adaptation to jointly optimize textual\nreasoning and action generation on both standard VLM corpora and a curated\n650K-sample VLA-IT dataset. On in-domain SimplerEnv tasks, InstructVLA achieves\n30.5% improvement over SpatialVLA. To evaluate generalization, we introduce\nSimplerEnv-Instruct, an 80-task benchmark requiring closed-loop control and\nhigh-level instruction understanding, where it outperforms a fine-tuned OpenVLA\nby 92% and an action expert aided by GPT-4o by 29%. Additionally, InstructVLA\nsurpasses baseline VLMs on multimodal tasks and exhibits inference-time scaling\nby leveraging textual reasoning to boost manipulation performance in both\nsimulated and real-world settings. These results demonstrate InstructVLA's\npotential for bridging intuitive and steerable human-robot interaction with\nefficient policy learning.\n","authors":["Shuai Yang","Hao Li","Yilun Chen","Bin Wang","Yang Tian","Tai Wang","Hanqing Wang","Feng Zhao","Yiyi Liao","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2507.17520v1.pdf","comment":"38 pages"},{"id":"http://arxiv.org/abs/2505.02586v3","updated":"2025-07-23T13:55:47Z","published":"2025-05-05T11:39:51Z","title":"RGBX-DiffusionDet: A Framework for Multi-Modal RGB-X Object Detection\n  Using DiffusionDet","summary":"  This work introduces RGBX-DiffusionDet, an object detection framework\nextending the DiffusionDet model to fuse the heterogeneous 2D data (X) with RGB\nimagery via an adaptive multimodal encoder. To enable cross-modal interaction,\nwe design the dynamic channel reduction within a convolutional block attention\nmodule (DCR-CBAM), which facilitates cross-talk between subnetworks by\ndynamically highlighting salient channel features. Furthermore, the dynamic\nmulti-level aggregation block (DMLAB) is proposed to refine spatial feature\nrepresentations through adaptive multiscale fusion. Finally, novel\nregularization losses that enforce channel saliency and spatial selectivity are\nintroduced, leading to compact and discriminative feature embeddings. Extensive\nexperiments using RGB-Depth (KITTI), a novel annotated RGB-Polarimetric\ndataset, and RGB-Infrared (M$^3$FD) benchmark dataset were conducted. We\ndemonstrate consistent superiority of the proposed approach over the baseline\nRGB-only DiffusionDet. The modular architecture maintains the original decoding\ncomplexity, ensuring efficiency. These results establish the proposed\nRGBX-DiffusionDet as a flexible multimodal object detection approach, providing\nnew insights into integrating diverse 2D sensing modalities into\ndiffusion-based detection pipelines.\n","authors":["Eliraz Orfaig","Inna Stainvas","Igal Bilik"],"pdf_url":"https://arxiv.org/pdf/2505.02586v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17515v1","updated":"2025-07-23T13:52:27Z","published":"2025-07-23T13:52:27Z","title":"URPO: A Unified Reward & Policy Optimization Framework for Large\n  Language Models","summary":"  Large-scale alignment pipelines typically pair a policy model with a\nseparately trained reward model whose parameters remain frozen during\nreinforcement learning (RL). This separation creates a complex,\nresource-intensive pipeline and suffers from a performance ceiling due to a\nstatic reward signal. We propose a novel framework, Unified Reward & Policy\nOptimization (URPO), that unifies instruction-following (\"player\") and reward\nmodeling (\"referee\") within a single model and a single training phase. Our\nmethod recasts all alignment data-including preference pairs, verifiable\nreasoning, and open-ended instructions-into a unified generative format\noptimized by a single Group-Relative Policy Optimization (GRPO) loop. This\nenables the model to learn from ground-truth preferences and verifiable logic\nwhile simultaneously generating its own rewards for open-ended tasks.\nExperiments on the Qwen2.5-7B model demonstrate URPO's superiority. Our unified\nmodel significantly outperforms a strong baseline using a separate generative\nreward model, boosting the instruction-following score on AlpacaEval from 42.24\nto 44.84 and the composite reasoning average from 32.66 to 35.66. Furthermore,\nURPO cultivates a superior internal evaluator as a byproduct of training,\nachieving a RewardBench score of 85.15 and surpassing the dedicated reward\nmodel it replaces (83.55). By eliminating the need for a separate reward model\nand fostering a co-evolutionary dynamic between generation and evaluation, URPO\npresents a simpler, more efficient, and more effective path towards robustly\naligned language models.\n","authors":["Songshuo Lu","Hua Wang","Zhi Chen","Yaohua Tang"],"pdf_url":"https://arxiv.org/pdf/2507.17515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17511v1","updated":"2025-07-23T13:49:25Z","published":"2025-07-23T13:49:25Z","title":"Accelerating Parallel Diffusion Model Serving with Residual Compression","summary":"  Diffusion models produce realistic images and videos but require substantial\ncomputational resources, necessitating multi-accelerator parallelism for\nreal-time deployment. However, parallel inference introduces significant\ncommunication overhead from exchanging large activations between devices,\nlimiting efficiency and scalability. We present CompactFusion, a compression\nframework that significantly reduces communication while preserving generation\nquality. Our key observation is that diffusion activations exhibit strong\ntemporal redundancy-adjacent steps produce highly similar activations,\nsaturating bandwidth with near-duplicate data carrying little new information.\nTo address this inefficiency, we seek a more compact representation that\nencodes only the essential information. CompactFusion achieves this via\nResidual Compression that transmits only compressed residuals (step-wise\nactivation differences). Based on empirical analysis and theoretical\njustification, we show that it effectively removes redundant data, enabling\nsubstantial data reduction while maintaining high fidelity. We also integrate\nlightweight error feedback to prevent error accumulation. CompactFusion\nestablishes a new paradigm for parallel diffusion inference, delivering lower\nlatency and significantly higher generation quality than prior methods. On\n4xL20, it achieves 3.0x speedup while greatly improving fidelity. It also\nuniquely supports communication-heavy strategies like sequence parallelism on\nslow networks, achieving 6.7x speedup over prior overlap-based method.\nCompactFusion applies broadly across diffusion models and parallel settings,\nand integrates easily without requiring pipeline rework. Portable\nimplementation demonstrated on xDiT is publicly available at\nhttps://github.com/Cobalt-27/CompactFusion\n","authors":["Jiajun Luo","Yicheng Xiao","Jianru Xu","Yangxiu You","Rongwei Lu","Chen Tang","Jingyan Jiang","Zhi Wang"],"pdf_url":"https://arxiv.org/pdf/2507.17511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17508v1","updated":"2025-07-23T13:47:33Z","published":"2025-07-23T13:47:33Z","title":"Illicit object detection in X-ray imaging using deep learning\n  techniques: A comparative evaluation","summary":"  Automated X-ray inspection is crucial for efficient and unobtrusive security\nscreening in various public settings. However, challenges such as object\nocclusion, variations in the physical properties of items, diversity in X-ray\nscanning devices, and limited training data hinder accurate and reliable\ndetection of illicit items. Despite the large body of research in the field,\nreported experimental evaluations are often incomplete, with frequently\nconflicting outcomes. To shed light on the research landscape and facilitate\nfurther research, a systematic, detailed, and thorough comparative evaluation\nof recent Deep Learning (DL)-based methods for X-ray object detection is\nconducted. For this, a comprehensive evaluation framework is developed,\ncomposed of: a) Six recent, large-scale, and widely used public datasets for\nX-ray illicit item detection (OPIXray, CLCXray, SIXray, EDS, HiXray, and\nPIDray), b) Ten different state-of-the-art object detection schemes covering\nall main categories in the literature, including generic Convolutional Neural\nNetwork (CNN), custom CNN, generic transformer, and hybrid CNN-transformer\narchitectures, and c) Various detection (mAP50 and mAP50:95) and\ntime/computational-complexity (inference time (ms), parameter size (M), and\ncomputational load (GFLOPS)) metrics. A thorough analysis of the results leads\nto critical observations and insights, emphasizing key aspects such as: a)\nOverall behavior of the object detection schemes, b) Object-level detection\nperformance, c) Dataset-specific observations, and d) Time efficiency and\ncomputational complexity analysis. To support reproducibility of the reported\nexperimental results, the evaluation code and model weights are made publicly\navailable at https://github.com/jgenc/xray-comparative-evaluation.\n","authors":["Jorgen Cani","Christos Diou","Spyridon Evangelatos","Vasileios Argyriou","Panagiotis Radoglou-Grammatikis","Panagiotis Sarigiannidis","Iraklis Varlamis","Georgios Th. Papadopoulos"],"pdf_url":"https://arxiv.org/pdf/2507.17508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17501v1","updated":"2025-07-23T13:37:23Z","published":"2025-07-23T13:37:23Z","title":"DNT: a Deeply Normalized Transformer that can be trained by Momentum SGD","summary":"  Transformers have become the de facto backbone of modern deep learning, yet\ntheir training typically demands an advanced optimizer with adaptive learning\nrate like AdamW, rather than a momentum SGDW (mSGDW). Previous works show that\nit is mainly due to a heavy-tailed distribution of the gradients. In this\npaper, we introduce a Deeply Normalized Transformer (DNT), which is\nmeticulously engineered to overcome this limitation enabling seamless training\nwith vanilla mSGDW while yielding comparable performance to the Transformers\ntrained via AdamW. To be specific, in DNT, we strategically integrate\nnormalization techniques at proper positions in the Transformers to effectively\nmodulate the Jacobian matrices of each layer, balance the influence of weights,\nactivations, and their interactions, and thus enable the distributions of\ngradients concentrated. We provide both theoretical justifications of the\nnormalization technique used in our DNT and extensive empirical evaluation on\ntwo popular Transformer architectures to validate that: a) DNT outperforms its\ncounterparts (\\ie, ViT and GPT), and b) DNT can be effectively trained with\nvanilla mSGDW.\n","authors":["Xianbiao Qi","Marco Chen","Wenjie Xiao","Jiaquan Ye","Yelin He","Chun-Guang Li","Zhouchen Lin"],"pdf_url":"https://arxiv.org/pdf/2507.17501v1.pdf","comment":"We have introduced a novel architecture, Deeply Normalized\n  Transformer (DNT), which enables efficient training with vanilla momentum\n  SGDW (mSGDW), achieving performance on par with AdamW-optimized Transformers"},{"id":"http://arxiv.org/abs/2507.00748v2","updated":"2025-07-23T13:23:35Z","published":"2025-07-01T13:48:57Z","title":"Improving the Reasoning of Multi-Image Grounding in MLLMs via\n  Reinforcement Learning","summary":"  Recently, Multimodal Large Language Models (MLLMs) excel at visual grounding\nin single-image scenarios with textual references. However, their performance\ndegrades when handling real-world applications that involve complex multi-image\ncompositions and multi-modal instructions, revealing limitations in cross-image\nreasoning and generalization. To address these challenges, we adopt a\nReinforcement Learning (RL) based post-training strategy to improve the\nreasoning of MLLMs in multi-image grounding tasks. Our approach begins with\nsynthesizing high-quality chain-of-thought (CoT) data for cold-start\ninitialization, followed by supervised fine-tuning (SFT) using low-rank\nadaptation (LoRA). The cold-start training stage enables the model to identify\ncorrect solutions. Subsequently, we perform rejection sampling using the merged\nSFT model to curate high-quality RL data and leverage rule-based RL to guide\nthe model toward optimal reasoning paths. Extensive experimental results\ndemonstrate the effectiveness of our approach, yielding improvements of +9.04%\non MIG-Bench, +6.37% on MC-Bench, and +4.98% on several out-of-domain reasoning\ngrounding benchmarks compared to the SFT baseline. Furthermore, our method\nexhibits strong generalization in multi-image perception, with gains of +3.1%\nand +2.4% over the base model on BLINK and MMIU benchmarks, respectively.\n","authors":["Bob Zhang","Haoran Li","Tao Zhang","Cilin Yan","Jiayin Cai","Yanbin Hao"],"pdf_url":"https://arxiv.org/pdf/2507.00748v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2507.17489v1","updated":"2025-07-23T13:14:59Z","published":"2025-07-23T13:14:59Z","title":"DFDNet: Dynamic Frequency-Guided De-Flare Network","summary":"  Strong light sources in nighttime photography frequently produce flares in\nimages, significantly degrading visual quality and impacting the performance of\ndownstream tasks. While some progress has been made, existing methods continue\nto struggle with removing large-scale flare artifacts and repairing structural\ndamage in regions near the light source. We observe that these challenging\nflare artifacts exhibit more significant discrepancies from the reference\nimages in the frequency domain compared to the spatial domain. Therefore, this\npaper presents a novel dynamic frequency-guided deflare network (DFDNet) that\ndecouples content information from flare artifacts in the frequency domain,\neffectively removing large-scale flare artifacts. Specifically, DFDNet consists\nmainly of a global dynamic frequency-domain guidance (GDFG) module and a local\ndetail guidance module (LDGM). The GDFG module guides the network to perceive\nthe frequency characteristics of flare artifacts by dynamically optimizing\nglobal frequency domain features, effectively separating flare information from\ncontent information. Additionally, we design an LDGM via a contrastive learning\nstrategy that aligns the local features of the light source with the reference\nimage, reduces local detail damage from flare removal, and improves\nfine-grained image restoration. The experimental results demonstrate that the\nproposed method outperforms existing state-of-the-art methods in terms of\nperformance. The code is available at\n\\href{https://github.com/AXNing/DFDNet}{https://github.com/AXNing/DFDNet}.\n","authors":["Minglong Xue","Aoxiang Ning","Shivakumara Palaiahnakote","Mingliang Zhou"],"pdf_url":"https://arxiv.org/pdf/2507.17489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17486v1","updated":"2025-07-23T13:09:57Z","published":"2025-07-23T13:09:57Z","title":"Unsupervised anomaly detection using Bayesian flow networks: application\n  to brain FDG PET in the context of Alzheimer's disease","summary":"  Unsupervised anomaly detection (UAD) plays a crucial role in neuroimaging for\nidentifying deviations from healthy subject data and thus facilitating the\ndiagnosis of neurological disorders. In this work, we focus on Bayesian flow\nnetworks (BFNs), a novel class of generative models, which have not yet been\napplied to medical imaging or anomaly detection. BFNs combine the strength of\ndiffusion frameworks and Bayesian inference. We introduce AnoBFN, an extension\nof BFNs for UAD, designed to: i) perform conditional image generation under\nhigh levels of spatially correlated noise, and ii) preserve subject specificity\nby incorporating a recursive feedback from the input image throughout the\ngenerative process. We evaluate AnoBFN on the challenging task of Alzheimer's\ndisease-related anomaly detection in FDG PET images. Our approach outperforms\nother state-of-the-art methods based on VAEs (beta-VAE), GANs (f-AnoGAN), and\ndiffusion models (AnoDDPM), demonstrating its effectiveness at detecting\nanomalies while reducing false positive rates.\n","authors":["Hugues Roy","Reuben Dorent","Ninon Burgos"],"pdf_url":"https://arxiv.org/pdf/2507.17486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09068v2","updated":"2025-07-23T13:06:44Z","published":"2025-07-11T23:07:04Z","title":"Infinite Video Understanding","summary":"  The rapid advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have ushered in remarkable progress in video understanding.\nHowever, a fundamental challenge persists: effectively processing and\ncomprehending video content that extends beyond minutes or hours. While recent\nefforts like Video-XL-2 have demonstrated novel architectural solutions for\nextreme efficiency, and advancements in positional encoding such as HoPE and\nVideoRoPE++ aim to improve spatio-temporal understanding over extensive\ncontexts, current state-of-the-art models still encounter significant\ncomputational and memory constraints when faced with the sheer volume of visual\ntokens from lengthy sequences. Furthermore, maintaining temporal coherence,\ntracking complex events, and preserving fine-grained details over extended\nperiods remain formidable hurdles, despite progress in agentic reasoning\nsystems like Deep Video Discovery. This position paper posits that a logical,\nalbeit ambitious, next frontier for multimedia research is Infinite Video\nUnderstanding -- the capability for models to continuously process, understand,\nand reason about video data of arbitrary, potentially never-ending duration. We\nargue that framing Infinite Video Understanding as a blue-sky research\nobjective provides a vital north star for the multimedia, and the wider AI,\nresearch communities, driving innovation in areas such as streaming\narchitectures, persistent memory mechanisms, hierarchical and adaptive\nrepresentations, event-centric reasoning, and novel evaluation paradigms.\nDrawing inspiration from recent work on long/ultra-long video understanding and\nseveral closely related fields, we outline the core challenges and key research\ndirections towards achieving this transformative capability.\n","authors":["Dell Zhang","Xiangyu Chen","Jixiang Luo","Mengxi Jia","Changzhi Sun","Ruilong Ren","Jingren Liu","Hao Sun","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2507.09068v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12296v3","updated":"2025-07-23T13:04:12Z","published":"2025-01-21T17:03:06Z","title":"RALAD: Bridging the Real-to-Sim Domain Gap in Autonomous Driving with\n  Retrieval-Augmented Learning","summary":"  In the pursuit of robust autonomous driving systems, models trained on\nreal-world datasets often struggle to adapt to new environments, particularly\nwhen confronted with corner cases such as extreme weather conditions.\nCollecting these corner cases in the real world is non-trivial, which\nnecessitates the use of simulators for validation. However,the high\ncomputational cost and the domain gap in data distribution have hindered the\nseamless transition between real and simulated driving scenarios. To tackle\nthis challenge, we propose Retrieval-Augmented Learning for Autonomous Driving\n(RALAD), a novel framework designed to bridge the real-to-sim gap at a low\ncost. RALAD features three primary designs, including (1) domain adaptation via\nan enhanced Optimal Transport (OT) method that accounts for both individual and\ngrouped image distances, (2) a simple and unified framework that can be applied\nto various models, and (3) efficient fine-tuning techniques that freeze the\ncomputationally expensive layers while maintaining robustness. Experimental\nresults demonstrate that RALAD compensates for the performance degradation in\nsimulated environments while maintaining accuracy in real-world scenarios\nacross three different models. Taking Cross View as an example, the mIOU and\nmAP metrics in real-world scenarios remain stable before and after RALAD\nfine-tuning, while in simulated environments,the mIOU and mAP metrics are\nimproved by 10.30% and 12.29%, respectively. Moreover, the re-training cost of\nour approach is reduced by approximately 88.1%. Our code is available at\nhttps://github.com/JiachengZuo/RALAD.git.\n","authors":["Jiacheng Zuo","Haibo Hu","Zikang Zhou","Yufei Cui","Ziquan Liu","Jianping Wang","Nan Guan","Jin Wang","Chun Jason Xue"],"pdf_url":"https://arxiv.org/pdf/2501.12296v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17479v1","updated":"2025-07-23T13:01:19Z","published":"2025-07-23T13:01:19Z","title":"SRMambaV2: Biomimetic Attention for Sparse Point Cloud Upsampling in\n  Autonomous Driving","summary":"  Upsampling LiDAR point clouds in autonomous driving scenarios remains a\nsignificant challenge due to the inherent sparsity and complex 3D structures of\nthe data. Recent studies have attempted to address this problem by converting\nthe complex 3D spatial scenes into 2D image super-resolution tasks. However,\ndue to the sparse and blurry feature representation of range images, accurately\nreconstructing detailed and complex spatial topologies remains a major\ndifficulty. To tackle this, we propose a novel sparse point cloud upsampling\nmethod named SRMambaV2, which enhances the upsampling accuracy in long-range\nsparse regions while preserving the overall geometric reconstruction quality.\nSpecifically, inspired by human driver visual perception, we design a\nbiomimetic 2D selective scanning self-attention (2DSSA) mechanism to model the\nfeature distribution in distant sparse areas. Meanwhile, we introduce a\ndual-branch network architecture to enhance the representation of sparse\nfeatures. In addition, we introduce a progressive adaptive loss (PAL) function\nto further refine the reconstruction of fine-grained details during the\nupsampling process. Experimental results demonstrate that SRMambaV2 achieves\nsuperior performance in both qualitative and quantitative evaluations,\nhighlighting its effectiveness and practical value in automotive sparse point\ncloud upsampling tasks.\n","authors":["Chuang Chen","Xiaolin Qin","Jing Hu","Wenyi Ge"],"pdf_url":"https://arxiv.org/pdf/2507.17479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13708v2","updated":"2025-07-23T13:00:06Z","published":"2025-07-18T07:33:08Z","title":"PoemTale Diffusion: Minimising Information Loss in Poem to Image\n  Generation with Multi-Stage Prompt Refinement","summary":"  Recent advancements in text-to-image diffusion models have achieved\nremarkable success in generating realistic and diverse visual content. A\ncritical factor in this process is the model's ability to accurately interpret\ntextual prompts. However, these models often struggle with creative\nexpressions, particularly those involving complex, abstract, or highly\ndescriptive language. In this work, we introduce a novel training-free approach\ntailored to improve image generation for a unique form of creative language:\npoetic verse, which frequently features layered, abstract, and dual meanings.\nOur proposed PoemTale Diffusion approach aims to minimise the information that\nis lost during poetic text-to-image conversion by integrating a multi stage\nprompt refinement loop into Language Models to enhance the interpretability of\npoetic texts. To support this, we adapt existing state-of-the-art diffusion\nmodels by modifying their self-attention mechanisms with a consistent\nself-attention technique to generate multiple consistent images, which are then\ncollectively used to convey the poem's meaning. Moreover, to encourage research\nin the field of poetry, we introduce the P4I (PoemForImage) dataset, consisting\nof 1111 poems sourced from multiple online and offline resources. We engaged a\npanel of poetry experts for qualitative assessments. The results from both\nhuman and quantitative evaluations validate the efficacy of our method and\ncontribute a novel perspective to poem-to-image generation with enhanced\ninformation capture in the generated images.\n","authors":["Sofia Jamil","Bollampalli Areen Reddy","Raghvendra Kumar","Sriparna Saha","Koustava Goswami","K. J. Joseph"],"pdf_url":"https://arxiv.org/pdf/2507.13708v2.pdf","comment":"ECAI 2025"},{"id":"http://arxiv.org/abs/2507.17467v1","updated":"2025-07-23T12:46:51Z","published":"2025-07-23T12:46:51Z","title":"Probing Vision-Language Understanding through the Visual Entailment\n  Task: promises and pitfalls","summary":"  This study investigates the extent to which the Visual Entailment (VE) task\nserves as a reliable probe of vision-language understanding in multimodal\nlanguage models, using the LLaMA 3.2 11B Vision model as a test case. Beyond\nreporting performance metrics, we aim to interpret what these results reveal\nabout the underlying possibilities and limitations of the VE task. We conduct a\nseries of experiments across zero-shot, few-shot, and fine-tuning settings,\nexploring how factors such as prompt design, the number and order of in-context\nexamples and access to visual information might affect VE performance. To\nfurther probe the reasoning processes of the model, we used explanation-based\nevaluations. Results indicate that three-shot inference outperforms the\nzero-shot baselines. However, additional examples introduce more noise than\nthey provide benefits. Additionally, the order of the labels in the prompt is a\ncritical factor that influences the predictions. In the absence of visual\ninformation, the model has a strong tendency to hallucinate and imagine\ncontent, raising questions about the model's over-reliance on linguistic\npriors. Fine-tuning yields strong results, achieving an accuracy of 83.3% on\nthe e-SNLI-VE dataset and outperforming the state-of-the-art OFA-X model.\nAdditionally, the explanation evaluation demonstrates that the fine-tuned model\nprovides semantically meaningful explanations similar to those of humans, with\na BERTScore F1-score of 89.2%. We do, however, find comparable BERTScore\nresults in experiments with limited vision, questioning the visual grounding of\nthis task. Overall, our results highlight both the utility and limitations of\nVE as a diagnostic task for vision-language understanding and point to\ndirections for refining multimodal evaluation methods.\n","authors":["Elena Pitta","Tom Kouwenhoven","Tessa Verhoef"],"pdf_url":"https://arxiv.org/pdf/2507.17467v1.pdf","comment":"LUHME: 2nd Workshop on Language Understanding in the Human-Machine\n  Era"},{"id":"http://arxiv.org/abs/2507.17462v1","updated":"2025-07-23T12:41:11Z","published":"2025-07-23T12:41:11Z","title":"ERMV: Editing 4D Robotic Multi-view images to enhance embodied agents","summary":"  Robot imitation learning relies on 4D multi-view sequential images. However,\nthe high cost of data collection and the scarcity of high-quality data severely\nconstrain the generalization and application of embodied intelligence policies\nlike Vision-Language-Action (VLA) models. Data augmentation is a powerful\nstrategy to overcome data scarcity, but methods for editing 4D multi-view\nsequential images for manipulation tasks are currently lacking. Thus, we\npropose ERMV (Editing Robotic Multi-View 4D data), a novel data augmentation\nframework that efficiently edits an entire multi-view sequence based on\nsingle-frame editing and robot state conditions. This task presents three core\nchallenges: (1) maintaining geometric and appearance consistency across dynamic\nviews and long time horizons; (2) expanding the working window with low\ncomputational costs; and (3) ensuring the semantic integrity of critical\nobjects like the robot arm. ERMV addresses these challenges through a series of\ninnovations. First, to ensure spatio-temporal consistency in motion blur, we\nintroduce a novel Epipolar Motion-Aware Attention (EMA-Attn) mechanism that\nlearns pixel shift caused by movement before applying geometric constraints.\nSecond, to maximize the editing working window, ERMV pioneers a Sparse\nSpatio-Temporal (STT) module, which decouples the temporal and spatial views\nand remodels a single-frame multi-view problem through sparse sampling of the\nviews to reduce computational demands. Third, to alleviate error accumulation,\nwe incorporate a feedback intervention Mechanism, which uses a Multimodal Large\nLanguage Model (MLLM) to check editing inconsistencies and request targeted\nexpert guidance only when necessary. Extensive experiments demonstrate that\nERMV-augmented data significantly boosts the robustness and generalization of\nVLA models in both simulated and real-world environments.\n","authors":["Chang Nie","Guangming Wang","Zhe Lie","Hesheng Wang"],"pdf_url":"https://arxiv.org/pdf/2507.17462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06777v8","updated":"2025-07-23T12:32:35Z","published":"2024-06-10T20:25:18Z","title":"MolX: Enhancing Large Language Models for Molecular Understanding With A\n  Multi-Modal Extension","summary":"  Large Language Models (LLMs) with their strong task-handling capabilities\nhave shown remarkable advancements across a spectrum of fields, moving beyond\nnatural language understanding. However, their proficiency within the chemistry\ndomain remains restricted, especially in solving molecule-related tasks. This\nchallenge is attributed to their inherent limitations in comprehending\nmolecules using only common textual representations, i.e. SMILES strings. In\nthis study, we seek to enhance the ability of LLMs to comprehend molecules by\nequipping them with a multi-modal external module, termed MolX. Instead of\ndirectly using SMILES strings to represent a molecule, we utilize specific\nencoders to extract fine-grained features from both SMILES string and 2D\nmolecular graph representations for feeding into an LLM. A hand-crafted\nmolecular fingerprint is incorporated to leverage its embedded domain\nknowledge. To establish an alignment between MolX and the LLM's textual input\nspace, the model in which the LLM is frozen, is pre-trained with a strategy\nincluding a diverse set of tasks. Experimental evaluations show that our\nproposed method outperforms baselines across 4 downstream molecule-related\ntasks ranging from molecule-to-text translation to retrosynthesis, with and\nwithout fine-tuning the LLM, while only introducing a small number of trainable\nparameters--0.53\\% and 0.82\\%, respectively.\n","authors":["Khiem Le","Zhichun Guo","Kaiwen Dong","Xiaobao Huang","Bozhao Nan","Roshni Iyer","Xiangliang Zhang","Olaf Wiest","Wei Wang","Ting Hua","Nitesh V. Chawla"],"pdf_url":"https://arxiv.org/pdf/2406.06777v8.pdf","comment":"MLoG-GenAI@KDD '25 (Oral)"},{"id":"http://arxiv.org/abs/2504.19991v2","updated":"2025-07-23T12:31:13Z","published":"2025-04-28T17:09:10Z","title":"Mapping of Weed Management Methods in Orchards using Sentinel-2 and\n  PlanetScope Data","summary":"  Effective weed management is crucial for improving agricultural productivity,\nas weeds compete with crops for vital resources like nutrients and water.\nAccurate maps of weed management methods are essential for policymakers to\nassess farmer practices, evaluate impacts on vegetation health, biodiversity,\nand climate, as well as ensure compliance with policies and subsidies. However,\nmonitoring weed management methods is challenging as they commonly rely on\nground-based field surveys, which are often costly, time-consuming and subject\nto delays. In order to tackle this problem, we leverage earth observation data\nand Machine Learning (ML). Specifically, we developed separate ML models using\nSentinel-2 and PlanetScope satellite time series data, respectively, to\nclassify four distinct weed management methods (Mowing, Tillage,\nChemical-spraying, and No practice) in orchards. The findings demonstrate the\npotential of ML-driven remote sensing to enhance the efficiency and accuracy of\nweed management mapping in orchards.\n","authors":["Ioannis Kontogiorgakis","Iason Tsardanidis","Dimitrios Bormpoudakis","Ilias Tsoumas","Dimitra A. Loka","Christos Noulas","Alexandros Tsitouras","Charalampos Kontoes"],"pdf_url":"https://arxiv.org/pdf/2504.19991v2.pdf","comment":"Accepted for 2025 IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS 2025)"},{"id":"http://arxiv.org/abs/2507.17456v1","updated":"2025-07-23T12:30:19Z","published":"2025-07-23T12:30:19Z","title":"Dynamic Scoring with Enhanced Semantics for Training-Free Human-Object\n  Interaction Detection","summary":"  Human-Object Interaction (HOI) detection aims to identify humans and objects\nwithin images and interpret their interactions. Existing HOI methods rely\nheavily on large datasets with manual annotations to learn interactions from\nvisual cues. These annotations are labor-intensive to create, prone to\ninconsistency, and limit scalability to new domains and rare interactions. We\nargue that recent advances in Vision-Language Models (VLMs) offer untapped\npotential, particularly in enhancing interaction representation. While prior\nwork has injected such potential and even proposed training-free methods, there\nremain key gaps. Consequently, we propose a novel training-free HOI detection\nframework for Dynamic Scoring with enhanced semantics (DYSCO) that effectively\nutilizes textual and visual interaction representations within a multimodal\nregistry, enabling robust and nuanced interaction understanding. This registry\nincorporates a small set of visual cues and uses innovative interaction\nsignatures to improve the semantic alignment of verbs, facilitating effective\ngeneralization to rare interactions. Additionally, we propose a unique\nmulti-head attention mechanism that adaptively weights the contributions of the\nvisual and textual features. Experimental results demonstrate that our DYSCO\nsurpasses training-free state-of-the-art models and is competitive with\ntraining-based approaches, particularly excelling in rare interactions. Code is\navailable at https://github.com/francescotonini/dysco.\n","authors":["Francesco Tonini","Lorenzo Vaquero","Alessandro Conti","Cigdem Beyan","Elisa Ricci"],"pdf_url":"https://arxiv.org/pdf/2507.17456v1.pdf","comment":"Accepted to ACM Multimedia 2025"},{"id":"http://arxiv.org/abs/2503.06894v3","updated":"2025-07-23T12:27:38Z","published":"2025-03-10T03:50:25Z","title":"A Deep Learning Approach for Augmenting Perceptional Understanding of\n  Histopathology Images","summary":"  In Recent Years, Digital Technologies Have Made Significant Strides In\nAugmenting-Human-Health, Cognition, And Perception, Particularly Within The\nField Of Computational-Pathology. This Paper Presents A Novel Approach To\nEnhancing The Analysis Of Histopathology Images By Leveraging A\nMult-modal-Model That Combines Vision Transformers (Vit) With Gpt-2 For Image\nCaptioning. The Model Is Fine-Tuned On The Specialized Arch-Dataset, Which\nIncludes Dense Image Captions Derived From Clinical And Academic Resources, To\nCapture The Complexities Of Pathology Images Such As Tissue Morphologies,\nStaining Variations, And Pathological Conditions. By Generating Accurate,\nContextually Captions, The Model Augments The Cognitive Capabilities Of\nHealthcare Professionals, Enabling More Efficient Disease Classification,\nSegmentation, And Detection. The Model Enhances The Perception Of Subtle\nPathological Features In Images That Might Otherwise Go Unnoticed, Thereby\nImproving Diagnostic Accuracy. Our Approach Demonstrates The Potential For\nDigital Technologies To Augment Human Cognitive Abilities In Medical Image\nAnalysis, Providing Steps Toward More Personalized And Accurate Healthcare\nOutcomes.\n","authors":["Xiaoqian Hu"],"pdf_url":"https://arxiv.org/pdf/2503.06894v3.pdf","comment":"Accepted by International Conference on Semantic & Natural Language\n  Processing (SNLP 2025)"},{"id":"http://arxiv.org/abs/2507.17455v1","updated":"2025-07-23T12:23:03Z","published":"2025-07-23T12:23:03Z","title":"VLM-Guided Visual Place Recognition for Planet-Scale Geo-Localization","summary":"  Geo-localization from a single image at planet scale (essentially an advanced\nor extreme version of the kidnapped robot problem) is a fundamental and\nchallenging task in applications such as navigation, autonomous driving and\ndisaster response due to the vast diversity of locations, environmental\nconditions, and scene variations. Traditional retrieval-based methods for\ngeo-localization struggle with scalability and perceptual aliasing, while\nclassification-based approaches lack generalization and require extensive\ntraining data. Recent advances in vision-language models (VLMs) offer a\npromising alternative by leveraging contextual understanding and reasoning.\nHowever, while VLMs achieve high accuracy, they are often prone to\nhallucinations and lack interpretability, making them unreliable as standalone\nsolutions. In this work, we propose a novel hybrid geo-localization framework\nthat combines the strengths of VLMs with retrieval-based visual place\nrecognition (VPR) methods. Our approach first leverages a VLM to generate a\nprior, effectively guiding and constraining the retrieval search space. We then\nemploy a retrieval step, followed by a re-ranking mechanism that selects the\nmost geographically plausible matches based on feature similarity and proximity\nto the initially estimated coordinates. We evaluate our approach on multiple\ngeo-localization benchmarks and show that it consistently outperforms prior\nstate-of-the-art methods, particularly at street (up to 4.51%) and city level\n(up to 13.52%). Our results demonstrate that VLM-generated geographic priors in\ncombination with VPR lead to scalable, robust, and accurate geo-localization\nsystems.\n","authors":["Sania Waheed","Na Min An","Michael Milford","Sarvapali D. Ramchurn","Shoaib Ehsan"],"pdf_url":"https://arxiv.org/pdf/2507.17455v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.19166v2","updated":"2025-07-23T12:14:57Z","published":"2025-05-25T14:32:24Z","title":"JEDI: The Force of Jensen-Shannon Divergence in Disentangling Diffusion\n  Models","summary":"  We introduce JEDI, a test-time adaptation method that enhances subject\nseparation and compositional alignment in diffusion models without requiring\nretraining or external supervision. JEDI operates by minimizing semantic\nentanglement in attention maps using a novel Jensen-Shannon divergence based\nobjective. To improve efficiency, we leverage adversarial optimization,\nreducing the number of updating steps required. JEDI is model-agnostic and\napplicable to architectures such as Stable Diffusion 1.5 and 3.5, consistently\nimproving prompt alignment and disentanglement in complex scenes. Additionally,\nJEDI provides a lightweight, CLIP-free disentanglement score derived from\ninternal attention distributions, offering a principled benchmark for\ncompositional alignment under test-time conditions. Code and results are\navailable at https://ericbill21.github.io/JEDI/.\n","authors":["Eric Tillmann Bill","Enis Simsar","Thomas Hofmann"],"pdf_url":"https://arxiv.org/pdf/2505.19166v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17436v1","updated":"2025-07-23T11:51:06Z","published":"2025-07-23T11:51:06Z","title":"Dynamic-DINO: Fine-Grained Mixture of Experts Tuning for Real-time\n  Open-Vocabulary Object Detection","summary":"  The Mixture of Experts (MoE) architecture has excelled in Large\nVision-Language Models (LVLMs), yet its potential in real-time open-vocabulary\nobject detectors, which also leverage large-scale vision-language datasets but\nsmaller models, remains unexplored. This work investigates this domain,\nrevealing intriguing insights. In the shallow layers, experts tend to cooperate\nwith diverse peers to expand the search space. While in the deeper layers,\nfixed collaborative structures emerge, where each expert maintains 2-3 fixed\npartners and distinct expert combinations are specialized in processing\nspecific patterns. Concretely, we propose Dynamic-DINO, which extends Grounding\nDINO 1.5 Edge from a dense model to a dynamic inference framework via an\nefficient MoE-Tuning strategy. Additionally, we design a granularity\ndecomposition mechanism to decompose the Feed-Forward Network (FFN) of base\nmodel into multiple smaller expert networks, expanding the subnet search space.\nTo prevent performance degradation at the start of fine-tuning, we further\npropose a pre-trained weight allocation strategy for the experts, coupled with\na specific router initialization. During inference, only the input-relevant\nexperts are activated to form a compact subnet. Experiments show that,\npretrained with merely 1.56M open-source data, Dynamic-DINO outperforms\nGrounding DINO 1.5 Edge, pretrained on the private Grounding20M dataset.\n","authors":["Yehao Lu","Minghe Weng","Zekang Xiao","Rui Jiang","Wei Su","Guangcong Zheng","Ping Lu","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2507.17436v1.pdf","comment":"Accepted by ICCV 2025"},{"id":"http://arxiv.org/abs/2503.17032v2","updated":"2025-07-23T11:42:41Z","published":"2025-03-21T10:40:37Z","title":"TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented\n  Reality via 3D Gaussian Splatting","summary":"  Realistic 3D full-body talking avatars hold great potential in AR, with\napplications ranging from e-commerce live streaming to holographic\ncommunication. Despite advances in 3D Gaussian Splatting (3DGS) for lifelike\navatar creation, existing methods struggle with fine-grained control of facial\nexpressions and body movements in full-body talking tasks. Additionally, they\noften lack sufficient details and cannot run in real-time on mobile devices. We\npresent TaoAvatar, a high-fidelity, lightweight, 3DGS-based full-body talking\navatar driven by various signals. Our approach starts by creating a\npersonalized clothed human parametric template that binds Gaussians to\nrepresent appearances. We then pre-train a StyleUnet-based network to handle\ncomplex pose-dependent non-rigid deformation, which can capture high-frequency\nappearance details but is too resource-intensive for mobile devices. To\novercome this, we \"bake\" the non-rigid deformations into a lightweight\nMLP-based network using a distillation technique and develop blend shapes to\ncompensate for details. Extensive experiments show that TaoAvatar achieves\nstate-of-the-art rendering quality while running in real-time across various\ndevices, maintaining 90 FPS on high-definition stereo devices such as the Apple\nVision Pro.\n","authors":["Jianchuan Chen","Jingchuan Hu","Gaige Wang","Zhonghua Jiang","Tiansong Zhou","Zhiwen Chen","Chengfei Lv"],"pdf_url":"https://arxiv.org/pdf/2503.17032v2.pdf","comment":"Accepted by CVPR 2025 (Highlight), project page:\n  https://PixelAI-Team.github.io/TaoAvatar"},{"id":"http://arxiv.org/abs/2503.23956v3","updated":"2025-07-23T11:42:03Z","published":"2025-03-31T11:13:18Z","title":"AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference","summary":"  Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.\n","authors":["Kai Huang","Hao Zou","Bochen Wang","Ye Xi","Zhen Xie","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2503.23956v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17420v1","updated":"2025-07-23T11:23:02Z","published":"2025-07-23T11:23:02Z","title":"CAPRI-CT: Causal Analysis and Predictive Reasoning for Image Quality\n  Optimization in Computed Tomography","summary":"  In computed tomography (CT), achieving high image quality while minimizing\nradiation exposure remains a key clinical challenge. This paper presents\nCAPRI-CT, a novel causal-aware deep learning framework for Causal Analysis and\nPredictive Reasoning for Image Quality Optimization in CT imaging. CAPRI-CT\nintegrates image data with acquisition metadata (such as tube voltage, tube\ncurrent, and contrast agent types) to model the underlying causal relationships\nthat influence image quality. An ensemble of Variational Autoencoders (VAEs) is\nemployed to extract meaningful features and generate causal representations\nfrom observational data, including CT images and associated imaging parameters.\nThese input features are fused to predict the Signal-to-Noise Ratio (SNR) and\nsupport counterfactual inference, enabling what-if simulations, such as changes\nin contrast agents (types and concentrations) or scan parameters. CAPRI-CT is\ntrained and validated using an ensemble learning approach, achieving strong\npredictive performance. By facilitating both prediction and interpretability,\nCAPRI-CT provides actionable insights that could help radiologists and\ntechnicians design more efficient CT protocols without repeated physical scans.\nThe source code and dataset are publicly available at\nhttps://github.com/SnehaGeorge22/capri-ct.\n","authors":["Sneha George Gnanakalavathy","Hairil Abdul Razak","Robert Meertens","Jonathan E. Fieldsend","Xujiong Ye","Mohammed M. Abdelsamea"],"pdf_url":"https://arxiv.org/pdf/2507.17420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17412v1","updated":"2025-07-23T11:12:52Z","published":"2025-07-23T11:12:52Z","title":"Content-based 3D Image Retrieval and a ColBERT-inspired Re-ranking for\n  Tumor Flagging and Staging","summary":"  The increasing volume of medical images poses challenges for radiologists in\nretrieving relevant cases. Content-based image retrieval (CBIR) systems offer\npotential for efficient access to similar cases, yet lack standardized\nevaluation and comprehensive studies. Building on prior studies for tumor\ncharacterization via CBIR, this study advances CBIR research for volumetric\nmedical images through three key contributions: (1) a framework eliminating\nreliance on pre-segmented data and organ-specific datasets, aligning with large\nand unstructured image archiving systems, i.e. PACS in clinical practice; (2)\nintroduction of C-MIR, a novel volumetric re-ranking method adapting ColBERT's\ncontextualized late interaction mechanism for 3D medical imaging; (3)\ncomprehensive evaluation across four tumor sites using three feature extractors\nand three database configurations. Our evaluations highlight the significant\nadvantages of C-MIR. We demonstrate the successful adaptation of the late\ninteraction principle to volumetric medical images, enabling effective\ncontext-aware re-ranking. A key finding is C-MIR's ability to effectively\nlocalize the region of interest, eliminating the need for pre-segmentation of\ndatasets and offering a computationally efficient alternative to systems\nrelying on expensive data enrichment steps. C-MIR demonstrates promising\nimprovements in tumor flagging, achieving improved performance, particularly\nfor colon and lung tumors (p<0.05). C-MIR also shows potential for improving\ntumor staging, warranting further exploration of its capabilities. Ultimately,\nour work seeks to bridge the gap between advanced retrieval techniques and\ntheir practical applications in healthcare, paving the way for improved\ndiagnostic processes.\n","authors":["Farnaz Khun Jush","Steffen Vogler","Matthias Lenga"],"pdf_url":"https://arxiv.org/pdf/2507.17412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17406v1","updated":"2025-07-23T11:04:30Z","published":"2025-07-23T11:04:30Z","title":"Physics-based Human Pose Estimation from a Single Moving RGB Camera","summary":"  Most monocular and physics-based human pose tracking methods, while achieving\nstate-of-the-art results, suffer from artifacts when the scene does not have a\nstrictly flat ground plane or when the camera is moving. Moreover, these\nmethods are often evaluated on in-the-wild real world videos without\nground-truth data or on synthetic datasets, which fail to model the real world\nlight transport, camera motion, and pose-induced appearance and geometry\nchanges. To tackle these two problems, we introduce MoviCam, the first\nnon-synthetic dataset containing ground-truth camera trajectories of a\ndynamically moving monocular RGB camera, scene geometry, and 3D human motion\nwith human-scene contact labels. Additionally, we propose PhysDynPose, a\nphysics-based method that incorporates scene geometry and physical constraints\nfor more accurate human motion tracking in case of camera motion and non-flat\nscenes. More precisely, we use a state-of-the-art kinematics estimator to\nobtain the human pose and a robust SLAM method to capture the dynamic camera\ntrajectory, enabling the recovery of the human pose in the world frame. We then\nrefine the kinematic pose estimate using our scene-aware physics optimizer.\nFrom our new benchmark, we found that even state-of-the-art methods struggle\nwith this inherently challenging setting, i.e. a moving camera and non-planar\nenvironments, while our method robustly estimates both human and camera poses\nin world coordinates.\n","authors":["Ayce Idil Aytekin","Chuqiao Li","Diogo Luvizon","Rishabh Dabral","Martin Oswald","Marc Habermann","Christian Theobalt"],"pdf_url":"https://arxiv.org/pdf/2507.17406v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17402v1","updated":"2025-07-23T10:59:46Z","published":"2025-07-23T10:59:46Z","title":"HLFormer: Enhancing Partially Relevant Video Retrieval with Hyperbolic\n  Learning","summary":"  Partially Relevant Video Retrieval (PRVR) addresses the critical challenge of\nmatching untrimmed videos with text queries describing only partial content.\nExisting methods suffer from geometric distortion in Euclidean space that\nsometimes misrepresents the intrinsic hierarchical structure of videos and\noverlooks certain hierarchical semantics, ultimately leading to suboptimal\ntemporal modeling. To address this issue, we propose the first hyperbolic\nmodeling framework for PRVR, namely HLFormer, which leverages hyperbolic space\nlearning to compensate for the suboptimal hierarchical modeling capabilities of\nEuclidean space. Specifically, HLFormer integrates the Lorentz Attention Block\nand Euclidean Attention Block to encode video embeddings in hybrid spaces,\nusing the Mean-Guided Adaptive Interaction Module to dynamically fuse features.\nAdditionally, we introduce a Partial Order Preservation Loss to enforce \"text <\nvideo\" hierarchy through Lorentzian cone constraints. This approach further\nenhances cross-modal matching by reinforcing partial relevance between video\ncontent and text queries. Extensive experiments show that HLFormer outperforms\nstate-of-the-art methods. Code is released at\nhttps://github.com/lijun2005/ICCV25-HLFormer.\n","authors":["Li Jun","Wang Jinpeng","Tan Chaolei","Lian Niu","Chen Long","Zhang Min","Wang Yaowei","Xia Shu-Tao","Chen Bin"],"pdf_url":"https://arxiv.org/pdf/2507.17402v1.pdf","comment":"Accepted by ICCV'25. 13 pages, 6 figures, 4 tables"},{"id":"http://arxiv.org/abs/2507.01955v2","updated":"2025-07-23T10:52:38Z","published":"2025-07-02T17:59:07Z","title":"How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation\n  Models on Standard Computer Vision Tasks","summary":"  Multimodal foundation models, such as GPT-4o, have recently made remarkable\nprogress, but it is not clear where exactly these models stand in terms of\nunderstanding vision. In this paper, we benchmark the performance of popular\nmultimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0\nFlash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision\ntasks (semantic segmentation, object detection, image classification, depth and\nsurface normal prediction) using established datasets (e.g., COCO, ImageNet and\nits variants, etc).\n  The main challenges to performing this are: 1) most models are trained to\noutput text and cannot natively express versatile domains, such as segments or\n3D geometry, and 2) many leading models are proprietary and accessible only at\nan API level, i.e., there is no weight access to adapt them. We address these\nchallenges by translating standard vision tasks into equivalent text-promptable\nand API-compatible tasks via prompt chaining to create a standardized\nbenchmarking framework.\n  We observe that 1) the models are not close to the state-of-the-art\nspecialist models at any task. However, 2) they are respectable generalists;\nthis is remarkable as they are presumably trained on primarily image-text-based\ntasks. 3) They perform semantic tasks notably better than geometric ones. 4)\nWhile the prompt-chaining techniques affect performance, better models exhibit\nless sensitivity to prompt variations. 5) GPT-4o performs the best among\nnon-reasoning models, securing the top position in 4 out of 6 tasks, 6)\nreasoning models, e.g. o3, show improvements in geometric tasks, and 7) a\npreliminary analysis of models with native image generation, like the latest\nGPT-4o, shows they exhibit quirks like hallucinations and spatial\nmisalignments.\n","authors":["Rahul Ramachandran","Ali Garjani","Roman Bachmann","Andrei Atanov","Oğuzhan Fatih Kar","Amir Zamir"],"pdf_url":"https://arxiv.org/pdf/2507.01955v2.pdf","comment":"Project page at https://fm-vision-evals.epfl.ch/"},{"id":"http://arxiv.org/abs/2507.17394v1","updated":"2025-07-23T10:41:46Z","published":"2025-07-23T10:41:46Z","title":"HiProbe-VAD: Video Anomaly Detection via Hidden States Probing in\n  Tuning-Free Multimodal LLMs","summary":"  Video Anomaly Detection (VAD) aims to identify and locate deviations from\nnormal patterns in video sequences. Traditional methods often struggle with\nsubstantial computational demands and a reliance on extensive labeled datasets,\nthereby restricting their practical applicability. To address these\nconstraints, we propose HiProbe-VAD, a novel framework that leverages\npre-trained Multimodal Large Language Models (MLLMs) for VAD without requiring\nfine-tuning. In this paper, we discover that the intermediate hidden states of\nMLLMs contain information-rich representations, exhibiting higher sensitivity\nand linear separability for anomalies compared to the output layer. To\ncapitalize on this, we propose a Dynamic Layer Saliency Probing (DLSP)\nmechanism that intelligently identifies and extracts the most informative\nhidden states from the optimal intermediate layer during the MLLMs reasoning.\nThen a lightweight anomaly scorer and temporal localization module efficiently\ndetects anomalies using these extracted hidden states and finally generate\nexplanations. Experiments on the UCF-Crime and XD-Violence datasets demonstrate\nthat HiProbe-VAD outperforms existing training-free and most traditional\napproaches. Furthermore, our framework exhibits remarkable cross-model\ngeneralization capabilities in different MLLMs without any tuning, unlocking\nthe potential of pre-trained MLLMs for video anomaly detection and paving the\nway for more practical and scalable solutions.\n","authors":["Zhaolin Cai","Fan Li","Ziwei Zheng","Yanjun Qin"],"pdf_url":"https://arxiv.org/pdf/2507.17394v1.pdf","comment":"Accepted by ACM MM 2025"},{"id":"http://arxiv.org/abs/2507.17388v1","updated":"2025-07-23T10:32:20Z","published":"2025-07-23T10:32:20Z","title":"EndoGen: Conditional Autoregressive Endoscopic Video Generation","summary":"  Endoscopic video generation is crucial for advancing medical imaging and\nenhancing diagnostic capabilities. However, prior efforts in this field have\neither focused on static images, lacking the dynamic context required for\npractical applications, or have relied on unconditional generation that fails\nto provide meaningful references for clinicians. Therefore, in this paper, we\npropose the first conditional endoscopic video generation framework, namely\nEndoGen. Specifically, we build an autoregressive model with a tailored\nSpatiotemporal Grid-Frame Patterning (SGP) strategy. It reformulates the\nlearning of generating multiple frames as a grid-based image generation\npattern, which effectively capitalizes the inherent global dependency modeling\ncapabilities of autoregressive architectures. Furthermore, we propose a\nSemantic-Aware Token Masking (SAT) mechanism, which enhances the model's\nability to produce rich and diverse content by selectively focusing on\nsemantically meaningful regions during the generation process. Through\nextensive experiments, we demonstrate the effectiveness of our framework in\ngenerating high-quality, conditionally guided endoscopic content, and improves\nthe performance of downstream task of polyp segmentation. Code released at\nhttps://www.github.com/CUHK-AIM-Group/EndoGen.\n","authors":["Xinyu Liu","Hengyu Liu","Cheng Wang","Tianming Liu","Yixuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2507.17388v1.pdf","comment":"MICCAI 2025"},{"id":"http://arxiv.org/abs/2506.08735v3","updated":"2025-07-23T10:31:35Z","published":"2025-06-10T12:31:05Z","title":"InceptionMamba: An Efficient Hybrid Network with Large Band Convolution\n  and Bottleneck Mamba","summary":"  Within the family of convolutional neural networks, InceptionNeXt has shown\nexcellent competitiveness in image classification and a number of downstream\ntasks. Built on parallel one-dimensional strip convolutions, however, it\nsuffers from limited ability of capturing spatial dependencies along different\ndimensions and fails to fully explore spatial modeling in local neighborhood.\nBesides, inherent locality constraints of convolution operations are\ndetrimental to effective global context modeling. To overcome these\nlimitations, we propose a novel backbone architecture termed InceptionMamba in\nthis study. More specifically, the traditional one-dimensional strip\nconvolutions are replaced by orthogonal band convolutions in our InceptionMamba\nto achieve cohesive spatial modeling. Furthermore, global contextual modeling\ncan be achieved via a bottleneck Mamba module, facilitating enhanced\ncross-channel information fusion and enlarged receptive field. Extensive\nevaluations on classification and various downstream tasks demonstrate that the\nproposed InceptionMamba achieves state-of-the-art performance with superior\nparameter and computational efficiency. The source code will be available at\nhttps://github.com/Wake1021/InceptionMamba.\n","authors":["Yuhang Wang","Jun Li","Zhijian Wu","Jifeng Shen","Jianhua Xu","Wankou Yang"],"pdf_url":"https://arxiv.org/pdf/2506.08735v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17377v1","updated":"2025-07-23T10:20:52Z","published":"2025-07-23T10:20:52Z","title":"A Conditional Probability Framework for Compositional Zero-shot Learning","summary":"  Compositional Zero-Shot Learning (CZSL) aims to recognize unseen combinations\nof known objects and attributes by leveraging knowledge from previously seen\ncompositions. Traditional approaches primarily focus on disentangling\nattributes and objects, treating them as independent entities during learning.\nHowever, this assumption overlooks the semantic constraints and contextual\ndependencies inside a composition. For example, certain attributes naturally\npair with specific objects (e.g., \"striped\" applies to \"zebra\" or \"shirts\" but\nnot \"sky\" or \"water\"), while the same attribute can manifest differently\ndepending on context (e.g., \"young\" in \"young tree\" vs. \"young dog\"). Thus,\ncapturing attribute-object interdependence remains a fundamental yet\nlong-ignored challenge in CZSL. In this paper, we adopt a Conditional\nProbability Framework (CPF) to explicitly model attribute-object dependencies.\nWe decompose the probability of a composition into two components: the\nlikelihood of an object and the conditional likelihood of its attribute. To\nenhance object feature learning, we incorporate textual descriptors to\nhighlight semantically relevant image regions. These enhanced object features\nthen guide attribute learning through a cross-attention mechanism, ensuring\nbetter contextual alignment. By jointly optimizing object likelihood and\nconditional attribute likelihood, our method effectively captures compositional\ndependencies and generalizes well to unseen compositions. Extensive experiments\non multiple CZSL benchmarks demonstrate the superiority of our approach. Code\nis available at here.\n","authors":["Peng Wu","Qiuxia Lai","Hao Fang","Guo-Sen Xie","Yilong Yin","Xiankai Lu","Wenguan Wang"],"pdf_url":"https://arxiv.org/pdf/2507.17377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17373v1","updated":"2025-07-23T10:16:25Z","published":"2025-07-23T10:16:25Z","title":"SFUOD: Source-Free Unknown Object Detection","summary":"  Source-free object detection adapts a detector pre-trained on a source domain\nto an unlabeled target domain without requiring access to labeled source data.\nWhile this setting is practical as it eliminates the need for the source\ndataset during domain adaptation, it operates under the restrictive assumption\nthat only pre-defined objects from the source domain exist in the target\ndomain. This closed-set setting prevents the detector from detecting undefined\nobjects. To ease this assumption, we propose Source-Free Unknown Object\nDetection (SFUOD), a novel scenario which enables the detector to not only\nrecognize known objects but also detect undefined objects as unknown objects.\nTo this end, we propose CollaPAUL (Collaborative tuning and Principal\nAxis-based Unknown Labeling), a novel framework for SFUOD. Collaborative tuning\nenhances knowledge adaptation by integrating target-dependent knowledge from\nthe auxiliary encoder with source-dependent knowledge from the pre-trained\ndetector through a cross-domain attention mechanism. Additionally, principal\naxes-based unknown labeling assigns pseudo-labels to unknown objects by\nestimating objectness via principal axes projection and confidence scores from\nmodel predictions. The proposed CollaPAUL achieves state-of-the-art\nperformances on SFUOD benchmarks, and extensive experiments validate its\neffectiveness.\n","authors":["Keon-Hee Park","Seun-An Choe","Gyeong-Moon Park"],"pdf_url":"https://arxiv.org/pdf/2507.17373v1.pdf","comment":"This paper has been accepted by ICCV 2025"},{"id":"http://arxiv.org/abs/2507.17367v1","updated":"2025-07-23T10:04:25Z","published":"2025-07-23T10:04:25Z","title":"Exploring Spatial Diversity for Region-based Active Learning","summary":"  State-of-the-art methods for semantic segmentation are based on deep neural\nnetworks trained on large-scale labeled datasets. Acquiring such datasets would\nincur large annotation costs, especially for dense pixel-level prediction tasks\nlike semantic segmentation. We consider region-based active learning as a\nstrategy to reduce annotation costs while maintaining high performance. In this\nsetting, batches of informative image regions instead of entire images are\nselected for labeling. Importantly, we propose that enforcing local spatial\ndiversity is beneficial for active learning in this case, and to incorporate\nspatial diversity along with the traditional active selection criterion, e.g.,\ndata sample uncertainty, in a unified optimization framework for region-based\nactive learning. We apply this framework to the Cityscapes and PASCAL VOC\ndatasets and demonstrate that the inclusion of spatial diversity effectively\nimproves the performance of uncertainty-based and feature diversity-based\nactive learning methods. Our framework achieves $95\\%$ performance of fully\nsupervised methods with only $5-9\\%$ of the labeled pixels, outperforming all\nstate-of-the-art region-based active learning methods for semantic\nsegmentation.\n","authors":["Lile Cai","Xun Xu","Lining Zhang","Chuan-Sheng Foo"],"pdf_url":"https://arxiv.org/pdf/2507.17367v1.pdf","comment":"published in IEEE Transactions on Image Processing, 2021"},{"id":"http://arxiv.org/abs/2410.22365v2","updated":"2025-07-23T09:53:03Z","published":"2024-10-28T09:00:28Z","title":"Vascular Segmentation of Functional Ultrasound Images using Deep\n  Learning","summary":"  Segmentation of medical images is a fundamental task with numerous\napplications. While MRI, CT, and PET modalities have significantly benefited\nfrom deep learning segmentation techniques, more recent modalities, like\nfunctional ultrasound (fUS), have seen limited progress. fUS is a non invasive\nimaging method that measures changes in cerebral blood volume (CBV) with high\nspatio-temporal resolution. However, distinguishing arterioles from venules in\nfUS is challenging due to opposing blood flow directions within the same pixel.\nUltrasound localization microscopy (ULM) can enhance resolution by tracking\nmicrobubble contrast agents but is invasive, and lacks dynamic CBV\nquantification. In this paper, we introduce the first deep learning-based\nsegmentation tool for fUS images, capable of differentiating signals from\ndifferent vascular compartments, based on ULM automatic annotation and enabling\ndynamic CBV quantification. We evaluate various UNet architectures on fUS\nimages of rat brains, achieving competitive segmentation performance, with 90%\naccuracy, a 71% F1 score, and an IoU of 0.59, using only 100 temporal frames\nfrom a fUS stack. These results are comparable to those from tubular structure\nsegmentation in other imaging modalities. Additionally, models trained on\nresting-state data generalize well to images captured during visual\nstimulation, highlighting robustness. This work offers a non-invasive,\ncost-effective alternative to ULM, enhancing fUS data interpretation and\nimproving understanding of vessel function. Our pipeline shows high linear\ncorrelation coefficients between signals from predicted and actual compartments\nin both cortical and deeper regions, showcasing its ability to accurately\ncapture blood flow dynamics.\n","authors":["Hana Sebia","Thomas Guyet","Mickaël Pereira","Marco Valdebenito","Hugues Berry","Benjamin Vidal"],"pdf_url":"https://arxiv.org/pdf/2410.22365v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.19996v2","updated":"2025-07-23T09:50:45Z","published":"2025-04-28T17:16:40Z","title":"Monitoring digestate application on agricultural crops using Sentinel-2\n  Satellite imagery","summary":"  The widespread use of Exogenous Organic Matter in agriculture necessitates\nmonitoring to assess its effects on soil and crop health. This study evaluates\noptical Sentinel-2 satellite imagery for detecting digestate application, a\npractice that enhances soil fertility but poses environmental risks like\nmicroplastic contamination and nitrogen losses. In the first instance,\nSentinel-2 satellite image time series (SITS) analysis of specific indices\n(EOMI, NDVI, EVI) was used to characterize EOM's spectral behavior after\napplication on the soils of four different crop types in Thessaly, Greece.\nFurthermore, Machine Learning (ML) models (namely Random Forest, k-NN, Gradient\nBoosting and a Feed-Forward Neural Network), were used to investigate digestate\npresence detection, achieving F1-scores up to 0.85. The findings highlight the\npotential of combining remote sensing and ML for scalable and cost-effective\nmonitoring of EOM applications, supporting precision agriculture and\nsustainability.\n","authors":["Andreas Kalogeras","Dimitrios Bormpoudakis","Iason Tsardanidis","Dimitra A. Loka","Charalampos Kontoes"],"pdf_url":"https://arxiv.org/pdf/2504.19996v2.pdf","comment":"Accepted for 2025 IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS 2025)"},{"id":"http://arxiv.org/abs/2507.17359v1","updated":"2025-07-23T09:44:11Z","published":"2025-07-23T09:44:11Z","title":"Exploring Active Learning for Semiconductor Defect Segmentation","summary":"  The development of X-Ray microscopy (XRM) technology has enabled\nnon-destructive inspection of semiconductor structures for defect\nidentification. Deep learning is widely used as the state-of-the-art approach\nto perform visual analysis tasks. However, deep learning based models require\nlarge amount of annotated data to train. This can be time-consuming and\nexpensive to obtain especially for dense prediction tasks like semantic\nsegmentation. In this work, we explore active learning (AL) as a potential\nsolution to alleviate the annotation burden. We identify two unique challenges\nwhen applying AL on semiconductor XRM scans: large domain shift and severe\nclass-imbalance. To address these challenges, we propose to perform contrastive\npretraining on the unlabelled data to obtain the initialization weights for\neach AL cycle, and a rareness-aware acquisition function that favors the\nselection of samples containing rare classes. We evaluate our method on a\nsemiconductor dataset that is compiled from XRM scans of high bandwidth memory\nstructures composed of logic and memory dies, and demonstrate that our method\nachieves state-of-the-art performance.\n","authors":["Lile Cai","Ramanpreet Singh Pahwa","Xun Xu","Jie Wang","Richard Chang","Lining Zhang","Chuan-Sheng Foo"],"pdf_url":"https://arxiv.org/pdf/2507.17359v1.pdf","comment":"accepted to ICIP 2022"},{"id":"http://arxiv.org/abs/2507.17351v1","updated":"2025-07-23T09:34:09Z","published":"2025-07-23T09:34:09Z","title":"Exploring Active Learning for Label-Efficient Training of Semantic\n  Neural Radiance Field","summary":"  Neural Radiance Field (NeRF) models are implicit neural scene representation\nmethods that offer unprecedented capabilities in novel view synthesis.\nSemantically-aware NeRFs not only capture the shape and radiance of a scene,\nbut also encode semantic information of the scene. The training of\nsemantically-aware NeRFs typically requires pixel-level class labels, which can\nbe prohibitively expensive to collect. In this work, we explore active learning\nas a potential solution to alleviate the annotation burden. We investigate\nvarious design choices for active learning of semantically-aware NeRF,\nincluding selection granularity and selection strategies. We further propose a\nnovel active learning strategy that takes into account 3D geometric constraints\nin sample selection. Our experiments demonstrate that active learning can\neffectively reduce the annotation cost of training semantically-aware NeRF,\nachieving more than 2X reduction in annotation cost compared to random\nsampling.\n","authors":["Yuzhe Zhu","Lile Cai","Kangkang Lu","Fayao Liu","Xulei Yang"],"pdf_url":"https://arxiv.org/pdf/2507.17351v1.pdf","comment":"Accepted to ICME 2025"},{"id":"http://arxiv.org/abs/2507.01630v2","updated":"2025-07-23T09:22:32Z","published":"2025-07-02T11:59:32Z","title":"Prompt Guidance and Human Proximal Perception for HOT Prediction with\n  Regional Joint Loss","summary":"  The task of Human-Object conTact (HOT) detection involves identifying the\nspecific areas of the human body that are touching objects. Nevertheless,\ncurrent models are restricted to just one type of image, often leading to too\nmuch segmentation in areas with little interaction, and struggling to maintain\ncategory consistency within specific regions. To tackle this issue, a HOT\nframework, termed \\textbf{P3HOT}, is proposed, which blends \\textbf{P}rompt\nguidance and human \\textbf{P}roximal \\textbf{P}erception. To begin with, we\nutilize a semantic-driven prompt mechanism to direct the network's attention\ntowards the relevant regions based on the correlation between image and text.\nThen a human proximal perception mechanism is employed to dynamically perceive\nkey depth range around the human, using learnable parameters to effectively\neliminate regions where interactions are not expected. Calculating depth\nresolves the uncertainty of the overlap between humans and objects in a 2D\nperspective, providing a quasi-3D viewpoint. Moreover, a Regional Joint Loss\n(RJLoss) has been created as a new loss to inhibit abnormal categories in the\nsame area. A new evaluation metric called ``AD-Acc.'' is introduced to address\nthe shortcomings of existing methods in addressing negative samples.\nComprehensive experimental results demonstrate that our approach achieves\nstate-of-the-art performance in four metrics across two benchmark datasets.\nSpecifically, our model achieves an improvement of \\textbf{0.7}$\\uparrow$,\n\\textbf{2.0}$\\uparrow$, \\textbf{1.6}$\\uparrow$, and \\textbf{11.0}$\\uparrow$ in\nSC-Acc., mIoU, wIoU, and AD-Acc. metrics, respectively, on the HOT-Annotated\ndataset. The sources code are available at\nhttps://github.com/YuxiaoWang-AI/P3HOT.\n","authors":["Yuxiao Wang","Yu Lei","Zhenao Wei","Weiying Xue","Xinyu Jiang","Nan Zhuang","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2507.01630v2.pdf","comment":"Accepted by ICCV 2025"},{"id":"http://arxiv.org/abs/2507.17343v1","updated":"2025-07-23T09:12:25Z","published":"2025-07-23T09:12:25Z","title":"Principled Multimodal Representation Learning","summary":"  Multimodal representation learning seeks to create a unified representation\nspace by integrating diverse data modalities to improve multimodal\nunderstanding. Traditional methods often depend on pairwise contrastive\nlearning, which relies on a predefined anchor modality, restricting alignment\nacross all modalities. Recent advances have investigated the simultaneous\nalignment of multiple modalities, yet several challenges remain, such as\nlimitations imposed by fixed anchor points and instability arising from\noptimizing the product of singular values. To address the challenges, in this\npaper, we propose Principled Multimodal Representation Learning (PMRL), a novel\nframework that achieves simultaneous alignment of multiple modalities without\nanchor dependency in a more stable manner. Specifically, grounded in the\ntheoretical insight that full alignment corresponds to a rank-1 Gram matrix,\nPMRL optimizes the dominant singular value of the representation matrix to\nalign modalities along a shared leading direction. We propose a softmax-based\nloss function that treats singular values as logits to prioritize the largest\nsingular value. Besides, instance-wise contrastive regularization on the\nleading eigenvectors maintains inter-instance separability and prevents\nrepresentation collapse. Extensive experiments across diverse tasks demonstrate\nPMRL's superiority compared to baseline methods. The source code will be\npublicly available.\n","authors":["Xiaohao Liu","Xiaobo Xia","See-Kiong Ng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2507.17343v1.pdf","comment":"32 pages, 9 figures, 10 tables"},{"id":"http://arxiv.org/abs/2507.17342v1","updated":"2025-07-23T09:11:25Z","published":"2025-07-23T09:11:25Z","title":"DeMo++: Motion Decoupling for Autonomous Driving","summary":"  Motion forecasting and planning are tasked with estimating the trajectories\nof traffic agents and the ego vehicle, respectively, to ensure the safety and\nefficiency of autonomous driving systems in dynamically changing environments.\nState-of-the-art methods typically adopt a one-query-one-trajectory paradigm,\nwhere each query corresponds to a unique trajectory for predicting multi-mode\ntrajectories. While this paradigm can produce diverse motion intentions, it\noften falls short in modeling the intricate spatiotemporal evolution of\ntrajectories, which can lead to collisions or suboptimal outcomes. To overcome\nthis limitation, we propose DeMo++, a framework that decouples motion\nestimation into two distinct components: holistic motion intentions to capture\nthe diverse potential directions of movement, and fine spatiotemporal states to\ntrack the agent's dynamic progress within the scene and enable a\nself-refinement capability. Further, we introduce a cross-scene trajectory\ninteraction mechanism to explore the relationships between motions in adjacent\nscenes. This allows DeMo++ to comprehensively model both the diversity of\nmotion intentions and the spatiotemporal evolution of each trajectory. To\neffectively implement this framework, we developed a hybrid model combining\nAttention and Mamba. This architecture leverages the strengths of both\nmechanisms for efficient scene information aggregation and precise trajectory\nstate sequence modeling. Extensive experiments demonstrate that DeMo++ achieves\nstate-of-the-art performance across various benchmarks, including motion\nforecasting (Argoverse 2 and nuScenes), motion planning (nuPlan), and\nend-to-end planning (NAVSIM).\n","authors":["Bozhou Zhang","Nan Song","Xiatian Zhu","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.17342v1.pdf","comment":"Journal extension of NeurIPS 2024. arXiv admin note: substantial text\n  overlap with arXiv:2410.05982"},{"id":"http://arxiv.org/abs/2507.17335v1","updated":"2025-07-23T09:03:01Z","published":"2025-07-23T09:03:01Z","title":"TransLPRNet: Lite Vision-Language Network for Single/Dual-line Chinese\n  License Plate Recognition","summary":"  License plate recognition in open environments is widely applicable across\nvarious domains; however, the diversity of license plate types and imaging\nconditions presents significant challenges. To address the limitations\nencountered by CNN and CRNN-based approaches in license plate recognition, this\npaper proposes a unified solution that integrates a lightweight visual encoder\nwith a text decoder, within a pre-training framework tailored for single and\ndouble-line Chinese license plates. To mitigate the scarcity of double-line\nlicense plate datasets, we constructed a single/double-line license plate\ndataset by synthesizing images, applying texture mapping onto real scenes, and\nblending them with authentic license plate images. Furthermore, to enhance the\nsystem's recognition accuracy, we introduce a perspective correction network\n(PTN) that employs license plate corner coordinate regression as an implicit\nvariable, supervised by license plate view classification information. This\nnetwork offers improved stability, interpretability, and low annotation costs.\nThe proposed algorithm achieves an average recognition accuracy of 99.34% on\nthe corrected CCPD test set under coarse localization disturbance. When\nevaluated under fine localization disturbance, the accuracy further improves to\n99.58%. On the double-line license plate test set, it achieves an average\nrecognition accuracy of 98.70%, with processing speeds reaching up to 167\nframes per second, indicating strong practical applicability.\n","authors":["Guangzhu Xu","Zhi Ke","Pengcheng Zuo","Bangjun Lei"],"pdf_url":"https://arxiv.org/pdf/2507.17335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17334v1","updated":"2025-07-23T09:02:09Z","published":"2025-07-23T09:02:09Z","title":"Temporal Point-Supervised Signal Reconstruction: A Human-Annotation-Free\n  Framework for Weak Moving Target Detection","summary":"  In low-altitude surveillance and early warning systems, detecting weak moving\ntargets remains a significant challenge due to low signal energy, small spatial\nextent, and complex background clutter. Existing methods struggle with\nextracting robust features and suffer from the lack of reliable annotations. To\naddress these limitations, we propose a novel Temporal Point-Supervised (TPS)\nframework that enables high-performance detection of weak targets without any\nmanual annotations.Instead of conventional frame-based detection, our framework\nreformulates the task as a pixel-wise temporal signal modeling problem, where\nweak targets manifest as short-duration pulse-like responses. A Temporal Signal\nReconstruction Network (TSRNet) is developed under the TPS paradigm to\nreconstruct these transient signals.TSRNet adopts an encoder-decoder\narchitecture and integrates a Dynamic Multi-Scale Attention (DMSAttention)\nmodule to enhance its sensitivity to diverse temporal patterns. Additionally, a\ngraph-based trajectory mining strategy is employed to suppress false alarms and\nensure temporal consistency.Extensive experiments on a purpose-built low-SNR\ndataset demonstrate that our framework outperforms state-of-the-art methods\nwhile requiring no human annotations. It achieves strong detection performance\nand operates at over 1000 FPS, underscoring its potential for real-time\ndeployment in practical scenarios.\n","authors":["Weihua Gao","Chunxu Ren","Wenlong Niu","Xiaodong Peng"],"pdf_url":"https://arxiv.org/pdf/2507.17334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17327v1","updated":"2025-07-23T08:52:48Z","published":"2025-07-23T08:52:48Z","title":"CartoonAlive: Towards Expressive Live2D Modeling from Single Portraits","summary":"  With the rapid advancement of large foundation models, AIGC, cloud rendering,\nand real-time motion capture technologies, digital humans are now capable of\nachieving synchronized facial expressions and body movements, engaging in\nintelligent dialogues driven by natural language, and enabling the fast\ncreation of personalized avatars. While current mainstream approaches to\ndigital humans primarily focus on 3D models and 2D video-based representations,\ninteractive 2D cartoon-style digital humans have received relatively less\nattention. Compared to 3D digital humans that require complex modeling and high\nrendering costs, and 2D video-based solutions that lack flexibility and\nreal-time interactivity, 2D cartoon-style Live2D models offer a more efficient\nand expressive alternative. By simulating 3D-like motion through layered\nsegmentation without the need for traditional 3D modeling, Live2D enables\ndynamic and real-time manipulation. In this technical report, we present\nCartoonAlive, an innovative method for generating high-quality Live2D digital\nhumans from a single input portrait image. CartoonAlive leverages the shape\nbasis concept commonly used in 3D face modeling to construct facial blendshapes\nsuitable for Live2D. It then infers the corresponding blendshape weights based\non facial keypoints detected from the input image. This approach allows for the\nrapid generation of a highly expressive and visually accurate Live2D model that\nclosely resembles the input portrait, within less than half a minute. Our work\nprovides a practical and scalable solution for creating interactive 2D cartoon\ncharacters, opening new possibilities in digital content creation and virtual\ncharacter animation. The project homepage is\nhttps://human3daigc.github.io/CartoonAlive_webpage/.\n","authors":["Chao He","Jianqiang Ren","Jianjing Xiang","Xiejie Shen"],"pdf_url":"https://arxiv.org/pdf/2507.17327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15986v2","updated":"2025-07-23T08:42:32Z","published":"2025-03-20T09:36:31Z","title":"SpiLiFormer: Enhancing Spiking Transformers with Lateral Inhibition","summary":"  Spiking Neural Networks (SNNs) based on Transformers have garnered\nsignificant attention due to their superior performance and high energy\nefficiency. However, the spiking attention modules of most existing\nTransformer-based SNNs are adapted from those of analog Transformers, failing\nto fully address the issue of over-allocating attention to irrelevant contexts.\nTo fix this fundamental yet overlooked issue, we propose a Lateral\nInhibition-inspired Spiking Transformer (SpiLiFormer). It emulates the brain's\nlateral inhibition mechanism, guiding the model to enhance attention to\nrelevant tokens while suppressing attention to irrelevant ones. Our model\nachieves state-of-the-art (SOTA) performance across multiple datasets,\nincluding CIFAR-10 (+0.45%), CIFAR-100 (+0.48%), CIFAR10-DVS (+2.70%),\nN-Caltech101 (+1.94%), and ImageNet-1K (+1.6%). Notably, on the ImageNet-1K\ndataset, SpiLiFormer (69.9M parameters, 4 time steps, 384 resolution)\noutperforms E-SpikeFormer (173.0M parameters, 8 time steps, 384 resolution), a\nSOTA spiking Transformer, by 0.46% using only 39% of the parameters and half\nthe time steps. The code and model checkpoints are publicly available at\nhttps://github.com/KirinZheng/SpiLiFormer.\n","authors":["Zeqi Zheng","Yanchen Huang","Yingchao Yu","Zizheng Zhu","Junfeng Tang","Zhaofei Yu","Yaochu Jin"],"pdf_url":"https://arxiv.org/pdf/2503.15986v2.pdf","comment":"Accepted by ICCV 2025. The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2507.17312v1","updated":"2025-07-23T08:29:26Z","published":"2025-07-23T08:29:26Z","title":"CasP: Improving Semi-Dense Feature Matching Pipeline Leveraging Cascaded\n  Correspondence Priors for Guidance","summary":"  Semi-dense feature matching methods have shown strong performance in\nchallenging scenarios. However, the existing pipeline relies on a global search\nacross the entire feature map to establish coarse matches, limiting further\nimprovements in accuracy and efficiency. Motivated by this limitation, we\npropose a novel pipeline, CasP, which leverages cascaded correspondence priors\nfor guidance. Specifically, the matching stage is decomposed into two\nprogressive phases, bridged by a region-based selective cross-attention\nmechanism designed to enhance feature discriminability. In the second phase,\none-to-one matches are determined by restricting the search range to the\none-to-many prior areas identified in the first phase. Additionally, this\npipeline benefits from incorporating high-level features, which helps reduce\nthe computational costs of low-level feature extraction. The acceleration gains\nof CasP increase with higher resolution, and our lite model achieves a speedup\nof $\\sim2.2\\times$ at a resolution of 1152 compared to the most efficient\nmethod, ELoFTR. Furthermore, extensive experiments demonstrate its superiority\nin geometric estimation, particularly with impressive cross-domain\ngeneralization. These advantages highlight its potential for latency-sensitive\nand high-robustness applications, such as SLAM and UAV systems. Code is\navailable at https://github.com/pq-chen/CasP.\n","authors":["Peiqi Chen","Lei Yu","Yi Wan","Yingying Pei","Xinyi Liu","Yongxiang Yao","Yingying Zhang","Lixiang Ru","Liheng Zhong","Jingdong Chen","Ming Yang","Yongjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.17312v1.pdf","comment":"Accepted to ICCV 2025"},{"id":"http://arxiv.org/abs/2507.14456v3","updated":"2025-07-23T08:26:59Z","published":"2025-07-19T03:04:28Z","title":"GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for\n  End-to-End Autonomous Driving","summary":"  End-to-end autonomous driving requires adaptive and robust handling of\ncomplex and diverse traffic environments. However, prevalent single-mode\nplanning methods attempt to learn an overall policy while struggling to acquire\ndiversified driving skills to handle diverse scenarios. Therefore, this paper\nproposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework\nfeaturing a Global Expert, a Scene-Adaptive Experts Group, and equipped with a\nDual-aware Router. Specifically, the Global Expert is trained on the overall\ndataset, possessing robust performance. The Scene-Adaptive Experts are trained\non corresponding scene subsets, achieving adaptive performance. The Dual-aware\nRouter simultaneously considers scenario-level features and routing uncertainty\nto dynamically activate expert modules. Through the effective coupling of the\nGlobal Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,\nGEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS\noutperforms existing methods in the Bench2Drive closed-loop benchmark and\nachieves state-of-the-art performance in Driving Score and Success Rate, even\nwith only monocular vision input. Furthermore, ablation studies demonstrate\nsignificant improvements over the original single-expert baseline: 7.67% in\nDriving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The\ncode will be available at https://github.com/newbrains1/GEMINUS.\n","authors":["Chi Wan","Yixin Cui","Jiatong Du","Shuo Yang","Yulong Bai","Yanjun Huang"],"pdf_url":"https://arxiv.org/pdf/2507.14456v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.15680v3","updated":"2025-07-23T08:20:34Z","published":"2025-07-21T14:44:46Z","title":"Visual-Language Model Knowledge Distillation Method for Image Quality\n  Assessment","summary":"  Image Quality Assessment (IQA) is a core task in computer vision. Multimodal\nmethods based on vision-language models, such as CLIP, have demonstrated\nexceptional generalization capabilities in IQA tasks. To address the issues of\nexcessive parameter burden and insufficient ability to identify local distorted\nfeatures in CLIP for IQA, this study proposes a visual-language model knowledge\ndistillation method aimed at guiding the training of models with architectural\nadvantages using CLIP's IQA knowledge. First, quality-graded prompt templates\nwere designed to guide CLIP to output quality scores. Then, CLIP is fine-tuned\nto enhance its capabilities in IQA tasks. Finally, a modality-adaptive\nknowledge distillation strategy is proposed to achieve guidance from the CLIP\nteacher model to the student model. Our experiments were conducted on multiple\nIQA datasets, and the results show that the proposed method significantly\nreduces model complexity while outperforming existing IQA methods,\ndemonstrating strong potential for practical deployment.\n","authors":["Yongkang Hou","Jiarun Song"],"pdf_url":"https://arxiv.org/pdf/2507.15680v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06438v2","updated":"2025-07-23T08:18:50Z","published":"2025-01-11T04:56:07Z","title":"Qffusion: Controllable Portrait Video Editing via Quadrant-Grid\n  Attention Learning","summary":"  This paper presents Qffusion, a dual-frame-guided framework for portrait\nvideo editing. Specifically, we consider a design principle of ``animation for\nediting'', and train Qffusion as a general animation framework from two still\nreference images while we can use it for portrait video editing easily by\napplying modified start and end frames as references during inference.\nLeveraging the powerful generative power of Stable Diffusion, we propose a\nQuadrant-grid Arrangement (QGA) scheme for latent re-arrangement, which\narranges the latent codes of two reference images and that of four facial\nconditions into a four-grid fashion, separately. Then, we fuse features of\nthese two modalities and use self-attention for both appearance and temporal\nlearning, where representations at different times are jointly modeled under\nQGA. Our Qffusion can achieve stable video editing without additional networks\nor complex training stages, where only the input format of Stable Diffusion is\nmodified. Further, we propose a Quadrant-grid Propagation (QGP) inference\nstrategy, which enjoys a unique advantage on stable arbitrary-length video\ngeneration by processing reference and condition frames recursively. Through\nextensive experiments, Qffusion consistently outperforms state-of-the-art\ntechniques on portrait video editing. Project page:\nhttps://qffusion.github.io/page/.\n","authors":["Maomao Li","Lijian Lin","Yunfei Liu","Ye Zhu","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2501.06438v2.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2409.01534v2","updated":"2025-07-23T08:14:06Z","published":"2024-09-03T02:08:47Z","title":"Cross-domain Multi-step Thinking: Zero-shot Fine-grained Traffic Sign\n  Recognition in the Wild","summary":"  In this study, we propose Cross-domain Multi-step Thinking (CdMT) to improve\nzero-shot fine-grained traffic sign recognition (TSR) performance in the wild.\nZero-shot fine-grained TSR in the wild is challenging due to the cross-domain\nproblem between clean template traffic signs and real-world counterparts, and\nexisting approaches particularly struggle with cross-country TSR scenarios,\nwhere traffic signs typically differ between countries. The proposed CdMT\nframework tackles these challenges by leveraging the multi-step reasoning\ncapabilities of large multimodal models (LMMs). We introduce context,\ncharacteristic, and differential descriptions to design multiple thinking\nprocesses for LMMs. Context descriptions, which are enhanced by center\ncoordinate prompt optimization, enable the precise localization of target\ntraffic signs in complex road images and filter irrelevant responses via novel\nprior traffic sign hypotheses. Characteristic descriptions, which are derived\nfrom in-context learning with template traffic signs, bridge cross-domain gaps\nand enhance fine-grained TSR. Differential descriptions refine the multimodal\nreasoning ability of LMMs by distinguishing subtle differences among similar\nsigns. CdMT is independent of training data and requires only simple and\nuniform instructions, enabling it to achieve cross-country TSR. We conducted\nextensive experiments on three benchmark datasets and two real-world datasets\nfrom different countries. The proposed CdMT framework achieved superior\nperformance compared with other state-of-the-art methods on all five datasets,\nwith recognition accuracies of 0.93, 0.89, 0.97, 0.89, and 0.85 on the GTSRB,\nBTSD, TT-100K, Sapporo, and Yokohama datasets, respectively.\n","authors":["Yaozong Gan","Guang Li","Ren Togo","Keisuke Maeda","Takahiro Ogawa","Miki Haseyama"],"pdf_url":"https://arxiv.org/pdf/2409.01534v2.pdf","comment":"Published by Knowledge-Based Systems"},{"id":"http://arxiv.org/abs/2507.17304v1","updated":"2025-07-23T08:10:27Z","published":"2025-07-23T08:10:27Z","title":"Learning-based Stage Verification System in Manual Assembly Scenarios","summary":"  In the context of Industry 4.0, effective monitoring of multiple targets and\nstates during assembly processes is crucial, particularly when constrained to\nusing only visual sensors. Traditional methods often rely on either multiple\nsensor types or complex hardware setups to achieve high accuracy in monitoring,\nwhich can be cost-prohibitive and difficult to implement in dynamic industrial\nenvironments. This study presents a novel approach that leverages multiple\nmachine learning models to achieve precise monitoring under the limitation of\nusing a minimal number of visual sensors. By integrating state information from\nidentical timestamps, our method detects and confirms the current stage of the\nassembly process with an average accuracy exceeding 92%. Furthermore, our\napproach surpasses conventional methods by offering enhanced error detection\nand visuali-zation capabilities, providing real-time, actionable guidance to\noperators. This not only improves the accuracy and efficiency of assembly\nmonitoring but also re-duces dependency on expensive hardware solutions, making\nit a more practical choice for modern industrial applications.\n","authors":["Xingjian Zhang","Yutong Duan","Zaishu Chen"],"pdf_url":"https://arxiv.org/pdf/2507.17304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17303v1","updated":"2025-07-23T08:09:42Z","published":"2025-07-23T08:09:42Z","title":"A Versatile Pathology Co-pilot via Reasoning Enhanced Multimodal Large\n  Language Model","summary":"  Multimodal large language models (MLLMs) have emerged as powerful tools for\ncomputational pathology, offering unprecedented opportunities to integrate\npathological images with language context for comprehensive diagnostic\nanalysis. These models hold particular promise for automating complex tasks\nthat traditionally require expert interpretation of pathologists. However,\ncurrent MLLM approaches in pathology demonstrate significantly constrained\nreasoning capabilities, primarily due to their reliance on expensive\nchain-of-thought annotations. Additionally, existing methods remain limited to\nsimplex application of visual question answering (VQA) at region-of-interest\n(ROI) level, failing to address the full spectrum of diagnostic needs such as\nROI classification, detection, segmentation, whole-slide-image (WSI)\nclassification and VQA in clinical practice. In this study, we present\nSmartPath-R1, a versatile MLLM capable of simultaneously addressing both\nROI-level and WSI-level tasks while demonstrating robust pathological reasoning\ncapability. Our framework combines scale-dependent supervised fine-tuning and\ntask-aware reinforcement fine-tuning, which circumvents the requirement for\nchain-of-thought supervision by leveraging the intrinsic knowledge within MLLM.\nFurthermore, SmartPath-R1 integrates multiscale and multitask analysis through\na mixture-of-experts mechanism, enabling dynamic processing for diverse tasks.\nWe curate a large-scale dataset comprising 2.3M ROI samples and 188K WSI\nsamples for training and evaluation. Extensive experiments across 72 tasks\nvalidate the effectiveness and superiority of the proposed approach. This work\nrepresents a significant step toward developing versatile, reasoning-enhanced\nAI systems for precision pathology.\n","authors":["Zhe Xu","Ziyi Liu","Junlin Hou","Jiabo Ma","Cheng Jin","Yihui Wang","Zhixuan Chen","Zhengyu Zhang","Zhengrui Guo","Fengtao Zhou","Yingxue Xu","Xi Wang","Ronald Cheong Kin Chan","Li Liang","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2507.17303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16619v3","updated":"2025-07-23T08:05:06Z","published":"2024-11-25T17:58:43Z","title":"Human-Activity AGV Quality Assessment: A Benchmark Dataset and an\n  Objective Evaluation Metric","summary":"  AI-driven video generation techniques have made significant progress in\nrecent years. However, AI-generated videos (AGVs) involving human activities\noften exhibit substantial visual and semantic distortions, hindering the\npractical application of video generation technologies in real-world scenarios.\nTo address this challenge, we conduct a pioneering study on human activity AGV\nquality assessment, focusing on visual quality evaluation and the\nidentification of semantic distortions. First, we construct the AI-Generated\nHuman activity Video Quality Assessment (Human-AGVQA) dataset, consisting of\n6,000 AGVs derived from 15 popular text-to-video (T2V) models using 400 text\nprompts that describe diverse human activities. We conduct a subjective study\nto evaluate the human appearance quality, action continuity quality, and\noverall video quality of AGVs, and identify semantic issues of human body\nparts. Based on Human-AGVQA, we benchmark the performance of T2V models and\nanalyze their strengths and weaknesses in generating different categories of\nhuman activities. Second, we develop an objective evaluation metric, named\nAI-Generated Human activity Video Quality metric (GHVQ), to automatically\nanalyze the quality of human activity AGVs. GHVQ systematically extracts\nhuman-focused quality features, AI-generated content-aware quality features,\nand temporal continuity features, making it a comprehensive and explainable\nquality metric for human activity AGVs. The extensive experimental results show\nthat GHVQ outperforms existing quality metrics on the Human-AGVQA dataset by a\nlarge margin, demonstrating its efficacy in assessing the quality of human\nactivity AGVs. The Human-AGVQA dataset and GHVQ metric will be released at\nhttps://github.com/zczhang-sjtu/GHVQ.git.\n","authors":["Zhichao Zhang","Wei Sun","Xinyue Li","Yunhao Li","Qihang Ge","Jun Jia","Zicheng Zhang","Zhongpeng Ji","Fengyu Sun","Shangling Jui","Xiongkuo Min","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2411.16619v3.pdf","comment":"Accepted by ACMMM 2025"},{"id":"http://arxiv.org/abs/2507.17296v1","updated":"2025-07-23T07:57:35Z","published":"2025-07-23T07:57:35Z","title":"PointLAMA: Latent Attention meets Mamba for Efficient Point Cloud\n  Pretraining","summary":"  Mamba has recently gained widespread attention as a backbone model for point\ncloud modeling, leveraging a state-space architecture that enables efficient\nglobal sequence modeling with linear complexity. However, its lack of local\ninductive bias limits its capacity to capture fine-grained geometric structures\nin 3D data. To address this limitation, we propose \\textbf{PointLAMA}, a point\ncloud pretraining framework that combines task-aware point cloud serialization,\na hybrid encoder with integrated Latent Attention and Mamba blocks, and a\nconditional diffusion mechanism built upon the Mamba backbone. Specifically,\nthe task-aware point cloud serialization employs Hilbert/Trans-Hilbert\nspace-filling curves and axis-wise sorting to structurally align point tokens\nfor classification and segmentation tasks, respectively. Our lightweight Latent\nAttention block features a Point-wise Multi-head Latent Attention (PMLA)\nmodule, which is specifically designed to align with the Mamba architecture by\nleveraging the shared latent space characteristics of PMLA and Mamba. This\nenables enhanced local context modeling while preserving overall efficiency. To\nfurther enhance representation learning, we incorporate a conditional diffusion\nmechanism during pretraining, which denoises perturbed feature sequences\nwithout relying on explicit point-wise reconstruction. Experimental results\ndemonstrate that PointLAMA achieves competitive performance on multiple\nbenchmark datasets with minimal parameter count and FLOPs, validating its\neffectiveness for efficient point cloud pretraining.\n","authors":["Xuanyu Lin","Xiaona Zeng","Xianwei Zheng","Xutao Li"],"pdf_url":"https://arxiv.org/pdf/2507.17296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04134v2","updated":"2025-07-23T07:56:36Z","published":"2025-06-04T16:26:49Z","title":"UniCUE: Unified Recognition and Generation Framework for Chinese Cued\n  Speech Video-to-Speech Generation","summary":"  Cued Speech (CS) enhances lipreading through hand coding, providing precise\nspeech perception support for the hearing-impaired. CS Video-to-Speech\ngeneration (CSV2S) task aims to convert the CS visual expressions (CS videos)\nof hearing-impaired individuals into comprehensible speech signals. Direct\ngeneration of speech from CS video (called single CSV2S) yields poor\nperformance due to insufficient CS data. Current research mostly focuses on CS\nRecognition (CSR), which convert video content into linguistic text. Based on\nthis, one straightforward way of CSV2S is to combine CSR with a Text-to-Speech\nsystem. This combined architecture relies on text as an intermediate medium for\nstepwise cross-modal alignment, which may lead to error propagation and\ntemporal misalignment between speech and video dynamics. To address these\nchallenges, we propose a novel approach that directly generates speech from CS\nvideos without relying on intermediate text. Building upon this, we propose\nUniCUE, the first unified framework for CSV2S, whose core innovation lies in\nthe integration of the CSR task that provides fine-grained visual-semantic\ninformation to facilitate speech generation from CS videos. More precisely, (1)\na novel fine-grained semantic alignment pool to ensure precise mapping between\nvisual features and speech contents; (2) a VisioPhonetic adapter to bridge\ncross-task representations, ensuring seamless compatibility between two\ndistinct tasks (i.e., CSV2S and CSR); (3) a pose-aware visual processor is\nintroduced to enhance fine-grained spatiotemporal correlations between lip and\nhand movements in CS video. Experiments on our new established Chinese CS\ndataset show that our UniCUE achieves state-of-the-art performance across\nvarious metrics.\n","authors":["Jinting Wang","Shan Yang","Li Liu"],"pdf_url":"https://arxiv.org/pdf/2506.04134v2.pdf","comment":"10 pages, 10 figures"},{"id":"http://arxiv.org/abs/2507.07578v2","updated":"2025-07-23T07:51:06Z","published":"2025-07-10T09:28:54Z","title":"Diffusion-Guided Knowledge Distillation for Weakly-Supervised Low-Light\n  Semantic Segmentation","summary":"  Weakly-supervised semantic segmentation aims to assign category labels to\neach pixel using weak annotations, significantly reducing manual annotation\ncosts. Although existing methods have achieved remarkable progress in well-lit\nscenarios, their performance significantly degrades in low-light environments\ndue to two fundamental limitations: severe image quality degradation (e.g., low\ncontrast, noise, and color distortion) and the inherent constraints of weak\nsupervision. These factors collectively lead to unreliable class activation\nmaps and semantically ambiguous pseudo-labels, ultimately compromising the\nmodel's ability to learn discriminative feature representations. To address\nthese problems, we propose Diffusion-Guided Knowledge Distillation for\nWeakly-Supervised Low-light Semantic Segmentation (DGKD-WLSS), a novel\nframework that synergistically combines Diffusion-Guided Knowledge Distillation\n(DGKD) with Depth-Guided Feature Fusion (DGF2). DGKD aligns normal-light and\nlow-light features via diffusion-based denoising and knowledge distillation,\nwhile DGF2 integrates depth maps as illumination-invariant geometric priors to\nenhance structural feature learning. Extensive experiments demonstrate the\neffectiveness of DGKD-WLSS, which achieves state-of-the-art performance in\nweakly supervised semantic segmentation tasks under low-light conditions. The\nsource codes have been released at:https://github.com/ChunyanWang1/DGKD-WLSS.\n","authors":["Chunyan Wang","Dong Zhang","Jinhui Tang"],"pdf_url":"https://arxiv.org/pdf/2507.07578v2.pdf","comment":"Accepted by ACM Multimedia"},{"id":"http://arxiv.org/abs/2507.17281v1","updated":"2025-07-23T07:37:39Z","published":"2025-07-23T07:37:39Z","title":"Fully Automated SAM for Single-source Domain Generalization in Medical\n  Image Segmentation","summary":"  Although SAM-based single-source domain generalization models for medical\nimage segmentation can mitigate the impact of domain shift on the model in\ncross-domain scenarios, these models still face two major challenges. First,\nthe segmentation of SAM is highly dependent on domain-specific expert-annotated\nprompts, which prevents SAM from achieving fully automated medical image\nsegmentation and therefore limits its application in clinical settings. Second,\nproviding poor prompts (such as bounding boxes that are too small or too large)\nto the SAM prompt encoder can mislead SAM into generating incorrect mask\nresults. Therefore, we propose the FA-SAM, a single-source domain\ngeneralization framework for medical image segmentation that achieves fully\nautomated SAM. FA-SAM introduces two key innovations: an Auto-prompted\nGeneration Model (AGM) branch equipped with a Shallow Feature Uncertainty\nModeling (SUFM) module, and an Image-Prompt Embedding Fusion (IPEF) module\nintegrated into the SAM mask decoder. Specifically, AGM models the uncertainty\ndistribution of shallow features through the SUFM module to generate bounding\nbox prompts for the target domain, enabling fully automated segmentation with\nSAM. The IPEF module integrates multiscale information from SAM image\nembeddings and prompt embeddings to capture global and local details of the\ntarget object, enabling SAM to mitigate the impact of poor prompts. Extensive\nexperiments on publicly available prostate and fundus vessel datasets validate\nthe effectiveness of FA-SAM and highlight its potential to address the above\nchallenges.\n","authors":["Huanli Zhuo","Leilei Ma","Haifeng Zhao","Shiwei Zhou","Dengdi Sun","Yanping Fu"],"pdf_url":"https://arxiv.org/pdf/2507.17281v1.pdf","comment":"This manuscript has been accepted for presentation at the IEEE\n  International Conference on Systems, Man, and Cybernetics (IEEE SMC 2025) and\n  is copyrighted by IEEE"},{"id":"http://arxiv.org/abs/2505.22334v2","updated":"2025-07-23T07:37:08Z","published":"2025-05-28T13:21:38Z","title":"Advancing Multimodal Reasoning via Reinforcement Learning with Cold\n  Start","summary":"  Recent advancements in large language models (LLMs) have demonstrated\nimpressive chain-of-thought reasoning capabilities, with reinforcement learning\n(RL) playing a crucial role in this progress. While \"aha moment\"\npatterns--where models exhibit self-correction through reflection--are often\nattributed to emergent properties from RL, we first demonstrate that these\npatterns exist in multimodal LLMs (MLLMs) prior to RL training but may not\nnecessarily correlate with improved reasoning performance. Building on these\ninsights, we present a comprehensive study on enhancing multimodal reasoning\nthrough a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start\nwith structured chain-of-thought reasoning patterns, followed by (2)\nreinforcement learning via GRPO to further refine these capabilities. Our\nextensive experiments show that this combined approach consistently outperforms\nboth SFT-only and RL-only methods across challenging multimodal reasoning\nbenchmarks. The resulting models achieve state-of-the-art performance among\nopen-source MLLMs at both 3B and 7B scales, with our 7B model showing\nsubstantial improvements over base models (e.g., 66.3 %$\\rightarrow$73.4 % on\nMathVista, 62.9 %$\\rightarrow$70.4 % on We-Math) and our 3B model achieving\nperformance competitive with several 7B models. Overall, this work provides\npractical guidance for building advanced multimodal reasoning models. Our code\nis available at https://github.com/waltonfuture/RL-with-Cold-Start.\n","authors":["Lai Wei","Yuting Li","Kaipeng Zheng","Chen Wang","Yue Wang","Linghe Kong","Lichao Sun","Weiran Huang"],"pdf_url":"https://arxiv.org/pdf/2505.22334v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.14811v2","updated":"2025-07-23T07:36:08Z","published":"2025-07-20T04:00:53Z","title":"SegQuant: A Semantics-Aware and Generalizable Quantization Framework for\n  Diffusion Models","summary":"  Diffusion models have demonstrated exceptional generative capabilities but\nare computationally intensive, posing significant challenges for deployment in\nresource-constrained or latency-sensitive environments. Quantization offers an\neffective means to reduce model size and computational cost, with post-training\nquantization (PTQ) being particularly appealing due to its compatibility with\npre-trained models without requiring retraining or training data. However,\nexisting PTQ methods for diffusion models often rely on architecture-specific\nheuristics that limit their generalizability and hinder integration with\nindustrial deployment pipelines. To address these limitations, we propose\nSegQuant, a unified quantization framework that adaptively combines\ncomplementary techniques to enhance cross-model versatility. SegQuant consists\nof a segment-aware, graph-based quantization strategy (SegLinear) that captures\nstructural semantics and spatial heterogeneity, along with a dual-scale\nquantization scheme (DualScale) that preserves polarity-asymmetric activations,\nwhich is crucial for maintaining visual fidelity in generated outputs. SegQuant\nis broadly applicable beyond Transformer-based diffusion models, achieving\nstrong performance while ensuring seamless compatibility with mainstream\ndeployment tools.\n","authors":["Jiaji Zhang","Ruichao Sun","Hailiang Zhao","Jiaju Wu","Peng Chen","Hao Li","Yuying Liu","Xinkui Zhao","Kingsum Chow","Gang Xiong","Shuiguang Deng"],"pdf_url":"https://arxiv.org/pdf/2507.14811v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02591v3","updated":"2025-07-23T07:25:27Z","published":"2025-07-03T12:55:16Z","title":"AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video\n  Understanding","summary":"  The challenge of long video understanding lies in its high computational\ncomplexity and prohibitive memory cost, since the memory and computation\nrequired by transformer-based LLMs scale quadratically with input sequence\nlength. We propose AuroraLong to address this challenge by replacing the LLM\ncomponent in MLLMs with a linear RNN language model that handles input sequence\nof arbitrary length with constant-size hidden states. To further increase\nthroughput and efficiency, we combine visual token merge with linear RNN models\nby reordering the visual tokens by their sizes in ascending order. Despite\nhaving only 2B parameters and being trained exclusively on public data,\nAuroraLong achieves performance comparable to Transformer-based models of\nsimilar size trained on private datasets across multiple video benchmarks. This\ndemonstrates the potential of efficient, linear RNNs to democratize long video\nunderstanding by lowering its computational entry barrier. To our best\nknowledge, we are the first to use a linear RNN based LLM backbone in a\nLLaVA-like model for open-ended video understanding.\n","authors":["Weili Xu","Enxin Song","Wenhao Chai","Xuexiang Wen","Tian Ye","Gaoang Wang"],"pdf_url":"https://arxiv.org/pdf/2507.02591v3.pdf","comment":"ICCV 2025 Camera Ready"}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2401.01405v2","updated":"2025-07-23T23:44:10Z","published":"2024-01-02T19:00:17Z","title":"Quantifying the Uniqueness and Divisiveness of Presidential Discourse","summary":"  Do American presidents speak discernibly different from each other? If so, in\nwhat ways? And are these differences confined to any single medium of\ncommunication? To investigate these questions, this paper introduces a novel\nmetric of uniqueness based on large language models, develops a new lexicon for\ndivisive speech, and presents a framework for assessing the distinctive ways in\nwhich presidents speak about their political opponents. Applying these tools to\na variety of corpora of presidential speeches, we find considerable evidence\nthat Donald Trump's speech patterns diverge from those of all major party\nnominees for the presidency in recent history. Trump is significantly more\ndistinctive than his fellow Republicans, whose uniqueness values appear closer\nto those of the Democrats. Contributing to these differences is Trump's\nemployment of divisive and antagonistic language, particularly when targeting\nhis political opponents. These differences hold across a variety of measurement\nstrategies, arise on both the campaign trail and in official presidential\naddresses, and do not appear to be an artifact of secular changes in\npresidential communications.\n","authors":["Karen Zhou","Alexander A. Meitus","Milo Chase","Grace Wang","Anne Mykland","William Howell","Chenhao Tan"],"pdf_url":"https://arxiv.org/pdf/2401.01405v2.pdf","comment":"Published in PNAS Nexus:\n  https://academic.oup.com/pnasnexus/article/3/10/pgae431/7814873"},{"id":"http://arxiv.org/abs/2507.17988v1","updated":"2025-07-23T23:39:04Z","published":"2025-07-23T23:39:04Z","title":"Synthesis of timeline-based planning strategies avoiding determinization","summary":"  Qualitative timeline-based planning models domains as sets of independent,\nbut\n  interacting, components whose behaviors over time, the timelines, are\ngoverned\n  by sets of qualitative temporal constraints (ordering relations), called\n  synchronization rules.\n  Its plan-existence problem has been shown to be PSPACE-complete; in\n  particular, PSPACE-membership has been proved via reduction to the\n  nonemptiness problem for nondeterministic finite automata.\n  However, nondeterministic automata cannot be directly used to synthesize\n  planning strategies as a costly determinization step is needed.\n  In this paper, we identify a fragment of qualitative timeline-based planning\n  whose plan-existence problem can be directly mapped into the nonemptiness\n  problem of deterministic finite automata, which can then\n  synthesize strategies.\n  In addition, we identify a maximal subset of Allen's relations that fits into\n  such a deterministic fragment.\n","authors":["Dario Della Monica","Angelo Montanari","Pietro Sala"],"pdf_url":"https://arxiv.org/pdf/2507.17988v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2410.22757"},{"id":"http://arxiv.org/abs/2507.17985v1","updated":"2025-07-23T23:23:38Z","published":"2025-07-23T23:23:38Z","title":"Decoding Instructional Dialogue: Human-AI Collaborative Analysis of\n  Teacher Use of AI Tool at Scale","summary":"  The integration of large language models (LLMs) into educational tools has\nthe potential to substantially impact how teachers plan instruction, support\ndiverse learners, and engage in professional reflection. Yet little is known\nabout how educators actually use these tools in practice and how their\ninteractions with AI can be meaningfully studied at scale. This paper presents\na human-AI collaborative methodology for large-scale qualitative analysis of\nover 140,000 educator-AI messages drawn from a generative AI platform used by\nK-12 teachers. Through a four-phase coding pipeline, we combined inductive\ntheme discovery, codebook development, structured annotation, and model\nbenchmarking to examine patterns of educator engagement and evaluate the\nperformance of LLMs in qualitative coding tasks. We developed a hierarchical\ncodebook aligned with established teacher evaluation frameworks, capturing\neducators' instructional goals, contextual needs, and pedagogical strategies.\nOur findings demonstrate that LLMs, particularly Claude 3.5 Haiku, can reliably\nsupport theme identification, extend human recognition in complex scenarios,\nand outperform open-weight models in both accuracy and structural reliability.\nThe analysis also reveals substantive patterns in how educators inquire AI to\nenhance instructional practices (79.7 percent of total conversations), create\nor adapt content (76.1 percent), support assessment and feedback loop (46.9\npercent), attend to student needs for tailored instruction (43.3 percent), and\nassist other professional responsibilities (34.2 percent), highlighting\nemerging AI-related competencies that have direct implications for teacher\npreparation and professional development. This study offers a scalable,\ntransparent model for AI-augmented qualitative research and provides\nfoundational insights into the evolving role of generative AI in educational\npractice.\n","authors":["Alex Liu","Lief Esbenshade","Shawon Sarkar","Victor Tian","Zachary Zhang","Kevin He","Min Sun"],"pdf_url":"https://arxiv.org/pdf/2507.17985v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17984v1","updated":"2025-07-23T23:23:18Z","published":"2025-07-23T23:23:18Z","title":"Machine Unlearning of Traffic State Estimation and Prediction","summary":"  Data-driven traffic state estimation and prediction (TSEP) relies heavily on\ndata sources that contain sensitive information. While the abundance of data\nhas fueled significant breakthroughs, particularly in machine learning-based\nmethods, it also raises concerns regarding privacy, cybersecurity, and data\nfreshness. These issues can erode public trust in intelligent transportation\nsystems. Recently, regulations have introduced the \"right to be forgotten\",\nallowing users to request the removal of their private data from models. As\nmachine learning models can remember old data, simply removing it from back-end\ndatabases is insufficient in such systems. To address these challenges, this\nstudy introduces a novel learning paradigm for TSEP-Machine Unlearning\nTSEP-which enables a trained TSEP model to selectively forget\nprivacy-sensitive, poisoned, or outdated data. By empowering models to\n\"unlearn,\" we aim to enhance the trustworthiness and reliability of data-driven\ntraffic TSEP.\n","authors":["Xin Wang","R. Tyrrell Rockafellar"," Xuegang"," Ban"],"pdf_url":"https://arxiv.org/pdf/2507.17984v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01108v2","updated":"2025-07-23T23:13:37Z","published":"2025-02-03T06:56:40Z","title":"Pulse-PPG: An Open-Source Field-Trained PPG Foundation Model for\n  Wearable Applications Across Lab and Field Settings","summary":"  Photoplethysmography (PPG)-based foundation models are gaining traction due\nto the widespread use of PPG in biosignal monitoring and their potential to\ngeneralize across diverse health applications. In this paper, we introduce\nPulse-PPG, the first open-source PPG foundation model trained exclusively on\nraw PPG data collected over a 100-day field study with 120 participants.\nExisting PPG foundation models are either open-source but trained on clinical\ndata or closed-source, limiting their applicability in real-world settings. We\nevaluate Pulse-PPG across multiple datasets and downstream tasks, comparing its\nperformance against a state-of-the-art foundation model trained on clinical\ndata. Our results demonstrate that Pulse-PPG, trained on uncurated field data,\nexhibits superior generalization across clinical and mobile health applications\nin both lab and field settings. This suggests that exposure to real-world\nvariability enables the model to learn fine-grained representations, making it\nmore adaptable across tasks. Furthermore, pre-training on field data\nsurprisingly outperforms its pre-training on clinical data in many tasks,\nreinforcing the importance of training on real-world, diverse datasets. To\nencourage further advancements in robust foundation models leveraging field\ndata, we plan to release Pulse-PPG, providing researchers with a powerful\nresource for developing more generalizable PPG-based models.\n","authors":["Mithun Saha","Maxwell A. Xu","Wanting Mao","Sameer Neupane","James M. Rehg","Santosh Kumar"],"pdf_url":"https://arxiv.org/pdf/2502.01108v2.pdf","comment":"Saha and Xu are co-first authors"},{"id":"http://arxiv.org/abs/2507.17978v1","updated":"2025-07-23T22:57:08Z","published":"2025-07-23T22:57:08Z","title":"MeAJOR Corpus: A Multi-Source Dataset for Phishing Email Detection","summary":"  Phishing emails continue to pose a significant threat to cybersecurity by\nexploiting human vulnerabilities through deceptive content and malicious\npayloads. While Machine Learning (ML) models are effective at detecting\nphishing threats, their performance largely relies on the quality and diversity\nof the training data. This paper presents MeAJOR (Merged email Assets from\nJoint Open-source Repositories) Corpus, a novel, multi-source phishing email\ndataset designed to overcome critical limitations in existing resources. It\nintegrates 135894 samples representing a broad number of phishing tactics and\nlegitimate emails, with a wide spectrum of engineered features. We evaluated\nthe dataset's utility for phishing detection research through systematic\nexperiments with four classification models (RF, XGB, MLP, and CNN) across\nmultiple feature configurations. Results highlight the dataset's effectiveness,\nachieving 98.34% F1 with XGB. By integrating broad features from multiple\ncategories, our dataset provides a reusable and consistent resource, while\naddressing common challenges like class imbalance, generalisability and\nreproducibility.\n","authors":["Paulo Mendes","Eva Maia","Isabel Praça"],"pdf_url":"https://arxiv.org/pdf/2507.17978v1.pdf","comment":"8 pages, 2 tables, WI-IAT 2025 conference"},{"id":"http://arxiv.org/abs/2507.17977v1","updated":"2025-07-23T22:51:09Z","published":"2025-07-23T22:51:09Z","title":"Improving the Computational Efficiency and Explainability of\n  GeoAggregator","summary":"  Accurate modeling and explaining geospatial tabular data (GTD) are critical\nfor understanding geospatial phenomena and their underlying processes. Recent\nwork has proposed a novel transformer-based deep learning model named\nGeoAggregator (GA) for this purpose, and has demonstrated that it outperforms\nother statistical and machine learning approaches. In this short paper, we\nfurther improve GA by 1) developing an optimized pipeline that accelerates the\ndataloading process and streamlines the forward pass of GA to achieve better\ncomputational efficiency; and 2) incorporating a model ensembling strategy and\na post-hoc model explanation function based on the GeoShapley framework to\nenhance model explainability. We validate the functionality and efficiency of\nthe proposed strategies by applying the improved GA model to synthetic\ndatasets. Experimental results show that our implementation improves the\nprediction accuracy and inference speed of GA compared to the original\nimplementation. Moreover, explanation experiments indicate that GA can\neffectively captures the inherent spatial effects in the designed synthetic\ndataset. The complete pipeline has been made publicly available for community\nuse (https://github.com/ruid7181/GA-sklearn).\n","authors":["Rui Deng","Ziqi Li","Mingshu Wang"],"pdf_url":"https://arxiv.org/pdf/2507.17977v1.pdf","comment":"4 pages, 3 figures"},{"id":"http://arxiv.org/abs/2507.17974v1","updated":"2025-07-23T22:45:30Z","published":"2025-07-23T22:45:30Z","title":"Natural Language Processing for Tigrinya: Current State and Future\n  Directions","summary":"  Despite being spoken by millions of people, Tigrinya remains severely\nunderrepresented in Natural Language Processing (NLP) research. This work\npresents a comprehensive survey of NLP research for Tigrinya, analyzing over 40\nstudies spanning more than a decade of work from 2011 to 2025. We\nsystematically review the current state of computational resources, models, and\napplications across ten distinct downstream tasks, including morphological\nprocessing, machine translation, speech recognition, and question-answering.\nOur analysis reveals a clear trajectory from foundational, rule-based systems\nto modern neural architectures, with progress consistently unlocked by resource\ncreation milestones. We identify key challenges rooted in Tigrinya's\nmorphological complexity and resource scarcity, while highlighting promising\nresearch directions, including morphology-aware modeling, cross-lingual\ntransfer, and community-centered resource development. This work serves as both\na comprehensive reference for researchers and a roadmap for advancing Tigrinya\nNLP. A curated metadata of the surveyed studies and resources is made publicly\navailable.\\footnote{Tigrinya NLP Anthology:\nhttps://github.com/fgaim/tigrinya-nlp-anthology.\n","authors":["Fitsum Gaim","Jong C. Park"],"pdf_url":"https://arxiv.org/pdf/2507.17974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17958v1","updated":"2025-07-23T22:02:56Z","published":"2025-07-23T22:02:56Z","title":"VIBE: Video-Input Brain Encoder for fMRI Response Modeling","summary":"  We present VIBE, a two-stage Transformer that fuses multi-modal video, audio,\nand text features to predict fMRI activity. Representations from open-source\nmodels (Qwen2.5, BEATs, Whisper, SlowFast, V-JEPA) are merged by a\nmodality-fusion transformer and temporally decoded by a prediction transformer\nwith rotary embeddings. Trained on 65 hours of movie data from the CNeuroMod\ndataset and ensembled across 20 seeds, VIBE attains mean parcel-wise Pearson\ncorrelations of 32.25 on in-distribution Friends S07 and 21.25 on six\nout-of-distribution films. An earlier iteration of the same architecture\nobtained 0.3198 and 0.2096, respectively, winning Phase-1 and placing second\noverall in the Algonauts 2025 Challenge.\n","authors":["Daniel Carlstrom Schad","Shrey Dixit","Janis Keck","Viktor Studenyak","Aleksandr Shpilevoi","Andrej Bicanski"],"pdf_url":"https://arxiv.org/pdf/2507.17958v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05094v2","updated":"2025-07-23T21:57:22Z","published":"2024-10-07T14:48:56Z","title":"On the Structure of Game Provenance and its Applications","summary":"  Provenance in databases has been thoroughly studied for positive and for\nrecursive queries, then for first-order (FO) queries, i.e., having negation but\nno recursion. Query evaluation can be understood as a two-player game where the\nopponents argue whether or not a tuple is in the query answer. This\ngame-theoretic approach yields a natural provenance model for FO queries,\nunifying how and why-not provenance. Here, we study the fine-grain structure of\ngame provenance. A game $G=(V,E)$ consists of positions $V$ and moves $E$ and\ncan be solved by computing the well-founded model of a single, unstratifiable\nrule: \\[ \\text{win}(X) \\leftarrow \\text{move}(X, Y), \\neg \\, \\text{win}(Y). \\]\nIn the solved game $G^{\\lambda}$, the value of a position $x\\,{\\in}\\,V$ is\neither won, lost, or drawn. This value is explained by the provenance\n$\\mathscr{P}$(x), i.e., certain (annotated) edges reachable from $x$. We\nidentify seven edge types that give rise to new kinds of provenance, i.e.,\npotential, actual, and primary, and demonstrate that \"not all moves are created\nequal\". We describe the new provenance types, show how they can be computed\nwhile solving games, and discuss applications, e.g., for abstract argumentation\nframeworks.\n","authors":["Shawn Bowers","Yilin Xia","Bertram Ludäscher"],"pdf_url":"https://arxiv.org/pdf/2410.05094v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13238v2","updated":"2025-07-23T21:50:22Z","published":"2025-07-17T15:47:49Z","title":"Multilingual LLMs Are Not Multilingual Thinkers: Evidence from Hindi\n  Analogy Evaluation","summary":"  Analogies test a model's ability to infer implicit relationships between\nconcepts, making them a key benchmark for evaluating reasoning capabilities.\nWhile large language models (LLMs) are widely evaluated for reasoning in\nEnglish, their abilities in Indic languages remain understudied, limiting our\nunderstanding of whether these models generalize across languages. To address\nthis gap, we introduce a new Hindi Analogy Test Set (HATS), comprising 405\nmultiple-choice questions sourced from Indian government exams. We benchmark\nstate-of-the-art multilingual LLMs using various prompting strategies and\nintroduce a grounded Chain of Thought approach that leverages cognitive\ntheories of analogical reasoning. This approach improves model performance on\nHindi analogy questions. Our experiments show that models perform best with\nEnglish prompts, irrespective of the prompting strategy. Our test set addresses\nthe lack of a critical resource to evaluate LLM reasoning capabilities in\nHindi.\n","authors":["Ashray Gupta","Rohan Joseph","Sunny Rai"],"pdf_url":"https://arxiv.org/pdf/2507.13238v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17951v1","updated":"2025-07-23T21:46:37Z","published":"2025-07-23T21:46:37Z","title":"Are LLM Belief Updates Consistent with Bayes' Theorem?","summary":"  Do larger and more capable language models learn to update their \"beliefs\"\nabout propositions more consistently with Bayes' theorem when presented with\nevidence in-context? To test this, we formulate a Bayesian Coherence\nCoefficient (BCC) metric and generate a dataset with which to measure the BCC.\nWe measure BCC for multiple pre-trained-only language models across five model\nfamilies, comparing against the number of model parameters, the amount of\ntraining data, and model scores on common benchmarks. Our results provide\nevidence for our hypothesis that larger and more capable pre-trained language\nmodels assign credences that are more coherent with Bayes' theorem. These\nresults have important implications for our understanding and governance of\nLLMs.\n","authors":["Sohaib Imran","Ihor Kendiukhov","Matthew Broerman","Aditya Thomas","Riccardo Campanella","Rob Lamb","Peter M. Atkinson"],"pdf_url":"https://arxiv.org/pdf/2507.17951v1.pdf","comment":"Accepted at the ICML 2025 Workshop on Assessing World Models"},{"id":"http://arxiv.org/abs/2507.17948v1","updated":"2025-07-23T21:32:50Z","published":"2025-07-23T21:32:50Z","title":"VERIRAG: Healthcare Claim Verification via Statistical Audit in\n  Retrieval-Augmented Generation","summary":"  Retrieval-augmented generation (RAG) systems are increasingly adopted in\nclinical decision support, yet they remain methodologically blind-they retrieve\nevidence but cannot vet its scientific quality. A paper claiming \"Antioxidant\nproteins decreased after alloferon treatment\" and a rigorous multi-laboratory\nreplication study will be treated as equally credible, even if the former\nlacked scientific rigor or was even retracted. To address this challenge, we\nintroduce VERIRAG, a framework that makes three notable contributions: (i) the\nVeritable, an 11-point checklist that evaluates each source for methodological\nrigor, including data integrity and statistical validity; (ii) a Hard-to-Vary\n(HV) Score, a quantitative aggregator that weights evidence by its quality and\ndiversity; and (iii) a Dynamic Acceptance Threshold, which calibrates the\nrequired evidence based on how extraordinary a claim is. Across four\ndatasets-comprising retracted, conflicting, comprehensive, and settled science\ncorpora-the VERIRAG approach consistently outperforms all baselines, achieving\nabsolute F1 scores ranging from 0.53 to 0.65, representing a 10 to 14 point\nimprovement over the next-best method in each respective dataset. We will\nrelease all materials necessary for reproducing our results.\n","authors":["Shubham Mohole","Hongjun Choi","Shusen Liu","Christine Klymko","Shashank Kushwaha","Derek Shi","Wesam Sakla","Sainyam Galhotra","Ruben Glatt"],"pdf_url":"https://arxiv.org/pdf/2507.17948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17944v1","updated":"2025-07-23T21:26:33Z","published":"2025-07-23T21:26:33Z","title":"Evaluating the Performance of AI Text Detectors, Few-Shot and\n  Chain-of-Thought Prompting Using DeepSeek Generated Text","summary":"  Large language models (LLMs) have rapidly transformed the creation of written\nmaterials. LLMs have led to questions about writing integrity, thereby driving\nthe creation of artificial intelligence (AI) detection technologies.\nAdversarial attacks, such as standard and humanized paraphrasing, inhibit\ndetectors' ability to detect machine-generated text. Previous studies have\nmainly focused on ChatGPT and other well-known LLMs and have shown varying\naccuracy across detectors. However, there is a clear gap in the literature\nabout DeepSeek, a recently published LLM. Therefore, in this work, we\ninvestigate whether six generally accessible AI detection tools -- AI Text\nClassifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, and GPTZero -- can\nconsistently recognize text generated by DeepSeek. The detectors were exposed\nto the aforementioned adversarial attacks. We also considered DeepSeek as a\ndetector by performing few-shot prompting and chain-of-thought reasoning (CoT)\nfor classifying AI and human-written text. We collected 49 human-authored\nquestion-answer pairs from before the LLM era and generated matching responses\nusing DeepSeek-v3, producing 49 AI-generated samples. Then, we applied\nadversarial techniques such as paraphrasing and humanizing to add 196 more\nsamples. These were used to challenge detector robustness and assess accuracy\nimpact. While QuillBot and Copyleaks showed near-perfect performance on\noriginal and paraphrased DeepSeek text, others -- particularly AI Text\nClassifier and GPT-2 -- showed inconsistent results. The most effective attack\nwas humanization, reducing accuracy to 71% for Copyleaks, 58% for QuillBot, and\n52% for GPTZero. Few-shot and CoT prompting showed high accuracy, with the best\nfive-shot result misclassifying only one of 49 samples (AI recall 96%, human\nrecall 100%).\n","authors":["Hulayyil Alshammari","Praveen Rao"],"pdf_url":"https://arxiv.org/pdf/2507.17944v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03699v3","updated":"2025-07-23T21:26:26Z","published":"2025-02-06T01:22:06Z","title":"LLM Alignment as Retriever Optimization: An Information Retrieval\n  Perspective","summary":"  Large Language Models (LLMs) have revolutionized artificial intelligence with\ncapabilities in reasoning, coding, and communication, driving innovation across\nindustries. Their true potential depends on effective alignment to ensure\ncorrect, trustworthy and ethical behavior, addressing challenges like\nmisinformation, hallucinations, bias and misuse. While existing Reinforcement\nLearning (RL)-based alignment methods are notoriously complex, direct\noptimization approaches offer a simpler alternative. In this work, we introduce\na novel direct optimization approach for LLM alignment by drawing on\nestablished Information Retrieval (IR) principles. We present a systematic\nframework that bridges LLM alignment and IR methodologies, mapping LLM\ngeneration and reward models to IR's retriever-reranker paradigm. Building on\nthis foundation, we propose LLM Alignment as Retriever Preference Optimization\n(LarPO), a new alignment method that enhances overall alignment quality.\nExtensive experiments validate LarPO's effectiveness with 38.9 % and 13.7 %\naveraged improvement on AlpacaEval2 and MixEval-Hard respectively. Our work\nopens new avenues for advancing LLM alignment by integrating IR foundations,\noffering a promising direction for future research.\n","authors":["Bowen Jin","Jinsung Yoon","Zhen Qin","Ziqi Wang","Wei Xiong","Yu Meng","Jiawei Han","Sercan O. Arik"],"pdf_url":"https://arxiv.org/pdf/2502.03699v3.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2507.17942v1","updated":"2025-07-23T21:22:35Z","published":"2025-07-23T21:22:35Z","title":"Minimax Data Sanitization with Distortion Constraint and Adversarial\n  Inference","summary":"  We study a privacy-preserving data-sharing setting where a privatizer\ntransforms private data into a sanitized version observed by an authorized\nreconstructor and two unauthorized adversaries, each with access to side\ninformation correlated with the private data.\n  The reconstructor is evaluated under a distortion function, while each\nadversary is evaluated using a separate loss function. The privatizer ensures\nthe reconstructor distortion remains below a fixed threshold while maximizing\nthe minimum loss across the two adversaries. This two-adversary setting models\ncases where individual users cannot reconstruct the data accurately, but their\ncombined side information enables estimation within the distortion threshold.\nThe privatizer maximizes individual loss while permitting accurate\nreconstruction only through collaboration. This echoes secret-sharing\nprinciples, but with lossy rather than perfect recovery. We frame this as a\nconstrained data-driven minimax optimization problem and propose a data-driven\ntraining procedure that alternately updates the privatizer, reconstructor, and\nadversaries. We also analyze the Gaussian and binary cases as special scenarios\nwhere optimal solutions can be obtained. These theoretical optimal results are\nbenchmarks for evaluating the proposed minimax training approach.\n","authors":["Amirarsalan Moatazedian","Yauhen Yakimenka","Rémi A. Chou","Jörg Kliewer"],"pdf_url":"https://arxiv.org/pdf/2507.17942v1.pdf","comment":"Accepted to IEEE ITW 2025"},{"id":"http://arxiv.org/abs/2507.17937v1","updated":"2025-07-23T21:11:47Z","published":"2025-07-23T21:11:47Z","title":"Bob's Confetti: Phonetic Memorization Attacks in Music and Video\n  Generation","summary":"  Lyrics-to-Song (LS2) generation models promise end-to-end music synthesis\nfrom text, yet their vulnerability to training data memorization remains\nunderexplored. We introduce Adversarial PhoneTic Prompting (APT), a novel\nattack where lyrics are semantically altered while preserving their acoustic\nstructure through homophonic substitutions (e.g., Eminem's famous \"mom's\nspaghetti\" $\\rightarrow$ \"Bob's confetti\"). Despite these distortions, we\nuncover a powerful form of sub-lexical memorization: models like SUNO and YuE\nregenerate outputs strikingly similar to known training content, achieving high\nsimilarity across audio-domain metrics, including CLAP, AudioJudge, and\nCoverID. This vulnerability persists across multiple languages and genres. More\nsurprisingly, we discover that phoneme-altered lyrics alone can trigger visual\nmemorization in text-to-video models. When prompted with phonetically modified\nlyrics from Lose Yourself, Veo 3 reconstructs visual elements from the original\nmusic video -- including character appearance and scene composition -- despite\nno visual cues in the prompt. We term this phenomenon phonetic-to-visual\nregurgitation. Together, these findings expose a critical vulnerability in\ntranscript-conditioned multimodal generation: phonetic prompting alone can\nunlock memorized audiovisual content, raising urgent questions about copyright,\nsafety, and content provenance in modern generative systems. Example\ngenerations are available on our demo page (jrohsc.github.io/music_attack/).\n","authors":["Jaechul Roh","Zachary Novack","Yuefeng Peng","Niloofar Mireshghallah","Taylor Berg-Kirkpatrick","Amir Houmansadr"],"pdf_url":"https://arxiv.org/pdf/2507.17937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.15465v2","updated":"2025-07-23T20:55:41Z","published":"2025-07-21T10:18:33Z","title":"The New LLM Bottleneck: A Systems Perspective on Latent Attention and\n  Mixture-of-Experts","summary":"  Computational workloads composing traditional Transformer models are starkly\nbifurcated. Multi-Head Attention (MHA) is memory-bound, with low arithmetic\nintensity, while feedforward layers are compute-bound. This dichotomy has long\nmotivated research into specialized hardware to mitigate the MHA bottleneck.\n  This paper argues that recent architectural shifts, namely Multi-head Latent\nAttention (MLA) and Mixture-of-Experts (MoE), challenge the premise of\nspecialized attention hardware. We make two key observations. First, the\narithmetic intensity of MLA is over two orders of magnitude greater than that\nof MHA, shifting it close to a compute-bound regime well-suited for modern\naccelerators like GPUs. Second, by distributing MoE experts across a pool of\naccelerators, their arithmetic intensity can be tuned through batching to match\nthat of the dense layers, creating a more balanced computational profile.\n  These findings reveal a diminishing need for specialized attention hardware.\nThe central challenge for next-generation Transformers is no longer\naccelerating a single memory-bound layer. Instead, the focus must shift to\ndesigning balanced systems with sufficient compute, memory capacity, memory\nbandwidth, and high-bandwidth interconnects to manage the diverse demands of\nlarge-scale models.\n","authors":["Sungmin Yun","Seonyong Park","Hwayong Nam","Younjoo Lee","Gunjun Lee","Kwanhee Kyung","Sangpyo Kim","Nam Sung Kim","Jongmin Kim","Hyungyo Kim","Juhwan Cho","Seungmin Baek","Jung Ho Ahn"],"pdf_url":"https://arxiv.org/pdf/2507.15465v2.pdf","comment":"15 pages, 11 figures"},{"id":"http://arxiv.org/abs/2507.17927v1","updated":"2025-07-23T20:53:40Z","published":"2025-07-23T20:53:40Z","title":"SMARTAPS: Tool-augmented LLMs for Operations Management","summary":"  Large language models (LLMs) present intriguing opportunities to enhance user\ninteraction with traditional algorithms and tools in real-world applications.\nAn advanced planning system (APS) is a sophisticated software that leverages\noptimization to help operations planners create, interpret, and modify an\noperational plan. While highly beneficial, many customers are priced out of\nusing an APS due to the ongoing costs of consultants responsible for\ncustomization and maintenance. To address the need for a more accessible APS\nexpressed by supply chain planners, we present SmartAPS, a conversational\nsystem built on a tool-augmented LLM. Our system provides operations planners\nwith an intuitive natural language chat interface, allowing them to query\ninformation, perform counterfactual reasoning, receive recommendations, and\nexecute scenario analysis to better manage their operation. A short video\ndemonstrating the system has been released: https://youtu.be/KtIrJjlDbyw\n","authors":["Timothy Tin Long Yu","Mahdi Mostajabdaveh","Jabo Serge Byusa","Rindra Ramamonjison","Giuseppe Carenini","Kun Mao","Zirui Zhou","Yong Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.17927v1.pdf","comment":"https://aaai.org/conference/aaai/aaai-25/bridge-ai-orms/"},{"id":"http://arxiv.org/abs/2507.09871v2","updated":"2025-07-23T20:53:29Z","published":"2025-07-14T02:53:14Z","title":"Task Priors: Enhancing Model Evaluation by Considering the Entire Space\n  of Downstream Tasks","summary":"  The grand goal of AI research, and particularly Self Supervised Learning\n(SSL), is to produce systems that can successfully solve any possible task. In\ncontrast, current evaluation methods available to AI researchers typically rely\non a fixed collection of hand-picked downstream benchmarks. Hence, a large\namount of effort is put into designing and searching for large collection of\nevaluation tasks that can serve as a proxy of our grand goal. We argue that\nsuch a rigid evaluation protocol creates a silent bottleneck in AI research. To\nremedy that, we define a probabilistic space of downstream tasks obtained by\nadopting a distribution of tasks and by defining Task Priors. Under this view,\none can evaluate a model's performance over the set of all possible downstream\ntasks. Our framework is the first to provide answers to key questions such as\n(i) what is the average performance of my model over all possible downstream\ntasks weighted by the probability to encounter each task? or (ii) what is the\nvariance of my model's performance across all downstream tasks under the\ndefined Task Priors? Beyond establishing a new standard for evaluation, we\nbelieve that Task Priors will accelerate the pace of research in SSL - where\ndownstream task evaluation is the sole qualitative signal that researchers have\naccess to.\n","authors":["Niket Patel","Randall Balestriero"],"pdf_url":"https://arxiv.org/pdf/2507.09871v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17924v1","updated":"2025-07-23T20:44:25Z","published":"2025-07-23T20:44:25Z","title":"UrbanPulse: A Cross-City Deep Learning Framework for Ultra-Fine-Grained\n  Population Transfer Prediction","summary":"  Accurate population flow prediction is essential for urban planning,\ntransportation management, and public health. Yet existing methods face key\nlimitations: traditional models rely on static spatial assumptions, deep\nlearning models struggle with cross-city generalization, and Large Language\nModels (LLMs) incur high computational costs while failing to capture spatial\nstructure. Moreover, many approaches sacrifice resolution by clustering Points\nof Interest (POIs) or restricting coverage to subregions, limiting their\nutility for city-wide analytics. We introduce UrbanPulse, a scalable deep\nlearning framework that delivers ultra-fine-grained, city-wide OD flow\npredictions by treating each POI as an individual node. It combines a temporal\ngraph convolutional encoder with a transformer-based decoder to model\nmulti-scale spatiotemporal dependencies. To ensure robust generalization across\nurban contexts, UrbanPulse employs a three-stage transfer learning strategy:\npretraining on large-scale urban graphs, cold-start adaptation, and\nreinforcement learning fine-tuning.Evaluated on over 103 million cleaned GPS\nrecords from three metropolitan areas in California, UrbanPulse achieves\nstate-of-the-art accuracy and scalability. Through efficient transfer learning,\nUrbanPulse takes a key step toward making high-resolution, AI-powered urban\nforecasting deployable in practice across diverse cities.\n","authors":["Hongrong Yang","Markus Schlaepfer"],"pdf_url":"https://arxiv.org/pdf/2507.17924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17922v1","updated":"2025-07-23T20:39:14Z","published":"2025-07-23T20:39:14Z","title":"From Seed to Harvest: Augmenting Human Creativity with AI for\n  Red-teaming Text-to-Image Models","summary":"  Text-to-image (T2I) models have become prevalent across numerous\napplications, making their robust evaluation against adversarial attacks a\ncritical priority. Continuous access to new and challenging adversarial prompts\nacross diverse domains is essential for stress-testing these models for\nresilience against novel attacks from multiple vectors. Current techniques for\ngenerating such prompts are either entirely authored by humans or synthetically\ngenerated. On the one hand, datasets of human-crafted adversarial prompts are\noften too small in size and imbalanced in their cultural and contextual\nrepresentation. On the other hand, datasets of synthetically-generated prompts\nachieve scale, but typically lack the realistic nuances and creative\nadversarial strategies found in human-crafted prompts. To combine the strengths\nof both human and machine approaches, we propose Seed2Harvest, a hybrid\nred-teaming method for guided expansion of culturally diverse, human-crafted\nadversarial prompt seeds. The resulting prompts preserve the characteristics\nand attack patterns of human prompts while maintaining comparable average\nattack success rates (0.31 NudeNet, 0.36 SD NSFW, 0.12 Q16). Our expanded\ndataset achieves substantially higher diversity with 535 unique geographic\nlocations and a Shannon entropy of 7.48, compared to 58 locations and 5.28\nentropy in the original dataset. Our work demonstrates the importance of\nhuman-machine collaboration in leveraging human creativity and machine\ncomputational capacity to achieve comprehensive, scalable red-teaming for\ncontinuous T2I model safety evaluation.\n","authors":["Jessica Quaye","Charvi Rastogi","Alicia Parrish","Oana Inel","Minsuk Kahng","Lora Aroyo","Vijay Janapa Reddi"],"pdf_url":"https://arxiv.org/pdf/2507.17922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11783v2","updated":"2025-07-23T20:10:43Z","published":"2025-07-15T22:52:44Z","title":"EEG Foundation Models: A Critical Review of Current Progress and Future\n  Directions","summary":"  Patterns of electrical brain activity recorded via electroencephalography\n(EEG) offer immense value for scientific and clinical investigations. The\ninability of supervised EEG encoders to learn robust EEG patterns and their\nover-reliance on expensive signal annotations have sparked a transition towards\ngeneral-purpose self-supervised EEG encoders, i.e., EEG foundation models\n(EEG-FMs), for robust and scalable EEG feature extraction. However, the\nreal-world readiness of early EEG-FMs and the rubric for long-term research\nprogress remain unclear. A systematic and comprehensive review of\nfirst-generation EEG-FMs is therefore necessary to understand the current\nstate-of-the-art and identify key directions for future EEG-FMs. To that end,\nthis study reviews 10 early EEG-FMs and presents a critical synthesis of their\nmethodology, empirical findings, and outstanding research gaps. We find that\nmost EEG-FMs adopt a sequence-based modeling scheme that relies on\ntransformer-based backbones and the reconstruction of masked sequences for\nself-supervision. However, model evaluations remain heterogeneous and largely\nlimited, making it challenging to assess their practical off-the-shelf utility.\nIn addition to adopting standardized and realistic evaluations, future work\nshould demonstrate more substantial scaling effects and make principled and\ntrustworthy choices throughout the EEG representation learning pipeline. We\nbelieve that developing benchmarks, software tools, technical methodologies,\nand applications in collaboration with domain experts may further advance the\ntranslational utility and real-world adoption of EEG-FMs.\n","authors":["Gayal Kuruppu","Neeraj Wagh","Yogatheesan Varatharajah"],"pdf_url":"https://arxiv.org/pdf/2507.11783v2.pdf","comment":"20 pages, 5 figures, 3 tables (main + supplement)"},{"id":"http://arxiv.org/abs/2507.17907v1","updated":"2025-07-23T20:07:53Z","published":"2025-07-23T20:07:53Z","title":"Deep learning-aided inverse design of porous metamaterials","summary":"  The ultimate aim of the study is to explore the inverse design of porous\nmetamaterials using a deep learning-based generative framework. Specifically,\nwe develop a property-variational autoencoder (pVAE), a variational autoencoder\n(VAE) augmented with a regressor, to generate structured metamaterials with\ntailored hydraulic properties, such as porosity and permeability. While this\nwork uses the lattice Boltzmann method (LBM) to generate intrinsic permeability\ntensor data for limited porous microstructures, a convolutional neural network\n(CNN) is trained using a bottom-up approach to predict effective hydraulic\nproperties. This significantly reduces the computational cost compared to\ndirect LBM simulations. The pVAE framework is trained on two datasets: a\nsynthetic dataset of artificial porous microstructures and CT-scan images of\nvolume elements from real open-cell foams. The encoder-decoder architecture of\nthe VAE captures key microstructural features, mapping them into a compact and\ninterpretable latent space for efficient structure-property exploration. The\nstudy provides a detailed analysis and interpretation of the latent space,\ndemonstrating its role in structure-property mapping, interpolation, and\ninverse design. This approach facilitates the generation of new metamaterials\nwith desired properties. The datasets and codes used in this study will be made\nopen-access to support further research.\n","authors":["Phu Thien Nguyen","Yousef Heider","Dennis M. Kochmann","Fadi Aldakheel"],"pdf_url":"https://arxiv.org/pdf/2507.17907v1.pdf","comment":"31 pages, 29 figures"},{"id":"http://arxiv.org/abs/2507.17896v1","updated":"2025-07-23T19:48:12Z","published":"2025-07-23T19:48:12Z","title":"VeriMinder: Mitigating Analytical Vulnerabilities in NL2SQL","summary":"  Application systems using natural language interfaces to databases (NLIDBs)\nhave democratized data analysis. This positive development has also brought\nforth an urgent challenge to help users who might use these systems without a\nbackground in statistical analysis to formulate bias-free analytical questions.\nAlthough significant research has focused on text-to-SQL generation accuracy,\naddressing cognitive biases in analytical questions remains underexplored. We\npresent VeriMinder, https://veriminder.ai, an interactive system for detecting\nand mitigating such analytical vulnerabilities. Our approach introduces three\nkey innovations: (1) a contextual semantic mapping framework for biases\nrelevant to specific analysis contexts (2) an analytical framework that\noperationalizes the Hard-to-Vary principle and guides users in systematic data\nanalysis (3) an optimized LLM-powered system that generates high-quality,\ntask-specific prompts using a structured process involving multiple candidates,\ncritic feedback, and self-reflection.\n  User testing confirms the merits of our approach. In direct user experience\nevaluation, 82.5% participants reported positively impacting the quality of the\nanalysis. In comparative evaluation, VeriMinder scored significantly higher\nthan alternative approaches, at least 20% better when considered for metrics of\nthe analysis's concreteness, comprehensiveness, and accuracy. Our system,\nimplemented as a web application, is set to help users avoid \"wrong question\"\nvulnerability during data analysis. VeriMinder code base with prompts,\nhttps://reproducibility.link/veriminder, is available as an MIT-licensed\nopen-source software to facilitate further research and adoption within the\ncommunity.\n","authors":["Shubham Mohole","Sainyam Galhotra"],"pdf_url":"https://arxiv.org/pdf/2507.17896v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17893v1","updated":"2025-07-23T19:42:51Z","published":"2025-07-23T19:42:51Z","title":"Action-List Reinforcement Learning Syndrome Decoding for Binary Linear\n  Block Codes","summary":"  This paper explores the application of reinforcement learning techniques to\nenhance the performance of decoding of linear block codes based on flipping\nbits and finding optimal decisions. We describe the methodology for mapping the\niterative decoding process into Markov Decision Processes (MDPs) and propose\ndifferent methods to reduce the number of states in the MDP. A truncated MDP is\nproposed to reduce the number of states in the MDP by learning a Hamming ball\nwith a specified radius around codewords. We then propose a general scheme for\nreinforcement learning based decoders applicable to any class of codes to\nimprove the performance of decoders. We call this scheme an action-list\ndecoding. We design an action-list decoder based on the Deep-Q network values\nthat substantially enhance performance. We also get benefit of automorphism\ngroup of code to further improve the code performance. Additionally, we propose\na feedback-based method to exploit and enhance the performance of existing\nhigh-performing decoders by applying reinforcement learning algorithms after\nthe existing decoders. These approaches effectively reduces the complexity of\nthe reinforcement learning block. Finally, we present experimental results for\nthe Low-Density Parity Check (LDPC) codes over the Binary Symmetric Channel\n(BSC) to demonstrate the efficiency of the proposed methods.\n","authors":["Milad Taghipour","Bane Vasic"],"pdf_url":"https://arxiv.org/pdf/2507.17893v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.08537v2","updated":"2025-07-23T19:39:27Z","published":"2025-03-11T15:27:17Z","title":"Chemical reasoning in LLMs unlocks strategy-aware synthesis planning and\n  reaction mechanism elucidation","summary":"  While automated chemical tools excel at specific tasks, they have struggled\nto capture the strategic thinking that characterizes expert chemical reasoning.\nHere we demonstrate that large language models (LLMs) can serve as powerful\ntools enabling chemical analysis. When integrated with traditional search\nalgorithms, they enable a new approach to computer-aided synthesis that mirrors\nhuman expert thinking. Rather than using LLMs to directly manipulate chemical\nstructures, we leverage their ability to evaluate chemical strategies and guide\nsearch algorithms toward chemically meaningful solutions. We demonstrate this\nparadigm through two fundamental challenges: strategy-aware retrosynthetic\nplanning and mechanism elucidation. In retrosynthetic planning, our system\nallows chemists to specify desired synthetic strategies in natural language --\nfrom protecting group strategies to global feasibility assessment -- and uses\ntraditional or LLM-guided Monte Carlo Tree Search to find routes that satisfy\nthese constraints. In mechanism elucidation, LLMs guide the search for\nplausible reaction mechanisms by combining chemical principles with systematic\nexploration. This approach shows strong performance across diverse chemical\ntasks, with newer and larger models demonstrating increasingly sophisticated\nchemical reasoning. Our approach establishes a new paradigm for computer-aided\nchemistry that combines the strategic understanding of LLMs with the precision\nof traditional chemical tools, opening possibilities for more intuitive and\npowerful chemical automation systems.\n","authors":["Andres M Bran","Theo A Neukomm","Daniel P Armstrong","Zlatko Jončev","Philippe Schwaller"],"pdf_url":"https://arxiv.org/pdf/2503.08537v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13180v3","updated":"2025-07-23T19:22:35Z","published":"2025-04-17T17:59:56Z","title":"PerceptionLM: Open-Access Data and Models for Detailed Visual\n  Understanding","summary":"  Vision-language models are integral to computer vision research, yet many\nhigh-performing models remain closed-source, obscuring their data, design and\ntraining recipe. The research community has responded by using distillation\nfrom black-box models to label training data, achieving strong benchmark\nresults, at the cost of measurable scientific progress. However, without\nknowing the details of the teacher model and its data sources, scientific\nprogress remains difficult to measure. In this paper, we study building a\nPerception Language Model (PLM) in a fully open and reproducible framework for\ntransparent research in image and video understanding. We analyze standard\ntraining pipelines without distillation from proprietary models and explore\nlarge-scale synthetic data to identify critical data gaps, particularly in\ndetailed video understanding. To bridge these gaps, we release 2.8M\nhuman-labeled instances of fine-grained video question-answer pairs and\nspatio-temporally grounded video captions. Additionally, we introduce\nPLM-VideoBench, a suite for evaluating challenging video understanding tasks\nfocusing on the ability to reason about \"what\", \"where\", \"when\", and \"how\" of a\nvideo. We make our work fully reproducible by providing data, training recipes,\ncode & models. https://github.com/facebookresearch/perception_models\n","authors":["Jang Hyun Cho","Andrea Madotto","Effrosyni Mavroudi","Triantafyllos Afouras","Tushar Nagarajan","Muhammad Maaz","Yale Song","Tengyu Ma","Shuming Hu","Suyog Jain","Miguel Martin","Huiyu Wang","Hanoona Rasheed","Peize Sun","Po-Yao Huang","Daniel Bolya","Nikhila Ravi","Shashank Jain","Tammy Stark","Shane Moon","Babak Damavandi","Vivian Lee","Andrew Westbury","Salman Khan","Philipp Krähenbühl","Piotr Dollár","Lorenzo Torresani","Kristen Grauman","Christoph Feichtenhofer"],"pdf_url":"https://arxiv.org/pdf/2504.13180v3.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2507.17874v1","updated":"2025-07-23T18:58:42Z","published":"2025-07-23T18:58:42Z","title":"I2I-STRADA -- Information to Insights via Structured Reasoning Agent for\n  Data Analysis","summary":"  Recent advances in agentic systems for data analysis have emphasized\nautomation of insight generation through multi-agent frameworks, and\norchestration layers. While these systems effectively manage tasks like query\ntranslation, data transformation, and visualization, they often overlook the\nstructured reasoning process underlying analytical thinking. Reasoning large\nlanguage models (LLMs) used for multi-step problem solving are trained as\ngeneral-purpose problem solvers. As a result, their reasoning or thinking steps\ndo not adhere to fixed processes for specific tasks. Real-world data analysis\nrequires a consistent cognitive workflow: interpreting vague goals, grounding\nthem in contextual knowledge, constructing abstract plans, and adapting\nexecution based on intermediate outcomes. We introduce I2I-STRADA\n(Information-to-Insight via Structured Reasoning Agent for Data Analysis), an\nagentic architecture designed to formalize this reasoning process. I2I-STRADA\nfocuses on modeling how analysis unfolds via modular sub-tasks that reflect the\ncognitive steps of analytical reasoning. Evaluations on the DABstep and DABench\nbenchmarks show that I2I-STRADA outperforms prior systems in planning coherence\nand insight alignment, highlighting the importance of structured cognitive\nworkflows in agent design for data analysis.\n","authors":["SaiBarath Sundar","Pranav Satheesan","Udayaadithya Avadhanam"],"pdf_url":"https://arxiv.org/pdf/2507.17874v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.03170v2","updated":"2025-07-23T18:41:23Z","published":"2025-05-28T18:52:40Z","title":"PALADIN : Robust Neural Fingerprinting for Text-to-Image Diffusion\n  Models","summary":"  The risk of misusing text-to-image generative models for malicious uses,\nespecially due to the open-source development of such models, has become a\nserious concern. As a risk mitigation strategy, attributing generative models\nwith neural fingerprinting is emerging as a popular technique. There has been a\nplethora of recent work that aim for addressing neural fingerprinting. A\ntrade-off between the attribution accuracy and generation quality of such\nmodels has been studied extensively. None of the existing methods yet achieved\n100% attribution accuracy. However, any model with less than cent percent\naccuracy is practically non-deployable. In this work, we propose an accurate\nmethod to incorporate neural fingerprinting for text-to-image diffusion models\nleveraging the concepts of cyclic error correcting codes from the literature of\ncoding theory.\n","authors":["Murthy L","Subarna Tripathi"],"pdf_url":"https://arxiv.org/pdf/2506.03170v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17860v1","updated":"2025-07-23T18:33:27Z","published":"2025-07-23T18:33:27Z","title":"Towards Facilitated Fairness Assessment of AI-based Skin Lesion\n  Classifiers Through GenAI-based Image Synthesis","summary":"  Recent advancements in Deep Learning and its application on the edge hold\ngreat potential for the revolution of routine screenings for skin cancers like\nMelanoma. Along with the anticipated benefits of this technology, potential\ndangers arise from unforseen and inherent biases. Thus, assessing and improving\nthe fairness of such systems is of utmost importance. A key challenge in\nfairness assessment is to ensure that the evaluation dataset is sufficiently\nrepresentative of different Personal Identifiable Information (PII) (sex, age,\nand race) and other minority groups. Against the backdrop of this challenge,\nthis study leverages the state-of-the-art Generative AI (GenAI) LightningDiT\nmodel to assess the fairness of publicly available melanoma classifiers. The\nresults suggest that fairness assessment using highly realistic synthetic data\nis a promising direction. Yet, our findings indicate that verifying fairness\nbecomes difficult when the melanoma-detection model used for evaluation is\ntrained on data that differ from the dataset underpinning the synthetic images.\nNonetheless, we propose that our approach offers a valuable new avenue for\nemploying synthetic data to gauge and enhance fairness in medical-imaging GenAI\nsystems.\n","authors":["Ko Watanabe. Stanislav Frolov. Adriano Lucieri. Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2507.17860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17853v1","updated":"2025-07-23T18:20:46Z","published":"2025-07-23T18:20:46Z","title":"Detail++: Training-Free Detail Enhancer for Text-to-Image Diffusion\n  Models","summary":"  Recent advances in text-to-image (T2I) generation have led to impressive\nvisual results. However, these models still face significant challenges when\nhandling complex prompt, particularly those involving multiple subjects with\ndistinct attributes. Inspired by the human drawing process, which first\noutlines the composition and then incrementally adds details, we propose\nDetail++, a training-free framework that introduces a novel Progressive Detail\nInjection (PDI) strategy to address this limitation. Specifically, we decompose\na complex prompt into a sequence of simplified sub-prompts, guiding the\ngeneration process in stages. This staged generation leverages the inherent\nlayout-controlling capacity of self-attention to first ensure global\ncomposition, followed by precise refinement. To achieve accurate binding\nbetween attributes and corresponding subjects, we exploit cross-attention\nmechanisms and further introduce a Centroid Alignment Loss at test time to\nreduce binding noise and enhance attribute consistency. Extensive experiments\non T2I-CompBench and a newly constructed style composition benchmark\ndemonstrate that Detail++ significantly outperforms existing methods,\nparticularly in scenarios involving multiple objects and complex stylistic\nconditions.\n","authors":["Lifeng Chen","Jiner Wang","Zihao Pan","Beier Zhu","Xiaofeng Yang","Chi Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.17853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17850v1","updated":"2025-07-23T18:17:26Z","published":"2025-07-23T18:17:26Z","title":"Performance Evaluation and Threat Mitigation in Large-scale 5G Core\n  Deployment","summary":"  The deployment of large-scale software-based 5G core functions presents\nsignificant challenges due to their reliance on optimized and intelligent\nresource provisioning for their services. Many studies have focused on\nanalyzing the impact of resource allocation for complex deployments using\nmathematical models, queue theories, or even Artificial Intelligence (AI). This\npaper elucidates the effects of chaotic workloads, generated by Distributed\nDenial of Service (DDoS) on different Network Functions (NFs) on User Equipment\nregistration performance. Our findings highlight the necessity of diverse\nresource profiles to ensure Service-Level Agreement (SLA) compliance in\nlarge-scale 5G core deployments. Additionally, our analysis of packet capture\napproaches demonstrates the potential of kernel-based monitoring for scalable\nsecurity threat defense. Finally, our empirical evaluation provides insights\ninto the effective deployment of 5G NFs in complex scenarios.\n","authors":["Rodrigo Moreira","Larissa F. Rodrigues Moreira","Flávio de Oliveira Silva"],"pdf_url":"https://arxiv.org/pdf/2507.17850v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17844v1","updated":"2025-07-23T18:11:39Z","published":"2025-07-23T18:11:39Z","title":"SV3.3B: A Sports Video Understanding Model for Action Recognition","summary":"  This paper addresses the challenge of automated sports video analysis, which\nhas traditionally been limited by computationally intensive models requiring\nserver-side processing and lacking fine-grained understanding of athletic\nmovements. Current approaches struggle to capture the nuanced biomechanical\ntransitions essential for meaningful sports analysis, often missing critical\nphases like preparation, execution, and follow-through that occur within\nseconds. To address these limitations, we introduce SV3.3B, a lightweight 3.3B\nparameter video understanding model that combines novel temporal motion\ndifference sampling with self-supervised learning for efficient on-device\ndeployment. Our approach employs a DWT-VGG16-LDA based keyframe extraction\nmechanism that intelligently identifies the 16 most representative frames from\nsports sequences, followed by a V-DWT-JEPA2 encoder pretrained through\nmask-denoising objectives and an LLM decoder fine-tuned for sports action\ndescription generation. Evaluated on a subset of the NSVA basketball dataset,\nSV3.3B achieves superior performance across both traditional text generation\nmetrics and sports-specific evaluation criteria, outperforming larger\nclosed-source models including GPT-4o variants while maintaining significantly\nlower computational requirements. Our model demonstrates exceptional capability\nin generating technically detailed and analytically rich sports descriptions,\nachieving 29.2% improvement over GPT-4o in ground truth validation metrics,\nwith substantial improvements in information density, action complexity, and\nmeasurement precision metrics essential for comprehensive athletic analysis.\nModel Available at https://huggingface.co/sportsvision/SV3.3B.\n","authors":["Sai Varun Kodathala","Yashwanth Reddy Vutukoori","Rakesh Vunnam"],"pdf_url":"https://arxiv.org/pdf/2507.17844v1.pdf","comment":"8 pages, 6 figures, 4 tables. Submitted to AIxSET 2025"},{"id":"http://arxiv.org/abs/2507.17748v1","updated":"2025-07-23T17:59:02Z","published":"2025-07-23T17:59:02Z","title":"Large Learning Rates Simultaneously Achieve Robustness to Spurious\n  Correlations and Compressibility","summary":"  Robustness and resource-efficiency are two highly desirable properties for\nmodern machine learning models. However, achieving them jointly remains a\nchallenge. In this paper, we position high learning rates as a facilitator for\nsimultaneously achieving robustness to spurious correlations and network\ncompressibility. We demonstrate that large learning rates also produce\ndesirable representation properties such as invariant feature utilization,\nclass separation, and activation sparsity. Importantly, our findings indicate\nthat large learning rates compare favorably to other hyperparameters and\nregularization methods, in consistently satisfying these properties in tandem.\nIn addition to demonstrating the positive effect of large learning rates across\ndiverse spurious correlation datasets, models, and optimizers, we also present\nstrong evidence that the previously documented success of large learning rates\nin standard classification tasks is likely due to its effect on addressing\nhidden/rare spurious correlations in the training dataset.\n","authors":["Melih Barsbey","Lucas Prieto","Stefanos Zafeiriou","Tolga Birdal"],"pdf_url":"https://arxiv.org/pdf/2507.17748v1.pdf","comment":"Accepted at ICCV 2025, 23 pages"},{"id":"http://arxiv.org/abs/2507.17747v1","updated":"2025-07-23T17:58:14Z","published":"2025-07-23T17:58:14Z","title":"Pretraining on the Test Set Is No Longer All You Need: A Debate-Driven\n  Approach to QA Benchmarks","summary":"  As frontier language models increasingly saturate standard QA benchmarks,\nconcerns about data contamination, memorization, and escalating dataset\ncreation costs persist. We propose a debate-driven evaluation paradigm that\ntransforms any existing QA dataset into structured adversarial debates--where\none model is given the official answer to defend, and another constructs and\ndefends an alternative answer--adjudicated by a judge model blind to the\ncorrect solution. By forcing multi-round argumentation, this approach\nsubstantially increases difficulty while penalizing shallow memorization, yet\nreuses QA items to reduce curation overhead. We make two main contributions:\n(1) an evaluation pipeline to systematically convert QA tasks into debate-based\nassessments, and (2) a public benchmark that demonstrates our paradigm's\neffectiveness on a subset of MMLU-Pro questions, complete with standardized\nprotocols and reference models. Empirical results validate the robustness of\nthe method and its effectiveness against data contamination--a Llama 3.1 model\nfine-tuned on test questions showed dramatic accuracy improvements (50% -> 82%)\nbut performed worse in debates. Results also show that even weaker judges can\nreliably differentiate stronger debaters, highlighting how debate-based\nevaluation can scale to future, more capable systems while maintaining a\nfraction of the cost of creating new benchmarks. Overall, our framework\nunderscores that \"pretraining on the test set is no longer all you need,\"\noffering a sustainable path for measuring the genuine reasoning ability of\nadvanced language models.\n","authors":["Linbo Cao","Jinman Zhao"],"pdf_url":"https://arxiv.org/pdf/2507.17747v1.pdf","comment":"22 pages, 7 figures. Accepted to COLM 2025. Code available at:\n  github.com/l6cao/Debate-Driven-Evaluation"},{"id":"http://arxiv.org/abs/2507.17746v1","updated":"2025-07-23T17:57:55Z","published":"2025-07-23T17:57:55Z","title":"Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains","summary":"  Extending Reinforcement Learning with Verifiable Rewards (RLVR) to real-world\ntasks often requires balancing objective and subjective evaluation criteria.\nHowever, many such tasks lack a single, unambiguous ground truth-making it\ndifficult to define reliable reward signals for post-training language models.\nWhile traditional preference-based methods offer a workaround, they rely on\nopaque reward functions that are difficult to interpret and prone to spurious\ncorrelations. We introduce $\\textbf{Rubrics as Rewards}$ (RaR), a framework\nthat uses structured, checklist-style rubrics as interpretable reward signals\nfor on-policy training with GRPO. Our best RaR method yields up to a $28\\%$\nrelative improvement on HealthBench-1k compared to simple Likert-based\napproaches, while matching or surpassing the performance of reward signals\nderived from expert-written references. By treating rubrics as structured\nreward signals, we show that RaR enables smaller-scale judge models to better\nalign with human preferences and sustain robust performance across model\nscales.\n","authors":["Anisha Gunjal","Anthony Wang","Elaine Lau","Vaskar Nath","Bing Liu","Sean Hendryx"],"pdf_url":"https://arxiv.org/pdf/2507.17746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17745v1","updated":"2025-07-23T17:57:16Z","published":"2025-07-23T17:57:16Z","title":"Ultra3D: Efficient and High-Fidelity 3D Generation with Part Attention","summary":"  Recent advances in sparse voxel representations have significantly improved\nthe quality of 3D content generation, enabling high-resolution modeling with\nfine-grained geometry. However, existing frameworks suffer from severe\ncomputational inefficiencies due to the quadratic complexity of attention\nmechanisms in their two-stage diffusion pipelines. In this work, we propose\nUltra3D, an efficient 3D generation framework that significantly accelerates\nsparse voxel modeling without compromising quality. Our method leverages the\ncompact VecSet representation to efficiently generate a coarse object layout in\nthe first stage, reducing token count and accelerating voxel coordinate\nprediction. To refine per-voxel latent features in the second stage, we\nintroduce Part Attention, a geometry-aware localized attention mechanism that\nrestricts attention computation within semantically consistent part regions.\nThis design preserves structural continuity while avoiding unnecessary global\nattention, achieving up to 6.7x speed-up in latent generation. To support this\nmechanism, we construct a scalable part annotation pipeline that converts raw\nmeshes into part-labeled sparse voxels. Extensive experiments demonstrate that\nUltra3D supports high-resolution 3D generation at 1024 resolution and achieves\nstate-of-the-art performance in both visual fidelity and user preference.\n","authors":["Yiwen Chen","Zhihao Li","Yikai Wang","Hu Zhang","Qin Li","Chi Zhang","Guosheng Lin"],"pdf_url":"https://arxiv.org/pdf/2507.17745v1.pdf","comment":"Project Page: https://buaacyw.github.io/ultra3d/"},{"id":"http://arxiv.org/abs/2507.17744v1","updated":"2025-07-23T17:57:09Z","published":"2025-07-23T17:57:09Z","title":"Yume: An Interactive World Generation Model","summary":"  Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/.\n","authors":["Xiaofeng Mao","Shaoheng Lin","Zhen Li","Chuanhao Li","Wenshuo Peng","Tong He","Jiangmiao Pang","Mingmin Chi","Yu Qiao","Kaipeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.17744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.01939v2","updated":"2025-07-23T17:47:04Z","published":"2025-07-02T17:49:52Z","title":"SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars","summary":"  In recent years, large language models (LLMs) have transformed natural\nlanguage understanding through vast datasets and large-scale parameterization.\nInspired by this success, we present SpecCLIP, a foundation model framework\nthat extends LLM-inspired methodologies to stellar spectral analysis. Stellar\nspectra, akin to structured language, encode rich physical and chemical\ninformation about stars. By training foundation models on large-scale spectral\ndatasets, our goal is to learn robust and informative embeddings that support\ndiverse downstream applications. As a proof of concept, SpecCLIP involves\npre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed\nby contrastive alignment using the CLIP (Contrastive Language-Image\nPre-training) framework, adapted to associate spectra from different\ninstruments. This alignment is complemented by auxiliary decoders that preserve\nspectrum-specific information and enable translation (prediction) between\nspectral types, with the former achieved by maximizing mutual information\nbetween embeddings and input spectra. The result is a cross-spectrum framework\nenabling intrinsic calibration and flexible applications across instruments. We\ndemonstrate that fine-tuning these models on moderate-sized labeled datasets\nimproves adaptability to tasks such as stellar-parameter estimation and\nchemical-abundance determination. SpecCLIP also enhances the accuracy and\nprecision of parameter estimates benchmarked against external survey data.\nAdditionally, its similarity search and cross-spectrum prediction capabilities\noffer potential for anomaly detection. Our results suggest that contrastively\ntrained foundation models enriched with spectrum-aware decoders can advance\nprecision stellar spectroscopy.\n","authors":["Xiaosheng Zhao","Yang Huang","Guirong Xue","Xiao Kong","Jifeng Liu","Xiaoyu Tang","Timothy C. Beers","Yuan-Sen Ting","A-Li Luo"],"pdf_url":"https://arxiv.org/pdf/2507.01939v2.pdf","comment":"27 pages, 8 figures, 5 tables. Updated with minor corrections to flux\n  normalization, and to related tables and figures. Submitted to AAS Journals.\n  Comments welcome"},{"id":"http://arxiv.org/abs/2507.17731v1","updated":"2025-07-23T17:44:29Z","published":"2025-07-23T17:44:29Z","title":"Flow Matching Meets Biology and Life Science: A Survey","summary":"  Over the past decade, advances in generative modeling, such as generative\nadversarial networks, masked autoencoders, and diffusion models, have\nsignificantly transformed biological research and discovery, enabling\nbreakthroughs in molecule design, protein generation, drug discovery, and\nbeyond. At the same time, biological applications have served as valuable\ntestbeds for evaluating the capabilities of generative models. Recently, flow\nmatching has emerged as a powerful and efficient alternative to diffusion-based\ngenerative modeling, with growing interest in its application to problems in\nbiology and life sciences. This paper presents the first comprehensive survey\nof recent developments in flow matching and its applications in biological\ndomains. We begin by systematically reviewing the foundations and variants of\nflow matching, and then categorize its applications into three major areas:\nbiological sequence modeling, molecule generation and design, and peptide and\nprotein generation. For each, we provide an in-depth review of recent progress.\nWe also summarize commonly used datasets and software tools, and conclude with\na discussion of potential future directions. The corresponding curated\nresources are available at\nhttps://github.com/Violet24K/Awesome-Flow-Matching-Meets-Biology.\n","authors":["Zihao Li","Zhichen Zeng","Xiao Lin","Feihao Fang","Yanru Qu","Zhe Xu","Zhining Liu","Xuying Ning","Tianxin Wei","Ge Liu","Hanghang Tong","Jingrui He"],"pdf_url":"https://arxiv.org/pdf/2507.17731v1.pdf","comment":"Preprint, 27 pages"},{"id":"http://arxiv.org/abs/2507.17730v1","updated":"2025-07-23T17:44:10Z","published":"2025-07-23T17:44:10Z","title":"Online Submission and Evaluation System Design for Competition\n  Operations","summary":"  Research communities have developed benchmark datasets across domains to\ncompare the performance of algorithms and techniques However, tracking the\nprogress in these research areas is not easy, as publications appear in\ndifferent venues at the same time, and many of them claim to represent the\nstate-of-the-art. To address this, research communities often organise periodic\ncompetitions to evaluate the performance of various algorithms and techniques,\nthereby tracking advancements in the field. However, these competitions pose a\nsignificant operational burden. The organisers must manage and evaluate a large\nvolume of submissions. Furthermore, participants typically develop their\nsolutions in diverse environments, leading to compatibility issues during the\nevaluation of their submissions. This paper presents an online competition\nsystem that automates the submission and evaluation process for a competition.\nThe competition system allows organisers to manage large numbers of submissions\nefficiently, utilising isolated environments to evaluate submissions. This\nsystem has already been used successfully for several competitions, including\nthe Grid-Based Pathfinding Competition and the League of Robot Runners\ncompetition.\n","authors":["Zhe Chen","Daniel Harabor","Ryan Hechnenberger","Nathan R. Sturtevant"],"pdf_url":"https://arxiv.org/pdf/2507.17730v1.pdf","comment":"This work was presented at the Workshop on the International Planning\n  Competition (WIPC 2024)"},{"id":"http://arxiv.org/abs/2505.20424v2","updated":"2025-07-23T17:39:54Z","published":"2025-05-26T18:17:07Z","title":"Robot Operation of Home Appliances by Reading User Manuals","summary":"  Operating home appliances, among the most common tools in every household, is\na critical capability for assistive home robots. This paper presents ApBot, a\nrobot system that operates novel household appliances by \"reading\" their user\nmanuals. ApBot faces multiple challenges: (i) infer goal-conditioned partial\npolicies from their unstructured, textual descriptions in a user manual\ndocument, (ii) ground the policies to the appliance in the physical world, and\n(iii) execute the policies reliably over potentially many steps, despite\ncompounding errors. To tackle these challenges, ApBot constructs a structured,\nsymbolic model of an appliance from its manual, with the help of a large\nvision-language model (VLM). It grounds the symbolic actions visually to\ncontrol panel elements. Finally, ApBot closes the loop by updating the model\nbased on visual feedback. Our experiments show that across a wide range of\nsimulated and real-world appliances, ApBot achieves consistent and\nstatistically significant improvements in task success rate, compared with\nstate-of-the-art large VLMs used directly as control policies. These results\nsuggest that a structured internal representations plays an important role in\nrobust robot operation of home appliances, especially, complex ones.\n","authors":["Jian Zhang","Hanbo Zhang","Anxing Xiao","David Hsu"],"pdf_url":"https://arxiv.org/pdf/2505.20424v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17725v1","updated":"2025-07-23T17:35:48Z","published":"2025-07-23T17:35:48Z","title":"On the Interaction of Compressibility and Adversarial Robustness","summary":"  Modern neural networks are expected to simultaneously satisfy a host of\ndesirable properties: accurate fitting to training data, generalization to\nunseen inputs, parameter and computational efficiency, and robustness to\nadversarial perturbations. While compressibility and robustness have each been\nstudied extensively, a unified understanding of their interaction still remains\nelusive. In this work, we develop a principled framework to analyze how\ndifferent forms of compressibility - such as neuron-level sparsity and spectral\ncompressibility - affect adversarial robustness. We show that these forms of\ncompression can induce a small number of highly sensitive directions in the\nrepresentation space, which adversaries can exploit to construct effective\nperturbations. Our analysis yields a simple yet instructive robustness bound,\nrevealing how neuron and spectral compressibility impact $L_\\infty$ and $L_2$\nrobustness via their effects on the learned representations. Crucially, the\nvulnerabilities we identify arise irrespective of how compression is achieved -\nwhether via regularization, architectural bias, or implicit learning dynamics.\nThrough empirical evaluations across synthetic and realistic tasks, we confirm\nour theoretical predictions, and further demonstrate that these vulnerabilities\npersist under adversarial training and transfer learning, and contribute to the\nemergence of universal adversarial perturbations. Our findings show a\nfundamental tension between structured compressibility and robustness, and\nsuggest new pathways for designing models that are both efficient and secure.\n","authors":["Melih Barsbey","Antônio H. Ribeiro","Umut Şimşekli","Tolga Birdal"],"pdf_url":"https://arxiv.org/pdf/2507.17725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17718v1","updated":"2025-07-23T17:30:14Z","published":"2025-07-23T17:30:14Z","title":"AI Telephone Surveying: Automating Quantitative Data Collection with an\n  AI Interviewer","summary":"  With the rise of voice-enabled artificial intelligence (AI) systems,\nquantitative survey researchers have access to a new data-collection mode: AI\ntelephone surveying. By using AI to conduct phone interviews, researchers can\nscale quantitative studies while balancing the dual goals of human-like\ninteractivity and methodological rigor. Unlike earlier efforts that used\ninteractive voice response (IVR) technology to automate these surveys, voice AI\nenables a more natural and adaptive respondent experience as it is more robust\nto interruptions, corrections, and other idiosyncrasies of human speech.\n  We built and tested an AI system to conduct quantitative surveys based on\nlarge language models (LLM), automatic speech recognition (ASR), and speech\nsynthesis technologies. The system was specifically designed for quantitative\nresearch, and strictly adhered to research best practices like question order\nrandomization, answer order randomization, and exact wording.\n  To validate the system's effectiveness, we deployed it to conduct two pilot\nsurveys with the SSRS Opinion Panel and followed-up with a separate\nhuman-administered survey to assess respondent experiences. We measured three\nkey metrics: the survey completion rates, break-off rates, and respondent\nsatisfaction scores. Our results suggest that shorter instruments and more\nresponsive AI interviewers may contribute to improvements across all three\nmetrics studied.\n","authors":["Danny D. Leybzon","Shreyas Tirumala","Nishant Jain","Summer Gillen","Michael Jackson","Cameron McPhee","Jennifer Schmidt"],"pdf_url":"https://arxiv.org/pdf/2507.17718v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17717v1","updated":"2025-07-23T17:28:31Z","published":"2025-07-23T17:28:31Z","title":"From Feedback to Checklists: Grounded Evaluation of AI-Generated\n  Clinical Notes","summary":"  AI-generated clinical notes are increasingly used in healthcare, but\nevaluating their quality remains a challenge due to high subjectivity and\nlimited scalability of expert review. Existing automated metrics often fail to\nalign with real-world physician preferences. To address this, we propose a\npipeline that systematically distills real user feedback into structured\nchecklists for note evaluation. These checklists are designed to be\ninterpretable, grounded in human feedback, and enforceable by LLM-based\nevaluators. Using deidentified data from over 21,000 clinical encounters,\nprepared in accordance with the HIPAA safe harbor standard, from a deployed AI\nmedical scribe system, we show that our feedback-derived checklist outperforms\nbaseline approaches in our offline evaluations in coverage, diversity, and\npredictive power for human ratings. Extensive experiments confirm the\nchecklist's robustness to quality-degrading perturbations, significant\nalignment with clinician preferences, and practical value as an evaluation\nmethodology. In offline research settings, the checklist can help identify\nnotes likely to fall below our chosen quality thresholds.\n","authors":["Karen Zhou","John Giorgi","Pranav Mani","Peng Xu","Davis Liang","Chenhao Tan"],"pdf_url":"https://arxiv.org/pdf/2507.17717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18079v3","updated":"2025-07-23T17:26:05Z","published":"2025-05-23T16:37:36Z","title":"Deep Video Discovery: Agentic Search with Tool Use for Long-form Video\n  Understanding","summary":"  Long-form video understanding presents significant challenges due to\nextensive temporal-spatial complexity and the difficulty of question answering\nunder such extended contexts. While Large Language Models (LLMs) have\ndemonstrated considerable advancements in video analysis capabilities and long\ncontext handling, they continue to exhibit limitations when processing\ninformation-dense hour-long videos. To overcome such limitations, we propose\nthe Deep Video Discovery agent to leverage an agentic search strategy over\nsegmented video clips. Different from previous video agents manually designing\na rigid workflow, our approach emphasizes the autonomous nature of agents. By\nproviding a set of search-centric tools on multi-granular video database, our\nDVD agent leverages the advanced reasoning capability of LLM to plan on its\ncurrent observation state, strategically selects tools, formulates appropriate\nparameters for actions, and iteratively refines its internal reasoning in light\nof the gathered information. We perform comprehensive evaluation on multiple\nlong video understanding benchmarks that demonstrates the advantage of the\nentire system design. Our DVD agent achieves SOTA performance, significantly\nsurpassing prior works by a large margin on the challenging LVBench dataset.\nComprehensive ablation studies and in-depth tool analyses are also provided,\nyielding insights to further advance intelligent agents tailored for long-form\nvideo understanding tasks. The code has been released in\nhttps://github.com/microsoft/DeepVideoDiscovery.\n","authors":["Xiaoyi Zhang","Zhaoyang Jia","Zongyu Guo","Jiahao Li","Bin Li","Houqiang Li","Yan Lu"],"pdf_url":"https://arxiv.org/pdf/2505.18079v3.pdf","comment":"V3 draft. Under review"},{"id":"http://arxiv.org/abs/2412.14382v3","updated":"2025-07-23T17:09:55Z","published":"2024-12-18T22:32:13Z","title":"Balans: Multi-Armed Bandits-based Adaptive Large Neighborhood Search for\n  Mixed-Integer Programming Problem","summary":"  Mixed-integer programming (MIP) is a powerful paradigm for modeling and\nsolving various important combinatorial optimization problems. Recently,\nlearning-based approaches have shown a potential to speed up MIP solving via\noffline training that then guides important design decisions during the search.\nHowever, a significant drawback of these methods is their heavy reliance on\noffline training, which requires collecting training datasets and\ncomputationally costly training epochs yet offering only limited generalization\nto unseen (larger) instances. In this paper, we propose Balans, an adaptive\nmeta-solver for MIPs with online learning capability that does not require any\nsupervision or apriori training. At its core, Balans is based on adaptive\nlarge-neighborhood search, operating on top of an MIP solver by successive\napplications of destroy and repair neighborhood operators. During the search,\nthe selection among different neighborhood definitions is guided on the fly for\nthe instance at hand via multi-armed bandit algorithms. Our extensive\nexperiments on hard optimization instances show that Balans offers significant\nperformance gains over the default MIP solver, is better than committing to any\nsingle best neighborhood, and improves over the state-of-the-art\nlarge-neighborhood search for MIPs. Finally, we release Balans as a highly\nconfigurable, MIP solver agnostic, open-source software.\n","authors":["Junyang Cai","Serdar Kadioglu","Bistra Dilkina"],"pdf_url":"https://arxiv.org/pdf/2412.14382v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17699v1","updated":"2025-07-23T17:04:20Z","published":"2025-07-23T17:04:20Z","title":"Thinking Isn't an Illusion: Overcoming the Limitations of Reasoning\n  Models via Tool Augmentations","summary":"  Large Reasoning Models (LRMs) have become a central focus in today's large\nlanguage model (LLM) research, where models are designed to output a\nstep-by-step thinking process before arriving at a final answer to handle\ncomplex reasoning tasks. Despite their promise, recent empirical studies (e.g.,\n[Shojaee et al., 2025] from Apple) suggest that this thinking process may not\nactually enhance reasoning ability, where LLMs without explicit reasoning\nactually outperform LRMs on tasks with low or high complexity. In this work, we\nrevisit these findings and investigate whether the limitations of LRMs persist\nwhen tool augmentations are introduced. We incorporate two types of tools,\nPython interpreters and scratchpads, and evaluate three representative LLMs and\ntheir LRM counterparts on Apple's benchmark reasoning puzzles. Our results show\nthat, with proper tool use, LRMs consistently outperform their non-reasoning\ncounterparts across all levels of task complexity. These findings challenge the\nrecent narrative that reasoning is an illusion and highlight the potential of\ntool-augmented LRMs for solving complex problems.\n","authors":["Zhao Song","Song Yue","Jiahao Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.17699v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17695v1","updated":"2025-07-23T17:01:23Z","published":"2025-07-23T17:01:23Z","title":"Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks","summary":"  Large Language Model (LLM)-based autonomous agents are expected to play a\nvital role in the evolution of 6G networks, by empowering real-time\ndecision-making related to management and service provisioning to end-users.\nThis shift facilitates the transition from a specialized intelligence approach,\nwhere artificial intelligence (AI) algorithms handle isolated tasks, to\nartificial general intelligence (AGI)-driven networks, where agents possess\nbroader reasoning capabilities and can manage diverse network functions. In\nthis paper, we introduce a novel agentic paradigm that combines LLMs with\nreal-time optimization algorithms towards Trustworthy AI, defined as symbiotic\nagents. Optimizers at the LLM's input-level provide bounded uncertainty\nsteering for numerically precise tasks, whereas output-level optimizers\nsupervised by the LLM enable adaptive real-time control. We design and\nimplement two novel agent types including: (i) Radio Access Network optimizers,\nand (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We\nfurther propose an end-to-end architecture for AGI networks and evaluate it on\na 5G testbed capturing channel fluctuations from moving vehicles. Results show\nthat symbiotic agents reduce decision errors fivefold compared to standalone\nLLM-based agents, while smaller language models (SLM) achieve similar accuracy\nwith a 99.9% reduction in GPU resource overhead and in near-real-time loops of\n82 ms. A multi-agent demonstration for collaborative RAN on the real-world\ntestbed highlights significant flexibility in service-level agreement and\nresource allocation, reducing RAN over-utilization by approximately 44%.\nDrawing on our findings and open-source implementations, we introduce the\nsymbiotic paradigm as the foundation for next-generation, AGI-driven\nnetworks-systems designed to remain adaptable, efficient, and trustworthy even\nas LLMs advance.\n","authors":["Ilias Chatzistefanidis","Navid Nikaein"],"pdf_url":"https://arxiv.org/pdf/2507.17695v1.pdf","comment":"Submitted to Computer Networks AI for 6G"},{"id":"http://arxiv.org/abs/2507.17691v1","updated":"2025-07-23T16:57:32Z","published":"2025-07-23T16:57:32Z","title":"CASCADE: LLM-Powered JavaScript Deobfuscator at Google","summary":"  Software obfuscation, particularly prevalent in JavaScript, hinders code\ncomprehension and analysis, posing significant challenges to software testing,\nstatic analysis, and malware detection. This paper introduces CASCADE, a novel\nhybrid approach that integrates the advanced coding capabilities of Gemini with\nthe deterministic transformation capabilities of a compiler Intermediate\nRepresentation (IR), specifically JavaScript IR (JSIR). By employing Gemini to\nidentify critical prelude functions, the foundational components underlying the\nmost prevalent obfuscation techniques, and leveraging JSIR for subsequent code\ntransformations, CASCADE effectively recovers semantic elements like original\nstrings and API names, and reveals original program behaviors. This method\novercomes limitations of existing static and dynamic deobfuscation techniques,\neliminating hundreds to thousands of hardcoded rules while achieving\nreliability and flexibility. CASCADE is already deployed in Google's production\nenvironment, demonstrating substantial improvements in JavaScript deobfuscation\nefficiency and reducing reverse engineering efforts.\n","authors":["Shan Jiang","Pranoy Kovuri","David Tao","Zhixun Tan"],"pdf_url":"https://arxiv.org/pdf/2507.17691v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15606v2","updated":"2025-07-23T16:48:01Z","published":"2025-06-18T16:30:02Z","title":"LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning","summary":"  Large Language Models (LLMs) have become indispensable in real-world\napplications. However, their widespread adoption raises significant safety\nconcerns, particularly in responding to socially harmful questions. Despite\nsubstantial efforts to improve model safety through alignment, aligned models\ncan still have their safety protections undermined by subsequent fine-tuning -\neven when the additional training data appears benign. In this paper, we\nempirically demonstrate that this vulnerability stems from the sensitivity of\nsafety-critical low-rank subspaces in LLM parameters to fine-tuning. Building\non this insight, we propose a novel training-free method, termed Low-Rank\nExtrapolation (LoX), to enhance safety robustness by extrapolating the safety\nsubspace of an aligned LLM. Our experimental results confirm the effectiveness\nof LoX, demonstrating significant improvements in robustness against both\nbenign and malicious fine-tuning attacks while preserving the model's\nadaptability to new tasks. For instance, LoX leads to 11% to 54% absolute\nreductions in attack success rates (ASR) facing benign or malicious fine-tuning\nattacks. By investigating the ASR landscape of parameters, we attribute the\nsuccess of LoX to that the extrapolation moves LLM parameters to a flatter\nzone, thereby less sensitive to perturbations. The code is available at\ngithub.com/VITA-Group/LoX.\n","authors":["Gabriel J. Perin","Runjin Chen","Xuxi Chen","Nina S. T. Hirata","Zhangyang Wang","Junyuan Hong"],"pdf_url":"https://arxiv.org/pdf/2506.15606v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02371v2","updated":"2025-07-23T16:44:22Z","published":"2025-02-04T14:52:10Z","title":"RAPID-Net: Accurate Pocket Identification for Binding-Site-Agnostic\n  Docking","summary":"  Accurate identification of druggable pockets and their features is essential\nfor structure-based drug design and effective downstream docking. Here, we\npresent RAPID-Net, a deep learning-based algorithm designed for the accurate\nprediction of binding pockets and seamless integration with docking pipelines.\nOn the PoseBusters benchmark, RAPID-Net-guided AutoDock Vina achieves 54.9% of\nTop-1 poses with RMSD < 2 A and satisfying the PoseBusters chemical-validity\ncriterion, compared to 49.1% for DiffBindFR. On the most challenging time split\nof PoseBusters aiming to assess generalization ability (structures submitted\nafter September 30, 2021), RAPID-Net-guided AutoDock Vina achieves 53.1% of\nTop-1 poses with RMSD < 2 A and PB-valid, versus 59.5% for AlphaFold 3.\nNotably, in 92.2% of cases, RAPID-Net-guided Vina samples at least one pose\nwith RMSD < 2 A (regardless of its rank), indicating that pose ranking, rather\nthan sampling, is the primary accuracy bottleneck. The lightweight inference,\nscalability, and competitive accuracy of RAPID-Net position it as a viable\noption for large-scale virtual screening campaigns. Across diverse benchmark\ndatasets, RAPID-Net outperforms other pocket prediction tools, including\nPUResNet and Kalasanty, in both docking accuracy and pocket-ligand intersection\nrates. Furthermore, we demonstrate the potential of RAPID-Net to accelerate the\ndevelopment of novel therapeutics by highlighting its performance on\npharmacologically relevant targets. RAPID-Net accurately identifies distal\nfunctional sites, offering new opportunities for allosteric inhibitor design.\nIn the case of the RNA-dependent RNA polymerase of SARS-CoV-2, RAPID-Net\nuncovers a wider array of potential binding pockets than existing predictors,\nwhich typically annotate only the orthosteric pocket and overlook secondary\ncavities.\n","authors":["Yaroslav Balytskyi","Inna Hubenko","Alina Balytska","Christopher V. Kelly"],"pdf_url":"https://arxiv.org/pdf/2502.02371v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17680v1","updated":"2025-07-23T16:42:51Z","published":"2025-07-23T16:42:51Z","title":"Simulating multiple human perspectives in socio-ecological systems using\n  large language models","summary":"  Understanding socio-ecological systems requires insights from diverse\nstakeholder perspectives, which are often hard to access. To enable\nalternative, simulation-based exploration of different stakeholder\nperspectives, we develop the HoPeS (Human-Oriented Perspective Shifting)\nmodelling framework. HoPeS employs agents powered by large language models\n(LLMs) to represent various stakeholders; users can step into the agent roles\nto experience perspectival differences. A simulation protocol serves as a\n\"scaffold\" to streamline multiple perspective-taking simulations, supporting\nusers in reflecting on, transitioning between, and integrating across\nperspectives. A prototype system is developed to demonstrate HoPeS in the\ncontext of institutional dynamics and land use change, enabling both\nnarrative-driven and numerical experiments. In an illustrative experiment, a\nuser successively adopts the perspectives of a system observer and a researcher\n- a role that analyses data from the embedded land use model to inform\nevidence-based decision-making for other LLM agents representing various\ninstitutions. Despite the user's effort to recommend technically sound\npolicies, discrepancies persist between the policy recommendation and\nimplementation due to stakeholders' competing advocacies, mirroring real-world\nmisalignment between researcher and policymaker perspectives. The user's\nreflection highlights the subjective feelings of frustration and disappointment\nas a researcher, especially due to the challenge of maintaining political\nneutrality while attempting to gain political influence. Despite this, the user\nexhibits high motivation to experiment with alternative narrative framing\nstrategies, suggesting the system's potential in exploring different\nperspectives. Further system and protocol refinement are likely to enable new\nforms of interdisciplinary collaboration in socio-ecological simulations.\n","authors":["Yongchao Zeng","Calum Brown","Ioannis Kyriakou","Ronja Hotz","Mark Rounsevell"],"pdf_url":"https://arxiv.org/pdf/2507.17680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17668v1","updated":"2025-07-23T16:31:38Z","published":"2025-07-23T16:31:38Z","title":"How Should We Meta-Learn Reinforcement Learning Algorithms?","summary":"  The process of meta-learning algorithms from data, instead of relying on\nmanual design, is growing in popularity as a paradigm for improving the\nperformance of machine learning systems. Meta-learning shows particular promise\nfor reinforcement learning (RL), where algorithms are often adapted from\nsupervised or unsupervised learning despite their suboptimality for RL.\nHowever, until now there has been a severe lack of comparison between different\nmeta-learning algorithms, such as using evolution to optimise over black-box\nfunctions or LLMs to propose code. In this paper, we carry out this empirical\ncomparison of the different approaches when applied to a range of meta-learned\nalgorithms which target different parts of the RL pipeline. In addition to\nmeta-train and meta-test performance, we also investigate factors including the\ninterpretability, sample cost and train time for each meta-learning algorithm.\nBased on these findings, we propose several guidelines for meta-learning new RL\nalgorithms which will help ensure that future learned algorithms are as\nperformant as possible.\n","authors":["Alexander David Goldie","Zilin Wang","Jakob Nicolaus Foerster","Shimon Whiteson"],"pdf_url":"https://arxiv.org/pdf/2507.17668v1.pdf","comment":"Accepted paper at Reinforcement Learning Conference (RLC) 2025"},{"id":"http://arxiv.org/abs/2501.13926v2","updated":"2025-07-23T16:09:10Z","published":"2025-01-23T18:59:43Z","title":"Can We Generate Images with CoT? Let's Verify and Reinforce Image\n  Generation Step by Step","summary":"  Chain-of-Thought (CoT) reasoning has been extensively explored in large\nmodels to tackle complex understanding tasks. However, it still remains an open\nquestion whether such strategies can be applied to verifying and reinforcing\nimage generation scenarios. In this paper, we provide the first comprehensive\ninvestigation of the potential of CoT reasoning to enhance autoregressive image\ngeneration. We focus on three techniques: scaling test-time computation for\nverification, aligning model preferences with Direct Preference Optimization\n(DPO), and integrating these techniques for complementary effects. Our results\ndemonstrate that these approaches can be effectively adapted and combined to\nsignificantly improve image generation performance. Furthermore, given the\npivotal role of reward models in our findings, we propose the Potential\nAssessment Reward Model (PARM) and PARM++, specialized for autoregressive image\ngeneration. PARM adaptively assesses each generation step through a potential\nassessment approach, merging the strengths of existing reward models, and\nPARM++ further introduces a reflection mechanism to self-correct the generated\nunsatisfactory image, which is the first to incorporate reflection in\nautoregressive image generation. Using our investigated reasoning strategies,\nwe enhance a baseline model, Show-o, to achieve superior results, with a\nsignificant +24% improvement on the GenEval benchmark, surpassing Stable\nDiffusion 3 by +15%. We hope our study provides unique insights and paves a new\npath for integrating CoT reasoning with autoregressive image generation. Code\nand models are released at https://github.com/ZiyuGuo99/Image-Generation-CoT\n","authors":["Ziyu Guo","Renrui Zhang","Chengzhuo Tong","Zhizheng Zhao","Rui Huang","Haoquan Zhang","Manyuan Zhang","Jiaming Liu","Shanghang Zhang","Peng Gao","Hongsheng Li","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2501.13926v2.pdf","comment":"Journal Version. Code and models are released at\n  https://github.com/ZiyuGuo99/Image-Generation-CoT"},{"id":"http://arxiv.org/abs/2403.14459v2","updated":"2025-07-23T15:48:23Z","published":"2024-03-21T15:06:14Z","title":"Multi-Level Explanations for Generative Language Models","summary":"  Despite the increasing use of large language models (LLMs) for\ncontext-grounded tasks like summarization and question-answering, understanding\nwhat makes an LLM produce a certain response is challenging. We propose\nMulti-Level Explanations for Generative Language Models (MExGen), a technique\nto provide explanations for context-grounded text generation. MExGen assigns\nscores to parts of the context to quantify their influence on the model's\noutput. It extends attribution methods like LIME and SHAP to LLMs used in\ncontext-grounded tasks where (1) inference cost is high, (2) input text is\nlong, and (3) the output is text. We conduct a systematic evaluation, both\nautomated and human, of perturbation-based attribution methods for\nsummarization and question answering. The results show that our framework can\nprovide more faithful explanations of generated output than available\nalternatives, including LLM self-explanations. We open-source code for MExGen\nas part of the ICX360 toolkit: https://github$.$com/IBM/ICX360.\n","authors":["Lucas Monteiro Paes","Dennis Wei","Hyo Jin Do","Hendrik Strobelt","Ronny Luss","Amit Dhurandhar","Manish Nagireddy","Karthikeyan Natesan Ramamurthy","Prasanna Sattigeri","Werner Geyer","Soumya Ghosh"],"pdf_url":"https://arxiv.org/pdf/2403.14459v2.pdf","comment":"Accepted as an oral presentation at ACL 2025. Code available at\n  https://github.com/IBM/ICX360"},{"id":"http://arxiv.org/abs/2507.17616v1","updated":"2025-07-23T15:47:34Z","published":"2025-07-23T15:47:34Z","title":"Vision Transformer attention alignment with human visual perception in\n  aesthetic object evaluation","summary":"  Visual attention mechanisms play a crucial role in human perception and\naesthetic evaluation. Recent advances in Vision Transformers (ViTs) have\ndemonstrated remarkable capabilities in computer vision tasks, yet their\nalignment with human visual attention patterns remains underexplored,\nparticularly in aesthetic contexts. This study investigates the correlation\nbetween human visual attention and ViT attention mechanisms when evaluating\nhandcrafted objects. We conducted an eye-tracking experiment with 30\nparticipants (9 female, 21 male, mean age 24.6 years) who viewed 20 artisanal\nobjects comprising basketry bags and ginger jars. Using a Pupil Labs\neye-tracker, we recorded gaze patterns and generated heat maps representing\nhuman visual attention. Simultaneously, we analyzed the same objects using a\npre-trained ViT model with DINO (Self-DIstillation with NO Labels), extracting\nattention maps from each of the 12 attention heads. We compared human and ViT\nattention distributions using Kullback-Leibler divergence across varying\nGaussian parameters (sigma=0.1 to 3.0). Statistical analysis revealed optimal\ncorrelation at sigma=2.4 +-0.03, with attention head #12 showing the strongest\nalignment with human visual patterns. Significant differences were found\nbetween attention heads, with heads #7 and #9 demonstrating the greatest\ndivergence from human attention (p< 0.05, Tukey HSD test). Results indicate\nthat while ViTs exhibit more global attention patterns compared to human focal\nattention, certain attention heads can approximate human visual behavior,\nparticularly for specific object features like buckles in basketry items. These\nfindings suggest potential applications of ViT attention mechanisms in product\ndesign and aesthetic evaluation, while highlighting fundamental differences in\nattention strategies between human perception and current AI models.\n","authors":["Miguel Carrasco","César González-Martín","José Aranda","Luis Oliveros"],"pdf_url":"https://arxiv.org/pdf/2507.17616v1.pdf","comment":"25 pages, 15 figures"},{"id":"http://arxiv.org/abs/2404.16417v2","updated":"2025-07-23T15:23:03Z","published":"2024-04-25T08:49:29Z","title":"Constructing Optimal Noise Channels for Enhanced Robustness in Quantum\n  Machine Learning","summary":"  With the rapid advancement of Quantum Machine Learning (QML), the critical\nneed to enhance security measures against adversarial attacks and protect QML\nmodels becomes increasingly evident. In this work, we outline the connection\nbetween quantum noise channels and differential privacy (DP), by constructing a\nfamily of noise channels which are inherently $\\epsilon$-DP: $(\\alpha,\n\\gamma)$-channels. Through this approach, we successfully replicate the\n$\\epsilon$-DP bounds observed for depolarizing and random rotation channels,\nthereby affirming the broad generality of our framework. Additionally, we use a\nsemi-definite program to construct an optimally robust channel. In a\nsmall-scale experimental evaluation, we demonstrate the benefits of using our\noptimal noise channel over depolarizing noise, particularly in enhancing\nadversarial accuracy. Moreover, we assess how the variables $\\alpha$ and\n$\\gamma$ affect the certifiable robustness and investigate how different\nencoding methods impact the classifier's robustness.\n","authors":["David Winderl","Nicola Franco","Jeanette Miriam Lorenz"],"pdf_url":"https://arxiv.org/pdf/2404.16417v2.pdf","comment":"QML technical track at IEEE QCE 2025"},{"id":"http://arxiv.org/abs/2507.11588v2","updated":"2025-07-23T15:22:26Z","published":"2025-07-15T14:47:01Z","title":"SToFM: a Multi-scale Foundation Model for Spatial Transcriptomics","summary":"  Spatial Transcriptomics (ST) technologies provide biologists with rich\ninsights into single-cell biology by preserving spatial context of cells.\nBuilding foundational models for ST can significantly enhance the analysis of\nvast and complex data sources, unlocking new perspectives on the intricacies of\nbiological tissues. However, modeling ST data is inherently challenging due to\nthe need to extract multi-scale information from tissue slices containing vast\nnumbers of cells. This process requires integrating macro-scale tissue\nmorphology, micro-scale cellular microenvironment, and gene-scale gene\nexpression profile. To address this challenge, we propose SToFM, a multi-scale\nSpatial Transcriptomics Foundation Model. SToFM first performs multi-scale\ninformation extraction on each ST slice, to construct a set of ST sub-slices\nthat aggregate macro-, micro- and gene-scale information. Then an SE(2)\nTransformer is used to obtain high-quality cell representations from the\nsub-slices. Additionally, we construct \\textbf{SToCorpus-88M}, the largest\nhigh-resolution spatial transcriptomics corpus for pretraining. SToFM achieves\noutstanding performance on a variety of downstream tasks, such as tissue region\nsemantic segmentation and cell type annotation, demonstrating its comprehensive\nunderstanding of ST data through capturing and integrating multi-scale\ninformation.\n","authors":["Suyuan Zhao","Yizhen Luo","Ganbo Yang","Yan Zhong","Hao Zhou","Zaiqing Nie"],"pdf_url":"https://arxiv.org/pdf/2507.11588v2.pdf","comment":"Accpeted by ICML 2025"},{"id":"http://arxiv.org/abs/2507.17580v1","updated":"2025-07-23T15:14:53Z","published":"2025-07-23T15:14:53Z","title":"Enhancing Quantum Federated Learning with Fisher Information-Based\n  Optimization","summary":"  Federated Learning (FL) has become increasingly popular across different\nsectors, offering a way for clients to work together to train a global model\nwithout sharing sensitive data. It involves multiple rounds of communication\nbetween the global model and participating clients, which introduces several\nchallenges like high communication costs, heterogeneous client data, prolonged\nprocessing times, and increased vulnerability to privacy threats. In recent\nyears, the convergence of federated learning and parameterized quantum circuits\nhas sparked significant research interest, with promising implications for\nfields such as healthcare and finance. By enabling decentralized training of\nquantum models, it allows clients or institutions to collaboratively enhance\nmodel performance and outcomes while preserving data privacy. Recognizing that\nFisher information can quantify the amount of information that a quantum state\ncarries under parameter changes, thereby providing insight into its geometric\nand statistical properties. We intend to leverage this property to address the\naforementioned challenges. In this work, we propose a Quantum Federated\nLearning (QFL) algorithm that makes use of the Fisher information computed on\nlocal client models, with data distributed across heterogeneous partitions.\nThis approach identifies the critical parameters that significantly influence\nthe quantum model's performance, ensuring they are preserved during the\naggregation process. Our research assessed the effectiveness and feasibility of\nQFL by comparing its performance against other variants, and exploring the\nbenefits of incorporating Fisher information in QFL settings. Experimental\nresults on ADNI and MNIST datasets demonstrate the effectiveness of our\napproach in achieving better performance and robustness against the quantum\nfederated averaging method.\n","authors":["Amandeep Singh Bhatia","Sabre Kais"],"pdf_url":"https://arxiv.org/pdf/2507.17580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.19550v4","updated":"2025-07-23T15:09:33Z","published":"2025-05-26T06:13:15Z","title":"Turing Test 2.0: The General Intelligence Threshold","summary":"  With the rise of artificial intelligence (A.I.) and large language models\nlike ChatGPT, a new race for achieving artificial general intelligence (A.G.I)\nhas started. While many speculate how and when A.I. will achieve A.G.I., there\nis no clear agreement on how A.G.I. can be detected in A.I. models, even when\npopular tools like the Turing test (and its modern variations) are used to\nmeasure their intelligence. In this work, we discuss why traditional methods\nlike the Turing test do not suffice for measuring or detecting A.G.I. and\nprovide a new, practical method that can be used to decide if a system\n(computer or any other) has reached or surpassed A.G.I. To achieve this, we\nmake two new contributions. First, we present a clear definition for general\nintelligence (G.I.) and set a G.I. Threshold (G.I.T.) that can be used to\ndistinguish between systems that achieve A.G.I. and systems that do not.\nSecond, we present a new framework on how to construct tests that can detect if\na system has achieved G.I. in a simple, comprehensive, and clear-cut fail/pass\nway. We call this novel framework the Turing test 2.0. We then demonstrate\nreal-life examples of applying tests that follow our Turing test 2.0 framework\non modern A.I. models.\n","authors":["Georgios Mappouras"],"pdf_url":"https://arxiv.org/pdf/2505.19550v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.04224v2","updated":"2025-07-23T15:08:40Z","published":"2025-07-06T03:28:24Z","title":"Fairness Evaluation of Large Language Models in Academic Library\n  Reference Services","summary":"  As libraries explore large language models (LLMs) for use in virtual\nreference services, a key question arises: Can LLMs serve all users equitably,\nregardless of demographics or social status? While they offer great potential\nfor scalable support, LLMs may also reproduce societal biases embedded in their\ntraining data, risking the integrity of libraries' commitment to equitable\nservice. To address this concern, we evaluate whether LLMs differentiate\nresponses across user identities by prompting six state-of-the-art LLMs to\nassist patrons differing in sex, race/ethnicity, and institutional role. We\nfound no evidence of differentiation by race or ethnicity, and only minor\nevidence of stereotypical bias against women in one model. LLMs demonstrated\nnuanced accommodation of institutional roles through the use of linguistic\nchoices related to formality, politeness, and domain-specific vocabularies,\nreflecting professional norms rather than discriminatory treatment. These\nfindings suggest that current LLMs show a promising degree of readiness to\nsupport equitable and contextually appropriate communication in academic\nlibrary reference services.\n","authors":["Haining Wang","Jason Clark","Yueru Yan","Star Bradley","Ruiyang Chen","Yiqiong Zhang","Hengyi Fu","Zuoyu Tian"],"pdf_url":"https://arxiv.org/pdf/2507.04224v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.14000v3","updated":"2025-07-23T15:07:06Z","published":"2025-07-18T15:14:56Z","title":"Photonic Fabric Platform for AI Accelerators","summary":"  This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM\n(PFA), a photonic-enabled switch and memory subsystem that delivers low\nlatency, high bandwidth, and low per-bit energy. By integrating high-bandwidth\nHBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D\nelectro-optical system-in-package, the PFA offers up to 32 TB of shared memory\nalongside 115 Tbps of all-to-all digital switching. The Photonic FabricTM\nenables distributed AI training and inference to execute parallelism strategies\nmore efficiently. The Photonic Fabric removes the silicon beachfront constraint\nthat limits the fixed memory-to-compute ratio observed in virtually all current\nXPU accelerator designs. Replacing a local HBM stack on an XPU with a chiplet\nthat connects to the Photonic Fabric increases its memory capacity and\ncorrespondingly its memory bandwidth by offering a flexible path to scaling\nwell beyond the limitations of on-package HBM alone. We introduce CelestiSim, a\nlightweight analytical simulator validated on NVIDIA H100 and H200 systems. It\nis used to evaluate the performance of LLM reference and energy savings on PFA,\nwithout any significant change to the GPU core design. With the PFA, the\nsimulation results show that up to 3.66x throughput and 1.40x latency\nimprovements in LLM inference at 405B parameters, up to 7.04x throughput and\n1.41x latency improvements at 1T parameters, and 60-90% energy savings in data\nmovement for heavy collective operations in all LLM training scenarios. While\nthese results are shown for NVIDIA GPUs, they can be applied similarly to other\nAI accelerator designs (XPUs) that share the same fundamental limitation of\nfixed memory to compute.\n","authors":["Jing Ding","Trung Diep"],"pdf_url":"https://arxiv.org/pdf/2507.14000v3.pdf","comment":"12 pages, 14 figures, 5 tables"},{"id":"http://arxiv.org/abs/2505.10016v2","updated":"2025-07-23T15:01:56Z","published":"2025-05-15T06:58:45Z","title":"Application of YOLOv8 in monocular downward multiple Car Target\n  detection","summary":"  Autonomous driving technology is progressively transforming traditional car\ndriving methods, marking a significant milestone in modern transportation.\nObject detection serves as a cornerstone of autonomous systems, playing a vital\nrole in enhancing driving safety, enabling autonomous functionality, improving\ntraffic efficiency, and facilitating effective emergency responses. However,\ncurrent technologies such as radar for environmental perception, cameras for\nroad perception, and vehicle sensor networks face notable challenges, including\nhigh costs, vulnerability to weather and lighting conditions, and limited\nresolution.To address these limitations, this paper presents an improved\nautonomous target detection network based on YOLOv8. By integrating structural\nreparameterization technology, a bidirectional pyramid structure network model,\nand a novel detection pipeline into the YOLOv8 framework, the proposed approach\nachieves highly efficient and precise detection of multi-scale, small, and\nremote objects. Experimental results demonstrate that the enhanced model can\neffectively detect both large and small objects with a detection accuracy of\n65%, showcasing significant advancements over traditional methods.This improved\nmodel holds substantial potential for real-world applications and is\nwell-suited for autonomous driving competitions, such as the Formula Student\nAutonomous China (FSAC), particularly excelling in scenarios involving\nsingle-target and small-object detection.\n","authors":["Shijie Lyu"],"pdf_url":"https://arxiv.org/pdf/2505.10016v2.pdf","comment":"This submission included authors who did not consent to the\n  submission. The paper is being withdrawn until authorship issues are resolved"},{"id":"http://arxiv.org/abs/2505.10027v2","updated":"2025-07-23T15:01:44Z","published":"2025-05-15T07:17:03Z","title":"ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model\n  Super-Resolution Reconstruction","summary":"  With the rapid advancement of remote sensing technology, super-resolution\nimage reconstruction is of great research and practical significance. Existing\ndeep learning methods have made progress but still face limitations in handling\ncomplex scenes and preserving image details. This paper proposes a\nreinforcement learning-based latent diffusion model (LDM) fine-tuning method\nfor remote sensing image super-resolution. The method constructs a\nreinforcement learning environment with states, actions, and rewards,\noptimizing decision objectives through proximal policy optimization (PPO)\nduring the reverse denoising process of the LDM model. Experiments on the\nRESISC45 dataset show significant improvements over the baseline model in PSNR,\nSSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11,\nand LPIPS reducing by 0.06-0.10, particularly in structured and complex natural\nscenes. The results demonstrate the method's effectiveness in enhancing\nsuper-resolution quality and adaptability across scenes.\n","authors":["Shijie Lyu"],"pdf_url":"https://arxiv.org/pdf/2505.10027v2.pdf","comment":"This submission included authors who did not consent to the\n  submission. The paper is being withdrawn until authorship issues are resolved"},{"id":"http://arxiv.org/abs/2405.08427v2","updated":"2025-07-23T14:35:12Z","published":"2024-05-14T08:42:49Z","title":"Impact of Stickers on Multimodal Sentiment and Intent in Social Media: A\n  New Task, Dataset and Baseline","summary":"  Stickers are increasingly used in social media to express sentiment and\nintent. Despite their significant impact on sentiment analysis and intent\nrecognition, little research has been conducted in this area. To address this\ngap, we propose a new task: \\textbf{M}ultimodal chat \\textbf{S}entiment\n\\textbf{A}nalysis and \\textbf{I}ntent \\textbf{R}ecognition involving\n\\textbf{S}tickers (MSAIRS). Additionally, we introduce a novel multimodal\ndataset containing Chinese chat records and stickers excerpted from several\nmainstream social media platforms. Our dataset includes paired data with the\nsame text but different stickers, the same sticker but different contexts, and\nvarious stickers consisting of the same images with different texts, allowing\nus to better understand the impact of stickers on chat sentiment and intent. We\nalso propose an effective multimodal joint model, MMSAIR, featuring\ndifferential vector construction and cascaded attention mechanisms for enhanced\nmultimodal fusion. Our experiments demonstrate the necessity and effectiveness\nof jointly modeling sentiment and intent, as they mutually reinforce each\nother's recognition accuracy. MMSAIR significantly outperforms traditional\nmodels and advanced MLLMs, demonstrating the challenge and uniqueness of\nsticker interpretation in social media. Our dataset and code are available on\nhttps://github.com/FakerBoom/MSAIRS-Dataset.\n","authors":["Yuanchen Shi","Biao Ma","Longyin Zhang","Fang Kong"],"pdf_url":"https://arxiv.org/pdf/2405.08427v2.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2507.17539v1","updated":"2025-07-23T14:19:30Z","published":"2025-07-23T14:19:30Z","title":"Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration\n  Through Clinical Cognitive Chain Reasoning","summary":"  Multimodal large language models (MLLMs) demonstrate significant potential in\nthe field of medical diagnosis. However, they face critical challenges in\nspecialized domains such as ophthalmology, particularly the fragmentation of\nannotation granularity and inconsistencies in clinical reasoning logic, which\nhinder precise cross-modal understanding. This paper introduces FundusExpert,\nan ophthalmology-specific MLLM with integrated positioning-diagnosis reasoning\ncapabilities, along with FundusGen, a dataset constructed through the\nintelligent Fundus-Engine system. Fundus-Engine automates localization and\nleverages MLLM-based semantic expansion to integrate global disease\nclassification, local object detection, and fine-grained feature analysis\nwithin a single fundus image. Additionally, by constructing a clinically\naligned cognitive chain, it guides the model to generate interpretable\nreasoning paths. FundusExpert, fine-tuned with instruction data from FundusGen,\nachieves the best performance in ophthalmic question-answering tasks,\nsurpassing the average accuracy of the 40B MedRegA by 26.6%. It also excels in\nzero-shot report generation tasks, achieving a clinical consistency of 77.0%,\nsignificantly outperforming GPT-4o's 47.6%. Furthermore, we reveal a scaling\nlaw between data quality and model capability ($L \\propto N^{0.068}$),\ndemonstrating that the cognitive alignment annotations in FundusGen enhance\ndata utilization efficiency. By integrating region-level localization with\ndiagnostic reasoning chains, our work develops a scalable, clinically-aligned\nMLLM and explores a pathway toward bridging the visual-language gap in specific\nMLLMs. Our project can be found at https://github.com/MeteorElf/FundusExpert.\n","authors":["Xinyao Liu","Diping Song"],"pdf_url":"https://arxiv.org/pdf/2507.17539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.07773v3","updated":"2025-07-23T14:15:28Z","published":"2025-05-12T17:23:34Z","title":"Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for\n  Mathematical Problem Solving","summary":"  Large Language Models (LLMs) often struggle with mathematical reasoning tasks\nrequiring precise, verifiable computation. While Reinforcement Learning (RL)\nfrom outcome-based rewards enhances text-based reasoning, understanding how\nagents autonomously learn to leverage external tools like code execution\nremains crucial. We investigate RL from outcome-based rewards for\nTool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously\ngenerate and execute Python code for mathematical problems without supervised\ntool-use examples. Our central contribution is we demonstrate that as RL\ntraining progresses, key metrics scale predictably. Specifically, we observe\nstrong positive correlations where increased training steps lead to increases\nin the spontaneous code execution frequency, the average response length, and,\ncritically, the final task accuracy. This suggests a quantifiable relationship\nbetween computational effort invested in training and the emergence of\neffective, tool-augmented reasoning strategies. We implement a robust framework\nfeaturing a decoupled code execution environment and validate our findings\nacross standard RL algorithms and frameworks. Experiments show ZeroTIR\nsignificantly surpasses non-tool ZeroRL baselines on challenging math\nbenchmarks. Our findings provide a foundational understanding of how autonomous\ntool use is acquired and scales within Agent RL, offering a reproducible\nbenchmark for future studies. Code is released at\n\\href{https://github.com/yyht/openrlhf_async_pipline}{https://github.com/yyht/openrlhf\\_async\\_pipline}.\n","authors":["Xinji Mai","Haotian Xu","Xing W","Weinong Wang","Jian Hu","Yingying Zhang","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.07773v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17534v1","updated":"2025-07-23T14:13:19Z","published":"2025-07-23T14:13:19Z","title":"Federated Majorize-Minimization: Beyond Parameter Aggregation","summary":"  This paper proposes a unified approach for designing stochastic optimization\nalgorithms that robustly scale to the federated learning setting. Our work\nstudies a class of Majorize-Minimization (MM) problems, which possesses a\nlinearly parameterized family of majorizing surrogate functions. This framework\nencompasses (proximal) gradient-based algorithms for (regularized) smooth\nobjectives, the Expectation Maximization algorithm, and many problems seen as\nvariational surrogate MM. We show that our framework motivates a unifying\nalgorithm called Stochastic Approximation Stochastic Surrogate MM (\\SSMM),\nwhich includes previous stochastic MM procedures as special instances. We then\nextend \\SSMM\\ to the federated setting, while taking into consideration common\nbottlenecks such as data heterogeneity, partial participation, and\ncommunication constraints; this yields \\QSMM. The originality of \\QSMM\\ is to\nlearn locally and then aggregate information characterizing the\n\\textit{surrogate majorizing function}, contrary to classical algorithms which\nlearn and aggregate the \\textit{original parameter}. Finally, to showcase the\nflexibility of this methodology beyond our theoretical setting, we use it to\ndesign an algorithm for computing optimal transport maps in the federated\nsetting.\n","authors":["Aymeric Dieuleveut","Gersende Fort","Mahmoud Hegazy","Hoi-To Wai"],"pdf_url":"https://arxiv.org/pdf/2507.17534v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17526v1","updated":"2025-07-23T14:07:33Z","published":"2025-07-23T14:07:33Z","title":"Integrating Physics-Based and Data-Driven Approaches for Probabilistic\n  Building Energy Modeling","summary":"  Building energy modeling is a key tool for optimizing the performance of\nbuilding energy systems. Historically, a wide spectrum of methods has been\nexplored -- ranging from conventional physics-based models to purely\ndata-driven techniques. Recently, hybrid approaches that combine the strengths\nof both paradigms have gained attention. These include strategies such as\nlearning surrogates for physics-based models, modeling residuals between\nsimulated and observed data, fine-tuning surrogates with real-world\nmeasurements, using physics-based outputs as additional inputs for data-driven\nmodels, and integrating the physics-based output into the loss function the\ndata-driven model. Despite this progress, two significant research gaps remain.\nFirst, most hybrid methods focus on deterministic modeling, often neglecting\nthe inherent uncertainties caused by factors like weather fluctuations and\noccupant behavior. Second, there has been little systematic comparison within a\nprobabilistic modeling framework. This study addresses these gaps by evaluating\nfive representative hybrid approaches for probabilistic building energy\nmodeling, focusing on quantile predictions of building thermodynamics in a\nreal-world case study. Our results highlight two main findings. First, the\nperformance of hybrid approaches varies across different building room types,\nbut residual learning with a Feedforward Neural Network performs best on\naverage. Notably, the residual approach is the only model that produces\nphysically intuitive predictions when applied to out-of-distribution test data.\nSecond, Quantile Conformal Prediction is an effective procedure for calibrating\nquantile predictions in case of indoor temperature modeling.\n","authors":["Leandro Von Krannichfeldt","Kristina Orehounig","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2507.17526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.00358v2","updated":"2025-07-23T14:00:39Z","published":"2025-07-01T01:09:06Z","title":"Data-Driven Exploration for a Class of Continuous-Time Indefinite\n  Linear--Quadratic Reinforcement Learning Problems","summary":"  We study reinforcement learning (RL) for the same class of continuous-time\nstochastic linear--quadratic (LQ) control problems as in\n\\cite{huang2024sublinear}, where volatilities depend on both states and\ncontrols while states are scalar-valued and running control rewards are absent.\nWe propose a model-free, data-driven exploration mechanism that adaptively\nadjusts entropy regularization by the critic and policy variance by the actor.\nUnlike the constant or deterministic exploration schedules employed in\n\\cite{huang2024sublinear}, which require extensive tuning for implementations\nand ignore learning progresses during iterations, our adaptive exploratory\napproach boosts learning efficiency with minimal tuning. Despite its\nflexibility, our method achieves a sublinear regret bound that matches the\nbest-known model-free results for this class of LQ problems, which were\npreviously derived only with fixed exploration schedules. Numerical experiments\ndemonstrate that adaptive explorations accelerate convergence and improve\nregret performance compared to the non-adaptive model-free and model-based\ncounterparts.\n","authors":["Yilie Huang","Xun Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2507.00358v2.pdf","comment":"37 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.13574v2","updated":"2025-07-23T13:55:50Z","published":"2024-03-20T13:14:29Z","title":"Enhancing Sequential Recommender with Large Language Models for Joint\n  Video and Comment Recommendation","summary":"  Nowadays, reading or writing comments on captivating videos has emerged as a\ncritical part of the viewing experience on online video platforms. However,\nexisting recommender systems primarily focus on users' interaction behaviors\nwith videos, neglecting comment content and interaction in user preference\nmodeling. In this paper, we propose a novel recommendation approach called\nLSVCR that utilizes user interaction histories with both videos and comments to\njointly perform personalized video and comment recommendation. Specifically,\nour approach comprises two key components: sequential recommendation (SR) model\nand supplemental large language model (LLM) recommender. The SR model functions\nas the primary recommendation backbone (retained in deployment) of our method\nfor efficient user preference modeling. Concurrently, we employ a LLM as the\nsupplemental recommender (discarded in deployment) to better capture underlying\nuser preferences derived from heterogeneous interaction behaviors. In order to\nintegrate the strengths of the SR model and the supplemental LLM recommender,\nwe introduce a two-stage training paradigm. The first stage, personalized\npreference alignment, aims to align the preference representations from both\ncomponents, thereby enhancing the semantics of the SR model. The second stage,\nrecommendation-oriented fine-tuning, involves fine-tuning the\nalignment-enhanced SR model according to specific objectives. Extensive\nexperiments in both video and comment recommendation tasks demonstrate the\neffectiveness of LSVCR. Moreover, online A/B testing on KuaiShou platform\nverifies the practical benefits of our approach. In particular, we attain a\ncumulative gain of 4.13% in comment watch time.\n","authors":["Bowen Zheng","Zihan Lin","Enze Liu","Chen Yang","Enyang Bai","Cheng Ling","Wayne Xin Zhao","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.13574v2.pdf","comment":"Accepted by RecSys2025"},{"id":"http://arxiv.org/abs/2507.17518v1","updated":"2025-07-23T13:55:35Z","published":"2025-07-23T13:55:35Z","title":"Enabling Cyber Security Education through Digital Twins and Generative\n  AI","summary":"  Digital Twins (DTs) are gaining prominence in cybersecurity for their ability\nto replicate complex IT (Information Technology), OT (Operational Technology),\nand IoT (Internet of Things) infrastructures, allowing for real time\nmonitoring, threat analysis, and system simulation. This study investigates how\nintegrating DTs with penetration testing tools and Large Language Models (LLMs)\ncan enhance cybersecurity education and operational readiness. By simulating\nrealistic cyber environments, this approach offers a practical, interactive\nframework for exploring vulnerabilities and defensive strategies. At the core\nof this research is the Red Team Knife (RTK), a custom penetration testing\ntoolkit aligned with the Cyber Kill Chain model. RTK is designed to guide\nlearners through key phases of cyberattacks, including reconnaissance,\nexploitation, and response within a DT powered ecosystem. The incorporation of\nLarge Language Models (LLMs) further enriches the experience by providing\nintelligent, real-time feedback, natural language threat explanations, and\nadaptive learning support during training exercises. This combined DT LLM\nframework is currently being piloted in academic settings to develop hands on\nskills in vulnerability assessment, threat detection, and security operations.\nInitial findings suggest that the integration significantly improves the\neffectiveness and relevance of cybersecurity training, bridging the gap between\ntheoretical knowledge and real-world application. Ultimately, the research\ndemonstrates how DTs and LLMs together can transform cybersecurity education to\nmeet evolving industry demands.\n","authors":["Vita Santa Barletta","Vito Bavaro","Miriana Calvano","Antonio Curci","Antonio Piccinno","Davide Pio Posa"],"pdf_url":"https://arxiv.org/pdf/2507.17518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17514v1","updated":"2025-07-23T13:51:23Z","published":"2025-07-23T13:51:23Z","title":"TAI Scan Tool: A RAG-Based Tool With Minimalistic Input for Trustworthy\n  AI Self-Assessment","summary":"  This paper introduces the TAI Scan Tool, a RAG-based TAI self-assessment tool\nwith minimalistic input. The current version of the tool supports the legal TAI\nassessment, with a particular emphasis on facilitating compliance with the AI\nAct. It involves a two-step approach with a pre-screening and an assessment\nphase. The assessment output of the system includes insight regarding the\nrisk-level of the AI system according to the AI Act, while at the same time\nretrieving relevant articles to aid with compliance and notify on their\nobligations. Our qualitative evaluation using use-case scenarios yields\npromising results, correctly predicting risk levels while retrieving relevant\narticles across three distinct semantic groups. Furthermore, interpretation of\nresults shows that the tool's reasoning relies on comparison with the setting\nof high-risk systems, a behaviour attributed to their deployment requiring\ncareful consideration, and therefore frequently presented within the AI Act.\n","authors":["Athanasios Davvetas","Xenia Ziouvelou","Ypatia Dami","Alexis Kaponis","Konstantina Giouvanopoulou","Michael Papademas"],"pdf_url":"https://arxiv.org/pdf/2507.17514v1.pdf","comment":"9 pages, 1 figure, 4 tables"},{"id":"http://arxiv.org/abs/2507.17513v1","updated":"2025-07-23T13:51:06Z","published":"2025-07-23T13:51:06Z","title":"HOTA: Hamiltonian framework for Optimal Transport Advection","summary":"  Optimal transport (OT) has become a natural framework for guiding the\nprobability flows. Yet, the majority of recent generative models assume trivial\ngeometry (e.g., Euclidean) and rely on strong density-estimation assumptions,\nyielding trajectories that do not respect the true principles of optimality in\nthe underlying manifold. We present Hamiltonian Optimal Transport Advection\n(HOTA), a Hamilton-Jacobi-Bellman based method that tackles the dual dynamical\nOT problem explicitly through Kantorovich potentials, enabling efficient and\nscalable trajectory optimization. Our approach effectively evades the need for\nexplicit density modeling, performing even when the cost functionals are\nnon-smooth. Empirically, HOTA outperforms all baselines in standard benchmarks,\nas well as in custom datasets with non-differentiable costs, both in terms of\nfeasibility and optimality.\n","authors":["Nazar Buzun","Daniil Shlenskii","Maxim Bobrin","Dmitry V. Dylov"],"pdf_url":"https://arxiv.org/pdf/2507.17513v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17512v1","updated":"2025-07-23T13:51:04Z","published":"2025-07-23T13:51:04Z","title":"Can One Domain Help Others? A Data-Centric Study on Multi-Domain\n  Reasoning via Reinforcement Learning","summary":"  Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful paradigm for enhancing the reasoning capabilities of LLMs. Existing\nresearch has predominantly concentrated on isolated reasoning domains such as\nmathematical problem-solving, coding tasks, or logical reasoning. However, real\nworld reasoning scenarios inherently demand an integrated application of\nmultiple cognitive skills. Despite this, the interplay among these reasoning\nskills under reinforcement learning remains poorly understood. To bridge this\ngap, we present a systematic investigation of multi-domain reasoning within the\nRLVR framework, explicitly focusing on three primary domains: mathematical\nreasoning, code generation, and logical puzzle solving. We conduct a\ncomprehensive study comprising four key components: (1) Leveraging the GRPO\nalgorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the\nmodels' in-domain improvements and cross-domain generalization capabilities\nwhen trained on single-domain datasets. (2) Additionally, we examine the\nintricate interactions including mutual enhancements and conflicts that emerge\nduring combined cross-domain training. (3) To further understand the influence\nof SFT on RL, we also analyze and compare performance differences between base\nand instruct models under identical RL configurations. (4) Furthermore, we\ndelve into critical RL training details, systematically exploring the impacts\nof curriculum learning strategies, variations in reward design, and\nlanguage-specific factors. Through extensive experiments, our results offer\nsignificant insights into the dynamics governing domain interactions, revealing\nkey factors influencing both specialized and generalizable reasoning\nperformance. These findings provide valuable guidance for optimizing RL\nmethodologies to foster comprehensive, multi-domain reasoning capabilities in\nLLMs.\n","authors":["Yu Li","Zhuoshi Pan","Honglin Lin","Mengyuan Sun","Conghui He","Lijun Wu"],"pdf_url":"https://arxiv.org/pdf/2507.17512v1.pdf","comment":"27 pages, 24 figures"},{"id":"http://arxiv.org/abs/2507.17494v1","updated":"2025-07-23T13:23:43Z","published":"2025-07-23T13:23:43Z","title":"To Trust or Not to Trust: On Calibration in ML-based Resource Allocation\n  for Wireless Networks","summary":"  In next-generation communications and networks, machine learning (ML) models\nare expected to deliver not only accurate predictions but also well-calibrated\nconfidence scores that reflect the true likelihood of correct decisions. This\npaper studies the calibration performance of an ML-based outage predictor\nwithin a single-user, multi-resource allocation framework. We first establish\nkey theoretical properties of this system's outage probability (OP) under\nperfect calibration. Importantly, we show that as the number of resources\ngrows, the OP of a perfectly calibrated predictor approaches the expected\noutput conditioned on it being below the classification threshold. In contrast,\nwhen only one resource is available, the system's OP equals the model's overall\nexpected output. We then derive the OP conditions for a perfectly calibrated\npredictor. These findings guide the choice of the classification threshold to\nachieve a desired OP, helping system designers meet specific reliability\nrequirements. We also demonstrate that post-processing calibration cannot\nimprove the system's minimum achievable OP, as it does not introduce new\ninformation about future channel states. Additionally, we show that\nwell-calibrated models are part of a broader class of predictors that\nnecessarily improve OP. In particular, we establish a monotonicity condition\nthat the accuracy-confidence function must satisfy for such improvement to\noccur. To demonstrate these theoretical properties, we conduct a rigorous\nsimulation-based analysis using post-processing calibration techniques: Platt\nscaling and isotonic regression. As part of this framework, the predictor is\ntrained using an outage loss function specifically designed for this system.\nFurthermore, this analysis is performed on Rayleigh fading channels with\ntemporal correlation captured by Clarke's 2D model, which accounts for receiver\nmobility.\n","authors":["Rashika Raina","Nidhi Simmons","David E. Simmons","Michel Daoud Yacoub","Trung Q. Duong"],"pdf_url":"https://arxiv.org/pdf/2507.17494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17493v1","updated":"2025-07-23T13:19:02Z","published":"2025-07-23T13:19:02Z","title":"Automated Hybrid Grounding Using Structural and Data-Driven Heuristics","summary":"  The grounding bottleneck poses one of the key challenges that hinders the\nwidespread adoption of Answer Set Programming in industry. Hybrid Grounding is\na step in alleviating the bottleneck by combining the strength of standard\nbottom-up grounding with recently proposed techniques where rule bodies are\ndecoupled during grounding. However, it has remained unclear when hybrid\ngrounding shall use body-decoupled grounding and when to use standard bottom-up\ngrounding. In this paper, we address this issue by developing automated hybrid\ngrounding: we introduce a splitting algorithm based on data-structural\nheuristics that detects when to use body-decoupled grounding and when standard\ngrounding is beneficial. We base our heuristics on the structure of rules and\nan estimation procedure that incorporates the data of the instance. The\nexperiments conducted on our prototypical implementation demonstrate promising\nresults, which show an improvement on hard-to-ground scenarios, whereas on\nhard-to-solve instances we approach state-of-the-art performance.\n","authors":["Alexander Beiser","Markus Hecher","Stefan Woltran"],"pdf_url":"https://arxiv.org/pdf/2507.17493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17487v1","updated":"2025-07-23T13:10:33Z","published":"2025-07-23T13:10:33Z","title":"CQE under Epistemic Dependencies: Algorithms and Experiments (extended\n  version)","summary":"  We investigate Controlled Query Evaluation (CQE) over ontologies, where\ninformation disclosure is regulated by epistemic dependencies (EDs), a family\nof logical rules recently proposed for the CQE framework. In particular, we\ncombine EDs with the notion of optimal GA censors, i.e. maximal sets of ground\natoms that are entailed by the ontology and can be safely revealed. We focus on\nanswering Boolean unions of conjunctive queries (BUCQs) with respect to the\nintersection of all optimal GA censors - an approach that has been shown in\nother contexts to ensure strong security guarantees with favorable\ncomputational behavior. First, we characterize the security of this\nintersection-based approach and identify a class of EDs (namely, full EDs) for\nwhich it remains safe. Then, for a subclass of EDs and for DL-Lite_R\nontologies, we show that answering BUCQs in the above CQE semantics is in AC^0\nin data complexity by presenting a suitable, detailed first-order rewriting\nalgorithm. Finally, we report on experiments conducted in two different\nevaluation scenarios, showing the practical feasibility of our rewriting\nfunction.\n","authors":["Lorenzo Marconi","Flavia Ricci","Riccardo Rosati"],"pdf_url":"https://arxiv.org/pdf/2507.17487v1.pdf","comment":"Extended version of paper accepted at the 24th International Semantic\n  Web Conference (ISWC 2025)"},{"id":"http://arxiv.org/abs/2507.17486v1","updated":"2025-07-23T13:09:57Z","published":"2025-07-23T13:09:57Z","title":"Unsupervised anomaly detection using Bayesian flow networks: application\n  to brain FDG PET in the context of Alzheimer's disease","summary":"  Unsupervised anomaly detection (UAD) plays a crucial role in neuroimaging for\nidentifying deviations from healthy subject data and thus facilitating the\ndiagnosis of neurological disorders. In this work, we focus on Bayesian flow\nnetworks (BFNs), a novel class of generative models, which have not yet been\napplied to medical imaging or anomaly detection. BFNs combine the strength of\ndiffusion frameworks and Bayesian inference. We introduce AnoBFN, an extension\nof BFNs for UAD, designed to: i) perform conditional image generation under\nhigh levels of spatially correlated noise, and ii) preserve subject specificity\nby incorporating a recursive feedback from the input image throughout the\ngenerative process. We evaluate AnoBFN on the challenging task of Alzheimer's\ndisease-related anomaly detection in FDG PET images. Our approach outperforms\nother state-of-the-art methods based on VAEs (beta-VAE), GANs (f-AnoGAN), and\ndiffusion models (AnoDDPM), demonstrating its effectiveness at detecting\nanomalies while reducing false positive rates.\n","authors":["Hugues Roy","Reuben Dorent","Ninon Burgos"],"pdf_url":"https://arxiv.org/pdf/2507.17486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09068v2","updated":"2025-07-23T13:06:44Z","published":"2025-07-11T23:07:04Z","title":"Infinite Video Understanding","summary":"  The rapid advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have ushered in remarkable progress in video understanding.\nHowever, a fundamental challenge persists: effectively processing and\ncomprehending video content that extends beyond minutes or hours. While recent\nefforts like Video-XL-2 have demonstrated novel architectural solutions for\nextreme efficiency, and advancements in positional encoding such as HoPE and\nVideoRoPE++ aim to improve spatio-temporal understanding over extensive\ncontexts, current state-of-the-art models still encounter significant\ncomputational and memory constraints when faced with the sheer volume of visual\ntokens from lengthy sequences. Furthermore, maintaining temporal coherence,\ntracking complex events, and preserving fine-grained details over extended\nperiods remain formidable hurdles, despite progress in agentic reasoning\nsystems like Deep Video Discovery. This position paper posits that a logical,\nalbeit ambitious, next frontier for multimedia research is Infinite Video\nUnderstanding -- the capability for models to continuously process, understand,\nand reason about video data of arbitrary, potentially never-ending duration. We\nargue that framing Infinite Video Understanding as a blue-sky research\nobjective provides a vital north star for the multimedia, and the wider AI,\nresearch communities, driving innovation in areas such as streaming\narchitectures, persistent memory mechanisms, hierarchical and adaptive\nrepresentations, event-centric reasoning, and novel evaluation paradigms.\nDrawing inspiration from recent work on long/ultra-long video understanding and\nseveral closely related fields, we outline the core challenges and key research\ndirections towards achieving this transformative capability.\n","authors":["Dell Zhang","Xiangyu Chen","Jixiang Luo","Mengxi Jia","Changzhi Sun","Ruilong Ren","Jingren Liu","Hao Sun","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2507.09068v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17482v1","updated":"2025-07-23T13:04:13Z","published":"2025-07-23T13:04:13Z","title":"LTLZinc: a Benchmarking Framework for Continual Learning and\n  Neuro-Symbolic Temporal Reasoning","summary":"  Neuro-symbolic artificial intelligence aims to combine neural architectures\nwith symbolic approaches that can represent knowledge in a human-interpretable\nformalism. Continual learning concerns with agents that expand their knowledge\nover time, improving their skills while avoiding to forget previously learned\nconcepts. Most of the existing approaches for neuro-symbolic artificial\nintelligence are applied to static scenarios only, and the challenging setting\nwhere reasoning along the temporal dimension is necessary has been seldom\nexplored. In this work we introduce LTLZinc, a benchmarking framework that can\nbe used to generate datasets covering a variety of different problems, against\nwhich neuro-symbolic and continual learning methods can be evaluated along the\ntemporal and constraint-driven dimensions. Our framework generates expressive\ntemporal reasoning and continual learning tasks from a linear temporal logic\nspecification over MiniZinc constraints, and arbitrary image classification\ndatasets. Fine-grained annotations allow multiple neural and neuro-symbolic\ntraining settings on the same generated datasets. Experiments on six\nneuro-symbolic sequence classification and four class-continual learning tasks\ngenerated by LTLZinc, demonstrate the challenging nature of temporal learning\nand reasoning, and highlight limitations of current state-of-the-art methods.\nWe release the LTLZinc generator and ten ready-to-use tasks to the\nneuro-symbolic and continual learning communities, in the hope of fostering\nresearch towards unified temporal learning and reasoning frameworks.\n","authors":["Luca Salvatore Lorello","Nikolaos Manginas","Marco Lippi","Stefano Melacci"],"pdf_url":"https://arxiv.org/pdf/2507.17482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12296v3","updated":"2025-07-23T13:04:12Z","published":"2025-01-21T17:03:06Z","title":"RALAD: Bridging the Real-to-Sim Domain Gap in Autonomous Driving with\n  Retrieval-Augmented Learning","summary":"  In the pursuit of robust autonomous driving systems, models trained on\nreal-world datasets often struggle to adapt to new environments, particularly\nwhen confronted with corner cases such as extreme weather conditions.\nCollecting these corner cases in the real world is non-trivial, which\nnecessitates the use of simulators for validation. However,the high\ncomputational cost and the domain gap in data distribution have hindered the\nseamless transition between real and simulated driving scenarios. To tackle\nthis challenge, we propose Retrieval-Augmented Learning for Autonomous Driving\n(RALAD), a novel framework designed to bridge the real-to-sim gap at a low\ncost. RALAD features three primary designs, including (1) domain adaptation via\nan enhanced Optimal Transport (OT) method that accounts for both individual and\ngrouped image distances, (2) a simple and unified framework that can be applied\nto various models, and (3) efficient fine-tuning techniques that freeze the\ncomputationally expensive layers while maintaining robustness. Experimental\nresults demonstrate that RALAD compensates for the performance degradation in\nsimulated environments while maintaining accuracy in real-world scenarios\nacross three different models. Taking Cross View as an example, the mIOU and\nmAP metrics in real-world scenarios remain stable before and after RALAD\nfine-tuning, while in simulated environments,the mIOU and mAP metrics are\nimproved by 10.30% and 12.29%, respectively. Moreover, the re-training cost of\nour approach is reduced by approximately 88.1%. Our code is available at\nhttps://github.com/JiachengZuo/RALAD.git.\n","authors":["Jiacheng Zuo","Haibo Hu","Zikang Zhou","Yufei Cui","Ziquan Liu","Jianping Wang","Nan Guan","Jin Wang","Chun Jason Xue"],"pdf_url":"https://arxiv.org/pdf/2501.12296v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17477v1","updated":"2025-07-23T13:00:00Z","published":"2025-07-23T13:00:00Z","title":"An Uncertainty-Driven Adaptive Self-Alignment Framework for Large\n  Language Models","summary":"  Large Language Models (LLMs) have demonstrated remarkable progress in\ninstruction following and general-purpose reasoning. However, achieving\nhigh-quality alignment with human intent and safety norms without human\nannotations remains a fundamental challenge. In this work, we propose an\nUncertainty-Driven Adaptive Self-Alignment (UDASA) framework designed to\nimprove LLM alignment in a fully automated manner. UDASA first generates\nmultiple responses for each input and quantifies output uncertainty across\nthree dimensions: semantics, factuality, and value alignment. Based on these\nuncertainty scores, the framework constructs preference pairs and categorizes\ntraining samples into three stages, conservative, moderate, and exploratory,\naccording to their uncertainty difference. The model is then optimized\nprogressively across these stages. In addition, we conduct a series of\npreliminary studies to validate the core design assumptions and provide strong\nempirical motivation for the proposed framework. Experimental results show that\nUDASA outperforms existing alignment methods across multiple tasks, including\nharmlessness, helpfulness, truthfulness, and controlled sentiment generation,\nsignificantly improving model performance.\n","authors":["Haoran Sun","Zekun Zhang","Shaoning Zeng"],"pdf_url":"https://arxiv.org/pdf/2507.17477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17476v1","updated":"2025-07-23T12:56:31Z","published":"2025-07-23T12:56:31Z","title":"MultiNRC: A Challenging and Native Multilingual Reasoning Evaluation\n  Benchmark for LLMs","summary":"  Although recent Large Language Models (LLMs) have shown rapid improvement on\nreasoning benchmarks in English, the evaluation of such LLMs' multilingual\nreasoning capability across diverse languages and cultural contexts remains\nlimited. Existing multilingual reasoning benchmarks are typically constructed\nby translating existing English reasoning benchmarks, biasing these benchmarks\ntowards reasoning problems with context in English language/cultures. In this\nwork, we introduce the Multilingual Native Reasoning Challenge (MultiNRC), a\nbenchmark designed to assess LLMs on more than 1,000 native, linguistic and\nculturally grounded reasoning questions written by native speakers in French,\nSpanish, and Chinese. MultiNRC covers four core reasoning categories:\nlanguage-specific linguistic reasoning, wordplay & riddles, cultural/tradition\nreasoning, and math reasoning with cultural relevance. For cultural/tradition\nreasoning and math reasoning with cultural relevance, we also provide English\nequivalent translations of the multilingual questions by manual translation\nfrom native speakers fluent in English. This set of English equivalents can\nprovide a direct comparison of LLM reasoning capacity in other languages vs.\nEnglish on the same reasoning questions. We systematically evaluate current 14\nleading LLMs covering most LLM families on MultiNRC and its English equivalent\nset. The results show that (1) current LLMs are still not good at native\nmultilingual reasoning, with none scoring above 50% on MultiNRC; (2) LLMs\nexhibit distinct strengths and weaknesses in handling linguistic, cultural, and\nlogical reasoning tasks; (3) Most models perform substantially better in math\nreasoning in English compared to in original languages (+10%), indicating\npersistent challenges with culturally grounded knowledge.\n","authors":["Alexander R. Fabbri","Diego Mares","Jorge Flores","Meher Mankikar","Ernesto Hernandez","Dean Lee","Bing Liu","Chen Xing"],"pdf_url":"https://arxiv.org/pdf/2507.17476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17472v1","updated":"2025-07-23T12:52:38Z","published":"2025-07-23T12:52:38Z","title":"BGM-HAN: A Hierarchical Attention Network for Accurate and Fair Decision\n  Assessment on Semi-Structured Profiles","summary":"  Human decision-making in high-stakes domains often relies on expertise and\nheuristics, but is vulnerable to hard-to-detect cognitive biases that threaten\nfairness and long-term outcomes. This work presents a novel approach to\nenhancing complex decision-making workflows through the integration of\nhierarchical learning alongside various enhancements. Focusing on university\nadmissions as a representative high-stakes domain, we propose BGM-HAN, an\nenhanced Byte-Pair Encoded, Gated Multi-head Hierarchical Attention Network,\ndesigned to effectively model semi-structured applicant data. BGM-HAN captures\nmulti-level representations that are crucial for nuanced assessment, improving\nboth interpretability and predictive performance. Experimental results on real\nadmissions data demonstrate that our proposed model significantly outperforms\nboth state-of-the-art baselines from traditional machine learning to large\nlanguage models, offering a promising framework for augmenting decision-making\nin domains where structure, context, and fairness matter. Source code is\navailable at: https://github.com/junhua/bgm-han.\n","authors":["Junhua Liu","Roy Ka-Wei Lee","Kwan Hui Lim"],"pdf_url":"https://arxiv.org/pdf/2507.17472v1.pdf","comment":"Accepted at ASONAM 2025"},{"id":"http://arxiv.org/abs/2507.17470v1","updated":"2025-07-23T12:51:03Z","published":"2025-07-23T12:51:03Z","title":"Demonstration of Efficient Predictive Surrogates for Large-scale Quantum\n  Processors","summary":"  The ongoing development of quantum processors is driving breakthroughs in\nscientific discovery. Despite this progress, the formidable cost of fabricating\nlarge-scale quantum processors means they will remain rare for the foreseeable\nfuture, limiting their widespread application. To address this bottleneck, we\nintroduce the concept of predictive surrogates, which are classical learning\nmodels designed to emulate the mean-value behavior of a given quantum processor\nwith provably computational efficiency. In particular, we propose two\npredictive surrogates that can substantially reduce the need for quantum\nprocessor access in diverse practical scenarios. To demonstrate their potential\nin advancing digital quantum simulation, we use these surrogates to emulate a\nquantum processor with up to 20 programmable superconducting qubits, enabling\nefficient pre-training of variational quantum eigensolvers for families of\ntransverse-field Ising models and identification of non-equilibrium Floquet\nsymmetry-protected topological phases. Experimental results reveal that the\npredictive surrogates not only reduce measurement overhead by orders of\nmagnitude, but can also surpass the performance of conventional,\nquantum-resource-intensive approaches. Collectively, these findings establish\npredictive surrogates as a practical pathway to broadening the impact of\nadvanced quantum processors.\n","authors":["Wei-You Liao","Yuxuan Du","Xinbiao Wang","Tian-Ci Tian","Yong Luo","Bo Du","Dacheng Tao","He-Liang Huang"],"pdf_url":"https://arxiv.org/pdf/2507.17470v1.pdf","comment":"53 pages, 15 figures, comments are welcome"},{"id":"http://arxiv.org/abs/2507.17467v1","updated":"2025-07-23T12:46:51Z","published":"2025-07-23T12:46:51Z","title":"Probing Vision-Language Understanding through the Visual Entailment\n  Task: promises and pitfalls","summary":"  This study investigates the extent to which the Visual Entailment (VE) task\nserves as a reliable probe of vision-language understanding in multimodal\nlanguage models, using the LLaMA 3.2 11B Vision model as a test case. Beyond\nreporting performance metrics, we aim to interpret what these results reveal\nabout the underlying possibilities and limitations of the VE task. We conduct a\nseries of experiments across zero-shot, few-shot, and fine-tuning settings,\nexploring how factors such as prompt design, the number and order of in-context\nexamples and access to visual information might affect VE performance. To\nfurther probe the reasoning processes of the model, we used explanation-based\nevaluations. Results indicate that three-shot inference outperforms the\nzero-shot baselines. However, additional examples introduce more noise than\nthey provide benefits. Additionally, the order of the labels in the prompt is a\ncritical factor that influences the predictions. In the absence of visual\ninformation, the model has a strong tendency to hallucinate and imagine\ncontent, raising questions about the model's over-reliance on linguistic\npriors. Fine-tuning yields strong results, achieving an accuracy of 83.3% on\nthe e-SNLI-VE dataset and outperforming the state-of-the-art OFA-X model.\nAdditionally, the explanation evaluation demonstrates that the fine-tuned model\nprovides semantically meaningful explanations similar to those of humans, with\na BERTScore F1-score of 89.2%. We do, however, find comparable BERTScore\nresults in experiments with limited vision, questioning the visual grounding of\nthis task. Overall, our results highlight both the utility and limitations of\nVE as a diagnostic task for vision-language understanding and point to\ndirections for refining multimodal evaluation methods.\n","authors":["Elena Pitta","Tom Kouwenhoven","Tessa Verhoef"],"pdf_url":"https://arxiv.org/pdf/2507.17467v1.pdf","comment":"LUHME: 2nd Workshop on Language Understanding in the Human-Machine\n  Era"},{"id":"http://arxiv.org/abs/2406.06777v8","updated":"2025-07-23T12:32:35Z","published":"2024-06-10T20:25:18Z","title":"MolX: Enhancing Large Language Models for Molecular Understanding With A\n  Multi-Modal Extension","summary":"  Large Language Models (LLMs) with their strong task-handling capabilities\nhave shown remarkable advancements across a spectrum of fields, moving beyond\nnatural language understanding. However, their proficiency within the chemistry\ndomain remains restricted, especially in solving molecule-related tasks. This\nchallenge is attributed to their inherent limitations in comprehending\nmolecules using only common textual representations, i.e. SMILES strings. In\nthis study, we seek to enhance the ability of LLMs to comprehend molecules by\nequipping them with a multi-modal external module, termed MolX. Instead of\ndirectly using SMILES strings to represent a molecule, we utilize specific\nencoders to extract fine-grained features from both SMILES string and 2D\nmolecular graph representations for feeding into an LLM. A hand-crafted\nmolecular fingerprint is incorporated to leverage its embedded domain\nknowledge. To establish an alignment between MolX and the LLM's textual input\nspace, the model in which the LLM is frozen, is pre-trained with a strategy\nincluding a diverse set of tasks. Experimental evaluations show that our\nproposed method outperforms baselines across 4 downstream molecule-related\ntasks ranging from molecule-to-text translation to retrosynthesis, with and\nwithout fine-tuning the LLM, while only introducing a small number of trainable\nparameters--0.53\\% and 0.82\\%, respectively.\n","authors":["Khiem Le","Zhichun Guo","Kaiwen Dong","Xiaobao Huang","Bozhao Nan","Roshni Iyer","Xiangliang Zhang","Olaf Wiest","Wei Wang","Ting Hua","Nitesh V. Chawla"],"pdf_url":"https://arxiv.org/pdf/2406.06777v8.pdf","comment":"MLoG-GenAI@KDD '25 (Oral)"},{"id":"http://arxiv.org/abs/2503.06894v3","updated":"2025-07-23T12:27:38Z","published":"2025-03-10T03:50:25Z","title":"A Deep Learning Approach for Augmenting Perceptional Understanding of\n  Histopathology Images","summary":"  In Recent Years, Digital Technologies Have Made Significant Strides In\nAugmenting-Human-Health, Cognition, And Perception, Particularly Within The\nField Of Computational-Pathology. This Paper Presents A Novel Approach To\nEnhancing The Analysis Of Histopathology Images By Leveraging A\nMult-modal-Model That Combines Vision Transformers (Vit) With Gpt-2 For Image\nCaptioning. The Model Is Fine-Tuned On The Specialized Arch-Dataset, Which\nIncludes Dense Image Captions Derived From Clinical And Academic Resources, To\nCapture The Complexities Of Pathology Images Such As Tissue Morphologies,\nStaining Variations, And Pathological Conditions. By Generating Accurate,\nContextually Captions, The Model Augments The Cognitive Capabilities Of\nHealthcare Professionals, Enabling More Efficient Disease Classification,\nSegmentation, And Detection. The Model Enhances The Perception Of Subtle\nPathological Features In Images That Might Otherwise Go Unnoticed, Thereby\nImproving Diagnostic Accuracy. Our Approach Demonstrates The Potential For\nDigital Technologies To Augment Human Cognitive Abilities In Medical Image\nAnalysis, Providing Steps Toward More Personalized And Accurate Healthcare\nOutcomes.\n","authors":["Xiaoqian Hu"],"pdf_url":"https://arxiv.org/pdf/2503.06894v3.pdf","comment":"Accepted by International Conference on Semantic & Natural Language\n  Processing (SNLP 2025)"},{"id":"http://arxiv.org/abs/2507.17448v1","updated":"2025-07-23T12:13:06Z","published":"2025-07-23T12:13:06Z","title":"Reasoning-Driven Retrosynthesis Prediction with Large Language Models\n  via Reinforcement Learning","summary":"  Retrosynthesis planning, essential in organic synthesis and drug discovery,\nhas greatly benefited from recent AI-driven advancements. Nevertheless,\nexisting methods frequently face limitations in both applicability and\nexplainability. Traditional graph-based and sequence-to-sequence models often\nlack generalized chemical knowledge, leading to predictions that are neither\nconsistently accurate nor easily explainable. To address these challenges, we\nintroduce RetroDFM-R, a reasoning-based large language model (LLM) designed\nspecifically for chemical retrosynthesis. Leveraging large-scale reinforcement\nlearning guided by chemically verifiable rewards, RetroDFM-R significantly\nenhances prediction accuracy and explainability. Comprehensive evaluations\ndemonstrate that RetroDFM-R significantly outperforms state-of-the-art methods,\nachieving a top-1 accuracy of 65.0% on the USPTO-50K benchmark. Double-blind\nhuman assessments further validate the chemical plausibility and practical\nutility of RetroDFM-R's predictions. RetroDFM-R also accurately predicts\nmultistep retrosynthetic routes reported in the literature for both real-world\ndrug molecules and perovskite materials. Crucially, the model's explicit\nreasoning process provides human-interpretable insights, thereby enhancing\ntrust and practical value in real-world retrosynthesis applications.\n","authors":["Situo Zhang","Hanqi Li","Lu Chen","Zihan Zhao","Xuanze Lin","Zichen Zhu","Bo Chen","Xin Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2507.17448v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2507.17445v1","updated":"2025-07-23T12:07:21Z","published":"2025-07-23T12:07:21Z","title":"IndoorBEV: Joint Detection and Footprint Completion of Objects via\n  Mask-based Prediction in Indoor Scenarios for Bird's-Eye View Perception","summary":"  Detecting diverse objects within complex indoor 3D point clouds presents\nsignificant challenges for robotic perception, particularly with varied object\nshapes, clutter, and the co-existence of static and dynamic elements where\ntraditional bounding box methods falter. To address these limitations, we\npropose IndoorBEV, a novel mask-based Bird's-Eye View (BEV) method for indoor\nmobile robots.\n  In a BEV method, a 3D scene is projected into a 2D BEV grid which handles\nnaturally occlusions and provides a consistent top-down view aiding to\ndistinguish static obstacles from dynamic agents. The obtained 2D BEV results\nis directly usable to downstream robotic tasks like navigation, motion\nprediction, and planning. Our architecture utilizes an axis compact encoder and\na window-based backbone to extract rich spatial features from this BEV map. A\nquery-based decoder head then employs learned object queries to concurrently\npredict object classes and instance masks in the BEV space. This mask-centric\nformulation effectively captures the footprint of both static and dynamic\nobjects regardless of their shape, offering a robust alternative to bounding\nbox regression. We demonstrate the effectiveness of IndoorBEV on a custom\nindoor dataset featuring diverse object classes including static objects\n  and dynamic elements like robots and miscellaneous items, showcasing its\npotential for robust indoor scene understanding.\n","authors":["Haichuan Li","Changda Tian","Panos Trahanias","Tomi Westerlund"],"pdf_url":"https://arxiv.org/pdf/2507.17445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17442v1","updated":"2025-07-23T12:03:54Z","published":"2025-07-23T12:03:54Z","title":"Each to Their Own: Exploring the Optimal Embedding in RAG","summary":"  Recently, as Large Language Models (LLMs) have fundamentally impacted various\nfields, the methods for incorporating up-to-date information into LLMs or\nadding external knowledge to construct domain-specific models have garnered\nwide attention. Retrieval-Augmented Generation (RAG), serving as an\ninference-time scaling method, is notable for its low cost and minimal effort\nfor parameter tuning. However, due to heterogeneous training data and model\narchitecture, the variant embedding models used in RAG exhibit different\nbenefits across various areas, often leading to different similarity\ncalculation results and, consequently, varying response quality from LLMs. To\naddress this problem, we propose and examine two approaches to enhance RAG by\ncombining the benefits of multiple embedding models, named Mixture-Embedding\nRAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects\nretrievals from multiple embedding models based on standardized similarity;\nhowever, it does not outperform vanilla RAG. In contrast, Confident RAG\ngenerates responses multiple times using different embedding models and then\nselects the responses with the highest confidence level, demonstrating average\nimprovements of approximately 10% and 5% over vanilla LLMs and RAG,\nrespectively. The consistent results across different LLMs and embedding models\nindicate that Confident RAG is an efficient plug-and-play approach for various\ndomains. We will release our code upon publication.\n","authors":["Shiting Chen","Zijian Zhao","Jinsong Chen"],"pdf_url":"https://arxiv.org/pdf/2507.17442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17433v1","updated":"2025-07-23T11:46:13Z","published":"2025-07-23T11:46:13Z","title":"Fair Compromises in Participatory Budgeting: a Multi-Agent Deep\n  Reinforcement Learning Approach","summary":"  Participatory budgeting is a method of collectively understanding and\naddressing spending priorities where citizens vote on how a budget is spent, it\nis regularly run to improve the fairness of the distribution of public funds.\nParticipatory budgeting requires voters to make decisions on projects which can\nlead to ``choice overload\". A multi-agent reinforcement learning approach to\ndecision support can make decision making easier for voters by identifying\nvoting strategies that increase the winning proportion of their vote. This\nnovel approach can also support policymakers by highlighting aspects of\nelection design that enable fair compromise on projects. This paper presents a\nnovel, ethically aligned approach to decision support using multi-agent deep\nreinforcement learning modelling. This paper introduces a novel use of a\nbranching neural network architecture to overcome scalability challenges of\nmulti-agent reinforcement learning in a decentralized way. Fair compromises are\nfound through optimising voter actions towards greater representation of voter\npreferences in the winning set. Experimental evaluation with real-world\nparticipatory budgeting data reveals a pattern in fair compromise: that it is\nachievable through projects with smaller cost.\n","authors":["Hugh Adams","Srijoni Majumdar","Evangelos Pournaras"],"pdf_url":"https://arxiv.org/pdf/2507.17433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.23956v3","updated":"2025-07-23T11:42:03Z","published":"2025-03-31T11:13:18Z","title":"AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference","summary":"  Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.\n","authors":["Kai Huang","Hao Zou","Bochen Wang","Ye Xi","Zhen Xie","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2503.23956v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17418v1","updated":"2025-07-23T11:21:27Z","published":"2025-07-23T11:21:27Z","title":"Ctx2TrajGen: Traffic Context-Aware Microscale Vehicle Trajectories using\n  Generative Adversarial Imitation Learning","summary":"  Precise modeling of microscopic vehicle trajectories is critical for traffic\nbehavior analysis and autonomous driving systems. We propose Ctx2TrajGen, a\ncontext-aware trajectory generation framework that synthesizes realistic urban\ndriving behaviors using GAIL. Leveraging PPO and WGAN-GP, our model addresses\nnonlinear interdependencies and training instability inherent in microscopic\nsettings. By explicitly conditioning on surrounding vehicles and road geometry,\nCtx2TrajGen generates interaction-aware trajectories aligned with real-world\ncontext. Experiments on the drone-captured DRIFT dataset demonstrate superior\nperformance over existing methods in terms of realism, behavioral diversity,\nand contextual fidelity, offering a robust solution to data scarcity and domain\nshift without simulation.\n","authors":["Joobin Jin","Seokjun Hong","Gyeongseon Baek","Yeeun Kim","Byeongjoon Noh"],"pdf_url":"https://arxiv.org/pdf/2507.17418v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17412v1","updated":"2025-07-23T11:12:52Z","published":"2025-07-23T11:12:52Z","title":"Content-based 3D Image Retrieval and a ColBERT-inspired Re-ranking for\n  Tumor Flagging and Staging","summary":"  The increasing volume of medical images poses challenges for radiologists in\nretrieving relevant cases. Content-based image retrieval (CBIR) systems offer\npotential for efficient access to similar cases, yet lack standardized\nevaluation and comprehensive studies. Building on prior studies for tumor\ncharacterization via CBIR, this study advances CBIR research for volumetric\nmedical images through three key contributions: (1) a framework eliminating\nreliance on pre-segmented data and organ-specific datasets, aligning with large\nand unstructured image archiving systems, i.e. PACS in clinical practice; (2)\nintroduction of C-MIR, a novel volumetric re-ranking method adapting ColBERT's\ncontextualized late interaction mechanism for 3D medical imaging; (3)\ncomprehensive evaluation across four tumor sites using three feature extractors\nand three database configurations. Our evaluations highlight the significant\nadvantages of C-MIR. We demonstrate the successful adaptation of the late\ninteraction principle to volumetric medical images, enabling effective\ncontext-aware re-ranking. A key finding is C-MIR's ability to effectively\nlocalize the region of interest, eliminating the need for pre-segmentation of\ndatasets and offering a computationally efficient alternative to systems\nrelying on expensive data enrichment steps. C-MIR demonstrates promising\nimprovements in tumor flagging, achieving improved performance, particularly\nfor colon and lung tumors (p<0.05). C-MIR also shows potential for improving\ntumor staging, warranting further exploration of its capabilities. Ultimately,\nour work seeks to bridge the gap between advanced retrieval techniques and\ntheir practical applications in healthcare, paving the way for improved\ndiagnostic processes.\n","authors":["Farnaz Khun Jush","Steffen Vogler","Matthias Lenga"],"pdf_url":"https://arxiv.org/pdf/2507.17412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15225v3","updated":"2025-07-23T11:02:12Z","published":"2023-08-29T11:27:22Z","title":"From DDMs to DNNs: Using process data and models of decision-making to\n  improve human-AI interactions","summary":"  Over the past decades, cognitive neuroscientists and behavioral economists\nhave recognized the value of describing the process of decision making in\ndetail and modeling the emergence of decisions over time. For example, the time\nit takes to decide can reveal more about an agent's true hidden preferences\nthan only the decision itself. Similarly, data that track the ongoing decision\nprocess such as eye movements or neural recordings contain critical information\nthat can be exploited, even if no decision is made. Here, we argue that\nartificial intelligence (AI) research would benefit from a stronger focus on\ninsights about how decisions emerge over time and incorporate related process\ndata to improve AI predictions in general and human-AI interactions in\nparticular. First, we introduce a highly established computational framework\nthat assumes decisions to emerge from the noisy accumulation of evidence, and\nwe present related empirical work in psychology, neuroscience, and economics.\nNext, we discuss to what extent current approaches in multi-agent AI do or do\nnot incorporate process data and models of decision making. Finally, we outline\nhow a more principled inclusion of the evidence-accumulation framework into the\ntraining and use of AI can help to improve human-AI interactions in the future.\n","authors":["Mrugsen Nagsen Gopnarayan","Jaan Aru","Sebastian Gluth"],"pdf_url":"https://arxiv.org/pdf/2308.15225v3.pdf","comment":"Review paper, 17 pages, 2 figure"},{"id":"http://arxiv.org/abs/2507.17399v1","updated":"2025-07-23T10:54:24Z","published":"2025-07-23T10:54:24Z","title":"Millions of $\\text{GeAR}$-s: Extending GraphRAG to Millions of Documents","summary":"  Recent studies have explored graph-based approaches to retrieval-augmented\ngeneration, leveraging structured or semi-structured information -- such as\nentities and their relations extracted from documents -- to enhance retrieval.\nHowever, these methods are typically designed to address specific tasks, such\nas multi-hop question answering and query-focused summarisation, and therefore,\nthere is limited evidence of their general applicability across broader\ndatasets. In this paper, we aim to adapt a state-of-the-art graph-based RAG\nsolution: $\\text{GeAR}$ and explore its performance and limitations on the\nSIGIR 2025 LiveRAG Challenge.\n","authors":["Zhili Shen","Chenxin Diao","Pascual Merita","Pavlos Vougiouklis","Jeff Z. Pan"],"pdf_url":"https://arxiv.org/pdf/2507.17399v1.pdf","comment":"Accepted by SIGIR 2025 LiveRAG Challenge Program"},{"id":"http://arxiv.org/abs/2507.01955v2","updated":"2025-07-23T10:52:38Z","published":"2025-07-02T17:59:07Z","title":"How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation\n  Models on Standard Computer Vision Tasks","summary":"  Multimodal foundation models, such as GPT-4o, have recently made remarkable\nprogress, but it is not clear where exactly these models stand in terms of\nunderstanding vision. In this paper, we benchmark the performance of popular\nmultimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0\nFlash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision\ntasks (semantic segmentation, object detection, image classification, depth and\nsurface normal prediction) using established datasets (e.g., COCO, ImageNet and\nits variants, etc).\n  The main challenges to performing this are: 1) most models are trained to\noutput text and cannot natively express versatile domains, such as segments or\n3D geometry, and 2) many leading models are proprietary and accessible only at\nan API level, i.e., there is no weight access to adapt them. We address these\nchallenges by translating standard vision tasks into equivalent text-promptable\nand API-compatible tasks via prompt chaining to create a standardized\nbenchmarking framework.\n  We observe that 1) the models are not close to the state-of-the-art\nspecialist models at any task. However, 2) they are respectable generalists;\nthis is remarkable as they are presumably trained on primarily image-text-based\ntasks. 3) They perform semantic tasks notably better than geometric ones. 4)\nWhile the prompt-chaining techniques affect performance, better models exhibit\nless sensitivity to prompt variations. 5) GPT-4o performs the best among\nnon-reasoning models, securing the top position in 4 out of 6 tasks, 6)\nreasoning models, e.g. o3, show improvements in geometric tasks, and 7) a\npreliminary analysis of models with native image generation, like the latest\nGPT-4o, shows they exhibit quirks like hallucinations and spatial\nmisalignments.\n","authors":["Rahul Ramachandran","Ali Garjani","Roman Bachmann","Andrei Atanov","Oğuzhan Fatih Kar","Amir Zamir"],"pdf_url":"https://arxiv.org/pdf/2507.01955v2.pdf","comment":"Project page at https://fm-vision-evals.epfl.ch/"},{"id":"http://arxiv.org/abs/2502.02170v2","updated":"2025-07-23T10:48:02Z","published":"2025-02-04T09:44:41Z","title":"Graph Neural Networks for O-RAN Mobility Management: A Link Prediction\n  Approach","summary":"  Mobility performance has been a key focus in cellular networks up to 5G. To\nenhance handover (HO) performance, 3GPP introduced Conditional Handover (CHO)\nand Layer 1/Layer 2 Triggered Mobility (LTM) mechanisms in 5G. While these\nreactive HO strategies address the trade-off between HO failures (HOF) and\nping-pong effects, they often result in inefficient radio resource utilization\ndue to additional HO preparations. To overcome these challenges, this article\nproposes a proactive HO framework for mobility management in O-RAN, leveraging\nuser-cell link predictions to identify the optimal target cell for HO. We\nexplore various categories of Graph Neural Networks (GNNs) for link prediction\nand analyze the complexity of applying them to the mobility management domain.\nTwo GNN models are compared using a real-world dataset, with experimental\nresults demonstrating their ability to capture the dynamic and graph-structured\nnature of cellular networks. Finally, we present key insights from our study\nand outline future steps to enable the integration of GNN-based link prediction\nfor mobility management in O-RAN networks.\n","authors":["Ana Gonzalez Bermudez","Miquel Farreras","Milan Groshev","José Antonio Trujillo","Isabel de la Bandera","Raquel Barco"],"pdf_url":"https://arxiv.org/pdf/2502.02170v2.pdf","comment":"7 pages, 5 figures, 1 table. Submitted to IEEE Vehicular Technology\n  Magazine, Special Issue on \"AI for 6G O-RAN Intelligent, Cost-Efficient and\n  Secure Automation\". Version after Major Revision"},{"id":"http://arxiv.org/abs/2507.15455v2","updated":"2025-07-23T10:44:15Z","published":"2025-07-21T10:06:53Z","title":"Solving nonconvex Hamilton--Jacobi--Isaacs equations with PINN-based\n  policy iteration","summary":"  We propose a mesh-free policy iteration framework that combines classical\ndynamic programming with physics-informed neural networks (PINNs) to solve\nhigh-dimensional, nonconvex Hamilton--Jacobi--Isaacs (HJI) equations arising in\nstochastic differential games and robust control. The method alternates between\nsolving linear second-order PDEs under fixed feedback policies and updating the\ncontrols via pointwise minimax optimization using automatic differentiation.\nUnder standard Lipschitz and uniform ellipticity assumptions, we prove that the\nvalue function iterates converge locally uniformly to the unique viscosity\nsolution of the HJI equation. The analysis establishes equi-Lipschitz\nregularity of the iterates, enabling provable stability and convergence without\nrequiring convexity of the Hamiltonian. Numerical experiments demonstrate the\naccuracy and scalability of the method. In a two-dimensional stochastic\npath-planning game with a moving obstacle, our method matches finite-difference\nbenchmarks with relative $L^2$-errors below %10^{-2}%. In five- and\nten-dimensional publisher-subscriber differential games with anisotropic noise,\nthe proposed approach consistently outperforms direct PINN solvers, yielding\nsmoother value functions and lower residuals. Our results suggest that\nintegrating PINNs with policy iteration is a practical and theoretically\ngrounded method for solving high-dimensional, nonconvex HJI equations, with\npotential applications in robotics, finance, and multi-agent reinforcement\nlearning.\n","authors":["Hee Jun Yang","Minjung Gim","Yeoneung Kim"],"pdf_url":"https://arxiv.org/pdf/2507.15455v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17394v1","updated":"2025-07-23T10:41:46Z","published":"2025-07-23T10:41:46Z","title":"HiProbe-VAD: Video Anomaly Detection via Hidden States Probing in\n  Tuning-Free Multimodal LLMs","summary":"  Video Anomaly Detection (VAD) aims to identify and locate deviations from\nnormal patterns in video sequences. Traditional methods often struggle with\nsubstantial computational demands and a reliance on extensive labeled datasets,\nthereby restricting their practical applicability. To address these\nconstraints, we propose HiProbe-VAD, a novel framework that leverages\npre-trained Multimodal Large Language Models (MLLMs) for VAD without requiring\nfine-tuning. In this paper, we discover that the intermediate hidden states of\nMLLMs contain information-rich representations, exhibiting higher sensitivity\nand linear separability for anomalies compared to the output layer. To\ncapitalize on this, we propose a Dynamic Layer Saliency Probing (DLSP)\nmechanism that intelligently identifies and extracts the most informative\nhidden states from the optimal intermediate layer during the MLLMs reasoning.\nThen a lightweight anomaly scorer and temporal localization module efficiently\ndetects anomalies using these extracted hidden states and finally generate\nexplanations. Experiments on the UCF-Crime and XD-Violence datasets demonstrate\nthat HiProbe-VAD outperforms existing training-free and most traditional\napproaches. Furthermore, our framework exhibits remarkable cross-model\ngeneralization capabilities in different MLLMs without any tuning, unlocking\nthe potential of pre-trained MLLMs for video anomaly detection and paving the\nway for more practical and scalable solutions.\n","authors":["Zhaolin Cai","Fan Li","Ziwei Zheng","Yanjun Qin"],"pdf_url":"https://arxiv.org/pdf/2507.17394v1.pdf","comment":"Accepted by ACM MM 2025"},{"id":"http://arxiv.org/abs/2507.17389v1","updated":"2025-07-23T10:34:22Z","published":"2025-07-23T10:34:22Z","title":"Investigating Training Data Detection in AI Coders","summary":"  Recent advances in code large language models (CodeLLMs) have made them\nindispensable tools in modern software engineering. However, these models\noccasionally produce outputs that contain proprietary or sensitive code\nsnippets, raising concerns about potential non-compliant use of training data,\nand posing risks to privacy and intellectual property. To ensure responsible\nand compliant deployment of CodeLLMs, training data detection (TDD) has become\na critical task. While recent TDD methods have shown promise in natural\nlanguage settings, their effectiveness on code data remains largely\nunderexplored. This gap is particularly important given code's structured\nsyntax and distinct similarity criteria compared to natural language. To\naddress this, we conduct a comprehensive empirical study of seven\nstate-of-the-art TDD methods on source code data, evaluating their performance\nacross eight CodeLLMs. To support this evaluation, we introduce CodeSnitch, a\nfunction-level benchmark dataset comprising 9,000 code samples in three\nprogramming languages, each explicitly labeled as either included or excluded\nfrom CodeLLM training. Beyond evaluation on the original CodeSnitch, we design\ntargeted mutation strategies to test the robustness of TDD methods under three\ndistinct settings. These mutation strategies are grounded in the\nwell-established Type-1 to Type-4 code clone detection taxonomy. Our study\nprovides a systematic assessment of current TDD techniques for code and offers\ninsights to guide the development of more effective and robust detection\nmethods in the future.\n","authors":["Tianlin Li","Yunxiang Wei","Zhiming Li","Aishan Liu","Qing Guo","Xianglong Liu","Dongning Sun","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2507.17389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17791v1","updated":"2025-07-23T10:33:35Z","published":"2025-07-23T10:33:35Z","title":"Helix 1.0: An Open-Source Framework for Reproducible and Interpretable\n  Machine Learning on Tabular Scientific Data","summary":"  Helix is an open-source, extensible, Python-based software framework to\nfacilitate reproducible and interpretable machine learning workflows for\ntabular data. It addresses the growing need for transparent experimental data\nanalytics provenance, ensuring that the entire analytical process -- including\ndecisions around data transformation and methodological choices -- is\ndocumented, accessible, reproducible, and comprehensible to relevant\nstakeholders. The platform comprises modules for standardised data\npreprocessing, visualisation, machine learning model training, evaluation,\ninterpretation, results inspection, and model prediction for unseen data. To\nfurther empower researchers without formal training in data science to derive\nmeaningful and actionable insights, Helix features a user-friendly interface\nthat enables the design of computational experiments, inspection of outcomes,\nincluding a novel interpretation approach to machine learning decisions using\nlinguistic terms all within an integrated environment. Released under the MIT\nlicence, Helix is accessible via GitHub and PyPI, supporting community-driven\ndevelopment and promoting adherence to the FAIR principles.\n","authors":["Eduardo Aguilar-Bejarano","Daniel Lea","Karthikeyan Sivakumar","Jimiama M. Mase","Reza Omidvar","Ruizhe Li","Troy Kettle","James Mitchell-White","Morgan R Alexander","David A Winkler","Grazziela Figueredo"],"pdf_url":"https://arxiv.org/pdf/2507.17791v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2507.17373v1","updated":"2025-07-23T10:16:25Z","published":"2025-07-23T10:16:25Z","title":"SFUOD: Source-Free Unknown Object Detection","summary":"  Source-free object detection adapts a detector pre-trained on a source domain\nto an unlabeled target domain without requiring access to labeled source data.\nWhile this setting is practical as it eliminates the need for the source\ndataset during domain adaptation, it operates under the restrictive assumption\nthat only pre-defined objects from the source domain exist in the target\ndomain. This closed-set setting prevents the detector from detecting undefined\nobjects. To ease this assumption, we propose Source-Free Unknown Object\nDetection (SFUOD), a novel scenario which enables the detector to not only\nrecognize known objects but also detect undefined objects as unknown objects.\nTo this end, we propose CollaPAUL (Collaborative tuning and Principal\nAxis-based Unknown Labeling), a novel framework for SFUOD. Collaborative tuning\nenhances knowledge adaptation by integrating target-dependent knowledge from\nthe auxiliary encoder with source-dependent knowledge from the pre-trained\ndetector through a cross-domain attention mechanism. Additionally, principal\naxes-based unknown labeling assigns pseudo-labels to unknown objects by\nestimating objectness via principal axes projection and confidence scores from\nmodel predictions. The proposed CollaPAUL achieves state-of-the-art\nperformances on SFUOD benchmarks, and extensive experiments validate its\neffectiveness.\n","authors":["Keon-Hee Park","Seun-An Choe","Gyeong-Moon Park"],"pdf_url":"https://arxiv.org/pdf/2507.17373v1.pdf","comment":"This paper has been accepted by ICCV 2025"},{"id":"http://arxiv.org/abs/2312.15234v2","updated":"2025-07-23T10:11:55Z","published":"2023-12-23T11:57:53Z","title":"Towards Efficient Generative Large Language Model Serving: A Survey from\n  Algorithms to Systems","summary":"  In the rapidly evolving landscape of artificial intelligence (AI), generative\nlarge language models (LLMs) stand at the forefront, revolutionizing how we\ninteract with our data. However, the computational intensity and memory\nconsumption of deploying these models present substantial challenges in terms\nof serving efficiency, particularly in scenarios demanding low latency and high\nthroughput. This survey addresses the imperative need for efficient LLM serving\nmethodologies from a machine learning system (MLSys) research perspective,\nstanding at the crux of advanced AI innovations and practical system\noptimizations. We provide in-depth analysis, covering a spectrum of solutions,\nranging from cutting-edge algorithmic modifications to groundbreaking changes\nin system designs. The survey aims to provide a comprehensive understanding of\nthe current state and future directions in efficient LLM serving, offering\nvaluable insights for researchers and practitioners in overcoming the barriers\nof effective LLM deployment, thereby reshaping the future of AI.\n","authors":["Xupeng Miao","Gabriele Oliaro","Zhihao Zhang","Xinhao Cheng","Hongyi Jin","Tianqi Chen","Zhihao Jia"],"pdf_url":"https://arxiv.org/pdf/2312.15234v2.pdf","comment":"ACM Computing Surveys"},{"id":"http://arxiv.org/abs/2507.14219v2","updated":"2025-07-23T10:00:49Z","published":"2025-07-16T10:56:24Z","title":"Artificial Intelligence for Green Hydrogen Yield Prediction and Site\n  Suitability using SHAP-Based Composite Index: Focus on Oman","summary":"  As nations seek sustainable alternatives to fossil fuels, green hydrogen has\nemerged as a promising strategic pathway toward decarbonisation, particularly\nin solar-rich arid regions. However, identifying optimal locations for hydrogen\nproduction requires the integration of complex environmental, atmospheric, and\ninfrastructural factors, often compounded by limited availability of direct\nhydrogen yield data. This study presents a novel Artificial Intelligence (AI)\nframework for computing green hydrogen yield and site suitability index using\nmean absolute SHAP (SHapley Additive exPlanations) values. This framework\nconsists of a multi-stage pipeline of unsupervised multi-variable clustering,\nsupervised machine learning classifier and SHAP algorithm. The pipeline trains\non an integrated meteorological, topographic and temporal dataset and the\nresults revealed distinct spatial patterns of suitability and relative\ninfluence of the variables. With model predictive accuracy of 98%, the result\nalso showed that water proximity, elevation and seasonal variation are the most\ninfluential factors determining green hydrogen site suitability in Oman with\nmean absolute shap values of 2.470891, 2.376296 and 1.273216 respectively.\nGiven limited or absence of ground-truth yield data in many countries that have\ngreen hydrogen prospects and ambitions, this study offers an objective and\nreproducible alternative to subjective expert weightings, thus allowing the\ndata to speak for itself and potentially discover novel latent groupings\nwithout pre-imposed assumptions. This study offers industry stakeholders and\npolicymakers a replicable and scalable tool for green hydrogen infrastructure\nplanning and other decision making in data-scarce regions.\n","authors":["Obumneme Zimuzor Nwafor","Mohammed Abdul Majeed Al Hooti"],"pdf_url":"https://arxiv.org/pdf/2507.14219v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17365v1","updated":"2025-07-23T09:58:31Z","published":"2025-07-23T09:58:31Z","title":"DynaSearcher: Dynamic Knowledge Graph Augmented Search Agent via\n  Multi-Reward Reinforcement Learning","summary":"  Multi-step agentic retrieval systems based on large language models (LLMs)\nhave demonstrated remarkable performance in complex information search tasks.\nHowever, these systems still face significant challenges in practical\napplications, particularly in generating factually inconsistent intermediate\nqueries and inefficient search trajectories, which can lead to reasoning\ndeviations or redundant computations. To address these issues, we propose\nDynaSearcher, an innovative search agent enhanced by dynamic knowledge graphs\nand multi-reward reinforcement learning (RL). Specifically, our system\nleverages knowledge graphs as external structured knowledge to guide the search\nprocess by explicitly modeling entity relationships, thereby ensuring factual\nconsistency in intermediate queries and mitigating biases from irrelevant\ninformation. Furthermore, we employ a multi-reward RL framework for\nfine-grained control over training objectives such as retrieval accuracy,\nefficiency, and response quality. This framework promotes the generation of\nhigh-quality intermediate queries and comprehensive final answers, while\ndiscouraging unnecessary exploration and minimizing information omissions or\nredundancy. Experimental results demonstrate that our approach achieves\nstate-of-the-art answer accuracy on six multi-hop question answering datasets,\nmatching frontier LLMs while using only small-scale models and limited\ncomputational resources. Furthermore, our approach demonstrates strong\ngeneralization and robustness across diverse retrieval environments and\nlarger-scale models, highlighting its broad applicability.\n","authors":["Chuzhan Hao","Wenfeng Feng","Yuewei Zhang","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2507.17365v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2507.17788v1","updated":"2025-07-23T09:54:44Z","published":"2025-07-23T09:54:44Z","title":"Adaptive Repetition for Mitigating Position Bias in LLM-Based Ranking","summary":"  When using LLMs to rank items based on given criteria, or evaluate answers,\nthe order of candidate items can influence the model's final decision. This\nsensitivity to item positioning in a LLM's prompt is known as position bias.\nPrior research shows that this bias exists even in large models, though its\nseverity varies across models and tasks. In addition to position bias, LLMs\nalso exhibit varying degrees of low repetition consistency, where repeating the\nLLM call with the same candidate ordering can lead to different rankings. To\naddress both inconsistencies, a common approach is to prompt the model multiple\ntimes with different candidate orderings and aggregate the results via majority\nvoting. However, this repetition strategy, significantly increases\ncomputational costs. Extending prior findings, we observe that both the\ndirection -- favoring either the earlier or later candidate in the prompt --\nand magnitude of position bias across instances vary substantially, even within\na single dataset. This observation highlights the need for a per-instance\nmitigation strategy. To this end, we introduce a dynamic early-stopping method\nthat adaptively determines the number of repetitions required for each\ninstance. Evaluating our approach across three LLMs of varying sizes and on two\ntasks, namely re-ranking and alignment, we demonstrate that transitioning to a\ndynamic repetition strategy reduces the number of LLM calls by an average of\n81%, while preserving the accuracy. Furthermore, we propose a confidence-based\nadaptation to our early-stopping method, reducing LLM calls by an average of\n87% compared to static repetition, with only a slight accuracy trade-off\nrelative to our original early-stopping method.\n","authors":["Ali Vardasbi","Gustavo Penha","Claudia Hauff","Hugues Bouchard"],"pdf_url":"https://arxiv.org/pdf/2507.17788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22365v2","updated":"2025-07-23T09:53:03Z","published":"2024-10-28T09:00:28Z","title":"Vascular Segmentation of Functional Ultrasound Images using Deep\n  Learning","summary":"  Segmentation of medical images is a fundamental task with numerous\napplications. While MRI, CT, and PET modalities have significantly benefited\nfrom deep learning segmentation techniques, more recent modalities, like\nfunctional ultrasound (fUS), have seen limited progress. fUS is a non invasive\nimaging method that measures changes in cerebral blood volume (CBV) with high\nspatio-temporal resolution. However, distinguishing arterioles from venules in\nfUS is challenging due to opposing blood flow directions within the same pixel.\nUltrasound localization microscopy (ULM) can enhance resolution by tracking\nmicrobubble contrast agents but is invasive, and lacks dynamic CBV\nquantification. In this paper, we introduce the first deep learning-based\nsegmentation tool for fUS images, capable of differentiating signals from\ndifferent vascular compartments, based on ULM automatic annotation and enabling\ndynamic CBV quantification. We evaluate various UNet architectures on fUS\nimages of rat brains, achieving competitive segmentation performance, with 90%\naccuracy, a 71% F1 score, and an IoU of 0.59, using only 100 temporal frames\nfrom a fUS stack. These results are comparable to those from tubular structure\nsegmentation in other imaging modalities. Additionally, models trained on\nresting-state data generalize well to images captured during visual\nstimulation, highlighting robustness. This work offers a non-invasive,\ncost-effective alternative to ULM, enhancing fUS data interpretation and\nimproving understanding of vessel function. Our pipeline shows high linear\ncorrelation coefficients between signals from predicted and actual compartments\nin both cortical and deeper regions, showcasing its ability to accurately\ncapture blood flow dynamics.\n","authors":["Hana Sebia","Thomas Guyet","Mickaël Pereira","Marco Valdebenito","Hugues Berry","Benjamin Vidal"],"pdf_url":"https://arxiv.org/pdf/2410.22365v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.19996v2","updated":"2025-07-23T09:50:45Z","published":"2025-04-28T17:16:40Z","title":"Monitoring digestate application on agricultural crops using Sentinel-2\n  Satellite imagery","summary":"  The widespread use of Exogenous Organic Matter in agriculture necessitates\nmonitoring to assess its effects on soil and crop health. This study evaluates\noptical Sentinel-2 satellite imagery for detecting digestate application, a\npractice that enhances soil fertility but poses environmental risks like\nmicroplastic contamination and nitrogen losses. In the first instance,\nSentinel-2 satellite image time series (SITS) analysis of specific indices\n(EOMI, NDVI, EVI) was used to characterize EOM's spectral behavior after\napplication on the soils of four different crop types in Thessaly, Greece.\nFurthermore, Machine Learning (ML) models (namely Random Forest, k-NN, Gradient\nBoosting and a Feed-Forward Neural Network), were used to investigate digestate\npresence detection, achieving F1-scores up to 0.85. The findings highlight the\npotential of combining remote sensing and ML for scalable and cost-effective\nmonitoring of EOM applications, supporting precision agriculture and\nsustainability.\n","authors":["Andreas Kalogeras","Dimitrios Bormpoudakis","Iason Tsardanidis","Dimitra A. Loka","Charalampos Kontoes"],"pdf_url":"https://arxiv.org/pdf/2504.19996v2.pdf","comment":"Accepted for 2025 IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS 2025)"},{"id":"http://arxiv.org/abs/2507.17787v1","updated":"2025-07-23T09:50:17Z","published":"2025-07-23T09:50:17Z","title":"Hyperbolic Deep Learning for Foundation Models: A Survey","summary":"  Foundation models pre-trained on massive datasets, including large language\nmodels (LLMs), vision-language models (VLMs), and large multimodal models, have\ndemonstrated remarkable success in diverse downstream tasks. However, recent\nstudies have shown fundamental limitations of these models: (1) limited\nrepresentational capacity, (2) lower adaptability, and (3) diminishing\nscalability. These shortcomings raise a critical question: is Euclidean\ngeometry truly the optimal inductive bias for all foundation models, or could\nincorporating alternative geometric spaces enable models to better align with\nthe intrinsic structure of real-world data and improve reasoning processes?\nHyperbolic spaces, a class of non-Euclidean manifolds characterized by\nexponential volume growth with respect to distance, offer a mathematically\ngrounded solution. These spaces enable low-distortion embeddings of\nhierarchical structures (e.g., trees, taxonomies) and power-law distributions\nwith substantially fewer dimensions compared to Euclidean counterparts. Recent\nadvances have leveraged these properties to enhance foundation models,\nincluding improving LLMs' complex reasoning ability, VLMs' zero-shot\ngeneralization, and cross-modal semantic alignment, while maintaining parameter\nefficiency. This paper provides a comprehensive review of hyperbolic neural\nnetworks and their recent development for foundation models. We further outline\nkey challenges and research directions to advance the field.\n","authors":["Neil He","Hiren Madhu","Ngoc Bui","Menglin Yang","Rex Ying"],"pdf_url":"https://arxiv.org/pdf/2507.17787v1.pdf","comment":"11 Pages, SIGKDD 2025"},{"id":"http://arxiv.org/abs/2507.01630v2","updated":"2025-07-23T09:22:32Z","published":"2025-07-02T11:59:32Z","title":"Prompt Guidance and Human Proximal Perception for HOT Prediction with\n  Regional Joint Loss","summary":"  The task of Human-Object conTact (HOT) detection involves identifying the\nspecific areas of the human body that are touching objects. Nevertheless,\ncurrent models are restricted to just one type of image, often leading to too\nmuch segmentation in areas with little interaction, and struggling to maintain\ncategory consistency within specific regions. To tackle this issue, a HOT\nframework, termed \\textbf{P3HOT}, is proposed, which blends \\textbf{P}rompt\nguidance and human \\textbf{P}roximal \\textbf{P}erception. To begin with, we\nutilize a semantic-driven prompt mechanism to direct the network's attention\ntowards the relevant regions based on the correlation between image and text.\nThen a human proximal perception mechanism is employed to dynamically perceive\nkey depth range around the human, using learnable parameters to effectively\neliminate regions where interactions are not expected. Calculating depth\nresolves the uncertainty of the overlap between humans and objects in a 2D\nperspective, providing a quasi-3D viewpoint. Moreover, a Regional Joint Loss\n(RJLoss) has been created as a new loss to inhibit abnormal categories in the\nsame area. A new evaluation metric called ``AD-Acc.'' is introduced to address\nthe shortcomings of existing methods in addressing negative samples.\nComprehensive experimental results demonstrate that our approach achieves\nstate-of-the-art performance in four metrics across two benchmark datasets.\nSpecifically, our model achieves an improvement of \\textbf{0.7}$\\uparrow$,\n\\textbf{2.0}$\\uparrow$, \\textbf{1.6}$\\uparrow$, and \\textbf{11.0}$\\uparrow$ in\nSC-Acc., mIoU, wIoU, and AD-Acc. metrics, respectively, on the HOT-Annotated\ndataset. The sources code are available at\nhttps://github.com/YuxiaoWang-AI/P3HOT.\n","authors":["Yuxiao Wang","Yu Lei","Zhenao Wei","Weiying Xue","Xinyu Jiang","Nan Zhuang","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2507.01630v2.pdf","comment":"Accepted by ICCV 2025"},{"id":"http://arxiv.org/abs/2504.19155v2","updated":"2025-07-23T09:16:28Z","published":"2025-04-27T08:19:47Z","title":"Machine Learning-Based Modeling of the Anode Heel Effect in X-ray Beam\n  Monte Carlo Simulations","summary":"  To develop a machine learning-based framework for accurately modeling the\nanode heel effect in Monte Carlo simulations of X-ray imaging systems, enabling\nrealistic beam intensity profiles with minimal experimental calibration.\nMultiple regression models were trained to predict spatial intensity variations\nalong the anode-cathode axis using experimentally acquired weights derived from\nbeam measurements across different tube potentials. These weights captured the\nasymmetry introduced by the anode heel effect. A systematic fine-tuning\nprotocol was established to minimize the number of required measurements while\npreserving model accuracy. The models were implemented in the OpenGATE 10 and\nGGEMS Monte Carlo toolkits to evaluate their integration feasibility and\npredictive performance. Among the tested models, gradient boosting regression\n(GBR) delivered the highest accuracy, with prediction errors remaining below 5%\nacross all energy levels. The optimized fine-tuning strategy required only six\ndetector positions per energy level, reducing measurement effort by 65%. The\nmaximum error introduced through this fine-tuning process remained below 2%.\nDose actor comparisons within Monte Carlo simulations demonstrated that the\nGBR-based model closely replicated clinical beam profiles and significantly\noutperformed conventional symmetric beam models. This study presents a robust\nand generalizable method for incorporating the anode heel effect into Monte\nCarlo simulations using machine learning. By enabling accurate,\nenergy-dependent beam modeling with limited calibration data, the approach\nenhances simulation realism for applications in clinical dosimetry, image\nquality assessment, and radiation protection.\n","authors":["Hussein Harb","Didier Benoit","Axel Rannou","Chi-Hieu Pham","Valentin Tissot","Bahaa Nasr","Julien Bert"],"pdf_url":"https://arxiv.org/pdf/2504.19155v2.pdf","comment":"15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2507.15120v2","updated":"2025-07-23T09:09:15Z","published":"2025-07-20T20:49:21Z","title":"Automated planning with ontologies under coherence update semantics\n  (Extended Version)","summary":"  Standard automated planning employs first-order formulas under closed-world\nsemantics to achieve a goal with a given set of actions from an initial state.\nWe follow a line of research that aims to incorporate background knowledge into\nautomated planning problems, for example, by means of ontologies, which are\nusually interpreted under open-world semantics. We present a new approach for\nplanning with DL-Lite ontologies that combines the advantages of ontology-based\naction conditions provided by explicit-input knowledge and action bases (eKABs)\nand ontology-aware action effects under the coherence update semantics. We show\nthat the complexity of the resulting formalism is not higher than that of\nprevious approaches and provide an implementation via a polynomial compilation\ninto classical planning. An evaluation of existing and new benchmarks examines\nthe performance of a planning system on different variants of our compilation.\n","authors":["Stefan Borgwardt","Duy Nhu","Gabriele Röger"],"pdf_url":"https://arxiv.org/pdf/2507.15120v2.pdf","comment":"Extended version of a paper accepted at 22nd International Conference\n  on Principles of Knowledge Representation and Reasoning (KR 2025)"},{"id":"http://arxiv.org/abs/2507.17334v1","updated":"2025-07-23T09:02:09Z","published":"2025-07-23T09:02:09Z","title":"Temporal Point-Supervised Signal Reconstruction: A Human-Annotation-Free\n  Framework for Weak Moving Target Detection","summary":"  In low-altitude surveillance and early warning systems, detecting weak moving\ntargets remains a significant challenge due to low signal energy, small spatial\nextent, and complex background clutter. Existing methods struggle with\nextracting robust features and suffer from the lack of reliable annotations. To\naddress these limitations, we propose a novel Temporal Point-Supervised (TPS)\nframework that enables high-performance detection of weak targets without any\nmanual annotations.Instead of conventional frame-based detection, our framework\nreformulates the task as a pixel-wise temporal signal modeling problem, where\nweak targets manifest as short-duration pulse-like responses. A Temporal Signal\nReconstruction Network (TSRNet) is developed under the TPS paradigm to\nreconstruct these transient signals.TSRNet adopts an encoder-decoder\narchitecture and integrates a Dynamic Multi-Scale Attention (DMSAttention)\nmodule to enhance its sensitivity to diverse temporal patterns. Additionally, a\ngraph-based trajectory mining strategy is employed to suppress false alarms and\nensure temporal consistency.Extensive experiments on a purpose-built low-SNR\ndataset demonstrate that our framework outperforms state-of-the-art methods\nwhile requiring no human annotations. It achieves strong detection performance\nand operates at over 1000 FPS, underscoring its potential for real-time\ndeployment in practical scenarios.\n","authors":["Weihua Gao","Chunxu Ren","Wenlong Niu","Xiaodong Peng"],"pdf_url":"https://arxiv.org/pdf/2507.17334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14644v2","updated":"2025-07-23T08:55:02Z","published":"2025-01-24T17:05:00Z","title":"Optimizing Privacy-Utility Trade-off in Decentralized Learning with\n  Generalized Correlated Noise","summary":"  Decentralized learning enables distributed agents to collaboratively train a\nshared machine learning model without a central server, through local\ncomputation and peer-to-peer communication. Although each agent retains its\ndataset locally, sharing local models can still expose private information\nabout the local training datasets to adversaries. To mitigate privacy attacks,\na common strategy is to inject random artificial noise at each agent before\nexchanging local models between neighbors. However, this often leads to utility\ndegradation due to the negative effects of cumulated artificial noise on the\nlearning algorithm. In this work, we introduce CorN-DSGD, a novel\ncovariance-based framework for generating correlated privacy noise across\nagents, which unifies several state-of-the-art methods as special cases. By\nleveraging network topology and mixing weights, CorN-DSGD optimizes the noise\ncovariance to achieve network-wide noise cancellation. Experimental results\nshow that CorN-DSGD cancels more noise than existing pairwise correlation\nschemes, improving model performance under formal privacy guarantees.\n","authors":["Angelo Rodio","Zheng Chen","Erik G. Larsson"],"pdf_url":"https://arxiv.org/pdf/2501.14644v2.pdf","comment":"6 pages, 5 figures, accepted at IEEE ITW 2025"},{"id":"http://arxiv.org/abs/2503.13844v2","updated":"2025-07-23T08:38:14Z","published":"2025-03-18T02:33:38Z","title":"Towards Detecting Persuasion on Social Media: From Model Development to\n  Insights on Persuasion Strategies","summary":"  Political advertising plays a pivotal role in shaping public opinion and\ninfluencing electoral outcomes, often through subtle persuasive techniques\nembedded in broader propaganda strategies. Detecting these persuasive elements\nis crucial for enhancing voter awareness and ensuring transparency in\ndemocratic processes. This paper presents an integrated approach that bridges\nmodel development and real-world application through two interconnected\nstudies. First, we introduce a lightweight model for persuasive text detection\nthat achieves state-of-the-art performance in Subtask 3 of SemEval 2023 Task 3\nwhile requiring significantly fewer computational resources and training data\nthan existing methods. Second, we demonstrate the model's practical utility by\ncollecting the Australian Federal Election 2022 Facebook Ads (APA22) dataset,\npartially annotating a subset for persuasion, and fine-tuning the model to\nadapt from mainstream news to social media content. We then apply the\nfine-tuned model to label the remainder of the APA22 dataset, revealing\ndistinct patterns in how political campaigns leverage persuasion through\ndifferent funding strategies, word choices, demographic targeting, and temporal\nshifts in persuasion intensity as election day approaches. Our findings not\nonly underscore the necessity of domain-specific modeling for analyzing\npersuasion on social media but also show how uncovering these strategies can\nenhance transparency, inform voters, and promote accountability in digital\ncampaigns.\n","authors":["Elyas Meguellati","Stefano Civelli","Pietro Bernardelle","Shazia Sadiq","Irwin King","Gianluca Demartini"],"pdf_url":"https://arxiv.org/pdf/2503.13844v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11053v2","updated":"2025-07-23T08:36:26Z","published":"2023-12-18T09:35:43Z","title":"Conflict Detection for Temporal Knowledge Graphs:A Fast Constraint\n  Mining Algorithm and New Benchmarks","summary":"  Temporal facts, which are used to describe events that occur during specific\ntime periods, have become a topic of increased interest in the field of\nknowledge graph (KG) research. In terms of quality management, the introduction\nof time restrictions brings new challenges to maintaining the temporal\nconsistency of KGs. Previous studies rely on manually enumerated temporal\nconstraints to detect conflicts, which are labor-intensive and may have\ngranularity issues. To address this problem, we start from the common pattern\nof temporal facts and propose a pattern-based temporal constraint mining\nmethod, PaTeCon. Unlike previous studies, PaTeCon uses graph patterns and\nstatistical information relevant to the given KG to automatically generate\ntemporal constraints, without the need for human experts. In this paper, we\nillustrate how this method can be optimized to achieve significant speed\nimprovement. We also annotate Wikidata and Freebase to build two new benchmarks\nfor conflict detection. Extensive experiments demonstrate that our\npattern-based automatic constraint mining approach is highly effective in\ngenerating valuable temporal constraints.\n","authors":["Jianhao Chen","Junyang Ren","Wentao Ding","Haoyuan Ouyang","Wei Hu","Yuzhong Qu"],"pdf_url":"https://arxiv.org/pdf/2312.11053v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17309v1","updated":"2025-07-23T08:23:34Z","published":"2025-07-23T08:23:34Z","title":"Confounded Causal Imitation Learning with Instrumental Variables","summary":"  Imitation learning from demonstrations usually suffers from the confounding\neffects of unmeasured variables (i.e., unmeasured confounders) on the states\nand actions. If ignoring them, a biased estimation of the policy would be\nentailed. To break up this confounding gap, in this paper, we take the best of\nthe strong power of instrumental variables (IV) and propose a Confounded Causal\nImitation Learning (C2L) model. This model accommodates confounders that\ninfluence actions across multiple timesteps, rather than being restricted to\nimmediate temporal dependencies. We develop a two-stage imitation learning\nframework for valid IV identification and policy optimization. In particular,\nin the first stage, we construct a testing criterion based on the defined\npseudo-variable, with which we achieve identifying a valid IV for the C2L\nmodels. Such a criterion entails the sufficient and necessary identifiability\nconditions for IV validity. In the second stage, with the identified IV, we\npropose two candidate policy learning approaches: one is based on a simulator,\nwhile the other is offline. Extensive experiments verified the effectiveness of\nidentifying the valid IV as well as learning the policy.\n","authors":["Yan Zeng","Shenglan Nie","Feng Xie","Libo Huang","Peng Wu","Zhi Geng"],"pdf_url":"https://arxiv.org/pdf/2507.17309v1.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2409.01534v2","updated":"2025-07-23T08:14:06Z","published":"2024-09-03T02:08:47Z","title":"Cross-domain Multi-step Thinking: Zero-shot Fine-grained Traffic Sign\n  Recognition in the Wild","summary":"  In this study, we propose Cross-domain Multi-step Thinking (CdMT) to improve\nzero-shot fine-grained traffic sign recognition (TSR) performance in the wild.\nZero-shot fine-grained TSR in the wild is challenging due to the cross-domain\nproblem between clean template traffic signs and real-world counterparts, and\nexisting approaches particularly struggle with cross-country TSR scenarios,\nwhere traffic signs typically differ between countries. The proposed CdMT\nframework tackles these challenges by leveraging the multi-step reasoning\ncapabilities of large multimodal models (LMMs). We introduce context,\ncharacteristic, and differential descriptions to design multiple thinking\nprocesses for LMMs. Context descriptions, which are enhanced by center\ncoordinate prompt optimization, enable the precise localization of target\ntraffic signs in complex road images and filter irrelevant responses via novel\nprior traffic sign hypotheses. Characteristic descriptions, which are derived\nfrom in-context learning with template traffic signs, bridge cross-domain gaps\nand enhance fine-grained TSR. Differential descriptions refine the multimodal\nreasoning ability of LMMs by distinguishing subtle differences among similar\nsigns. CdMT is independent of training data and requires only simple and\nuniform instructions, enabling it to achieve cross-country TSR. We conducted\nextensive experiments on three benchmark datasets and two real-world datasets\nfrom different countries. The proposed CdMT framework achieved superior\nperformance compared with other state-of-the-art methods on all five datasets,\nwith recognition accuracies of 0.93, 0.89, 0.97, 0.89, and 0.85 on the GTSRB,\nBTSD, TT-100K, Sapporo, and Yokohama datasets, respectively.\n","authors":["Yaozong Gan","Guang Li","Ren Togo","Keisuke Maeda","Takahiro Ogawa","Miki Haseyama"],"pdf_url":"https://arxiv.org/pdf/2409.01534v2.pdf","comment":"Published by Knowledge-Based Systems"},{"id":"http://arxiv.org/abs/2507.17303v1","updated":"2025-07-23T08:09:42Z","published":"2025-07-23T08:09:42Z","title":"A Versatile Pathology Co-pilot via Reasoning Enhanced Multimodal Large\n  Language Model","summary":"  Multimodal large language models (MLLMs) have emerged as powerful tools for\ncomputational pathology, offering unprecedented opportunities to integrate\npathological images with language context for comprehensive diagnostic\nanalysis. These models hold particular promise for automating complex tasks\nthat traditionally require expert interpretation of pathologists. However,\ncurrent MLLM approaches in pathology demonstrate significantly constrained\nreasoning capabilities, primarily due to their reliance on expensive\nchain-of-thought annotations. Additionally, existing methods remain limited to\nsimplex application of visual question answering (VQA) at region-of-interest\n(ROI) level, failing to address the full spectrum of diagnostic needs such as\nROI classification, detection, segmentation, whole-slide-image (WSI)\nclassification and VQA in clinical practice. In this study, we present\nSmartPath-R1, a versatile MLLM capable of simultaneously addressing both\nROI-level and WSI-level tasks while demonstrating robust pathological reasoning\ncapability. Our framework combines scale-dependent supervised fine-tuning and\ntask-aware reinforcement fine-tuning, which circumvents the requirement for\nchain-of-thought supervision by leveraging the intrinsic knowledge within MLLM.\nFurthermore, SmartPath-R1 integrates multiscale and multitask analysis through\na mixture-of-experts mechanism, enabling dynamic processing for diverse tasks.\nWe curate a large-scale dataset comprising 2.3M ROI samples and 188K WSI\nsamples for training and evaluation. Extensive experiments across 72 tasks\nvalidate the effectiveness and superiority of the proposed approach. This work\nrepresents a significant step toward developing versatile, reasoning-enhanced\nAI systems for precision pathology.\n","authors":["Zhe Xu","Ziyi Liu","Junlin Hou","Jiabo Ma","Cheng Jin","Yihui Wang","Zhixuan Chen","Zhengyu Zhang","Zhengrui Guo","Fengtao Zhou","Yingxue Xu","Xi Wang","Ronald Cheong Kin Chan","Li Liang","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2507.17303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.02634v4","updated":"2025-07-23T08:07:19Z","published":"2025-06-03T08:51:38Z","title":"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider","summary":"  Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.\n","authors":["Jiahao Wang","Jinbo Han","Xingda Wei","Sijie Shen","Dingyan Zhang","Chenguang Fang","Rong Chen","Wenyuan Yu","Haibo Chen"],"pdf_url":"https://arxiv.org/pdf/2506.02634v4.pdf","comment":"Accepted by USENIX ATC'25"},{"id":"http://arxiv.org/abs/2507.03038v2","updated":"2025-07-23T08:06:29Z","published":"2025-07-03T05:49:18Z","title":"Cautious Next Token Prediction","summary":"  Next token prediction paradigm has been prevailing for autoregressive models\nin the era of LLMs. The current default sampling choice for popular LLMs is\ntemperature scaling together with nucleus sampling to balance diversity and\ncoherence. Nevertheless, such approach leads to inferior performance in various\nNLP tasks when the model is not certain about testing questions. To this end,\nwe propose a brand new training-free decoding strategy, dubbed as Cautious Next\nToken Prediction (CNTP). In the decoding process, if the model has\ncomparatively high prediction entropy at a certain step, we sample multiple\ntrials starting from the step independently and stop when encountering any\npunctuation. Then we select the trial with the lowest perplexity score viewed\nas the most probable and reliable trial path given the model's capacity. The\ntrial number is negatively correlated with the prediction confidence, i.e., the\nless confident the model is, the more trials it should sample. This is\nconsistent with human beings' behaviour: when feeling uncertain or unconfident,\none tends to think more creatively, exploring multiple thinking paths, to\ncautiously select the path one feels most confident about. Extensive\nexperiments on both LLMs and MLLMs show that our proposed CNTP approach\noutperforms existing standard decoding strategies consistently by a clear\nmargin. Moreover, the integration of CNTP with self consistency can further\nimprove over vanilla self consistency. We believe our proposed CNTP has the\npotential to become one of the default choices for LLM decoding. Code is\navailable at https://github.com/wyzjack/CNTP.\n","authors":["Yizhou Wang","Lingzhi Zhang","Yue Bai","Mang Tik Chiu","Zhengmian Hu","Mingyuan Zhang","Qihua Dong","Yu Yin","Sohrab Amirghodsi","Yun Fu"],"pdf_url":"https://arxiv.org/pdf/2507.03038v2.pdf","comment":"ACL 2025"},{"id":"http://arxiv.org/abs/2501.08518v2","updated":"2025-07-23T08:01:28Z","published":"2025-01-15T02:06:29Z","title":"Alleviating Seasickness through Brain-Computer Interface-based Attention\n  Shift","summary":"  Seasickness poses a widespread problem that adversely impacts both passenger\ncomfort and the operational efficiency of maritime crews. Although attention\nshift has been proposed as a potential method to alleviate symptoms of motion\nsickness, its efficacy remains to be rigorously validated, especially in\nmaritime environments. In this study, we develop an AI-driven brain-computer\ninterface (BCI) to realize sustained and practical attention shift by\nincorporating tasks such as breath counting. Forty-three participants completed\na real-world nautical experiment consisting of a real-feedback session, a\nresting session, and a pseudo-feedback session. Notably, 81.39\\% of the\nparticipants reported that the BCI intervention was effective. EEG analysis\nrevealed that the proposed system can effectively regulate motion sickness EEG\nsignatures, such as an decrease in total band power, along with an increase in\ntheta relative power and a decrease in beta relative power. Furthermore, an\nindicator of attentional focus, the theta/beta ratio, exhibited a significant\nreduction during the real-feedback session, providing further evidence to\nsupport the effectiveness of the BCI in shifting attention. Collectively, this\nstudy presents a novel nonpharmacological, portable, and effective approach for\nseasickness intervention, which has the potential to open up a brand-new\napplication domain for BCIs.\n","authors":["Xiaoyu Bao","Kailin Xu","Jiawei Zhu","Haiyun Huang","Kangning Li","Qiyun Huang","Yuanqing Li"],"pdf_url":"https://arxiv.org/pdf/2501.08518v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17297v1","updated":"2025-07-23T07:58:28Z","published":"2025-07-23T07:58:28Z","title":"On Temporal Guidance and Iterative Refinement in Audio Source Separation","summary":"  Spatial semantic segmentation of sound scenes (S5) involves the accurate\nidentification of active sound classes and the precise separation of their\nsources from complex acoustic mixtures. Conventional systems rely on a\ntwo-stage pipeline - audio tagging followed by label-conditioned source\nseparation - but are often constrained by the absence of fine-grained temporal\ninformation critical for effective separation. In this work, we address this\nlimitation by introducing a novel approach for S5 that enhances the synergy\nbetween the event detection and source separation stages. Our key contributions\nare threefold. First, we fine-tune a pre-trained Transformer to detect active\nsound classes. Second, we utilize a separate instance of this fine-tuned\nTransformer to perform sound event detection (SED), providing the separation\nmodule with detailed, time-varying guidance. Third, we implement an iterative\nrefinement mechanism that progressively enhances separation quality by\nrecursively reusing the separator's output from previous iterations. These\nadvancements lead to significant improvements in both audio tagging and source\nseparation performance, as demonstrated by our system's second-place finish in\nTask 4 of the DCASE Challenge 2025. Our implementation and model checkpoints\nare available in our GitHub repository: https://github.com/theMoro/dcase25task4 .\n","authors":["Tobias Morocutti","Jonathan Greif","Paul Primus","Florian Schmid","Gerhard Widmer"],"pdf_url":"https://arxiv.org/pdf/2507.17297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17291v1","updated":"2025-07-23T07:52:09Z","published":"2025-07-23T07:52:09Z","title":"Integrating Belief Domains into Probabilistic Logic Programs","summary":"  Probabilistic Logic Programming (PLP) under the Distribution Semantics is a\nleading approach to practical reasoning under uncertainty. An advantage of the\nDistribution Semantics is its suitability for implementation as a Prolog or\nPython library, available through two well-maintained implementations, namely\nProbLog and cplint/PITA. However, current formulations of the Distribution\nSemantics use point-probabilities, making it difficult to express epistemic\nuncertainty, such as arises from, for example, hierarchical classifications\nfrom computer vision models. Belief functions generalize probability measures\nas non-additive capacities, and address epistemic uncertainty via interval\nprobabilities. This paper introduces interval-based Capacity Logic Programs\nbased on an extension of the Distribution Semantics to include belief\nfunctions, and describes properties of the new framework that make it amenable\nto practical applications.\n","authors":["Damiano Azzolini","Fabrizio Riguzzi","Theresa Swift"],"pdf_url":"https://arxiv.org/pdf/2507.17291v1.pdf","comment":"Under consideration in Theory and Practice of Logic Programming\n  (TPLP)"},{"id":"http://arxiv.org/abs/2505.22334v2","updated":"2025-07-23T07:37:08Z","published":"2025-05-28T13:21:38Z","title":"Advancing Multimodal Reasoning via Reinforcement Learning with Cold\n  Start","summary":"  Recent advancements in large language models (LLMs) have demonstrated\nimpressive chain-of-thought reasoning capabilities, with reinforcement learning\n(RL) playing a crucial role in this progress. While \"aha moment\"\npatterns--where models exhibit self-correction through reflection--are often\nattributed to emergent properties from RL, we first demonstrate that these\npatterns exist in multimodal LLMs (MLLMs) prior to RL training but may not\nnecessarily correlate with improved reasoning performance. Building on these\ninsights, we present a comprehensive study on enhancing multimodal reasoning\nthrough a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start\nwith structured chain-of-thought reasoning patterns, followed by (2)\nreinforcement learning via GRPO to further refine these capabilities. Our\nextensive experiments show that this combined approach consistently outperforms\nboth SFT-only and RL-only methods across challenging multimodal reasoning\nbenchmarks. The resulting models achieve state-of-the-art performance among\nopen-source MLLMs at both 3B and 7B scales, with our 7B model showing\nsubstantial improvements over base models (e.g., 66.3 %$\\rightarrow$73.4 % on\nMathVista, 62.9 %$\\rightarrow$70.4 % on We-Math) and our 3B model achieving\nperformance competitive with several 7B models. Overall, this work provides\npractical guidance for building advanced multimodal reasoning models. Our code\nis available at https://github.com/waltonfuture/RL-with-Cold-Start.\n","authors":["Lai Wei","Yuting Li","Kaipeng Zheng","Chen Wang","Yue Wang","Linghe Kong","Lichao Sun","Weiran Huang"],"pdf_url":"https://arxiv.org/pdf/2505.22334v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.14811v2","updated":"2025-07-23T07:36:08Z","published":"2025-07-20T04:00:53Z","title":"SegQuant: A Semantics-Aware and Generalizable Quantization Framework for\n  Diffusion Models","summary":"  Diffusion models have demonstrated exceptional generative capabilities but\nare computationally intensive, posing significant challenges for deployment in\nresource-constrained or latency-sensitive environments. Quantization offers an\neffective means to reduce model size and computational cost, with post-training\nquantization (PTQ) being particularly appealing due to its compatibility with\npre-trained models without requiring retraining or training data. However,\nexisting PTQ methods for diffusion models often rely on architecture-specific\nheuristics that limit their generalizability and hinder integration with\nindustrial deployment pipelines. To address these limitations, we propose\nSegQuant, a unified quantization framework that adaptively combines\ncomplementary techniques to enhance cross-model versatility. SegQuant consists\nof a segment-aware, graph-based quantization strategy (SegLinear) that captures\nstructural semantics and spatial heterogeneity, along with a dual-scale\nquantization scheme (DualScale) that preserves polarity-asymmetric activations,\nwhich is crucial for maintaining visual fidelity in generated outputs. SegQuant\nis broadly applicable beyond Transformer-based diffusion models, achieving\nstrong performance while ensuring seamless compatibility with mainstream\ndeployment tools.\n","authors":["Jiaji Zhang","Ruichao Sun","Hailiang Zhao","Jiaju Wu","Peng Chen","Hao Li","Yuying Liu","Xinkui Zhao","Kingsum Chow","Gang Xiong","Shuiguang Deng"],"pdf_url":"https://arxiv.org/pdf/2507.14811v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01709v3","updated":"2025-07-23T07:22:26Z","published":"2025-05-03T06:17:18Z","title":"RoBridge: A Hierarchical Architecture Bridging Cognition and Execution\n  for General Robotic Manipulation","summary":"  Operating robots in open-ended scenarios with diverse tasks is a crucial\nresearch and application direction in robotics. While recent progress in\nnatural language processing and large multimodal models has enhanced robots'\nability to understand complex instructions, robot manipulation still faces the\nprocedural skill dilemma and the declarative skill dilemma in open\nenvironments. Existing methods often compromise cognitive and executive\ncapabilities. To address these challenges, in this paper, we propose RoBridge,\na hierarchical intelligent architecture for general robotic manipulation. It\nconsists of a high-level cognitive planner (HCP) based on a large-scale\npre-trained vision-language model (VLM), an invariant operable representation\n(IOR) serving as a symbolic bridge, and a generalist embodied agent (GEA).\nRoBridge maintains the declarative skill of VLM and unleashes the procedural\nskill of reinforcement learning, effectively bridging the gap between cognition\nand execution. RoBridge demonstrates significant performance improvements over\nexisting baselines, achieving a 75% success rate on new tasks and an 83%\naverage success rate in sim-to-real generalization using only five real-world\ndata samples per task. This work represents a significant step towards\nintegrating cognitive reasoning with physical execution in robotic systems,\noffering a new paradigm for general robotic manipulation.\n","authors":["Kaidong Zhang","Rongtao Xu","Pengzhen Ren","Junfan Lin","Hefeng Wu","Liang Lin","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2505.01709v3.pdf","comment":"project page: https://abliao.github.io/RoBridge/"},{"id":"http://arxiv.org/abs/2503.02382v2","updated":"2025-07-23T07:19:41Z","published":"2025-03-04T08:18:46Z","title":"An Efficient and Precise Training Data Construction Framework for\n  Process-supervised Reward Model in Mathematical Reasoning","summary":"  Enhancing the mathematical reasoning capabilities of Large Language Models\n(LLMs) is of great scientific and practical significance. Researchers typically\nemploy process-supervised reward models (PRMs) to guide the reasoning process,\neffectively improving the models' reasoning abilities. However, existing\nmethods for constructing process supervision training data, such as manual\nannotation and per-step Monte Carlo estimation, are often costly or suffer from\npoor quality. To address these challenges, this paper introduces a framework\ncalled EpicPRM, which annotates each intermediate reasoning step based on its\nquantified contribution and uses an adaptive binary search algorithm to enhance\nboth annotation precision and efficiency. Using this approach, we efficiently\nconstruct a high-quality process supervision training dataset named Epic50k,\nconsisting of 50k annotated intermediate steps. Compared to other publicly\navailable datasets, the PRM trained on Epic50k demonstrates significantly\nsuperior performance. Getting Epic50k at https://github.com/xiaolizh1/EpicPRM.\n","authors":["Wei Sun","Qianlong Du","Fuwei Cui","Jiajun Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.02382v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17273v1","updated":"2025-07-23T07:18:55Z","published":"2025-07-23T07:18:55Z","title":"Leveraging Knowledge Graphs and LLM Reasoning to Identify Operational\n  Bottlenecks for Warehouse Planning Assistance","summary":"  Analyzing large, complex output datasets from Discrete Event Simulations\n(DES) of warehouse operations to identify bottlenecks and inefficiencies is a\ncritical yet challenging task, often demanding significant manual effort or\nspecialized analytical tools. Our framework integrates Knowledge Graphs (KGs)\nand Large Language Model (LLM)-based agents to analyze complex Discrete Event\nSimulation (DES) output data from warehouse operations. It transforms raw DES\ndata into a semantically rich KG, capturing relationships between simulation\nevents and entities. An LLM-based agent uses iterative reasoning, generating\ninterdependent sub-questions. For each sub-question, it creates Cypher queries\nfor KG interaction, extracts information, and self-reflects to correct errors.\nThis adaptive, iterative, and self-correcting process identifies operational\nissues mimicking human analysis. Our DES approach for warehouse bottleneck\nidentification, tested with equipment breakdowns and process irregularities,\noutperforms baseline methods. For operational questions, it achieves\nnear-perfect pass rates in pinpointing inefficiencies. For complex\ninvestigative questions, we demonstrate its superior diagnostic ability to\nuncover subtle, interconnected issues. This work bridges simulation modeling\nand AI (KG+LLM), offering a more intuitive method for actionable insights,\nreducing time-to-insight, and enabling automated warehouse inefficiency\nevaluation and diagnosis.\n","authors":["Rishi Parekh","Saisubramaniam Gopalakrishnan","Zishan Ahmad","Anirudh Deodhar"],"pdf_url":"https://arxiv.org/pdf/2507.17273v1.pdf","comment":"12 pages, 2 figures"},{"id":"http://arxiv.org/abs/2507.17264v1","updated":"2025-07-23T07:01:44Z","published":"2025-07-23T07:01:44Z","title":"Understanding Prompt Programming Tasks and Questions","summary":"  Prompting foundation models (FMs) like large language models (LLMs) have\nenabled new AI-powered software features (e.g., text summarization) that\npreviously were only possible by fine-tuning FMs. Now, developers are embedding\nprompts in software, known as prompt programs. The process of prompt\nprogramming requires the developer to make many changes to their prompt. Yet,\nthe questions developers ask to update their prompt is unknown, despite the\nanswers to these questions affecting how developers plan their changes. With\nthe growing number of research and commercial prompt programming tools, it is\nunclear whether prompt programmers' needs are being adequately addressed. We\naddress these challenges by developing a taxonomy of 25 tasks prompt\nprogrammers do and 51 questions they ask, measuring the importance of each task\nand question. We interview 16 prompt programmers, observe 8 developers make\nprompt changes, and survey 50 developers. We then compare the taxonomy with 48\nresearch and commercial tools. We find that prompt programming is not\nwell-supported: all tasks are done manually, and 16 of the 51 questions --\nincluding a majority of the most important ones -- remain unanswered. Based on\nthis, we outline important opportunities for prompt programming tools.\n","authors":["Jenny T. Liang","Chenyang Yang","Agnia Sergeyuk","Travis D. Breaux","Brad A. Myers"],"pdf_url":"https://arxiv.org/pdf/2507.17264v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12404v2","updated":"2025-07-23T06:58:51Z","published":"2025-06-14T08:48:44Z","title":"EXGnet: a single-lead explainable-AI guided multiresolution network with\n  train-only quantitative features for trustworthy ECG arrhythmia\n  classification","summary":"  Deep learning has significantly propelled the performance of ECG arrhythmia\nclassification, yet its clinical adoption remains hindered by challenges in\ninterpretability and deployment on resource-constrained edge devices. To bridge\nthis gap, we propose EXGnet, a novel and reliable ECG arrhythmia classification\nnetwork tailored for single-lead signals, specifically designed to balance high\naccuracy, explainability, and edge compatibility. EXGnet integrates XAI\nsupervision during training via a normalized cross-correlation based loss,\ndirecting the model's attention to clinically relevant ECG regions, similar to\na cardiologist's focus. This supervision is driven by automatically generated\nground truth, derived through an innovative heart rate variability-based\napproach, without the need for manual annotation. To enhance classification\naccuracy without compromising deployment simplicity, we incorporate\nquantitative ECG features during training. These enrich the model with\nmulti-domain knowledge but are excluded during inference, keeping the model\nlightweight for edge deployment. Additionally, we introduce an innovative\nmultiresolution block to efficiently capture both short and long-term signal\nfeatures while maintaining computational efficiency. Rigorous evaluation on the\nChapman and Ningbo benchmark datasets validates the supremacy of EXGnet, which\nachieves average five-fold accuracies of 98.762% and 96.932%, and F1-scores of\n97.910% and 95.527%, respectively. Comprehensive ablation studies and both\nquantitative and qualitative interpretability assessment confirm that the XAI\nguidance is pivotal, demonstrably enhancing the model's focus and\ntrustworthiness. Overall, EXGnet sets a new benchmark by combining\nhigh-performance arrhythmia classification with interpretability, paving the\nway for more trustworthy and accessible portable ECG based health monitoring\nsystems.\n","authors":["Tushar Talukder Showrav","Soyabul Islam Lincoln","Md. Kamrul Hasan"],"pdf_url":"https://arxiv.org/pdf/2506.12404v2.pdf","comment":"17 pages, 8 figures"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2507.17984v1","updated":"2025-07-23T23:23:18Z","published":"2025-07-23T23:23:18Z","title":"Machine Unlearning of Traffic State Estimation and Prediction","summary":"  Data-driven traffic state estimation and prediction (TSEP) relies heavily on\ndata sources that contain sensitive information. While the abundance of data\nhas fueled significant breakthroughs, particularly in machine learning-based\nmethods, it also raises concerns regarding privacy, cybersecurity, and data\nfreshness. These issues can erode public trust in intelligent transportation\nsystems. Recently, regulations have introduced the \"right to be forgotten\",\nallowing users to request the removal of their private data from models. As\nmachine learning models can remember old data, simply removing it from back-end\ndatabases is insufficient in such systems. To address these challenges, this\nstudy introduces a novel learning paradigm for TSEP-Machine Unlearning\nTSEP-which enables a trained TSEP model to selectively forget\nprivacy-sensitive, poisoned, or outdated data. By empowering models to\n\"unlearn,\" we aim to enhance the trustworthiness and reliability of data-driven\ntraffic TSEP.\n","authors":["Xin Wang","R. Tyrrell Rockafellar"," Xuegang"," Ban"],"pdf_url":"https://arxiv.org/pdf/2507.17984v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01108v2","updated":"2025-07-23T23:13:37Z","published":"2025-02-03T06:56:40Z","title":"Pulse-PPG: An Open-Source Field-Trained PPG Foundation Model for\n  Wearable Applications Across Lab and Field Settings","summary":"  Photoplethysmography (PPG)-based foundation models are gaining traction due\nto the widespread use of PPG in biosignal monitoring and their potential to\ngeneralize across diverse health applications. In this paper, we introduce\nPulse-PPG, the first open-source PPG foundation model trained exclusively on\nraw PPG data collected over a 100-day field study with 120 participants.\nExisting PPG foundation models are either open-source but trained on clinical\ndata or closed-source, limiting their applicability in real-world settings. We\nevaluate Pulse-PPG across multiple datasets and downstream tasks, comparing its\nperformance against a state-of-the-art foundation model trained on clinical\ndata. Our results demonstrate that Pulse-PPG, trained on uncurated field data,\nexhibits superior generalization across clinical and mobile health applications\nin both lab and field settings. This suggests that exposure to real-world\nvariability enables the model to learn fine-grained representations, making it\nmore adaptable across tasks. Furthermore, pre-training on field data\nsurprisingly outperforms its pre-training on clinical data in many tasks,\nreinforcing the importance of training on real-world, diverse datasets. To\nencourage further advancements in robust foundation models leveraging field\ndata, we plan to release Pulse-PPG, providing researchers with a powerful\nresource for developing more generalizable PPG-based models.\n","authors":["Mithun Saha","Maxwell A. Xu","Wanting Mao","Sameer Neupane","James M. Rehg","Santosh Kumar"],"pdf_url":"https://arxiv.org/pdf/2502.01108v2.pdf","comment":"Saha and Xu are co-first authors"},{"id":"http://arxiv.org/abs/2507.17980v1","updated":"2025-07-23T23:02:10Z","published":"2025-07-23T23:02:10Z","title":"Machine Learning Workflow for Analysis of High-Dimensional Order\n  Parameter Space: A Case Study of Polymer Crystallization from Molecular\n  Dynamics Simulations","summary":"  Currently, identification of crystallization pathways in polymers is being\ncarried out using molecular simulation-based data on a preset cut-off point on\na single order parameter (OP) to define nucleated or crystallized regions.\nAside from sensitivity to cut-off, each of these OPs introduces its own\nsystematic biases. In this study, an integrated machine learning workflow is\npresented to accurately quantify crystallinity in polymeric systems using\natomistic molecular dynamics data. Each atom is represented by a\nhigh-dimensional feature vector that combines geometric, thermodynamic-like,\nand symmetry-based descriptors. Low dimensional embeddings are employed to\nexpose latent structural fingerprints within atomic environments. Subsequently,\nunsupervised clustering on the embeddings identified crystalline and amorphous\natoms with high fidelity. After generating high quality labels with\nmultidimensional data, we use supervised learning techniques to identify a\nminimal set of order parameters that can fully capture this label. Various\ntests were conducted to reduce the feature set, demonstrating that using only\nthree order parameters is sufficient to recreate the crystallization labels.\nBased on these observed OPs, the crystallinity index (C-index) is defined as\nthe logistic regression model's probability of crystallinity, remaining bimodal\nthroughout the process and achieving over 0.98 classification performance\n(AUC). Notably, a model trained on one or a few snapshots enables efficient\non-the-fly computation of crystallinity. Lastly, we demonstrate how the optimal\nC-index fit evolves during various stages of crystallization, supporting the\nhypothesis that entropy dominates early nucleation, while symmetry gains\nrelevance later. This workflow provides a data-driven strategy for OP selection\nand a metric to monitor structural transformations in large-scale polymer\nsimulations.\n","authors":["Elyar Tourani","Brian J. Edwards","Bamin Khomami"],"pdf_url":"https://arxiv.org/pdf/2507.17980v1.pdf","comment":"30 pages, 8 figures, 1 table"},{"id":"http://arxiv.org/abs/2507.17979v1","updated":"2025-07-23T23:00:24Z","published":"2025-07-23T23:00:24Z","title":"SIFOTL: A Principled, Statistically-Informed Fidelity-Optimization\n  Method for Tabular Learning","summary":"  Identifying the factors driving data shifts in tabular datasets is a\nsignificant challenge for analysis and decision support systems, especially\nthose focusing on healthcare. Privacy rules restrict data access, and noise\nfrom complex processes hinders analysis. To address this challenge, we propose\nSIFOTL (Statistically-Informed Fidelity-Optimization Method for Tabular\nLearning) that (i) extracts privacy-compliant data summary statistics, (ii)\nemploys twin XGBoost models to disentangle intervention signals from noise with\nassistance from LLMs, and (iii) merges XGBoost outputs via a Pareto-weighted\ndecision tree to identify interpretable segments responsible for the shift.\nUnlike existing analyses which may ignore noise or require full data access for\nLLM-based analysis, SIFOTL addresses both challenges using only privacy-safe\nsummary statistics. Demonstrating its real-world efficacy, for a MEPS panel\ndataset mimicking a new Medicare drug subsidy, SIFOTL achieves an F1 score of\n0.85, substantially outperforming BigQuery Contribution Analysis (F1=0.46) and\nstatistical tests (F1=0.20) in identifying the segment receiving the subsidy.\nFurthermore, across 18 diverse EHR datasets generated based on Synthea ABM,\nSIFOTL sustains F1 scores of 0.86-0.96 without noise and >= 0.75 even with\ninjected observational noise, whereas baseline average F1 scores range from\n0.19-0.67 under the same tests. SIFOTL, therefore, provides an interpretable,\nprivacy-conscious workflow that is empirically robust to observational noise.\n","authors":["Shubham Mohole","Sainyam Galhotra"],"pdf_url":"https://arxiv.org/pdf/2507.17979v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17977v1","updated":"2025-07-23T22:51:09Z","published":"2025-07-23T22:51:09Z","title":"Improving the Computational Efficiency and Explainability of\n  GeoAggregator","summary":"  Accurate modeling and explaining geospatial tabular data (GTD) are critical\nfor understanding geospatial phenomena and their underlying processes. Recent\nwork has proposed a novel transformer-based deep learning model named\nGeoAggregator (GA) for this purpose, and has demonstrated that it outperforms\nother statistical and machine learning approaches. In this short paper, we\nfurther improve GA by 1) developing an optimized pipeline that accelerates the\ndataloading process and streamlines the forward pass of GA to achieve better\ncomputational efficiency; and 2) incorporating a model ensembling strategy and\na post-hoc model explanation function based on the GeoShapley framework to\nenhance model explainability. We validate the functionality and efficiency of\nthe proposed strategies by applying the improved GA model to synthetic\ndatasets. Experimental results show that our implementation improves the\nprediction accuracy and inference speed of GA compared to the original\nimplementation. Moreover, explanation experiments indicate that GA can\neffectively captures the inherent spatial effects in the designed synthetic\ndataset. The complete pipeline has been made publicly available for community\nuse (https://github.com/ruid7181/GA-sklearn).\n","authors":["Rui Deng","Ziqi Li","Mingshu Wang"],"pdf_url":"https://arxiv.org/pdf/2507.17977v1.pdf","comment":"4 pages, 3 figures"},{"id":"http://arxiv.org/abs/2507.17963v1","updated":"2025-07-23T22:09:38Z","published":"2025-07-23T22:09:38Z","title":"Zero-Shot Dynamic Concept Personalization with Grid-Based LoRA","summary":"  Recent advances in text-to-video generation have enabled high-quality\nsynthesis from text and image prompts. While the personalization of dynamic\nconcepts, which capture subject-specific appearance and motion from a single\nvideo, is now feasible, most existing methods require per-instance fine-tuning,\nlimiting scalability. We introduce a fully zero-shot framework for dynamic\nconcept personalization in text-to-video models. Our method leverages\nstructured 2x2 video grids that spatially organize input and output pairs,\nenabling the training of lightweight Grid-LoRA adapters for editing and\ncomposition within these grids. At inference, a dedicated Grid Fill module\ncompletes partially observed layouts, producing temporally coherent and\nidentity preserving outputs. Once trained, the entire system operates in a\nsingle forward pass, generalizing to previously unseen dynamic concepts without\nany test-time optimization. Extensive experiments demonstrate high-quality and\nconsistent results across a wide range of subjects beyond trained concepts and\nediting scenarios.\n","authors":["Rameen Abdal","Or Patashnik","Ekaterina Deyneka","Hao Chen","Aliaksandr Siarohin","Sergey Tulyakov","Daniel Cohen-Or","Kfir Aberman"],"pdf_url":"https://arxiv.org/pdf/2507.17963v1.pdf","comment":"Project Page and Video :\n  https://snap-research.github.io/zero-shot-dynamic-concepts/"},{"id":"http://arxiv.org/abs/2507.17958v1","updated":"2025-07-23T22:02:56Z","published":"2025-07-23T22:02:56Z","title":"VIBE: Video-Input Brain Encoder for fMRI Response Modeling","summary":"  We present VIBE, a two-stage Transformer that fuses multi-modal video, audio,\nand text features to predict fMRI activity. Representations from open-source\nmodels (Qwen2.5, BEATs, Whisper, SlowFast, V-JEPA) are merged by a\nmodality-fusion transformer and temporally decoded by a prediction transformer\nwith rotary embeddings. Trained on 65 hours of movie data from the CNeuroMod\ndataset and ensembled across 20 seeds, VIBE attains mean parcel-wise Pearson\ncorrelations of 32.25 on in-distribution Friends S07 and 21.25 on six\nout-of-distribution films. An earlier iteration of the same architecture\nobtained 0.3198 and 0.2096, respectively, winning Phase-1 and placing second\noverall in the Algonauts 2025 Challenge.\n","authors":["Daniel Carlstrom Schad","Shrey Dixit","Janis Keck","Viktor Studenyak","Aleksandr Shpilevoi","Andrej Bicanski"],"pdf_url":"https://arxiv.org/pdf/2507.17958v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17953v1","updated":"2025-07-23T21:50:28Z","published":"2025-07-23T21:50:28Z","title":"Clo-HDnn: A 4.66 TFLOPS/W and 3.78 TOPS/W Continual On-Device Learning\n  Accelerator with Energy-efficient Hyperdimensional Computing via Progressive\n  Search","summary":"  Clo-HDnn is an on-device learning (ODL) accelerator designed for emerging\ncontinual learning (CL) tasks. Clo-HDnn integrates hyperdimensional computing\n(HDC) along with low-cost Kronecker HD Encoder and weight clustering feature\nextraction (WCFE) to optimize accuracy and efficiency. Clo-HDnn adopts\ngradient-free CL to efficiently update and store the learned knowledge in the\nform of class hypervectors. Its dual-mode operation enables bypassing costly\nfeature extraction for simpler datasets, while progressive search reduces\ncomplexity by up to 61% by encoding and comparing only partial query\nhypervectors. Achieving 4.66 TFLOPS/W (FE) and 3.78 TOPS/W (classifier),\nClo-HDnn delivers 7.77x and 4.85x higher energy efficiency compared to SOTA ODL\naccelerators.\n","authors":["Chang Eun Song","Weihong Xu","Keming Fan","Soumil Jain","Gopabandhu Hota","Haichao Yang","Leo Liu","Kerem Akarvardar","Meng-Fan Chang","Carlos H. Diaz","Gert Cauwenberghs","Tajana Rosing","Mingu Kang"],"pdf_url":"https://arxiv.org/pdf/2507.17953v1.pdf","comment":"Published in 2025 Symposium on VLSI Technology and Circuits (VLSI\n  Technology and Circuits), Kyoto, Japan, 2025"},{"id":"http://arxiv.org/abs/2412.09900v3","updated":"2025-07-23T21:24:40Z","published":"2024-12-13T06:35:55Z","title":"Analyzing Fairness of Computer Vision and Natural Language Processing\n  Models","summary":"  Machine learning (ML) algorithms play a critical role in decision-making\nacross various domains, such as healthcare, finance, education, and law\nenforcement. However, concerns about fairness and bias in these systems have\nraised significant ethical and social challenges. To address these challenges,\nthis research utilizes two prominent fairness libraries, Fairlearn by Microsoft\nand AIF360 by IBM. These libraries offer comprehensive frameworks for fairness\nanalysis, providing tools to evaluate fairness metrics, visualize results, and\nimplement bias mitigation algorithms. The study focuses on assessing and\nmitigating biases for unstructured datasets using Computer Vision (CV) and\nNatural Language Processing (NLP) models. The primary objective is to present a\ncomparative analysis of the performance of mitigation algorithms from the two\nfairness libraries. This analysis involves applying the algorithms\nindividually, one at a time, in one of the stages of the ML lifecycle,\npre-processing, in-processing, or post-processing, as well as sequentially\nacross more than one stage. The results reveal that some sequential\napplications improve the performance of mitigation algorithms by effectively\nreducing bias while maintaining the model's performance. Publicly available\ndatasets from Kaggle were chosen for this research, providing a practical\ncontext for evaluating fairness in real-world machine learning workflows.\n","authors":["Ahmed Rashed","Abdelkrim Kallich","Mohamed Eltayeb"],"pdf_url":"https://arxiv.org/pdf/2412.09900v3.pdf","comment":"25 pages, 8 table, 11 figures"},{"id":"http://arxiv.org/abs/2406.03674v3","updated":"2025-07-23T21:09:41Z","published":"2024-06-06T01:29:47Z","title":"Learning Safe Strategies for Value Maximizing Buyers in Uniform Price\n  Auctions","summary":"  We study the bidding problem in repeated uniform price multi-unit auctions\nfrom the perspective of a value-maximizing buyer. The buyer aims to maximize\ntheir cumulative value over $T$ rounds while adhering to per-round\nreturn-on-investment (RoI) constraints in a strategic (or adversarial)\nenvironment. Using an $m$-uniform bidding format, the buyer submits $m$\nbid-quantity pairs $(b_i, q_i)$ to demand $q_i$ units at bid $b_i$, with $m \\ll\nM$ in practice, where $M$ denotes the maximum demand of the buyer.\n  We introduce the notion of safe bidding strategies as those that satisfy the\nRoI constraints irrespective of competing bids. Despite the stringent\nrequirement, we show that these strategies satisfy a mild no-overbidding\ncondition, depend only on the valuation curve of the bidder, and the bidder can\nfocus on a finite subset without loss of generality. Though the subset size is\n$O(M^m)$, we design a polynomial-time learning algorithm that achieves\nsublinear regret, both in full-information and bandit settings, relative to the\nhindsight-optimal safe strategy.\n  We assess the robustness of safe strategies against the hindsight-optimal\nstrategy from a richer class. We define the richness ratio $\\alpha \\in (0,1]$\nas the minimum ratio of the value of the optimal safe strategy to that of the\noptimal strategy from richer class and construct hard instances showing the\ntightness of $\\alpha$. Our algorithm achieves $\\alpha$-approximate sublinear\nregret against these stronger benchmarks. Simulations on semi-synthetic auction\ndata show that empirical richness ratios significantly outperform the\ntheoretical worst-case bounds. The proposed safe strategies and learning\nalgorithm extend naturally to more nuanced buyer and competitor models.\n","authors":["Negin Golrezaei","Sourav Sahoo"],"pdf_url":"https://arxiv.org/pdf/2406.03674v3.pdf","comment":"84 pages, 5 figures. Appeared at ICML 2025"},{"id":"http://arxiv.org/abs/2507.17931v1","updated":"2025-07-23T21:08:29Z","published":"2025-07-23T21:08:29Z","title":"Quantum Machine Learning Playground","summary":"  This article introduces an innovative interactive visualization tool designed\nto demystify quantum machine learning (QML) algorithms. Our work is inspired by\nthe success of classical machine learning visualization tools, such as\nTensorFlow Playground, and aims to bridge the gap in visualization resources\nspecifically for the field of QML. The article includes a comprehensive\noverview of relevant visualization metaphors from both quantum computing and\nclassical machine learning, the development of an algorithm visualization\nconcept, and the design of a concrete implementation as an interactive web\napplication. By combining common visualization metaphors for the so-called data\nre-uploading universal quantum classifier as a representative QML model, this\narticle aims to lower the entry barrier to quantum computing and encourage\nfurther innovation in the field. The accompanying interactive application is a\nproposal for the first version of a quantum machine learning playground for\nlearning and exploring QML models.\n","authors":["Pascal Debus","Sebastian Issel","Kilian Tscharke"],"pdf_url":"https://arxiv.org/pdf/2507.17931v1.pdf","comment":"Accepted to IEEE Computer Graphics and Applications. Final version:\n  https://doi.org/10.1109/MCG.2024.3456288"},{"id":"http://arxiv.org/abs/2507.09871v2","updated":"2025-07-23T20:53:29Z","published":"2025-07-14T02:53:14Z","title":"Task Priors: Enhancing Model Evaluation by Considering the Entire Space\n  of Downstream Tasks","summary":"  The grand goal of AI research, and particularly Self Supervised Learning\n(SSL), is to produce systems that can successfully solve any possible task. In\ncontrast, current evaluation methods available to AI researchers typically rely\non a fixed collection of hand-picked downstream benchmarks. Hence, a large\namount of effort is put into designing and searching for large collection of\nevaluation tasks that can serve as a proxy of our grand goal. We argue that\nsuch a rigid evaluation protocol creates a silent bottleneck in AI research. To\nremedy that, we define a probabilistic space of downstream tasks obtained by\nadopting a distribution of tasks and by defining Task Priors. Under this view,\none can evaluate a model's performance over the set of all possible downstream\ntasks. Our framework is the first to provide answers to key questions such as\n(i) what is the average performance of my model over all possible downstream\ntasks weighted by the probability to encounter each task? or (ii) what is the\nvariance of my model's performance across all downstream tasks under the\ndefined Task Priors? Beyond establishing a new standard for evaluation, we\nbelieve that Task Priors will accelerate the pace of research in SSL - where\ndownstream task evaluation is the sole qualitative signal that researchers have\naccess to.\n","authors":["Niket Patel","Randall Balestriero"],"pdf_url":"https://arxiv.org/pdf/2507.09871v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17924v1","updated":"2025-07-23T20:44:25Z","published":"2025-07-23T20:44:25Z","title":"UrbanPulse: A Cross-City Deep Learning Framework for Ultra-Fine-Grained\n  Population Transfer Prediction","summary":"  Accurate population flow prediction is essential for urban planning,\ntransportation management, and public health. Yet existing methods face key\nlimitations: traditional models rely on static spatial assumptions, deep\nlearning models struggle with cross-city generalization, and Large Language\nModels (LLMs) incur high computational costs while failing to capture spatial\nstructure. Moreover, many approaches sacrifice resolution by clustering Points\nof Interest (POIs) or restricting coverage to subregions, limiting their\nutility for city-wide analytics. We introduce UrbanPulse, a scalable deep\nlearning framework that delivers ultra-fine-grained, city-wide OD flow\npredictions by treating each POI as an individual node. It combines a temporal\ngraph convolutional encoder with a transformer-based decoder to model\nmulti-scale spatiotemporal dependencies. To ensure robust generalization across\nurban contexts, UrbanPulse employs a three-stage transfer learning strategy:\npretraining on large-scale urban graphs, cold-start adaptation, and\nreinforcement learning fine-tuning.Evaluated on over 103 million cleaned GPS\nrecords from three metropolitan areas in California, UrbanPulse achieves\nstate-of-the-art accuracy and scalability. Through efficient transfer learning,\nUrbanPulse takes a key step toward making high-resolution, AI-powered urban\nforecasting deployable in practice across diverse cities.\n","authors":["Hongrong Yang","Markus Schlaepfer"],"pdf_url":"https://arxiv.org/pdf/2507.17924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17922v1","updated":"2025-07-23T20:39:14Z","published":"2025-07-23T20:39:14Z","title":"From Seed to Harvest: Augmenting Human Creativity with AI for\n  Red-teaming Text-to-Image Models","summary":"  Text-to-image (T2I) models have become prevalent across numerous\napplications, making their robust evaluation against adversarial attacks a\ncritical priority. Continuous access to new and challenging adversarial prompts\nacross diverse domains is essential for stress-testing these models for\nresilience against novel attacks from multiple vectors. Current techniques for\ngenerating such prompts are either entirely authored by humans or synthetically\ngenerated. On the one hand, datasets of human-crafted adversarial prompts are\noften too small in size and imbalanced in their cultural and contextual\nrepresentation. On the other hand, datasets of synthetically-generated prompts\nachieve scale, but typically lack the realistic nuances and creative\nadversarial strategies found in human-crafted prompts. To combine the strengths\nof both human and machine approaches, we propose Seed2Harvest, a hybrid\nred-teaming method for guided expansion of culturally diverse, human-crafted\nadversarial prompt seeds. The resulting prompts preserve the characteristics\nand attack patterns of human prompts while maintaining comparable average\nattack success rates (0.31 NudeNet, 0.36 SD NSFW, 0.12 Q16). Our expanded\ndataset achieves substantially higher diversity with 535 unique geographic\nlocations and a Shannon entropy of 7.48, compared to 58 locations and 5.28\nentropy in the original dataset. Our work demonstrates the importance of\nhuman-machine collaboration in leveraging human creativity and machine\ncomputational capacity to achieve comprehensive, scalable red-teaming for\ncontinuous T2I model safety evaluation.\n","authors":["Jessica Quaye","Charvi Rastogi","Alicia Parrish","Oana Inel","Minsuk Kahng","Lora Aroyo","Vijay Janapa Reddi"],"pdf_url":"https://arxiv.org/pdf/2507.17922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17921v1","updated":"2025-07-23T20:35:15Z","published":"2025-07-23T20:35:15Z","title":"Sliding Window Informative Canonical Correlation Analysis","summary":"  Canonical correlation analysis (CCA) is a technique for finding correlated\nsets of features between two datasets. In this paper, we propose a novel\nextension of CCA to the online, streaming data setting: Sliding Window\nInformative Canonical Correlation Analysis (SWICCA). Our method uses a\nstreaming principal component analysis (PCA) algorithm as a backend and uses\nthese outputs combined with a small sliding window of samples to estimate the\nCCA components in real time. We motivate and describe our algorithm, provide\nnumerical simulations to characterize its performance, and provide a\ntheoretical performance guarantee. The SWICCA method is applicable and scalable\nto extremely high dimensions, and we provide a real-data example that\ndemonstrates this capability.\n","authors":["Arvind Prasadan"],"pdf_url":"https://arxiv.org/pdf/2507.17921v1.pdf","comment":"22 pages, submitted"},{"id":"http://arxiv.org/abs/2503.15704v4","updated":"2025-07-23T20:34:43Z","published":"2025-03-19T21:35:02Z","title":"Tuning Sequential Monte Carlo Samplers via Greedy Incremental Divergence\n  Minimization","summary":"  The performance of sequential Monte Carlo (SMC) samplers heavily depends on\nthe tuning of the Markov kernels used in the path proposal. For SMC samplers\nwith unadjusted Markov kernels, standard tuning objectives, such as the\nMetropolis-Hastings acceptance rate or the expected-squared jump distance, are\nno longer applicable. While stochastic gradient-based end-to-end optimization\nhas been explored for tuning SMC samplers, they often incur excessive training\ncosts, even for tuning just the kernel step sizes. In this work, we propose a\ngeneral adaptation framework for tuning the Markov kernels in SMC samplers by\nminimizing the incremental Kullback-Leibler (KL) divergence between the\nproposal and target paths. For step size tuning, we provide a gradient- and\ntuning-free algorithm that is generally applicable for kernels such as Langevin\nMonte Carlo (LMC). We further demonstrate the utility of our approach by\nproviding a tailored scheme for tuning kinetic LMC used in SMC samplers. Our\nimplementations are able to obtain a full schedule of tuned parameters at the\ncost of a few vanilla SMC runs, which is a fraction of gradient-based\napproaches.\n","authors":["Kyurae Kim","Zuheng Xu","Jacob R. Gardner","Trevor Campbell"],"pdf_url":"https://arxiv.org/pdf/2503.15704v4.pdf","comment":"Accepted to ICML'25; v4: fixed typos"},{"id":"http://arxiv.org/abs/2507.17912v1","updated":"2025-07-23T20:22:20Z","published":"2025-07-23T20:22:20Z","title":"SETOL: A Semi-Empirical Theory of (Deep) Learning","summary":"  We present a SemiEmpirical Theory of Learning (SETOL) that explains the\nremarkable performance of State-Of-The-Art (SOTA) Neural Networks (NNs). We\nprovide a formal explanation of the origin of the fundamental quantities in the\nphenomenological theory of Heavy-Tailed Self-Regularization (HTSR): the\nheavy-tailed power-law layer quality metrics, alpha and alpha-hat. In prior\nwork, these metrics have been shown to predict trends in the test accuracies of\npretrained SOTA NN models, importantly, without needing access to either\ntesting or training data. Our SETOL uses techniques from statistical mechanics\nas well as advanced methods from random matrix theory and quantum chemistry.\nThe derivation suggests new mathematical preconditions for ideal learning,\nincluding a new metric, ERG, which is equivalent to applying a single step of\nthe Wilson Exact Renormalization Group. We test the assumptions and predictions\nof SETOL on a simple 3-layer multilayer perceptron (MLP), demonstrating\nexcellent agreement with the key theoretical assumptions. For SOTA NN models,\nwe show how to estimate the individual layer qualities of a trained NN by\nsimply computing the empirical spectral density (ESD) of the layer weight\nmatrices and plugging this ESD into our SETOL formulas. Notably, we examine the\nperformance of the HTSR alpha and the SETOL ERG layer quality metrics, and find\nthat they align remarkably well, both on our MLP and on SOTA NNs.\n","authors":["Charles H Martin","Christopher Hinrichs"],"pdf_url":"https://arxiv.org/pdf/2507.17912v1.pdf","comment":"139 pages, 28 figures. Code for experiments available at\n  https://github.com/charlesmartin14/SETOL_experiments"},{"id":"http://arxiv.org/abs/2507.11783v2","updated":"2025-07-23T20:10:43Z","published":"2025-07-15T22:52:44Z","title":"EEG Foundation Models: A Critical Review of Current Progress and Future\n  Directions","summary":"  Patterns of electrical brain activity recorded via electroencephalography\n(EEG) offer immense value for scientific and clinical investigations. The\ninability of supervised EEG encoders to learn robust EEG patterns and their\nover-reliance on expensive signal annotations have sparked a transition towards\ngeneral-purpose self-supervised EEG encoders, i.e., EEG foundation models\n(EEG-FMs), for robust and scalable EEG feature extraction. However, the\nreal-world readiness of early EEG-FMs and the rubric for long-term research\nprogress remain unclear. A systematic and comprehensive review of\nfirst-generation EEG-FMs is therefore necessary to understand the current\nstate-of-the-art and identify key directions for future EEG-FMs. To that end,\nthis study reviews 10 early EEG-FMs and presents a critical synthesis of their\nmethodology, empirical findings, and outstanding research gaps. We find that\nmost EEG-FMs adopt a sequence-based modeling scheme that relies on\ntransformer-based backbones and the reconstruction of masked sequences for\nself-supervision. However, model evaluations remain heterogeneous and largely\nlimited, making it challenging to assess their practical off-the-shelf utility.\nIn addition to adopting standardized and realistic evaluations, future work\nshould demonstrate more substantial scaling effects and make principled and\ntrustworthy choices throughout the EEG representation learning pipeline. We\nbelieve that developing benchmarks, software tools, technical methodologies,\nand applications in collaboration with domain experts may further advance the\ntranslational utility and real-world adoption of EEG-FMs.\n","authors":["Gayal Kuruppu","Neeraj Wagh","Yogatheesan Varatharajah"],"pdf_url":"https://arxiv.org/pdf/2507.11783v2.pdf","comment":"20 pages, 5 figures, 3 tables (main + supplement)"},{"id":"http://arxiv.org/abs/2507.17907v1","updated":"2025-07-23T20:07:53Z","published":"2025-07-23T20:07:53Z","title":"Deep learning-aided inverse design of porous metamaterials","summary":"  The ultimate aim of the study is to explore the inverse design of porous\nmetamaterials using a deep learning-based generative framework. Specifically,\nwe develop a property-variational autoencoder (pVAE), a variational autoencoder\n(VAE) augmented with a regressor, to generate structured metamaterials with\ntailored hydraulic properties, such as porosity and permeability. While this\nwork uses the lattice Boltzmann method (LBM) to generate intrinsic permeability\ntensor data for limited porous microstructures, a convolutional neural network\n(CNN) is trained using a bottom-up approach to predict effective hydraulic\nproperties. This significantly reduces the computational cost compared to\ndirect LBM simulations. The pVAE framework is trained on two datasets: a\nsynthetic dataset of artificial porous microstructures and CT-scan images of\nvolume elements from real open-cell foams. The encoder-decoder architecture of\nthe VAE captures key microstructural features, mapping them into a compact and\ninterpretable latent space for efficient structure-property exploration. The\nstudy provides a detailed analysis and interpretation of the latent space,\ndemonstrating its role in structure-property mapping, interpolation, and\ninverse design. This approach facilitates the generation of new metamaterials\nwith desired properties. The datasets and codes used in this study will be made\nopen-access to support further research.\n","authors":["Phu Thien Nguyen","Yousef Heider","Dennis M. Kochmann","Fadi Aldakheel"],"pdf_url":"https://arxiv.org/pdf/2507.17907v1.pdf","comment":"31 pages, 29 figures"},{"id":"http://arxiv.org/abs/2507.17903v1","updated":"2025-07-23T20:01:36Z","published":"2025-07-23T20:01:36Z","title":"Federated Learning for Large-Scale Cloud Robotic Manipulation:\n  Opportunities and Challenges","summary":"  Federated Learning (FL) is an emerging distributed machine learning paradigm,\nwhere the collaborative training of a model involves dynamic participation of\ndevices to achieve broad objectives. In contrast, classical machine learning\n(ML) typically requires data to be located on-premises for training, whereas FL\nleverages numerous user devices to train a shared global model without the need\nto share private data. Current robotic manipulation tasks are constrained by\nthe individual capabilities and speed of robots due to limited low-latency\ncomputing resources. Consequently, the concept of cloud robotics has emerged,\nallowing robotic applications to harness the flexibility and reliability of\ncomputing resources, effectively alleviating their computational demands across\nthe cloud-edge continuum. Undoubtedly, within this distributed computing\ncontext, as exemplified in cloud robotic manipulation scenarios, FL offers\nmanifold advantages while also presenting several challenges and opportunities.\nIn this paper, we present fundamental concepts of FL and their connection to\ncloud robotic manipulation. Additionally, we envision the opportunities and\nchallenges associated with realizing efficient and reliable cloud robotic\nmanipulation at scale through FL, where researchers adopt to design and verify\nFL models in either centralized or decentralized settings.\n","authors":["Obaidullah Zaland","Chanh Nguyen","Florian T. Pokorny","Monowar Bhuyan"],"pdf_url":"https://arxiv.org/pdf/2507.17903v1.pdf","comment":"Accepted for Presentation at IEEE International Conference on Machine\n  Learning and Cybernetics (ICMLC) 2025"},{"id":"http://arxiv.org/abs/2507.17897v1","updated":"2025-07-23T19:48:27Z","published":"2025-07-23T19:48:27Z","title":"Multimodal Recurrent Ensembles for Predicting Brain Responses to\n  Naturalistic Movies (Algonauts 2025)","summary":"  Accurately predicting distributed cortical responses to naturalistic stimuli\nrequires models that integrate visual, auditory and semantic information over\ntime. We present a hierarchical multimodal recurrent ensemble that maps\npretrained video, audio, and language embeddings to fMRI time series recorded\nwhile four subjects watched almost 80 hours of movies provided by the Algonauts\n2025 challenge. Modality-specific bidirectional RNNs encode temporal dynamics;\ntheir hidden states are fused and passed to a second recurrent layer, and\nlightweight subject-specific heads output responses for 1000 cortical parcels.\nTraining relies on a composite MSE-correlation loss and a curriculum that\ngradually shifts emphasis from early sensory to late association regions.\nAveraging 100 model variants further boosts robustness. The resulting system\nranked third on the competition leaderboard, achieving an overall Pearson r =\n0.2094 and the highest single-parcel peak score (mean r = 0.63) among all\nparticipants, with particularly strong gains for the most challenging subject\n(Subject 5). The approach establishes a simple, extensible baseline for future\nmultimodal brain-encoding benchmarks.\n","authors":["Semih Eren","Deniz Kucukahmetler","Nico Scherf"],"pdf_url":"https://arxiv.org/pdf/2507.17897v1.pdf","comment":"8 pages, 2 figures, 1 table. Invited report, CCN 2025 Algonauts\n  Project session (3rd-place team). Code:\n  https://github.com/erensemih/Algonauts2025_ModalityRNN"},{"id":"http://arxiv.org/abs/2507.17895v1","updated":"2025-07-23T19:46:08Z","published":"2025-07-23T19:46:08Z","title":"Lower Bounds for Public-Private Learning under Distribution Shift","summary":"  The most effective differentially private machine learning algorithms in\npractice rely on an additional source of purportedly public data. This paradigm\nis most interesting when the two sources combine to be more than the sum of\ntheir parts. However, there are settings such as mean estimation where we have\nstrong lower bounds, showing that when the two data sources have the same\ndistribution, there is no complementary value to combining the two data\nsources. In this work we extend the known lower bounds for public-private\nlearning to setting where the two data sources exhibit significant distribution\nshift. Our results apply to both Gaussian mean estimation where the two\ndistributions have different means, and to Gaussian linear regression where the\ntwo distributions exhibit parameter shift. We find that when the shift is small\n(relative to the desired accuracy), either public or private data must be\nsufficiently abundant to estimate the private parameter. Conversely, when the\nshift is large, public data provides no benefit.\n","authors":["Amrith Setlur","Pratiksha Thaker","Jonathan Ullman"],"pdf_url":"https://arxiv.org/pdf/2507.17895v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2507.17893v1","updated":"2025-07-23T19:42:51Z","published":"2025-07-23T19:42:51Z","title":"Action-List Reinforcement Learning Syndrome Decoding for Binary Linear\n  Block Codes","summary":"  This paper explores the application of reinforcement learning techniques to\nenhance the performance of decoding of linear block codes based on flipping\nbits and finding optimal decisions. We describe the methodology for mapping the\niterative decoding process into Markov Decision Processes (MDPs) and propose\ndifferent methods to reduce the number of states in the MDP. A truncated MDP is\nproposed to reduce the number of states in the MDP by learning a Hamming ball\nwith a specified radius around codewords. We then propose a general scheme for\nreinforcement learning based decoders applicable to any class of codes to\nimprove the performance of decoders. We call this scheme an action-list\ndecoding. We design an action-list decoder based on the Deep-Q network values\nthat substantially enhance performance. We also get benefit of automorphism\ngroup of code to further improve the code performance. Additionally, we propose\na feedback-based method to exploit and enhance the performance of existing\nhigh-performing decoders by applying reinforcement learning algorithms after\nthe existing decoders. These approaches effectively reduces the complexity of\nthe reinforcement learning block. Finally, we present experimental results for\nthe Low-Density Parity Check (LDPC) codes over the Binary Symmetric Channel\n(BSC) to demonstrate the efficiency of the proposed methods.\n","authors":["Milad Taghipour","Bane Vasic"],"pdf_url":"https://arxiv.org/pdf/2507.17893v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06785v2","updated":"2025-07-23T19:32:20Z","published":"2025-02-10T18:58:52Z","title":"DeepCrossAttention: Supercharging Transformer Residual Connections","summary":"  Transformer networks have achieved remarkable success across diverse domains,\nleveraging a variety of architectural innovations, including residual\nconnections. However, traditional residual connections, which simply sum the\noutputs of previous layers, can dilute crucial information. This work\nintroduces DeepCrossAttention (DCA), an approach that enhances residual\nlearning in transformers. DCA employs learnable, input-dependent weights to\ndynamically combine layer outputs, enabling the model to selectively focus on\nthe most relevant information in any of the previous layers. Furthermore, DCA\nincorporates depth-wise cross-attention, allowing for richer interactions\nbetween layers at different depths. Our language modeling experiments show that\nDCA achieves improved perplexity for a given training time. Moreover, DCA\nobtains the same model quality up to 3x faster while adding a negligible number\nof parameters. Theoretical analysis confirms that DCA provides an improved\ntrade-off between accuracy and model size when the ratio of collective layer\nranks to the ambient dimension falls below a critical threshold.\n","authors":["Mike Heddes","Adel Javanmard","Kyriakos Axiotis","Gang Fu","MohammadHossein Bateni","Vahab Mirrokni"],"pdf_url":"https://arxiv.org/pdf/2502.06785v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17887v1","updated":"2025-07-23T19:30:34Z","published":"2025-07-23T19:30:34Z","title":"Fourier Neural Operators for Non-Markovian Processes:Approximation\n  Theorems and Experiments","summary":"  This paper introduces an operator-based neural network, the mirror-padded\nFourier neural operator (MFNO), designed to learn the dynamics of stochastic\nsystems. MFNO extends the standard Fourier neural operator (FNO) by\nincorporating mirror padding, enabling it to handle non-periodic inputs. We\nrigorously prove that MFNOs can approximate solutions of path-dependent\nstochastic differential equations and Lipschitz transformations of fractional\nBrownian motions to an arbitrary degree of accuracy. Our theoretical analysis\nbuilds on Wong--Zakai type theorems and various approximation techniques.\nEmpirically, the MFNO exhibits strong resolution generalization--a property\nrarely seen in standard architectures such as LSTMs, TCNs, and DeepONet.\nFurthermore, our model achieves performance that is comparable or superior to\nthese baselines while offering significantly faster sample path generation than\nclassical numerical schemes.\n","authors":["Wonjae Lee","Taeyoung Kim","Hyungbin Park"],"pdf_url":"https://arxiv.org/pdf/2507.17887v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13180v3","updated":"2025-07-23T19:22:35Z","published":"2025-04-17T17:59:56Z","title":"PerceptionLM: Open-Access Data and Models for Detailed Visual\n  Understanding","summary":"  Vision-language models are integral to computer vision research, yet many\nhigh-performing models remain closed-source, obscuring their data, design and\ntraining recipe. The research community has responded by using distillation\nfrom black-box models to label training data, achieving strong benchmark\nresults, at the cost of measurable scientific progress. However, without\nknowing the details of the teacher model and its data sources, scientific\nprogress remains difficult to measure. In this paper, we study building a\nPerception Language Model (PLM) in a fully open and reproducible framework for\ntransparent research in image and video understanding. We analyze standard\ntraining pipelines without distillation from proprietary models and explore\nlarge-scale synthetic data to identify critical data gaps, particularly in\ndetailed video understanding. To bridge these gaps, we release 2.8M\nhuman-labeled instances of fine-grained video question-answer pairs and\nspatio-temporally grounded video captions. Additionally, we introduce\nPLM-VideoBench, a suite for evaluating challenging video understanding tasks\nfocusing on the ability to reason about \"what\", \"where\", \"when\", and \"how\" of a\nvideo. We make our work fully reproducible by providing data, training recipes,\ncode & models. https://github.com/facebookresearch/perception_models\n","authors":["Jang Hyun Cho","Andrea Madotto","Effrosyni Mavroudi","Triantafyllos Afouras","Tushar Nagarajan","Muhammad Maaz","Yale Song","Tengyu Ma","Shuming Hu","Suyog Jain","Miguel Martin","Huiyu Wang","Hanoona Rasheed","Peize Sun","Po-Yao Huang","Daniel Bolya","Nikhila Ravi","Shashank Jain","Tammy Stark","Shane Moon","Babak Damavandi","Vivian Lee","Andrew Westbury","Salman Khan","Philipp Krähenbühl","Piotr Dollár","Lorenzo Torresani","Kristen Grauman","Christoph Feichtenhofer"],"pdf_url":"https://arxiv.org/pdf/2504.13180v3.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2507.17881v1","updated":"2025-07-23T19:14:46Z","published":"2025-07-23T19:14:46Z","title":"A Supervised Machine Learning Framework for Multipactor Breakdown\n  Prediction in High-Power Radio Frequency Devices and Accelerator Components:\n  A Case Study in Planar Geometry","summary":"  Multipactor is a nonlinear electron avalanche phenomenon that can severely\nimpair the performance of high-power radio frequency (RF) devices and\naccelerator systems. Accurate prediction of multipactor susceptibility across\ndifferent materials and operational regimes remains a critical yet\ncomputationally intensive challenge in accelerator component design and RF\nengineering. This study presents the first application of supervised machine\nlearning (ML) for predicting multipactor susceptibility in two-surface planar\ngeometries. A simulation-derived dataset spanning six distinct secondary\nelectron yield (SEY) material profiles is used to train regression models -\nincluding Random Forest (RF), Extra Trees (ET), Extreme Gradient Boosting\n(XGBoost), and funnel-structured Multilayer Perceptrons (MLPs) - to predict the\ntime-averaged electron growth rate, ${\\delta}_{avg}$. Performance is evaluated\nusing Intersection over Union (IoU), Structural Similarity Index (SSIM), and\nPearson correlation coefficient. Tree-based models consistently outperform MLPs\nin generalizing across disjoint material domains. MLPs trained using a\nscalarized objective function that combines IoU and SSIM during Bayesian\nhyperparameter optimization with 5-fold cross-validation outperform those\ntrained with single-objective loss functions. Principal Component Analysis\nreveals that performance degradation for certain materials stems from disjoint\nfeature-space distributions, underscoring the need for broader dataset\ncoverage. This study demonstrates both the promise and limitations of ML-based\nmultipactor prediction and lays the groundwork for accelerated, data-driven\nmodeling in advanced RF and accelerator system design.\n","authors":["Asif Iqbal","John Verboncoeur","Peng Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.17881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17876v1","updated":"2025-07-23T19:05:37Z","published":"2025-07-23T19:05:37Z","title":"Look the Other Way: Designing 'Positive' Molecules with Negative Data\n  via Task Arithmetic","summary":"  The scarcity of molecules with desirable properties (i.e., 'positive'\nmolecules) is an inherent bottleneck for generative molecule design. To\nsidestep such obstacle, here we propose molecular task arithmetic: training a\nmodel on diverse and abundant negative examples to learn 'property directions'\n$--$ without accessing any positively labeled data $--$ and moving models in\nthe opposite property directions to generate positive molecules. When analyzed\non 20 zero-shot design experiments, molecular task arithmetic generated more\ndiverse and successful designs than models trained on positive molecules.\nMoreover, we employed molecular task arithmetic in dual-objective and few-shot\ndesign tasks. We find that molecular task arithmetic can consistently increase\nthe diversity of designs while maintaining desirable design properties. With\nits simplicity, data efficiency, and performance, molecular task arithmetic\nbears the potential to become the $\\textit{de-facto}$ transfer learning\nstrategy for de novo molecule design.\n","authors":["Rıza Özçelik","Sarah de Ruiter","Francesca Grisoni"],"pdf_url":"https://arxiv.org/pdf/2507.17876v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17869v1","updated":"2025-07-23T18:53:23Z","published":"2025-07-23T18:53:23Z","title":"Integrating Feature Selection and Machine Learning for Nitrogen\n  Assessment in Grapevine Leaves using In-Field Hyperspectral Imaging","summary":"  Nitrogen (N) is one of the most crucial nutrients in vineyards, affecting\nplant growth and subsequent products such as wine and juice. Because soil N has\nhigh spatial and temporal variability, it is desirable to accurately estimate\nthe N concentration of grapevine leaves and manage fertilization at the\nindividual plant level to optimally meet plant needs. In this study, we used\nin-field hyperspectral images with wavelengths ranging from $400 to 1000nm of\nfour different grapevine cultivars collected from distinct vineyards and over\ntwo growth stages during two growing seasons to develop models for predicting N\nconcentration at the leaf-level and canopy-level. After image processing, two\nfeature selection methods were employed to identify the optimal set of spectral\nbands that were responsive to leaf N concentrations. The selected spectral\nbands were used to train and test two different Machine Learning (ML) models,\nGradient Boosting and XGBoost, for predicting nitrogen concentrations. The\ncomparison of selected bands for both leaf-level and canopy-level datasets\nshowed that most of the spectral regions identified by the feature selection\nmethods were across both methods and the dataset types (leaf- and canopy-level\ndatasets), particularly in the key regions, 500-525nm, 650-690nm, 750-800nm,\nand 900-950nm. These findings indicated the robustness of these spectral\nregions for predicting nitrogen content. The results for N prediction\ndemonstrated that the ML model achieved an R square of 0.49 for canopy-level\ndata and an R square of 0.57 for leaf-level data, despite using different sets\nof selected spectral bands for each analysis level. The study demonstrated the\npotential of using in-field hyperspectral imaging and the use of spectral data\nin integrated feature selection and ML techniques to monitor N status in\nvineyards.\n","authors":["Atif Bilal Asad","Achyut Paudel","Safal Kshetri","Chenchen Kang","Salik Ram Khanal","Nataliya Shcherbatyuk","Pierre Davadant","R. Paul Schreiner","Santosh Kalauni","Manoj Karkee","Markus Keller"],"pdf_url":"https://arxiv.org/pdf/2507.17869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.22607v2","updated":"2025-07-23T18:51:06Z","published":"2025-06-27T20:09:50Z","title":"Learning Individual Reproductive Behavior from Aggregate Fertility Rates\n  via Neural Posterior Estimation","summary":"  Age-specific fertility rates (ASFRs) provide the most extensive record of\nreproductive change, but their aggregate nature obscures the individual-level\nbehavioral mechanisms that drive fertility trends. To bridge this micro-macro\ndivide, we introduce a likelihood-free Bayesian framework that couples a\ndemographically interpretable, individual-level simulation model of the\nreproductive process with Sequential Neural Posterior Estimation (SNPE). We\nshow that this framework successfully recovers core behavioral parameters\ngoverning contemporary fertility, including preferences for family size,\nreproductive timing, and contraceptive failure, using only ASFRs. The\nframework's effectiveness is validated on cohorts from four countries with\ndiverse fertility regimes. Most compellingly, the model, estimated solely on\naggregate data, successfully predicts out-of-sample distributions of\nindividual-level outcomes, including age at first sex, desired family size, and\nbirth intervals. Because our framework yields complete synthetic life\nhistories, it significantly reduces the data requirements for building\nmicrosimulation models and enables behaviorally explicit demographic forecasts.\n","authors":["Daniel Ciganda","Ignacio Campón","Iñaki Permanyer","Jakob H Macke"],"pdf_url":"https://arxiv.org/pdf/2506.22607v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.17999v2","updated":"2025-07-23T18:50:43Z","published":"2025-04-25T00:58:37Z","title":"Streaming, Fast and Slow: Cognitive Load-Aware Streaming for Efficient\n  LLM Serving","summary":"  Generative conversational interfaces powered by large language models (LLMs)\ntypically stream output token-by-token at a rate determined by computational\nbudget, often neglecting actual human reading speeds and the cognitive load\nassociated with the content. This mismatch frequently leads to inefficient use\nof computational resources. For example, in cloud-based services, streaming\ncontent faster than users can read appears unnecessary, resulting in wasted\ncomputational resources and potential delays for other users, particularly\nduring peak usage periods. To address this issue, we propose an adaptive\nstreaming method that dynamically adjusts the pacing of LLM streaming output in\nreal-time based on inferred cognitive load. Our approach estimates the\ncognitive load associated with streaming content and strategically slows down\nthe stream during complex or information-rich segments, thereby freeing\ncomputational resources for other users. We conducted a statistical analysis\nand simulation based on a statistical model derived from data collected in a\ncrowdsourced user study across various types of LLM-generated content. Our\nresults show that this adaptive method can effectively reduce computational\nconsumption while largely maintaining streaming speed above user's normal\nreading speed.\n","authors":["Chang Xiao","Brenda Yang"],"pdf_url":"https://arxiv.org/pdf/2504.17999v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.03170v2","updated":"2025-07-23T18:41:23Z","published":"2025-05-28T18:52:40Z","title":"PALADIN : Robust Neural Fingerprinting for Text-to-Image Diffusion\n  Models","summary":"  The risk of misusing text-to-image generative models for malicious uses,\nespecially due to the open-source development of such models, has become a\nserious concern. As a risk mitigation strategy, attributing generative models\nwith neural fingerprinting is emerging as a popular technique. There has been a\nplethora of recent work that aim for addressing neural fingerprinting. A\ntrade-off between the attribution accuracy and generation quality of such\nmodels has been studied extensively. None of the existing methods yet achieved\n100% attribution accuracy. However, any model with less than cent percent\naccuracy is practically non-deployable. In this work, we propose an accurate\nmethod to incorporate neural fingerprinting for text-to-image diffusion models\nleveraging the concepts of cyclic error correcting codes from the literature of\ncoding theory.\n","authors":["Murthy L","Subarna Tripathi"],"pdf_url":"https://arxiv.org/pdf/2506.03170v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17860v1","updated":"2025-07-23T18:33:27Z","published":"2025-07-23T18:33:27Z","title":"Towards Facilitated Fairness Assessment of AI-based Skin Lesion\n  Classifiers Through GenAI-based Image Synthesis","summary":"  Recent advancements in Deep Learning and its application on the edge hold\ngreat potential for the revolution of routine screenings for skin cancers like\nMelanoma. Along with the anticipated benefits of this technology, potential\ndangers arise from unforseen and inherent biases. Thus, assessing and improving\nthe fairness of such systems is of utmost importance. A key challenge in\nfairness assessment is to ensure that the evaluation dataset is sufficiently\nrepresentative of different Personal Identifiable Information (PII) (sex, age,\nand race) and other minority groups. Against the backdrop of this challenge,\nthis study leverages the state-of-the-art Generative AI (GenAI) LightningDiT\nmodel to assess the fairness of publicly available melanoma classifiers. The\nresults suggest that fairness assessment using highly realistic synthetic data\nis a promising direction. Yet, our findings indicate that verifying fairness\nbecomes difficult when the melanoma-detection model used for evaluation is\ntrained on data that differ from the dataset underpinning the synthetic images.\nNonetheless, we propose that our approach offers a valuable new avenue for\nemploying synthetic data to gauge and enhance fairness in medical-imaging GenAI\nsystems.\n","authors":["Ko Watanabe. Stanislav Frolov. Adriano Lucieri. Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2507.17860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.01256v2","updated":"2025-07-23T18:17:16Z","published":"2023-03-02T13:36:28Z","title":"Choosing Public Datasets for Private Machine Learning via Gradient\n  Subspace Distance","summary":"  Differentially private stochastic gradient descent privatizes model training\nby injecting noise into each iteration, where the noise magnitude increases\nwith the number of model parameters. Recent works suggest that we can reduce\nthe noise by leveraging public data for private machine learning, by projecting\ngradients onto a subspace prescribed by the public data. However, given a\nchoice of public datasets, it is not a priori clear which one may be most\nappropriate for the private task. We give an algorithm for selecting a public\ndataset by measuring a low-dimensional subspace distance between gradients of\nthe public and private examples. We provide theoretical analysis demonstrating\nthat the excess risk scales with this subspace distance. This distance is easy\nto compute and robust to modifications in the setting. Empirical evaluation\nshows that trained model accuracy is monotone in this distance.\n","authors":["Xin Gu","Gautam Kamath","Zhiwei Steven Wu"],"pdf_url":"https://arxiv.org/pdf/2303.01256v2.pdf","comment":"Accepted to SaTML 2025"},{"id":"http://arxiv.org/abs/2507.17804v1","updated":"2025-07-23T18:00:00Z","published":"2025-07-23T18:00:00Z","title":"On the Energy Distribution of the Galactic Center Excess' Sources","summary":"  The Galactic Center Excess (GCE) remains one of the defining mysteries\nuncovered by the Fermi $\\gamma$-ray Space Telescope. Although it may yet herald\nthe discovery of annihilating dark matter, weighing against that conclusion are\nanalyses showing the spatial structure of the emission appears more consistent\nwith a population of dim point sources. Technical limitations have restricted\nprior analyses to studying the point-source hypothesis purely spatially. All\nspectral information that could help disentangle the GCE from the complex and\nuncertain astrophysical emission was discarded. We demonstrate that a neural\nnetwork-aided simulation-based inference approach can overcome such limitations\nand thereby confront the point source explanation of the GCE with spatial and\nspectral data. The addition is profound: energy information drives the putative\npoint sources to be significantly dimmer, indicating either the GCE is truly\ndiffuse in nature or made of an exceptionally large number of sources.\nQuantitatively, for our best fit background model, the excess is essentially\nconsistent with Poisson emission as predicted by dark matter. If the excess is\ninstead due to point sources, our median prediction is ${\\cal O}(10^5)$ sources\nin the Galactic Center, or more than 35,000 sources at 90% confidence, both\nsignificantly larger than the hundreds of sources preferred by earlier\npoint-source analyses of the GCE.\n","authors":["Florian List","Yujin Park","Nicholas L. Rodd","Eve Schoen","Florian Wolf"],"pdf_url":"https://arxiv.org/pdf/2507.17804v1.pdf","comment":"7+20 pages, 2+20 figures, comments welcome"},{"id":"http://arxiv.org/abs/2507.17748v1","updated":"2025-07-23T17:59:02Z","published":"2025-07-23T17:59:02Z","title":"Large Learning Rates Simultaneously Achieve Robustness to Spurious\n  Correlations and Compressibility","summary":"  Robustness and resource-efficiency are two highly desirable properties for\nmodern machine learning models. However, achieving them jointly remains a\nchallenge. In this paper, we position high learning rates as a facilitator for\nsimultaneously achieving robustness to spurious correlations and network\ncompressibility. We demonstrate that large learning rates also produce\ndesirable representation properties such as invariant feature utilization,\nclass separation, and activation sparsity. Importantly, our findings indicate\nthat large learning rates compare favorably to other hyperparameters and\nregularization methods, in consistently satisfying these properties in tandem.\nIn addition to demonstrating the positive effect of large learning rates across\ndiverse spurious correlation datasets, models, and optimizers, we also present\nstrong evidence that the previously documented success of large learning rates\nin standard classification tasks is likely due to its effect on addressing\nhidden/rare spurious correlations in the training dataset.\n","authors":["Melih Barsbey","Lucas Prieto","Stefanos Zafeiriou","Tolga Birdal"],"pdf_url":"https://arxiv.org/pdf/2507.17748v1.pdf","comment":"Accepted at ICCV 2025, 23 pages"},{"id":"http://arxiv.org/abs/2507.17746v1","updated":"2025-07-23T17:57:55Z","published":"2025-07-23T17:57:55Z","title":"Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains","summary":"  Extending Reinforcement Learning with Verifiable Rewards (RLVR) to real-world\ntasks often requires balancing objective and subjective evaluation criteria.\nHowever, many such tasks lack a single, unambiguous ground truth-making it\ndifficult to define reliable reward signals for post-training language models.\nWhile traditional preference-based methods offer a workaround, they rely on\nopaque reward functions that are difficult to interpret and prone to spurious\ncorrelations. We introduce $\\textbf{Rubrics as Rewards}$ (RaR), a framework\nthat uses structured, checklist-style rubrics as interpretable reward signals\nfor on-policy training with GRPO. Our best RaR method yields up to a $28\\%$\nrelative improvement on HealthBench-1k compared to simple Likert-based\napproaches, while matching or surpassing the performance of reward signals\nderived from expert-written references. By treating rubrics as structured\nreward signals, we show that RaR enables smaller-scale judge models to better\nalign with human preferences and sustain robust performance across model\nscales.\n","authors":["Anisha Gunjal","Anthony Wang","Elaine Lau","Vaskar Nath","Bing Liu","Sean Hendryx"],"pdf_url":"https://arxiv.org/pdf/2507.17746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.01939v2","updated":"2025-07-23T17:47:04Z","published":"2025-07-02T17:49:52Z","title":"SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars","summary":"  In recent years, large language models (LLMs) have transformed natural\nlanguage understanding through vast datasets and large-scale parameterization.\nInspired by this success, we present SpecCLIP, a foundation model framework\nthat extends LLM-inspired methodologies to stellar spectral analysis. Stellar\nspectra, akin to structured language, encode rich physical and chemical\ninformation about stars. By training foundation models on large-scale spectral\ndatasets, our goal is to learn robust and informative embeddings that support\ndiverse downstream applications. As a proof of concept, SpecCLIP involves\npre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed\nby contrastive alignment using the CLIP (Contrastive Language-Image\nPre-training) framework, adapted to associate spectra from different\ninstruments. This alignment is complemented by auxiliary decoders that preserve\nspectrum-specific information and enable translation (prediction) between\nspectral types, with the former achieved by maximizing mutual information\nbetween embeddings and input spectra. The result is a cross-spectrum framework\nenabling intrinsic calibration and flexible applications across instruments. We\ndemonstrate that fine-tuning these models on moderate-sized labeled datasets\nimproves adaptability to tasks such as stellar-parameter estimation and\nchemical-abundance determination. SpecCLIP also enhances the accuracy and\nprecision of parameter estimates benchmarked against external survey data.\nAdditionally, its similarity search and cross-spectrum prediction capabilities\noffer potential for anomaly detection. Our results suggest that contrastively\ntrained foundation models enriched with spectrum-aware decoders can advance\nprecision stellar spectroscopy.\n","authors":["Xiaosheng Zhao","Yang Huang","Guirong Xue","Xiao Kong","Jifeng Liu","Xiaoyu Tang","Timothy C. Beers","Yuan-Sen Ting","A-Li Luo"],"pdf_url":"https://arxiv.org/pdf/2507.01939v2.pdf","comment":"27 pages, 8 figures, 5 tables. Updated with minor corrections to flux\n  normalization, and to related tables and figures. Submitted to AAS Journals.\n  Comments welcome"},{"id":"http://arxiv.org/abs/2507.17731v1","updated":"2025-07-23T17:44:29Z","published":"2025-07-23T17:44:29Z","title":"Flow Matching Meets Biology and Life Science: A Survey","summary":"  Over the past decade, advances in generative modeling, such as generative\nadversarial networks, masked autoencoders, and diffusion models, have\nsignificantly transformed biological research and discovery, enabling\nbreakthroughs in molecule design, protein generation, drug discovery, and\nbeyond. At the same time, biological applications have served as valuable\ntestbeds for evaluating the capabilities of generative models. Recently, flow\nmatching has emerged as a powerful and efficient alternative to diffusion-based\ngenerative modeling, with growing interest in its application to problems in\nbiology and life sciences. This paper presents the first comprehensive survey\nof recent developments in flow matching and its applications in biological\ndomains. We begin by systematically reviewing the foundations and variants of\nflow matching, and then categorize its applications into three major areas:\nbiological sequence modeling, molecule generation and design, and peptide and\nprotein generation. For each, we provide an in-depth review of recent progress.\nWe also summarize commonly used datasets and software tools, and conclude with\na discussion of potential future directions. The corresponding curated\nresources are available at\nhttps://github.com/Violet24K/Awesome-Flow-Matching-Meets-Biology.\n","authors":["Zihao Li","Zhichen Zeng","Xiao Lin","Feihao Fang","Yanru Qu","Zhe Xu","Zhining Liu","Xuying Ning","Tianxin Wei","Ge Liu","Hanghang Tong","Jingrui He"],"pdf_url":"https://arxiv.org/pdf/2507.17731v1.pdf","comment":"Preprint, 27 pages"},{"id":"http://arxiv.org/abs/2507.17726v1","updated":"2025-07-23T17:40:39Z","published":"2025-07-23T17:40:39Z","title":"Deep Generative Learning of Magnetic Frustration in Artificial Spin Ice\n  from Magnetic Force Microscopy Images","summary":"  Increasingly large datasets of microscopic images with atomic resolution\nfacilitate the development of machine learning methods to identify and analyze\nsubtle physical phenomena embedded within the images. In this work, microscopic\nimages of honeycomb lattice spin-ice samples serve as datasets from which we\nautomate the calculation of net magnetic moments and directional orientations\nof spin-ice configurations. In the first stage of our workflow, machine\nlearning models are trained to accurately predict magnetic moments and\ndirections within spin-ice structures. Variational Autoencoders (VAEs), an\nemergent unsupervised deep learning technique, are employed to generate\nhigh-quality synthetic magnetic force microscopy (MFM) images and extract\nlatent feature representations, thereby reducing experimental and segmentation\nerrors. The second stage of proposed methodology enables precise identification\nand prediction of frustrated vertices and nanomagnetic segments, effectively\ncorrelating structural and functional aspects of microscopic images. This\nfacilitates the design of optimized spin-ice configurations with controlled\nfrustration patterns, enabling potential on-demand synthesis.\n","authors":["Arnab Neogi","Suryakant Mishra","Prasad P Iyer","Tzu-Ming Lu","Ezra Bussmann","Sergei Tretiak","Andrew Crandall Jones","Jian-Xin Zhu"],"pdf_url":"https://arxiv.org/pdf/2507.17726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17725v1","updated":"2025-07-23T17:35:48Z","published":"2025-07-23T17:35:48Z","title":"On the Interaction of Compressibility and Adversarial Robustness","summary":"  Modern neural networks are expected to simultaneously satisfy a host of\ndesirable properties: accurate fitting to training data, generalization to\nunseen inputs, parameter and computational efficiency, and robustness to\nadversarial perturbations. While compressibility and robustness have each been\nstudied extensively, a unified understanding of their interaction still remains\nelusive. In this work, we develop a principled framework to analyze how\ndifferent forms of compressibility - such as neuron-level sparsity and spectral\ncompressibility - affect adversarial robustness. We show that these forms of\ncompression can induce a small number of highly sensitive directions in the\nrepresentation space, which adversaries can exploit to construct effective\nperturbations. Our analysis yields a simple yet instructive robustness bound,\nrevealing how neuron and spectral compressibility impact $L_\\infty$ and $L_2$\nrobustness via their effects on the learned representations. Crucially, the\nvulnerabilities we identify arise irrespective of how compression is achieved -\nwhether via regularization, architectural bias, or implicit learning dynamics.\nThrough empirical evaluations across synthetic and realistic tasks, we confirm\nour theoretical predictions, and further demonstrate that these vulnerabilities\npersist under adversarial training and transfer learning, and contribute to the\nemergence of universal adversarial perturbations. Our findings show a\nfundamental tension between structured compressibility and robustness, and\nsuggest new pathways for designing models that are both efficient and secure.\n","authors":["Melih Barsbey","Antônio H. Ribeiro","Umut Şimşekli","Tolga Birdal"],"pdf_url":"https://arxiv.org/pdf/2507.17725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.19664v5","updated":"2025-07-23T17:31:03Z","published":"2024-04-30T15:57:41Z","title":"Towards Generalist Robot Learning from Internet Video: A Survey","summary":"  Scaling deep learning to massive and diverse internet data has driven\nremarkable breakthroughs in domains such as video generation and natural\nlanguage processing. Robot learning, however, has thus far failed to replicate\nthis success and remains constrained by a scarcity of available data. Learning\nfrom videos (LfV) methods aim to address this data bottleneck by augmenting\ntraditional robot data with large-scale internet video. This video data\nprovides foundational information regarding physical dynamics, behaviours, and\ntasks, and can be highly informative for general-purpose robots.\n  This survey systematically examines the emerging field of LfV. We first\noutline essential concepts, including detailing fundamental LfV challenges such\nas distribution shift and missing action labels in video data. Next, we\ncomprehensively review current methods for extracting knowledge from\nlarge-scale internet video, overcoming LfV challenges, and improving robot\nlearning through video-informed training. The survey concludes with a critical\ndiscussion of future opportunities. Here, we emphasize the need for scalable\nfoundation model approaches that can leverage the full range of available\ninternet video and enhance the learning of robot policies and dynamics models.\nOverall, the survey aims to inform and catalyse future LfV research, driving\nprogress towards general-purpose robots.\n","authors":["Robert McCarthy","Daniel C. H. Tan","Dominik Schmidt","Fernando Acero","Nathan Herr","Yilun Du","Thomas G. Thuruthel","Zhibin Li"],"pdf_url":"https://arxiv.org/pdf/2404.19664v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.21427v2","updated":"2025-07-23T17:30:42Z","published":"2025-06-26T16:09:53Z","title":"Flow-Based Single-Step Completion for Efficient and Expressive Policy\n  Learning","summary":"  Generative models such as diffusion and flow-matching offer expressive\npolicies for offline reinforcement learning (RL) by capturing rich, multimodal\naction distributions, but their iterative sampling introduces high inference\ncosts and training instability due to gradient propagation across sampling\nsteps. We propose the \\textit{Single-Step Completion Policy} (SSCP), a\ngenerative policy trained with an augmented flow-matching objective to predict\ndirect completion vectors from intermediate flow samples, enabling accurate,\none-shot action generation. In an off-policy actor-critic framework, SSCP\ncombines the expressiveness of generative models with the training and\ninference efficiency of unimodal policies, without requiring long\nbackpropagation chains. Our method scales effectively to offline,\noffline-to-online, and online RL settings, offering substantial gains in speed\nand adaptability over diffusion-based baselines. We further extend SSCP to\ngoal-conditioned RL, enabling flat policies to exploit subgoal structures\nwithout explicit hierarchical inference. SSCP achieves strong results across\nstandard offline RL and behavior cloning benchmarks, positioning it as a\nversatile, expressive, and efficient framework for deep RL and sequential\ndecision-making.\n","authors":["Prajwal Koirala","Cody Fleming"],"pdf_url":"https://arxiv.org/pdf/2506.21427v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16209v3","updated":"2025-07-23T17:25:41Z","published":"2024-12-17T19:38:29Z","title":"Challenges learning from imbalanced data using tree-based models:\n  Prevalence estimates systematically depend on hyperparameters and can be\n  upwardly biased","summary":"  Imbalanced binary classification problems arise in many fields of study. When\nusing machine learning models for these problems, it is common to subsample the\nmajority class (i.e., undersampling) to create a (more) balanced dataset for\nmodel training. This biases the model's predictions because the model learns\nfrom a dataset that does not follow the same data generating process as new\ndata. One way of accounting for this bias is to analytically map the resulting\npredictions to new values based on the sampling rate for the majority class,\nwhich was used to create the training dataset. While this approach may work\nwell for some machine learning models, we show that calibrating a random forest\nthis way has unintended negative consequences, including prevalence estimates\nthat can be upwardly biased. These prevalence estimates depend on both i) the\nnumber of predictors considered at each split in the random forest; and ii) the\nsampling rate used. We explain the former using known properties of random\nforests and analytical calibration. However, in investigating the latter issue,\nwe made a surprising discovery - contrary to the widespread belief that\ndecision trees are biased towards the majority class, they actually can be\nbiased towards the minority class.\n","authors":["Nathan Phelps","Daniel J. Lizotte","Douglas G. Woolford"],"pdf_url":"https://arxiv.org/pdf/2412.16209v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17713v1","updated":"2025-07-23T17:25:14Z","published":"2025-07-23T17:25:14Z","title":"Sequential Bayesian Design for Efficient Surrogate Construction in the\n  Inversion of Darcy Flows","summary":"  Inverse problems governed by partial differential equations (PDEs) play a\ncrucial role in various fields, including computational science, image\nprocessing, and engineering. Particularly, Darcy flow equation is a fundamental\nequation in fluid mechanics, which plays a crucial role in understanding fluid\nflow through porous media. Bayesian methods provide an effective approach for\nsolving PDEs inverse problems, while their numerical implementation requires\nnumerous evaluations of computationally expensive forward solvers. Therefore,\nthe adoption of surrogate models with lower computational costs is essential.\nHowever, constructing a globally accurate surrogate model for high-dimensional\ncomplex problems demands high model capacity and large amounts of data. To\naddress this challenge, this study proposes an efficient locally accurate\nsurrogate that focuses on the high-probability regions of the true likelihood\nin inverse problems, with relatively low model complexity and few training data\nrequirements. Additionally, we introduce a sequential Bayesian design strategy\nto acquire the proposed surrogate since the high-probability region of the\nlikelihood is unknown. The strategy treats the posterior evolution process of\nsequential Bayesian design as a Gaussian process, enabling algorithmic\nacceleration through one-step ahead prior. The complete algorithmic framework\nis referred to as Sequential Bayesian design for locally accurate surrogate\n(SBD-LAS). Finally, three experiments based the Darcy flow equation demonstrate\nthe advantages of the proposed method in terms of both inversion accuracy and\ncomputational speed.\n","authors":["Hongji Wang","Hongqiao Wang","Jinyong Ying","Qingping Zhou"],"pdf_url":"https://arxiv.org/pdf/2507.17713v1.pdf","comment":"21 pages, 15 figures"},{"id":"http://arxiv.org/abs/2506.08274v3","updated":"2025-07-23T17:23:04Z","published":"2025-06-09T22:32:51Z","title":"The Impact of Feature Scaling In Machine Learning: Effects on Regression\n  and Classification Tasks","summary":"  This research addresses the critical lack of comprehensive studies on feature\nscaling by systematically evaluating 12 scaling techniques - including several\nless common transformations - across 14 different Machine Learning algorithms\nand 16 datasets for classification and regression tasks. We meticulously\nanalyzed impacts on predictive performance (using metrics such as accuracy,\nMAE, MSE, and $R^2$) and computational costs (training time, inference time,\nand memory usage). Key findings reveal that while ensemble methods (such as\nRandom Forest and gradient boosting models like XGBoost, CatBoost and LightGBM)\ndemonstrate robust performance largely independent of scaling, other widely\nused models such as Logistic Regression, SVMs, TabNet, and MLPs show\nsignificant performance variations highly dependent on the chosen scaler. This\nextensive empirical analysis, with all source code, experimental results, and\nmodel parameters made publicly available to ensure complete transparency and\nreproducibility, offers model-specific crucial guidance to practitioners on the\nneed for an optimal selection of feature scaling techniques.\n","authors":["João Manoel Herrera Pinheiro","Suzana Vilas Boas de Oliveira","Thiago Henrique Segreto Silva","Pedro Antonio Rabelo Saraiva","Enzo Ferreira de Souza","Ricardo V. Godoy","Leonardo André Ambrosio","Marcelo Becker"],"pdf_url":"https://arxiv.org/pdf/2506.08274v3.pdf","comment":"27 pages"},{"id":"http://arxiv.org/abs/2504.06566v4","updated":"2025-07-23T17:18:54Z","published":"2025-04-09T04:01:35Z","title":"Diffusion Factor Models: Generating High-Dimensional Returns with Factor\n  Structure","summary":"  Financial scenario simulation is essential for risk management and portfolio\noptimization, yet it remains challenging especially in high-dimensional and\nsmall data settings common in finance. We propose a diffusion factor model that\nintegrates latent factor structure into generative diffusion processes,\nbridging econometrics with modern generative AI to address the challenges of\nthe curse of dimensionality and data scarcity in financial simulation. By\nexploiting the low-dimensional factor structure inherent in asset returns, we\ndecompose the score function--a key component in diffusion models--using\ntime-varying orthogonal projections, and this decomposition is incorporated\ninto the design of neural network architectures. We derive rigorous statistical\nguarantees, establishing nonasymptotic error bounds for both score estimation\nat O(d^{5/2} n^{-2/(k+5)}) and generated distribution at O(d^{5/4}\nn^{-1/2(k+5)}), primarily driven by the intrinsic factor dimension k rather\nthan the number of assets d, surpassing the dimension-dependent limits in the\nclassical nonparametric statistics literature and making the framework viable\nfor markets with thousands of assets. Numerical studies confirm superior\nperformance in latent subspace recovery under small data regimes. Empirical\nanalysis demonstrates the economic significance of our framework in\nconstructing mean-variance optimal portfolios and factor portfolios. This work\npresents the first theoretical integration of factor structure with diffusion\nmodels, offering a principled approach for high-dimensional financial\nsimulation with limited data. Our code is available at\nhttps://github.com/xymmmm00/diffusion_factor_model.\n","authors":["Minshuo Chen","Renyuan Xu","Yumin Xu","Ruixun Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.06566v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17706v1","updated":"2025-07-23T17:12:19Z","published":"2025-07-23T17:12:19Z","title":"HydraOpt: Navigating the Efficiency-Performance Trade-off of Adapter\n  Merging","summary":"  Large language models (LLMs) often leverage adapters, such as low-rank-based\nadapters, to achieve strong performance on downstream tasks. However, storing a\nseparate adapter for each task significantly increases memory requirements,\nposing a challenge for resource-constrained environments such as mobile\ndevices. Although model merging techniques can reduce storage costs, they\ntypically result in substantial performance degradation. In this work, we\nintroduce HydraOpt, a new model merging technique that capitalizes on the\ninherent similarities between the matrices of low-rank adapters. Unlike\nexisting methods that produce a fixed trade-off between storage size and\nperformance, HydraOpt allows us to navigate this spectrum of efficiency and\nperformance. Our experiments show that HydraOpt significantly reduces storage\nsize (48% reduction) compared to storing all adapters, while achieving\ncompetitive performance (0.2-1.8% drop). Furthermore, it outperforms existing\nmerging techniques in terms of performance at the same or slightly worse\nstorage efficiency.\n","authors":["Taha Ceritli","Ondrej Bohdal","Mete Ozay","Jijoong Moon","Kyeng-Hun Lee","Hyeonmok Ko","Umberto Michieli"],"pdf_url":"https://arxiv.org/pdf/2507.17706v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14382v3","updated":"2025-07-23T17:09:55Z","published":"2024-12-18T22:32:13Z","title":"Balans: Multi-Armed Bandits-based Adaptive Large Neighborhood Search for\n  Mixed-Integer Programming Problem","summary":"  Mixed-integer programming (MIP) is a powerful paradigm for modeling and\nsolving various important combinatorial optimization problems. Recently,\nlearning-based approaches have shown a potential to speed up MIP solving via\noffline training that then guides important design decisions during the search.\nHowever, a significant drawback of these methods is their heavy reliance on\noffline training, which requires collecting training datasets and\ncomputationally costly training epochs yet offering only limited generalization\nto unseen (larger) instances. In this paper, we propose Balans, an adaptive\nmeta-solver for MIPs with online learning capability that does not require any\nsupervision or apriori training. At its core, Balans is based on adaptive\nlarge-neighborhood search, operating on top of an MIP solver by successive\napplications of destroy and repair neighborhood operators. During the search,\nthe selection among different neighborhood definitions is guided on the fly for\nthe instance at hand via multi-armed bandit algorithms. Our extensive\nexperiments on hard optimization instances show that Balans offers significant\nperformance gains over the default MIP solver, is better than committing to any\nsingle best neighborhood, and improves over the state-of-the-art\nlarge-neighborhood search for MIPs. Finally, we release Balans as a highly\nconfigurable, MIP solver agnostic, open-source software.\n","authors":["Junyang Cai","Serdar Kadioglu","Bistra Dilkina"],"pdf_url":"https://arxiv.org/pdf/2412.14382v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06565v5","updated":"2025-07-23T17:02:53Z","published":"2025-07-09T05:39:56Z","title":"A Mathematical Theory of Discursive Networks","summary":"  Large language models (LLMs) turn writing into a live exchange between humans\nand software. We characterize this new medium as a discursive network that\ntreats people and LLMs as equal nodes and tracks how their statements\ncirculate. We define the generation of erroneous information as invalidation\n(any factual, logical, or structural breach) and show it follows four hazards:\ndrift from truth, self-repair, fresh fabrication, and external detection. We\ndevelop a general mathematical model of discursive networks that shows that a\nnetwork governed only by drift and self-repair stabilizes at a modest error\nrate. Giving each false claim even a small chance of peer review shifts the\nsystem to a truth-dominant state. We operationalize peer review with the\nopen-source Flaws-of-Others (FOO) algorithm: a configurable loop in which any\nset of agents critique one another while a harmonizer merges their verdicts. We\nidentify an ethical transgression, epithesis, that occurs when humans fail to\nengage in the discursive network. The takeaway is practical and cultural:\nreliability in this new medium comes not from perfecting single models but from\nconnecting imperfect ones into networks that enforce mutual accountability.\n","authors":["Juan B. Gutiérrez"],"pdf_url":"https://arxiv.org/pdf/2507.06565v5.pdf","comment":"42 pages, 4 figures, 4 tables, 3 algorithm, 61 references"},{"id":"http://arxiv.org/abs/2507.17692v1","updated":"2025-07-23T16:57:43Z","published":"2025-07-23T16:57:43Z","title":"Joint Asymmetric Loss for Learning with Noisy Labels","summary":"  Learning with noisy labels is a crucial task for training accurate deep\nneural networks. To mitigate label noise, prior studies have proposed various\nrobust loss functions, particularly symmetric losses. Nevertheless, symmetric\nlosses usually suffer from the underfitting issue due to the overly strict\nconstraint. To address this problem, the Active Passive Loss (APL) jointly\noptimizes an active and a passive loss to mutually enhance the overall fitting\nability. Within APL, symmetric losses have been successfully extended, yielding\nadvanced robust loss functions. Despite these advancements, emerging\ntheoretical analyses indicate that asymmetric losses, a new class of robust\nloss functions, possess superior properties compared to symmetric losses.\nHowever, existing asymmetric losses are not compatible with advanced\noptimization frameworks such as APL, limiting their potential and\napplicability. Motivated by this theoretical gap and the prospect of asymmetric\nlosses, we extend the asymmetric loss to the more complex passive loss scenario\nand propose the Asymetric Mean Square Error (AMSE), a novel asymmetric loss. We\nrigorously establish the necessary and sufficient condition under which AMSE\nsatisfies the asymmetric condition. By substituting the traditional symmetric\npassive loss in APL with our proposed AMSE, we introduce a novel robust loss\nframework termed Joint Asymmetric Loss (JAL). Extensive experiments demonstrate\nthe effectiveness of our method in mitigating label noise. Code available at:\nhttps://github.com/cswjl/joint-asymmetric-loss\n","authors":["Jialiang Wang","Xianming Liu","Xiong Zhou","Gangfeng Hu","Deming Zhai","Junjun Jiang","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2507.17692v1.pdf","comment":"Accepted by ICCV 2025"},{"id":"http://arxiv.org/abs/2507.17691v1","updated":"2025-07-23T16:57:32Z","published":"2025-07-23T16:57:32Z","title":"CASCADE: LLM-Powered JavaScript Deobfuscator at Google","summary":"  Software obfuscation, particularly prevalent in JavaScript, hinders code\ncomprehension and analysis, posing significant challenges to software testing,\nstatic analysis, and malware detection. This paper introduces CASCADE, a novel\nhybrid approach that integrates the advanced coding capabilities of Gemini with\nthe deterministic transformation capabilities of a compiler Intermediate\nRepresentation (IR), specifically JavaScript IR (JSIR). By employing Gemini to\nidentify critical prelude functions, the foundational components underlying the\nmost prevalent obfuscation techniques, and leveraging JSIR for subsequent code\ntransformations, CASCADE effectively recovers semantic elements like original\nstrings and API names, and reveals original program behaviors. This method\novercomes limitations of existing static and dynamic deobfuscation techniques,\neliminating hundreds to thousands of hardcoded rules while achieving\nreliability and flexibility. CASCADE is already deployed in Google's production\nenvironment, demonstrating substantial improvements in JavaScript deobfuscation\nefficiency and reducing reverse engineering efforts.\n","authors":["Shan Jiang","Pranoy Kovuri","David Tao","Zhixun Tan"],"pdf_url":"https://arxiv.org/pdf/2507.17691v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15612v7","updated":"2025-07-23T16:56:21Z","published":"2024-10-21T03:16:32Z","title":"In-Trajectory Inverse Reinforcement Learning: Learn Incrementally Before\n  An Ongoing Trajectory Terminates","summary":"  Inverse reinforcement learning (IRL) aims to learn a reward function and a\ncorresponding policy that best fit the demonstrated trajectories of an expert.\nHowever, current IRL works cannot learn incrementally from an ongoing\ntrajectory because they have to wait to collect at least one complete\ntrajectory to learn. To bridge the gap, this paper considers the problem of\nlearning a reward function and a corresponding policy while observing the\ninitial state-action pair of an ongoing trajectory and keeping updating the\nlearned reward and policy when new state-action pairs of the ongoing trajectory\nare observed. We formulate this problem as an online bi-level optimization\nproblem where the upper level dynamically adjusts the learned reward according\nto the newly observed state-action pairs with the help of a meta-regularization\nterm, and the lower level learns the corresponding policy. We propose a novel\nalgorithm to solve this problem and guarantee that the algorithm achieves\nsub-linear local regret $O(\\sqrt{T}+\\log T+\\sqrt{T}\\log T)$. If the reward\nfunction is linear, we prove that the proposed algorithm achieves sub-linear\nregret $O(\\log T)$. Experiments are used to validate the proposed algorithm.\n","authors":["Shicheng Liu","Minghui Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.15612v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17688v1","updated":"2025-07-23T16:52:42Z","published":"2025-07-23T16:52:42Z","title":"Mindfulness Meditation and Respiration: Accelerometer-Based Respiration\n  Rate and Mindfulness Progress Estimation to Enhance App Engagement and\n  Mindfulness Skills","summary":"  Mindfulness training is widely recognized for its benefits in reducing\ndepression, anxiety, and loneliness. With the rise of smartphone-based\nmindfulness apps, digital meditation has become more accessible, but sustaining\nlong-term user engagement remains a challenge. This paper explores whether\nrespiration biosignal feedback and mindfulness skill estimation enhance system\nusability and skill development. We develop a smartphone's accelerometer-based\nrespiration tracking algorithm, eliminating the need for additional wearables.\nUnlike existing methods, our approach accurately captures slow breathing\npatterns typical of mindfulness meditation. Additionally, we introduce the\nfirst quantitative framework to estimate mindfulness skills-concentration,\nsensory clarity, and equanimity-based on accelerometer-derived respiration\ndata. We develop and test our algorithms on 261 mindfulness sessions in both\ncontrolled and real-world settings. A user study comparing an experimental\ngroup receiving biosignal feedback with a control group using a standard app\nshows that respiration feedback enhances system usability. Our respiration\ntracking model achieves a mean absolute error (MAE) of 1.6 breaths per minute,\nclosely aligning with ground truth data, while our mindfulness skill estimation\nattains F1 scores of 80-84% in tracking skill progression. By integrating\nrespiration tracking and mindfulness estimation into a commercial app, we\ndemonstrate the potential of smartphone sensors to enhance digital mindfulness\ntraining.\n","authors":["Mohammad Nur Hossain Khan","David creswell","Jordan Albert","Patrick O'Connell","Shawn Fallon","Mathew Polowitz","Xuhai \"orson\" Xu","Bashima islam"],"pdf_url":"https://arxiv.org/pdf/2507.17688v1.pdf","comment":"Accepted in Proc. ACM Interact. Mob. Wearable Ubiquitous Technology\n  (IMWUT)"},{"id":"http://arxiv.org/abs/2507.17687v1","updated":"2025-07-23T16:51:23Z","published":"2025-07-23T16:51:23Z","title":"Towards Effective Open-set Graph Class-incremental Learning","summary":"  Graph class-incremental learning (GCIL) allows graph neural networks (GNNs)\nto adapt to evolving graph analytical tasks by incrementally learning new class\nknowledge while retaining knowledge of old classes. Existing GCIL methods\nprimarily focus on a closed-set assumption, where all test samples are presumed\nto belong to previously known classes. Such an assumption restricts their\napplicability in real-world scenarios, where unknown classes naturally emerge\nduring inference, and are absent during training. In this paper, we explore a\nmore challenging open-set graph class-incremental learning scenario with two\nintertwined challenges: catastrophic forgetting of old classes, which impairs\nthe detection of unknown classes, and inadequate open-set recognition, which\ndestabilizes the retention of learned knowledge. To address the above problems,\na novel OGCIL framework is proposed, which utilizes pseudo-sample embedding\ngeneration to effectively mitigate catastrophic forgetting and enable robust\ndetection of unknown classes. To be specific, a prototypical conditional\nvariational autoencoder is designed to synthesize node embeddings for old\nclasses, enabling knowledge replay without storing raw graph data. To handle\nunknown classes, we employ a mixing-based strategy to generate\nout-of-distribution (OOD) samples from pseudo in-distribution and current node\nembeddings. A novel prototypical hypersphere classification loss is further\nproposed, which anchors in-distribution embeddings to their respective class\nprototypes, while repelling OOD embeddings away. Instead of assigning all\nunknown samples into one cluster, our proposed objective function explicitly\nmodels them as outliers through prototype-aware rejection regions, ensuring a\nrobust open-set recognition. Extensive experiments on five benchmarks\ndemonstrate the effectiveness of OGCIL over existing GCIL and open-set GNN\nmethods.\n","authors":["Jiazhen Chen","Zheng Ma","Sichao Fu","Mingbin Feng","Tony S. Wirjanto","Weihua Ou"],"pdf_url":"https://arxiv.org/pdf/2507.17687v1.pdf","comment":"Accepted by 33rd ACM International Conference on Multimedia (MM 2025)"},{"id":"http://arxiv.org/abs/2507.17686v1","updated":"2025-07-23T16:51:09Z","published":"2025-07-23T16:51:09Z","title":"Debiased maximum-likelihood estimators for hazard ratios under\n  machine-learning adjustment","summary":"  Previous studies have shown that hazard ratios between treatment groups\nestimated with the Cox model are uninterpretable because the indefinite\nbaseline hazard of the model fails to identify temporal change in the risk set\ncomposition due to treatment assignment and unobserved factors among multiple,\ncontradictory scenarios. To alleviate this problem, especially in studies based\non observational data with uncontrolled dynamic treatment and real-time\nmeasurement of many covariates, we propose abandoning the baseline hazard and\nusing machine learning to explicitly model the change in the risk set with or\nwithout latent variables. For this framework, we clarify the context in which\nhazard ratios can be causally interpreted, and then develop a method based on\nNeyman orthogonality to compute debiased maximum-likelihood estimators of\nhazard ratios. Computing the constructed estimators is more efficient than\ncomputing those based on weighted regression with marginal structural Cox\nmodels. Numerical simulations confirm that the proposed method identifies the\nground truth with minimal bias. These results lay the foundation for developing\na useful, alternative method for causal inference with uncontrolled,\nobservational data in modern epidemiology.\n","authors":["Takashi Hayakawa","Satoshi Asai"],"pdf_url":"https://arxiv.org/pdf/2507.17686v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15606v2","updated":"2025-07-23T16:48:01Z","published":"2025-06-18T16:30:02Z","title":"LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning","summary":"  Large Language Models (LLMs) have become indispensable in real-world\napplications. However, their widespread adoption raises significant safety\nconcerns, particularly in responding to socially harmful questions. Despite\nsubstantial efforts to improve model safety through alignment, aligned models\ncan still have their safety protections undermined by subsequent fine-tuning -\neven when the additional training data appears benign. In this paper, we\nempirically demonstrate that this vulnerability stems from the sensitivity of\nsafety-critical low-rank subspaces in LLM parameters to fine-tuning. Building\non this insight, we propose a novel training-free method, termed Low-Rank\nExtrapolation (LoX), to enhance safety robustness by extrapolating the safety\nsubspace of an aligned LLM. Our experimental results confirm the effectiveness\nof LoX, demonstrating significant improvements in robustness against both\nbenign and malicious fine-tuning attacks while preserving the model's\nadaptability to new tasks. For instance, LoX leads to 11% to 54% absolute\nreductions in attack success rates (ASR) facing benign or malicious fine-tuning\nattacks. By investigating the ASR landscape of parameters, we attribute the\nsuccess of LoX to that the extrapolation moves LLM parameters to a flatter\nzone, thereby less sensitive to perturbations. The code is available at\ngithub.com/VITA-Group/LoX.\n","authors":["Gabriel J. Perin","Runjin Chen","Xuxi Chen","Nina S. T. Hirata","Zhangyang Wang","Junyuan Hong"],"pdf_url":"https://arxiv.org/pdf/2506.15606v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17684v1","updated":"2025-07-23T16:46:03Z","published":"2025-07-23T16:46:03Z","title":"Generalized Dual Discriminator GANs","summary":"  Dual discriminator generative adversarial networks (D2 GANs) were introduced\nto mitigate the problem of mode collapse in generative adversarial networks. In\nD2 GANs, two discriminators are employed alongside a generator: one\ndiscriminator rewards high scores for samples from the true data distribution,\nwhile the other favors samples from the generator. In this work, we first\nintroduce dual discriminator $\\alpha$-GANs (D2 $\\alpha$-GANs), which combines\nthe strengths of dual discriminators with the flexibility of a tunable loss\nfunction, $\\alpha$-loss. We further generalize this approach to arbitrary\nfunctions defined on positive reals, leading to a broader class of models we\nrefer to as generalized dual discriminator generative adversarial networks. For\neach of these proposed models, we provide theoretical analysis and show that\nthe associated min-max optimization reduces to the minimization of a linear\ncombination of an $f$-divergence and a reverse $f$-divergence. This generalizes\nthe known simplification for D2-GANs, where the objective reduces to a linear\ncombination of the KL-divergence and the reverse KL-divergence. Finally, we\nperform experiments on 2D synthetic data and use multiple performance metrics\nto capture various advantages of our GANs.\n","authors":["Penukonda Naga Chandana","Tejas Srivastava","Gowtham R. Kurri","V. Lalitha"],"pdf_url":"https://arxiv.org/pdf/2507.17684v1.pdf","comment":"8 pages, 2 figures, extended version of a paper accepted for\n  presentation at ITW 2025"},{"id":"http://arxiv.org/abs/2502.02371v2","updated":"2025-07-23T16:44:22Z","published":"2025-02-04T14:52:10Z","title":"RAPID-Net: Accurate Pocket Identification for Binding-Site-Agnostic\n  Docking","summary":"  Accurate identification of druggable pockets and their features is essential\nfor structure-based drug design and effective downstream docking. Here, we\npresent RAPID-Net, a deep learning-based algorithm designed for the accurate\nprediction of binding pockets and seamless integration with docking pipelines.\nOn the PoseBusters benchmark, RAPID-Net-guided AutoDock Vina achieves 54.9% of\nTop-1 poses with RMSD < 2 A and satisfying the PoseBusters chemical-validity\ncriterion, compared to 49.1% for DiffBindFR. On the most challenging time split\nof PoseBusters aiming to assess generalization ability (structures submitted\nafter September 30, 2021), RAPID-Net-guided AutoDock Vina achieves 53.1% of\nTop-1 poses with RMSD < 2 A and PB-valid, versus 59.5% for AlphaFold 3.\nNotably, in 92.2% of cases, RAPID-Net-guided Vina samples at least one pose\nwith RMSD < 2 A (regardless of its rank), indicating that pose ranking, rather\nthan sampling, is the primary accuracy bottleneck. The lightweight inference,\nscalability, and competitive accuracy of RAPID-Net position it as a viable\noption for large-scale virtual screening campaigns. Across diverse benchmark\ndatasets, RAPID-Net outperforms other pocket prediction tools, including\nPUResNet and Kalasanty, in both docking accuracy and pocket-ligand intersection\nrates. Furthermore, we demonstrate the potential of RAPID-Net to accelerate the\ndevelopment of novel therapeutics by highlighting its performance on\npharmacologically relevant targets. RAPID-Net accurately identifies distal\nfunctional sites, offering new opportunities for allosteric inhibitor design.\nIn the case of the RNA-dependent RNA polymerase of SARS-CoV-2, RAPID-Net\nuncovers a wider array of potential binding pockets than existing predictors,\nwhich typically annotate only the orthosteric pocket and overlook secondary\ncavities.\n","authors":["Yaroslav Balytskyi","Inna Hubenko","Alina Balytska","Christopher V. Kelly"],"pdf_url":"https://arxiv.org/pdf/2502.02371v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.12309v5","updated":"2025-07-23T16:41:45Z","published":"2023-01-28T23:22:49Z","title":"On the Lipschitz Constant of Deep Networks and Double Descent","summary":"  Existing bounds on the generalization error of deep networks assume some form\nof smooth or bounded dependence on the input variable, falling short of\ninvestigating the mechanisms controlling such factors in practice. In this\nwork, we present an extensive experimental study of the empirical Lipschitz\nconstant of deep networks undergoing double descent, and highlight\nnon-monotonic trends strongly correlating with the test error. Building a\nconnection between parameter-space and input-space gradients for SGD around a\ncritical point, we isolate two important factors -- namely loss landscape\ncurvature and distance of parameters from initialization -- respectively\ncontrolling optimization dynamics around a critical point and bounding model\nfunction complexity, even beyond the training data. Our study presents novels\ninsights on implicit regularization via overparameterization, and effective\nmodel complexity for networks trained in practice.\n","authors":["Matteo Gamba","Hossein Azizpour","Mårten Björkman"],"pdf_url":"https://arxiv.org/pdf/2301.12309v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17668v1","updated":"2025-07-23T16:31:38Z","published":"2025-07-23T16:31:38Z","title":"How Should We Meta-Learn Reinforcement Learning Algorithms?","summary":"  The process of meta-learning algorithms from data, instead of relying on\nmanual design, is growing in popularity as a paradigm for improving the\nperformance of machine learning systems. Meta-learning shows particular promise\nfor reinforcement learning (RL), where algorithms are often adapted from\nsupervised or unsupervised learning despite their suboptimality for RL.\nHowever, until now there has been a severe lack of comparison between different\nmeta-learning algorithms, such as using evolution to optimise over black-box\nfunctions or LLMs to propose code. In this paper, we carry out this empirical\ncomparison of the different approaches when applied to a range of meta-learned\nalgorithms which target different parts of the RL pipeline. In addition to\nmeta-train and meta-test performance, we also investigate factors including the\ninterpretability, sample cost and train time for each meta-learning algorithm.\nBased on these findings, we propose several guidelines for meta-learning new RL\nalgorithms which will help ensure that future learned algorithms are as\nperformant as possible.\n","authors":["Alexander David Goldie","Zilin Wang","Jakob Nicolaus Foerster","Shimon Whiteson"],"pdf_url":"https://arxiv.org/pdf/2507.17668v1.pdf","comment":"Accepted paper at Reinforcement Learning Conference (RLC) 2025"},{"id":"http://arxiv.org/abs/2507.17662v1","updated":"2025-07-23T16:29:46Z","published":"2025-07-23T16:29:46Z","title":"Mammo-Mamba: A Hybrid State-Space and Transformer Architecture with\n  Sequential Mixture of Experts for Multi-View Mammography","summary":"  Breast cancer (BC) remains one of the leading causes of cancer-related\nmortality among women, despite recent advances in Computer-Aided Diagnosis\n(CAD) systems. Accurate and efficient interpretation of multi-view mammograms\nis essential for early detection, driving a surge of interest in Artificial\nIntelligence (AI)-powered CAD models. While state-of-the-art multi-view\nmammogram classification models are largely based on Transformer architectures,\ntheir computational complexity scales quadratically with the number of image\npatches, highlighting the need for more efficient alternatives. To address this\nchallenge, we propose Mammo-Mamba, a novel framework that integrates Selective\nState-Space Models (SSMs), transformer-based attention, and expert-driven\nfeature refinement into a unified architecture. Mammo-Mamba extends the\nMambaVision backbone by introducing the Sequential Mixture of Experts (SeqMoE)\nmechanism through its customized SecMamba block. The SecMamba is a modified\nMambaVision block that enhances representation learning in high-resolution\nmammographic images by enabling content-adaptive feature refinement. These\nblocks are integrated into the deeper stages of MambaVision, allowing the model\nto progressively adjust feature emphasis through dynamic expert gating,\neffectively mitigating the limitations of traditional Transformer models.\nEvaluated on the CBIS-DDSM benchmark dataset, Mammo-Mamba achieves superior\nclassification performance across all key metrics while maintaining\ncomputational efficiency.\n","authors":["Farnoush Bayatmakou","Reza Taleei","Nicole Simone","Arash Mohammadi"],"pdf_url":"https://arxiv.org/pdf/2507.17662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17650v1","updated":"2025-07-23T16:14:48Z","published":"2025-07-23T16:14:48Z","title":"XStacking: Explanation-Guided Stacked Ensemble Learning","summary":"  Ensemble Machine Learning (EML) techniques, especially stacking, have been\nshown to improve predictive performance by combining multiple base models.\nHowever, they are often criticized for their lack of interpretability. In this\npaper, we introduce XStacking, an effective and inherently explainable\nframework that addresses this limitation by integrating dynamic feature\ntransformation with model-agnostic Shapley additive explanations. This enables\nstacked models to retain their predictive accuracy while becoming inherently\nexplainable. We demonstrate the effectiveness of the framework on 29 datasets,\nachieving improvements in both the predictive effectiveness of the learning\nspace and the interpretability of the resulting models. XStacking offers a\npractical and scalable solution for responsible ML.\n","authors":["Moncef Garouani","Ayah Barhrhouj","Olivier Teste"],"pdf_url":"https://arxiv.org/pdf/2507.17650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17799v1","updated":"2025-07-23T16:11:44Z","published":"2025-07-23T16:11:44Z","title":"A Concept-based approach to Voice Disorder Detection","summary":"  Voice disorders affect a significant portion of the population, and the\nability to diagnose them using automated, non-invasive techniques would\nrepresent a substantial advancement in healthcare, improving the quality of\nlife of patients. Recent studies have demonstrated that artificial intelligence\nmodels, particularly Deep Neural Networks (DNNs), can effectively address this\ntask. However, due to their complexity, the decision-making process of such\nmodels often remain opaque, limiting their trustworthiness in clinical\ncontexts. This paper investigates an alternative approach based on Explainable\nAI (XAI), a field that aims to improve the interpretability of DNNs by\nproviding different forms of explanations. Specifically, this works focuses on\nconcept-based models such as Concept Bottleneck Model (CBM) and Concept\nEmbedding Model (CEM) and how they can achieve performance comparable to\ntraditional deep learning methods, while offering a more transparent and\ninterpretable decision framework.\n","authors":["Davide Ghia","Gabriele Ciravegna","Alkis Koudounas","Marco Fantini","Erika Crosetti","Giovanni Succo","Tania Cerquitelli"],"pdf_url":"https://arxiv.org/pdf/2507.17799v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17634v1","updated":"2025-07-23T16:02:06Z","published":"2025-07-23T16:02:06Z","title":"WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM\n  Pre-training","summary":"  Recent advances in learning rate (LR) scheduling have demonstrated the\neffectiveness of decay-free approaches that eliminate the traditional decay\nphase while maintaining competitive performance. Model merging techniques have\nemerged as particularly promising solutions in this domain. We present\nWarmup-Stable and Merge (WSM), a general framework that establishes a formal\nconnection between learning rate decay and model merging. WSM provides a\nunified theoretical foundation for emulating various decay strategies-including\ncosine decay, linear decay and inverse square root decay-as principled model\naveraging schemes, while remaining fully compatible with diverse optimization\nmethods. Through extensive experiments, we identify merge duration-the training\nwindow for checkpoint aggregation-as the most critical factor influencing model\nperformance, surpassing the importance of both checkpoint interval and merge\nquantity. Our framework consistently outperforms the widely-adopted\nWarmup-Stable-Decay (WSD) approach across multiple benchmarks, achieving\nsignificant improvements of +3.5% on MATH, +2.9% on HumanEval, and +5.5% on\nMMLU-Pro. The performance advantages extend to supervised fine-tuning\nscenarios, highlighting WSM's potential for long-term model refinement.\n","authors":["Changxin Tian","Jiapeng Wang","Qian Zhao","Kunlong Chen","Jia Liu","Ziqi Liu","Jiaxin Mao","Wayne Xin Zhao","Zhiqiang Zhang","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2507.17634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.11962v5","updated":"2025-07-23T16:01:17Z","published":"2023-02-23T12:18:28Z","title":"Unified Convergence Theory of Stochastic and Variance-Reduced Cubic\n  Newton Methods","summary":"  We study stochastic Cubic Newton methods for solving general possibly\nnon-convex minimization problems. We propose a new framework, which we call the\nhelper framework, that provides a unified view of the stochastic and\nvariance-reduced second-order algorithms equipped with global complexity\nguarantees. It can also be applied to learning with auxiliary information. Our\nhelper framework offers the algorithm designer high flexibility for\nconstructing and analyzing the stochastic Cubic Newton methods, allowing\narbitrary size batches, and the use of noisy and possibly biased estimates of\nthe gradients and Hessians, incorporating both the variance reduction and the\nlazy Hessian updates. We recover the best-known complexities for the stochastic\nand variance-reduced Cubic Newton, under weak assumptions on the noise. A\ndirect consequence of our theory is the new lazy stochastic second-order\nmethod, which significantly improves the arithmetic complexity for large\ndimension problems. We also establish complexity bounds for the classes of\ngradient-dominated objectives, that include convex and strongly convex\nproblems. For Auxiliary Learning, we show that using a helper (auxiliary\nfunction) can outperform training alone if a given similarity measure is small.\n","authors":["El Mahdi Chayti","Nikita Doikov","Martin Jaggi"],"pdf_url":"https://arxiv.org/pdf/2302.11962v5.pdf","comment":"Published in Transactions on Machine Learning Research"},{"id":"http://arxiv.org/abs/2507.16242v2","updated":"2025-07-23T15:59:38Z","published":"2025-07-22T05:26:28Z","title":"Toward a Lightweight and Robust Design for Caching","summary":"  The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.\n","authors":["Peng Chen","Hailiang Zhao","Jiaji Zhang","Xueyan Tang","Yixuan Wang","Shuiguang Deng"],"pdf_url":"https://arxiv.org/pdf/2507.16242v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2108.02283v2","updated":"2025-07-23T15:52:55Z","published":"2021-08-04T20:48:27Z","title":"Machine Learning Classification and Portfolio Allocation: with\n  Implications from Machine Uncertainty","summary":"  We use multi-class machine learning classifiers to identify the stocks that\noutperform or underperform other stocks. The resulting long-short portfolios\nachieve annual Sharpe ratios of 1.67 (value-weighted) and 3.35\n(equal-weighted), with annual alphas ranging from 29\\% to 48\\%. These results\npersist after controlling for machine learning regressions and remain robust\namong large-cap stocks. Machine uncertainty, as measured by predicted\nprobabilities, impairs the prediction performance. Stocks with higher machine\nuncertainty experience lower returns, particularly when human proxies of\ninformation uncertainty align with machine uncertainty. Consistent with the\nliterature, such an effect is driven by the past underperformers.\n","authors":["Yang Bai","Kuntara Pukthuanthong"],"pdf_url":"https://arxiv.org/pdf/2108.02283v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17616v1","updated":"2025-07-23T15:47:34Z","published":"2025-07-23T15:47:34Z","title":"Vision Transformer attention alignment with human visual perception in\n  aesthetic object evaluation","summary":"  Visual attention mechanisms play a crucial role in human perception and\naesthetic evaluation. Recent advances in Vision Transformers (ViTs) have\ndemonstrated remarkable capabilities in computer vision tasks, yet their\nalignment with human visual attention patterns remains underexplored,\nparticularly in aesthetic contexts. This study investigates the correlation\nbetween human visual attention and ViT attention mechanisms when evaluating\nhandcrafted objects. We conducted an eye-tracking experiment with 30\nparticipants (9 female, 21 male, mean age 24.6 years) who viewed 20 artisanal\nobjects comprising basketry bags and ginger jars. Using a Pupil Labs\neye-tracker, we recorded gaze patterns and generated heat maps representing\nhuman visual attention. Simultaneously, we analyzed the same objects using a\npre-trained ViT model with DINO (Self-DIstillation with NO Labels), extracting\nattention maps from each of the 12 attention heads. We compared human and ViT\nattention distributions using Kullback-Leibler divergence across varying\nGaussian parameters (sigma=0.1 to 3.0). Statistical analysis revealed optimal\ncorrelation at sigma=2.4 +-0.03, with attention head #12 showing the strongest\nalignment with human visual patterns. Significant differences were found\nbetween attention heads, with heads #7 and #9 demonstrating the greatest\ndivergence from human attention (p< 0.05, Tukey HSD test). Results indicate\nthat while ViTs exhibit more global attention patterns compared to human focal\nattention, certain attention heads can approximate human visual behavior,\nparticularly for specific object features like buckles in basketry items. These\nfindings suggest potential applications of ViT attention mechanisms in product\ndesign and aesthetic evaluation, while highlighting fundamental differences in\nattention strategies between human perception and current AI models.\n","authors":["Miguel Carrasco","César González-Martín","José Aranda","Luis Oliveros"],"pdf_url":"https://arxiv.org/pdf/2507.17616v1.pdf","comment":"25 pages, 15 figures"},{"id":"http://arxiv.org/abs/2507.17606v1","updated":"2025-07-23T15:39:39Z","published":"2025-07-23T15:39:39Z","title":"Time Deep Gradient Flow Method for pricing American options","summary":"  In this research, we explore neural network-based methods for pricing\nmultidimensional American put options under the BlackScholes and Heston model,\nextending up to five dimensions. We focus on two approaches: the Time Deep\nGradient Flow (TDGF) method and the Deep Galerkin Method (DGM). We extend the\nTDGF method to handle the free-boundary partial differential equation inherent\nin American options. We carefully design the sampling strategy during training\nto enhance performance. Both TDGF and DGM achieve high accuracy while\noutperforming conventional Monte Carlo methods in terms of computational speed.\nIn particular, TDGF tends to be faster during training than DGM.\n","authors":["Jasper Rou"],"pdf_url":"https://arxiv.org/pdf/2507.17606v1.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2404.11944v3","updated":"2025-07-23T15:34:21Z","published":"2024-04-18T06:47:30Z","title":"Trusted Multi-view Learning under Noisy Supervision","summary":"  Multi-view learning methods often focus on improving decision accuracy while\nneglecting the decision uncertainty, which significantly restricts their\napplications in safety-critical scenarios. To address this, trusted multi-view\nlearning methods estimate prediction uncertainties by learning class\ndistributions from each instance. However, these methods heavily rely on high\nquality ground-truth labels. This motivates us to delve into a new problem: how\nto develop a reliable multi-view learning model under the guidance of noisy\nlabels? We propose the Trusted Multi view Noise Refining (TMNR) method to\naddress this challenge by modeling label noise arising from low-quality data\nfeatures and easily-confused classes. TMNR employs evidential deep neural\nnetworks to construct view-specific opinions that capture both beliefs and\nuncertainty. These opinions are then transformed through noise correlation\nmatrices to align with the noisy supervision, where matrix elements are\nconstrained by sample uncertainty to reflect label reliability. Furthermore,\nconsidering the challenge of jointly optimizing the evidence network and noise\ncorrelation matrices under noisy supervision, we further propose Trusted\nMulti-view Noise Re-Refining (TMNR^2 ), which disentangles this complex\nco-training problem by establishing different training objectives for distinct\nmodules. TMNR^2 identifies potentially mislabeled samples through\nevidence-label consistency and generates pseudo-labels from neighboring\ninformation. By assigning clean samples to optimize evidential networks and\nnoisy samples to guide noise correlation matrices, respectively, TMNR^2 reduces\nmapping interference and achieves stabilizes training. Experimental results\ndemonstrate that TMNR^2 significantly outperforms baseline methods, with\naverage accuracy improvements of 7% on datasets with 50% label noise.\n","authors":["Yilin Zhang","Cai Xu","Han Jiang","Ziyu Guan","Wei Zhao","Xiaofei He","Murat Sensoy"],"pdf_url":"https://arxiv.org/pdf/2404.11944v3.pdf","comment":"v2 12 pages, accepted at IJCAI 2024; v3 is currently under review"},{"id":"http://arxiv.org/abs/2507.17603v1","updated":"2025-07-23T15:34:07Z","published":"2025-07-23T15:34:07Z","title":"Citation Recommendation using Deep Canonical Correlation Analysis","summary":"  Recent advances in citation recommendation have improved accuracy by\nleveraging multi-view representation learning to integrate the various\nmodalities present in scholarly documents. However, effectively combining\nmultiple data views requires fusion techniques that can capture complementary\ninformation while preserving the unique characteristics of each modality. We\npropose a novel citation recommendation algorithm that improves upon linear\nCanonical Correlation Analysis (CCA) methods by applying Deep CCA (DCCA), a\nneural network extension capable of capturing complex, non-linear relationships\nbetween distributed textual and graph-based representations of scientific\narticles. Experiments on the large-scale DBLP (Digital Bibliography & Library\nProject) citation network dataset demonstrate that our approach outperforms\nstate-of-the-art CCA-based methods, achieving relative improvements of over 11%\nin Mean Average Precision@10, 5% in Precision@10, and 7% in Recall@10. These\ngains reflect more relevant citation recommendations and enhanced ranking\nquality, suggesting that DCCA's non-linear transformations yield more\nexpressive latent representations than CCA's linear projections.\n","authors":["Conor McNamara","Effirul Ramlan"],"pdf_url":"https://arxiv.org/pdf/2507.17603v1.pdf","comment":"21 pages, 6 figures, 7 tables"},{"id":"http://arxiv.org/abs/2507.15917v2","updated":"2025-07-23T15:32:44Z","published":"2025-07-21T17:57:17Z","title":"HyDRA: A Hybrid-Driven Reasoning Architecture for Verifiable Knowledge\n  Graphs","summary":"  The synergy between symbolic knowledge, often represented by Knowledge Graphs\n(KGs), and the generative capabilities of neural networks is central to\nadvancing neurosymbolic AI. A primary bottleneck in realizing this potential is\nthe difficulty of automating KG construction, which faces challenges related to\noutput reliability, consistency, and verifiability. These issues can manifest\nas structural inconsistencies within the generated graphs, such as the\nformation of disconnected $\\textit{isolated islands}$ of data or the inaccurate\nconflation of abstract classes with specific instances. To address these\nchallenges, we propose HyDRA, a $\\textbf{Hy}$brid-$\\textbf{D}$riven\n$\\textbf{R}$easoning $\\textbf{A}$rchitecture designed for verifiable KG\nautomation. Given a domain or an initial set of documents, HyDRA first\nconstructs an ontology via a panel of collaborative neurosymbolic agents. These\nagents collaboratively agree on a set of competency questions (CQs) that define\nthe scope and requirements the ontology must be able to answer. Given these\nCQs, we build an ontology graph that subsequently guides the automated\nextraction of triplets for KG generation from arbitrary documents. Inspired by\ndesign-by-contracts (DbC) principles, our method leverages verifiable contracts\nas the primary control mechanism to steer the generative process of Large\nLanguage Models (LLMs). To verify the output of our approach, we extend beyond\nstandard benchmarks and propose an evaluation framework that assesses the\nfunctional correctness of the resulting KG by leveraging symbolic verifications\nas described by the neurosymbolic AI framework, $\\textit{SymbolicAI}$. This\nwork contributes a hybrid-driven architecture for improving the reliability of\nautomated KG construction and the exploration of evaluation methods for\nmeasuring the functional integrity of its output. The code is publicly\navailable.\n","authors":["Adrian Kaiser","Claudiu Leoveanu-Condrei","Ryan Gold","Marius-Constantin Dinu","Markus Hofmarcher"],"pdf_url":"https://arxiv.org/pdf/2507.15917v2.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2507.17798v1","updated":"2025-07-23T15:29:34Z","published":"2025-07-23T15:29:34Z","title":"Wasserstein GAN-Based Precipitation Downscaling with Optimal Transport\n  for Enhancing Perceptual Realism","summary":"  High-resolution (HR) precipitation prediction is essential for reducing\ndamage from stationary and localized heavy rainfall; however, HR precipitation\nforecasts using process-driven numerical weather prediction models remains\nchallenging. This study proposes using Wasserstein Generative Adversarial\nNetwork (WGAN) to perform precipitation downscaling with an optimal transport\ncost. In contrast to a conventional neural network trained with mean squared\nerror, the WGAN generated visually realistic precipitation fields with\nfine-scale structures even though the WGAN exhibited slightly lower performance\non conventional evaluation metrics. The learned critic of WGAN correlated well\nwith human perceptual realism. Case-based analysis revealed that large\ndiscrepancies in critic scores can help identify both unrealistic WGAN outputs\nand potential artifacts in the reference data. These findings suggest that the\nWGAN framework not only improves perceptual realism in precipitation\ndownscaling but also offers a new perspective for evaluating and\nquality-controlling precipitation datasets.\n","authors":["Kenta Shiraishi","Yuka Muto","Atsushi Okazaki","Shunji Kotsuki"],"pdf_url":"https://arxiv.org/pdf/2507.17798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00107v2","updated":"2025-07-23T15:24:16Z","published":"2024-10-31T18:02:30Z","title":"First, Learn What You Don't Know: Active Information Gathering for\n  Driving at the Limits of Handling","summary":"  Combining data-driven models that adapt online and model predictive control\n(MPC) has enabled effective control of nonlinear systems. However, when\ndeployed on unstable systems, online adaptation may not be fast enough to\nensure reliable simultaneous learning and control. For example, a controller on\na vehicle executing highly dynamic maneuvers--such as drifting to avoid an\nobstacle--may push the vehicle's tires to their friction limits, destabilizing\nthe vehicle and allowing modeling errors to quickly compound and cause a loss\nof control. To address this challenge, we present an active information\ngathering framework for identifying vehicle dynamics as quickly as possible. We\npropose an expressive vehicle dynamics model that leverages Bayesian last-layer\nmeta-learning to enable rapid online adaptation. The model's uncertainty\nestimates are used to guide informative data collection and quickly improve the\nmodel prior to deployment. Dynamic drifting experiments on a Toyota Supra show\nthat (i) the framework enables reliable control of a vehicle at the edge of\nstability, (ii) online adaptation alone may not suffice for zero-shot control\nand can lead to undesirable transient errors or spin-outs, and (iii) active\ndata collection helps achieve reliable performance.\n","authors":["Alexander Davydov","Franck Djeumou","Marcus Greiff","Makoto Suminaka","Michael Thompson","John Subosits","Thomas Lew"],"pdf_url":"https://arxiv.org/pdf/2411.00107v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16417v2","updated":"2025-07-23T15:23:03Z","published":"2024-04-25T08:49:29Z","title":"Constructing Optimal Noise Channels for Enhanced Robustness in Quantum\n  Machine Learning","summary":"  With the rapid advancement of Quantum Machine Learning (QML), the critical\nneed to enhance security measures against adversarial attacks and protect QML\nmodels becomes increasingly evident. In this work, we outline the connection\nbetween quantum noise channels and differential privacy (DP), by constructing a\nfamily of noise channels which are inherently $\\epsilon$-DP: $(\\alpha,\n\\gamma)$-channels. Through this approach, we successfully replicate the\n$\\epsilon$-DP bounds observed for depolarizing and random rotation channels,\nthereby affirming the broad generality of our framework. Additionally, we use a\nsemi-definite program to construct an optimally robust channel. In a\nsmall-scale experimental evaluation, we demonstrate the benefits of using our\noptimal noise channel over depolarizing noise, particularly in enhancing\nadversarial accuracy. Moreover, we assess how the variables $\\alpha$ and\n$\\gamma$ affect the certifiable robustness and investigate how different\nencoding methods impact the classifier's robustness.\n","authors":["David Winderl","Nicola Franco","Jeanette Miriam Lorenz"],"pdf_url":"https://arxiv.org/pdf/2404.16417v2.pdf","comment":"QML technical track at IEEE QCE 2025"},{"id":"http://arxiv.org/abs/2507.17797v1","updated":"2025-07-23T15:22:51Z","published":"2025-07-23T15:22:51Z","title":"GenSelect: A Generative Approach to Best-of-N","summary":"  Generative reward models with parallel sampling have enabled effective\ntest-time scaling for reasoning tasks. Current approaches employ pointwise\nscoring of individual solutions or pairwise comparisons. However, pointwise\nmethods underutilize LLMs' comparative abilities, while pairwise methods scale\ninefficiently with larger sampling budgets. We introduce GenSelect, where the\nLLM uses long reasoning to select the best solution among N candidates. This\nleverages LLMs' comparative strengths while scaling efficiently across parallel\nsampling budgets. For math reasoning, we demonstrate that reasoning models,\nsuch as QwQ and DeepSeek-R1-0528, excel at GenSelect, outperforming existing\nscoring approaches with simple prompting.\n","authors":["Shubham Toshniwal","Ivan Sorokin","Aleksander Ficek","Ivan Moshkov","Igor Gitman"],"pdf_url":"https://arxiv.org/pdf/2507.17797v1.pdf","comment":"Presented at the 2nd AI for MATH Workshop @ ICML"},{"id":"http://arxiv.org/abs/2507.11588v2","updated":"2025-07-23T15:22:26Z","published":"2025-07-15T14:47:01Z","title":"SToFM: a Multi-scale Foundation Model for Spatial Transcriptomics","summary":"  Spatial Transcriptomics (ST) technologies provide biologists with rich\ninsights into single-cell biology by preserving spatial context of cells.\nBuilding foundational models for ST can significantly enhance the analysis of\nvast and complex data sources, unlocking new perspectives on the intricacies of\nbiological tissues. However, modeling ST data is inherently challenging due to\nthe need to extract multi-scale information from tissue slices containing vast\nnumbers of cells. This process requires integrating macro-scale tissue\nmorphology, micro-scale cellular microenvironment, and gene-scale gene\nexpression profile. To address this challenge, we propose SToFM, a multi-scale\nSpatial Transcriptomics Foundation Model. SToFM first performs multi-scale\ninformation extraction on each ST slice, to construct a set of ST sub-slices\nthat aggregate macro-, micro- and gene-scale information. Then an SE(2)\nTransformer is used to obtain high-quality cell representations from the\nsub-slices. Additionally, we construct \\textbf{SToCorpus-88M}, the largest\nhigh-resolution spatial transcriptomics corpus for pretraining. SToFM achieves\noutstanding performance on a variety of downstream tasks, such as tissue region\nsemantic segmentation and cell type annotation, demonstrating its comprehensive\nunderstanding of ST data through capturing and integrating multi-scale\ninformation.\n","authors":["Suyuan Zhao","Yizhen Luo","Ganbo Yang","Yan Zhong","Hao Zhou","Zaiqing Nie"],"pdf_url":"https://arxiv.org/pdf/2507.11588v2.pdf","comment":"Accpeted by ICML 2025"},{"id":"http://arxiv.org/abs/2507.17580v1","updated":"2025-07-23T15:14:53Z","published":"2025-07-23T15:14:53Z","title":"Enhancing Quantum Federated Learning with Fisher Information-Based\n  Optimization","summary":"  Federated Learning (FL) has become increasingly popular across different\nsectors, offering a way for clients to work together to train a global model\nwithout sharing sensitive data. It involves multiple rounds of communication\nbetween the global model and participating clients, which introduces several\nchallenges like high communication costs, heterogeneous client data, prolonged\nprocessing times, and increased vulnerability to privacy threats. In recent\nyears, the convergence of federated learning and parameterized quantum circuits\nhas sparked significant research interest, with promising implications for\nfields such as healthcare and finance. By enabling decentralized training of\nquantum models, it allows clients or institutions to collaboratively enhance\nmodel performance and outcomes while preserving data privacy. Recognizing that\nFisher information can quantify the amount of information that a quantum state\ncarries under parameter changes, thereby providing insight into its geometric\nand statistical properties. We intend to leverage this property to address the\naforementioned challenges. In this work, we propose a Quantum Federated\nLearning (QFL) algorithm that makes use of the Fisher information computed on\nlocal client models, with data distributed across heterogeneous partitions.\nThis approach identifies the critical parameters that significantly influence\nthe quantum model's performance, ensuring they are preserved during the\naggregation process. Our research assessed the effectiveness and feasibility of\nQFL by comparing its performance against other variants, and exploring the\nbenefits of incorporating Fisher information in QFL settings. Experimental\nresults on ADNI and MNIST datasets demonstrate the effectiveness of our\napproach in achieving better performance and robustness against the quantum\nfederated averaging method.\n","authors":["Amandeep Singh Bhatia","Sabre Kais"],"pdf_url":"https://arxiv.org/pdf/2507.17580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17577v1","updated":"2025-07-23T15:11:25Z","published":"2025-07-23T15:11:25Z","title":"Boosting Ray Search Procedure of Hard-label Attacks with Transfer-based\n  Priors","summary":"  One of the most practical and challenging types of black-box adversarial\nattacks is the hard-label attack, where only the top-1 predicted label is\navailable. One effective approach is to search for the optimal ray direction\nfrom the benign image that minimizes the $\\ell_p$-norm distance to the\nadversarial region. The unique advantage of this approach is that it transforms\nthe hard-label attack into a continuous optimization problem. The objective\nfunction value is the ray's radius, which can be obtained via binary search at\na high query cost. Existing methods use a \"sign trick\" in gradient estimation\nto reduce the number of queries. In this paper, we theoretically analyze the\nquality of this gradient estimation and propose a novel prior-guided approach\nto improve ray search efficiency both theoretically and empirically.\nSpecifically, we utilize the transfer-based priors from surrogate models, and\nour gradient estimators appropriately integrate them by approximating the\nprojection of the true gradient onto the subspace spanned by these priors and\nrandom directions, in a query-efficient manner. We theoretically derive the\nexpected cosine similarities between the obtained gradient estimators and the\ntrue gradient, and demonstrate the improvement achieved by incorporating\npriors. Extensive experiments on the ImageNet and CIFAR-10 datasets show that\nour approach significantly outperforms 11 state-of-the-art methods in terms of\nquery efficiency.\n","authors":["Chen Ma","Xinjie Xu","Shuyu Cheng","Qi Xuan"],"pdf_url":"https://arxiv.org/pdf/2507.17577v1.pdf","comment":"Published at ICLR 2025 (Spotlight paper)"},{"id":"http://arxiv.org/abs/2405.15632v3","updated":"2025-07-23T14:57:55Z","published":"2024-05-24T15:17:51Z","title":"Federated Behavioural Planes: Explaining the Evolution of Client\n  Behaviour in Federated Learning","summary":"  Federated Learning (FL), a privacy-aware approach in distributed deep\nlearning environments, enables many clients to collaboratively train a model\nwithout sharing sensitive data, thereby reducing privacy risks. However,\nenabling human trust and control over FL systems requires understanding the\nevolving behaviour of clients, whether beneficial or detrimental for the\ntraining, which still represents a key challenge in the current literature. To\naddress this challenge, we introduce Federated Behavioural Planes (FBPs), a\nnovel method to analyse, visualise, and explain the dynamics of FL systems,\nshowing how clients behave under two different lenses: predictive performance\n(error behavioural space) and decision-making processes (counterfactual\nbehavioural space). Our experiments demonstrate that FBPs provide informative\ntrajectories describing the evolving states of clients and their contributions\nto the global model, thereby enabling the identification of clusters of clients\nwith similar behaviours. Leveraging the patterns identified by FBPs, we propose\na robust aggregation technique named Federated Behavioural Shields to detect\nmalicious or noisy client models, thereby enhancing security and surpassing the\nefficacy of existing state-of-the-art FL defense mechanisms. Our code is\npublicly available on GitHub.\n","authors":["Dario Fenoglio","Gabriele Dominici","Pietro Barbiero","Alberto Tonda","Martin Gjoreski","Marc Langheinrich"],"pdf_url":"https://arxiv.org/pdf/2405.15632v3.pdf","comment":"[v3] Pre-print of the paper accepted to NeurIPS 2024 (30 pages)"},{"id":"http://arxiv.org/abs/2504.20129v2","updated":"2025-07-23T14:53:46Z","published":"2025-04-28T17:21:16Z","title":"A Physically Driven Long Short Term Memory Model for Estimating Snow\n  Water Equivalent over the Continental United States","summary":"  Snow is an essential input for various land surface models. Seasonal snow\nestimates are available as snow water equivalent (SWE) from process-based\nreanalysis products or locally from in situ measurements. While the reanalysis\nproducts are computationally expensive and available at only fixed spatial and\ntemporal resolutions, the in situ measurements are highly localized and sparse.\nTo address these issues and enable the analysis of the effect of a large suite\nof physical, morphological, and geological conditions on the presence and\namount of snow, we build a Long Short-Term Memory (LSTM) network, which is able\nto estimate the SWE based on time series input of the various\nphysical/meteorological factors as well static spatial/morphological factors.\nSpecifically, this model breaks down the SWE estimation into two separate\ntasks: (i) a classification task that indicates the presence/absence of snow on\na specific day and (ii) a regression task that indicates the height of the SWE\non a specific day in the case of snow presence. The model is trained using\nphysical/in situ SWE measurements from the SNOw TELemetry (SNOTEL) snow pillows\nin the western United States. We will show that trained LSTM models have a\nclassification accuracy of $\\geq 93\\%$ for the presence of snow and a\ncoefficient of correlation of $\\sim 0.9$ concerning their SWE estimates. We\nwill also demonstrate that the models can generalize both spatially and\ntemporally to previously unseen data.\n","authors":["Arun M. Saranathan","Mahmoud Saeedimoghaddam","Brandon Smith","Deepthi Raghunandan","Grey Nearing","Craig Pelissier"],"pdf_url":"https://arxiv.org/pdf/2504.20129v2.pdf","comment":"Preprint of journal paper in preparation. Details: 24 pages, 8\n  figures"},{"id":"http://arxiv.org/abs/2507.17545v1","updated":"2025-07-23T14:22:42Z","published":"2025-07-23T14:22:42Z","title":"Scalable DC Optimization via Adaptive Frank-Wolfe Algorithms","summary":"  We consider the problem of minimizing a difference of (smooth) convex\nfunctions over a compact convex feasible region $P$, i.e., $\\min_{x \\in P} f(x)\n- g(x)$, with smooth $f$ and Lipschitz continuous $g$. This computational study\nbuilds upon and complements the framework of Maskan et al. [2025] by\nintegrating advanced Frank-Wolfe variants to reduce computational overhead. We\nempirically show that constrained DC problems can be efficiently solved using a\ncombination of the Blended Pairwise Conditional Gradients (BPCG) algorithm\n[Tsuji et al., 2022] with warm-starting and the adaptive error bound from\nMaskan et al. [2025]. The result is a highly efficient and scalable\nprojection-free algorithm for constrained DC optimization.\n","authors":["Sebastian Pokutta"],"pdf_url":"https://arxiv.org/pdf/2507.17545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17544v1","updated":"2025-07-23T14:20:46Z","published":"2025-07-23T14:20:46Z","title":"Optimal differentially private kernel learning with random projection","summary":"  Differential privacy has become a cornerstone in the development of\nprivacy-preserving learning algorithms. This work addresses optimizing\ndifferentially private kernel learning within the empirical risk minimization\n(ERM) framework. We propose a novel differentially private kernel ERM algorithm\nbased on random projection in the reproducing kernel Hilbert space using\nGaussian processes. Our method achieves minimax-optimal excess risk for both\nthe squared loss and Lipschitz-smooth convex loss functions under a local\nstrong convexity condition. We further show that existing approaches based on\nalternative dimension reduction techniques, such as random Fourier feature\nmappings or $\\ell_2$ regularization, yield suboptimal generalization\nperformance. Our key theoretical contribution also includes the derivation of\ndimension-free generalization bounds for objective perturbation-based private\nlinear ERM -- marking the first such result that does not rely on noisy\ngradient-based mechanisms. Additionally, we obtain sharper generalization\nbounds for existing differentially private kernel ERM algorithms. Empirical\nevaluations support our theoretical claims, demonstrating that random\nprojection enables statistically efficient and optimally private kernel\nlearning. These findings provide new insights into the design of differentially\nprivate algorithms and highlight the central role of dimension reduction in\nbalancing privacy and utility.\n","authors":["Bonwoo Lee","Cheolwoo Park","Jeongyoun Ahn"],"pdf_url":"https://arxiv.org/pdf/2507.17544v1.pdf","comment":"110 page, 12 figures"},{"id":"http://arxiv.org/abs/2507.17540v1","updated":"2025-07-23T14:19:33Z","published":"2025-07-23T14:19:33Z","title":"Clustering-based hard negative sampling for supervised contrastive\n  speaker verification","summary":"  In speaker verification, contrastive learning is gaining popularity as an\nalternative to the traditionally used classification-based approaches.\nContrastive methods can benefit from an effective use of hard negative pairs,\nwhich are different-class samples particularly challenging for a verification\nmodel due to their similarity. In this paper, we propose CHNS - a\nclustering-based hard negative sampling method, dedicated for supervised\ncontrastive speaker representation learning. Our approach clusters embeddings\nof similar speakers, and adjusts batch composition to obtain an optimal ratio\nof hard and easy negatives during contrastive loss calculation. Experimental\nevaluation shows that CHNS outperforms a baseline supervised contrastive\napproach with and without loss-based hard negative sampling, as well as a\nstate-of-the-art classification-based approach to speaker verification by as\nmuch as 18 % relative EER and minDCF on the VoxCeleb dataset using two\nlightweight model architectures.\n","authors":["Piotr Masztalski","Michał Romaniuk","Jakub Żak","Mateusz Matuszewski","Konrad Kowalczyk"],"pdf_url":"https://arxiv.org/pdf/2507.17540v1.pdf","comment":"Accepted to INTERSPEECH 2025"},{"id":"http://arxiv.org/abs/2507.17796v1","updated":"2025-07-23T14:15:31Z","published":"2025-07-23T14:15:31Z","title":"CoCAI: Copula-based Conformal Anomaly Identification for Multivariate\n  Time-Series","summary":"  We propose a novel framework that harnesses the power of generative\nartificial intelligence and copula-based modeling to address two critical\nchallenges in multivariate time-series analysis: delivering accurate\npredictions and enabling robust anomaly detection. Our method, Copula-based\nConformal Anomaly Identification for Multivariate Time-Series (CoCAI),\nleverages a diffusion-based model to capture complex dependencies within the\ndata, enabling high quality forecasting. The model's outputs are further\ncalibrated using a conformal prediction technique, yielding predictive regions\nwhich are statistically valid, i.e., cover the true target values with a\ndesired confidence level. Starting from these calibrated forecasts, robust\noutlier detection is performed by combining dimensionality reduction techniques\nwith copula-based modeling, providing a statistically grounded anomaly score.\nCoCAI benefits from an offline calibration phase that allows for minimal\noverhead during deployment and delivers actionable results rooted in\nestablished theoretical foundations. Empirical tests conducted on real\noperational data derived from water distribution and sewerage systems confirm\nCoCAI's effectiveness in accurately forecasting target sequences of data and in\nidentifying anomalous segments within them.\n","authors":["Nicholas A. Pearson","Francesca Zanello","Davide Russo","Luca Bortolussi","Francesca Cairoli"],"pdf_url":"https://arxiv.org/pdf/2507.17796v1.pdf","comment":"Accepted for Presentation at Runtime Verification 25"},{"id":"http://arxiv.org/abs/2507.17534v1","updated":"2025-07-23T14:13:19Z","published":"2025-07-23T14:13:19Z","title":"Federated Majorize-Minimization: Beyond Parameter Aggregation","summary":"  This paper proposes a unified approach for designing stochastic optimization\nalgorithms that robustly scale to the federated learning setting. Our work\nstudies a class of Majorize-Minimization (MM) problems, which possesses a\nlinearly parameterized family of majorizing surrogate functions. This framework\nencompasses (proximal) gradient-based algorithms for (regularized) smooth\nobjectives, the Expectation Maximization algorithm, and many problems seen as\nvariational surrogate MM. We show that our framework motivates a unifying\nalgorithm called Stochastic Approximation Stochastic Surrogate MM (\\SSMM),\nwhich includes previous stochastic MM procedures as special instances. We then\nextend \\SSMM\\ to the federated setting, while taking into consideration common\nbottlenecks such as data heterogeneity, partial participation, and\ncommunication constraints; this yields \\QSMM. The originality of \\QSMM\\ is to\nlearn locally and then aggregate information characterizing the\n\\textit{surrogate majorizing function}, contrary to classical algorithms which\nlearn and aggregate the \\textit{original parameter}. Finally, to showcase the\nflexibility of this methodology beyond our theoretical setting, we use it to\ndesign an algorithm for computing optimal transport maps in the federated\nsetting.\n","authors":["Aymeric Dieuleveut","Gersende Fort","Mahmoud Hegazy","Hoi-To Wai"],"pdf_url":"https://arxiv.org/pdf/2507.17534v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17141v2","updated":"2025-07-23T14:11:04Z","published":"2025-03-21T13:44:12Z","title":"HiFi-Stream: Streaming Speech Enhancement with Generative Adversarial\n  Networks","summary":"  Speech Enhancement techniques have become core technologies in mobile devices\nand voice software. Still, modern deep learning solutions often require high\namount of computational resources what makes their usage on low-resource\ndevices challenging. We present HiFi-Stream, an optimized version of recently\npublished HiFi++ model. Our experiments demonstrate that HiFi-Stream saves most\nof the qualities of the original model despite its size and computational\ncomplexity improved in comparison to the original HiFi++ making it one of the\nsmallest and fastest models available. The model is evaluated in streaming\nsetting where it demonstrates its superior performance in comparison to modern\nbaselines.\n","authors":["Ekaterina Dmitrieva","Maksim Kaledin"],"pdf_url":"https://arxiv.org/pdf/2503.17141v2.pdf","comment":"5 pages (4 content pages + 1 page of references)"},{"id":"http://arxiv.org/abs/2506.07770v2","updated":"2025-07-23T14:10:03Z","published":"2025-06-09T13:46:44Z","title":"Channel Estimation for RIS-Assisted mmWave Systems via Diffusion Models","summary":"  Reconfigurable intelligent surface (RIS) has been recognized as a promising\ntechnology for next-generation wireless communications. However, the\nperformance of RIS-assisted systems critically depends on accurate channel\nstate information (CSI). To address this challenge, this letter proposes a\nnovel channel estimation method for RIS-aided millimeter-wave (mmWave) systems\nbased on diffusion models (DMs). Specifically, the forward diffusion process of\nthe original signal is formulated to model the received signal as a noisy\nobservation within the framework of DMs. Subsequently, the channel estimation\ntask is formulated as the reverse diffusion process, and a sampling algorithm\nbased on denoising diffusion implicit models (DDIMs) is developed to enable\neffective inference. Furthermore, a lightweight neural network, termed BRCNet,\nis introduced to replace the conventional U-Net, significantly reducing the\nnumber of parameters and computational complexity. Extensive experiments\nconducted under various scenarios demonstrate that the proposed method\nconsistently outperforms existing baselines.\n","authors":["Yang Wang","Yin Xu","Cixiao Zhang","Zhiyong Chen","Mingzeng Dai","Haiming Wang","Bingchao Liu","Dazhi He","Meixia Tao"],"pdf_url":"https://arxiv.org/pdf/2506.07770v2.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2401.01100v3","updated":"2025-07-23T14:08:03Z","published":"2024-01-02T08:43:06Z","title":"Sampling-enabled scalable manifold learning unveils discriminative\n  cluster structure of high-dimensional data","summary":"  As a pivotal branch of machine learning, manifold learning uncovers the\nintrinsic low-dimensional structure within complex nonlinear manifolds in\nhigh-dimensional space for visualization, classification, clustering, and\ngaining key insights. Although existing techniques have achieved remarkable\nsuccesses, they suffer from extensive distortions of cluster structure, which\nhinders the understanding of underlying patterns. Scalability issues also limit\ntheir applicability for handling large-scale data. We hence propose a\nsampling-based Scalable manifold learning technique that enables Uniform and\nDiscriminative Embedding, namely SUDE, for large-scale and high-dimensional\ndata. It starts by seeking a set of landmarks to construct the low-dimensional\nskeleton of the entire data, and then incorporates the non-landmarks into the\nlearned space based on the constrained locally linear embedding (CLLE). We\nempirically validated the effectiveness of SUDE on synthetic datasets and\nreal-world benchmarks, and applied it to analyze single-cell data and detect\nanomalies in electrocardiogram (ECG) signals. SUDE exhibits distinct advantage\nin scalability with respect to data size and embedding dimension, and has\npromising performance in cluster separation, integrity, and global structure\npreservation. The experiments also demonstrate notable robustness in embedding\nquality as the sampling rate decreases.\n","authors":["Dehua Peng","Zhipeng Gui","Wenzhang Wei","Fa Li","Jie Gui","Huayi Wu","Jianya Gong"],"pdf_url":"https://arxiv.org/pdf/2401.01100v3.pdf","comment":"80 pages, 37 figures"},{"id":"http://arxiv.org/abs/2507.17530v1","updated":"2025-07-23T14:07:56Z","published":"2025-07-23T14:07:56Z","title":"Generalized Advantage Estimation for Distributional Policy Gradients","summary":"  Generalized Advantage Estimation (GAE) has been used to mitigate the\ncomputational complexity of reinforcement learning (RL) by employing an\nexponentially weighted estimation of the advantage function to reduce the\nvariance in policy gradient estimates. Despite its effectiveness, GAE is not\ndesigned to handle value distributions integral to distributional RL, which can\ncapture the inherent stochasticity in systems and is hence more robust to\nsystem noises. To address this gap, we propose a novel approach that utilizes\nthe optimal transport theory to introduce a Wasserstein-like directional\nmetric, which measures both the distance and the directional discrepancies\nbetween probability distributions. Using the exponentially weighted estimation,\nwe leverage this Wasserstein-like directional metric to derive distributional\nGAE (DGAE). Similar to traditional GAE, our proposed DGAE provides a\nlow-variance advantage estimate with controlled bias, making it well-suited for\npolicy gradient algorithms that rely on advantage estimation for policy\nupdates. We integrated DGAE into three different policy gradient methods.\nAlgorithms were evaluated across various OpenAI Gym environments and compared\nwith the baselines with traditional GAE to assess the performance.\n","authors":["Shahil Shaik","Jonathon M. Smereka","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2507.17530v1.pdf","comment":"6 pages, 3 figures, published at ACC 2025 Conference"},{"id":"http://arxiv.org/abs/2507.17528v1","updated":"2025-07-23T14:07:47Z","published":"2025-07-23T14:07:47Z","title":"Generalized Low-Rank Matrix Contextual Bandits with Graph Information","summary":"  The matrix contextual bandit (CB), as an extension of the well-known\nmulti-armed bandit, is a powerful framework that has been widely applied in\nsequential decision-making scenarios involving low-rank structure. In many\nreal-world scenarios, such as online advertising and recommender systems,\nadditional graph information often exists beyond the low-rank structure, that\nis, the similar relationships among users/items can be naturally captured\nthrough the connectivity among nodes in the corresponding graphs. However,\nexisting matrix CB methods fail to explore such graph information, and thereby\nmaking them difficult to generate effective decision-making policies. To fill\nin this void, we propose in this paper a novel matrix CB algorithmic framework\nthat builds upon the classical upper confidence bound (UCB) framework. This new\nframework can effectively integrate both the low-rank structure and graph\ninformation in a unified manner. Specifically, it involves first solving a\njoint nuclear norm and matrix Laplacian regularization problem, followed by the\nimplementation of a graph-based generalized linear version of the UCB\nalgorithm. Rigorous theoretical analysis demonstrates that our procedure\noutperforms several popular alternatives in terms of cumulative regret bound,\nowing to the effective utilization of graph information. A series of synthetic\nand real-world data experiments are conducted to further illustrate the merits\nof our procedure.\n","authors":["Yao Wang","Jiannan Li","Yue Kang","Shanxing Gao","Zhenxin Xiao"],"pdf_url":"https://arxiv.org/pdf/2507.17528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17526v1","updated":"2025-07-23T14:07:33Z","published":"2025-07-23T14:07:33Z","title":"Integrating Physics-Based and Data-Driven Approaches for Probabilistic\n  Building Energy Modeling","summary":"  Building energy modeling is a key tool for optimizing the performance of\nbuilding energy systems. Historically, a wide spectrum of methods has been\nexplored -- ranging from conventional physics-based models to purely\ndata-driven techniques. Recently, hybrid approaches that combine the strengths\nof both paradigms have gained attention. These include strategies such as\nlearning surrogates for physics-based models, modeling residuals between\nsimulated and observed data, fine-tuning surrogates with real-world\nmeasurements, using physics-based outputs as additional inputs for data-driven\nmodels, and integrating the physics-based output into the loss function the\ndata-driven model. Despite this progress, two significant research gaps remain.\nFirst, most hybrid methods focus on deterministic modeling, often neglecting\nthe inherent uncertainties caused by factors like weather fluctuations and\noccupant behavior. Second, there has been little systematic comparison within a\nprobabilistic modeling framework. This study addresses these gaps by evaluating\nfive representative hybrid approaches for probabilistic building energy\nmodeling, focusing on quantile predictions of building thermodynamics in a\nreal-world case study. Our results highlight two main findings. First, the\nperformance of hybrid approaches varies across different building room types,\nbut residual learning with a Feedforward Neural Network performs best on\naverage. Notably, the residual approach is the only model that produces\nphysically intuitive predictions when applied to out-of-distribution test data.\nSecond, Quantile Conformal Prediction is an effective procedure for calibrating\nquantile predictions in case of indoor temperature modeling.\n","authors":["Leandro Von Krannichfeldt","Kristina Orehounig","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2507.17526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17795v1","updated":"2025-07-23T14:01:16Z","published":"2025-07-23T14:01:16Z","title":"LSDM: LLM-Enhanced Spatio-temporal Diffusion Model for Service-Level\n  Mobile Traffic Prediction","summary":"  Service-level mobile traffic prediction for individual users is essential for\nnetwork efficiency and quality of service enhancement. However, current\nprediction methods are limited in their adaptability across different urban\nenvironments and produce inaccurate results due to the high uncertainty in\npersonal traffic patterns, the lack of detailed environmental context, and the\ncomplex dependencies among different network services. These challenges demand\nadvanced modeling techniques that can capture dynamic traffic distributions and\nrich environmental features. Inspired by the recent success of diffusion models\nin distribution modeling and Large Language Models (LLMs) in contextual\nunderstanding, we propose an LLM-Enhanced Spatio-temporal Diffusion Model\n(LSDM). LSDM integrates the generative power of diffusion models with the\nadaptive learning capabilities of transformers, augmented by the ability to\ncapture multimodal environmental information for modeling service-level\npatterns and dynamics. Extensive evaluations on real-world service-level\ndatasets demonstrate that the model excels in traffic usage predictions,\nshowing outstanding generalization and adaptability. After incorporating\ncontextual information via LLM, the performance improves by at least 2.83% in\nterms of the coefficient of determination. Compared to models of a similar\ntype, such as CSDI, the root mean squared error can be reduced by at least\n8.29%. The code and dataset will be available at:\nhttps://github.com/SoftYuaneR/LSDM.\n","authors":["Shiyuan Zhang","Tong Li","Zhu Xiao","Hongyang Du","Kaibin Huang"],"pdf_url":"https://arxiv.org/pdf/2507.17795v1.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2507.00358v2","updated":"2025-07-23T14:00:39Z","published":"2025-07-01T01:09:06Z","title":"Data-Driven Exploration for a Class of Continuous-Time Indefinite\n  Linear--Quadratic Reinforcement Learning Problems","summary":"  We study reinforcement learning (RL) for the same class of continuous-time\nstochastic linear--quadratic (LQ) control problems as in\n\\cite{huang2024sublinear}, where volatilities depend on both states and\ncontrols while states are scalar-valued and running control rewards are absent.\nWe propose a model-free, data-driven exploration mechanism that adaptively\nadjusts entropy regularization by the critic and policy variance by the actor.\nUnlike the constant or deterministic exploration schedules employed in\n\\cite{huang2024sublinear}, which require extensive tuning for implementations\nand ignore learning progresses during iterations, our adaptive exploratory\napproach boosts learning efficiency with minimal tuning. Despite its\nflexibility, our method achieves a sublinear regret bound that matches the\nbest-known model-free results for this class of LQ problems, which were\npreviously derived only with fixed exploration schedules. Numerical experiments\ndemonstrate that adaptive explorations accelerate convergence and improve\nregret performance compared to the non-adaptive model-free and model-based\ncounterparts.\n","authors":["Yilie Huang","Xun Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2507.00358v2.pdf","comment":"37 pages, 10 figures"},{"id":"http://arxiv.org/abs/2507.17513v1","updated":"2025-07-23T13:51:06Z","published":"2025-07-23T13:51:06Z","title":"HOTA: Hamiltonian framework for Optimal Transport Advection","summary":"  Optimal transport (OT) has become a natural framework for guiding the\nprobability flows. Yet, the majority of recent generative models assume trivial\ngeometry (e.g., Euclidean) and rely on strong density-estimation assumptions,\nyielding trajectories that do not respect the true principles of optimality in\nthe underlying manifold. We present Hamiltonian Optimal Transport Advection\n(HOTA), a Hamilton-Jacobi-Bellman based method that tackles the dual dynamical\nOT problem explicitly through Kantorovich potentials, enabling efficient and\nscalable trajectory optimization. Our approach effectively evades the need for\nexplicit density modeling, performing even when the cost functionals are\nnon-smooth. Empirically, HOTA outperforms all baselines in standard benchmarks,\nas well as in custom datasets with non-differentiable costs, both in terms of\nfeasibility and optimality.\n","authors":["Nazar Buzun","Daniil Shlenskii","Maxim Bobrin","Dmitry V. Dylov"],"pdf_url":"https://arxiv.org/pdf/2507.17513v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17512v1","updated":"2025-07-23T13:51:04Z","published":"2025-07-23T13:51:04Z","title":"Can One Domain Help Others? A Data-Centric Study on Multi-Domain\n  Reasoning via Reinforcement Learning","summary":"  Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful paradigm for enhancing the reasoning capabilities of LLMs. Existing\nresearch has predominantly concentrated on isolated reasoning domains such as\nmathematical problem-solving, coding tasks, or logical reasoning. However, real\nworld reasoning scenarios inherently demand an integrated application of\nmultiple cognitive skills. Despite this, the interplay among these reasoning\nskills under reinforcement learning remains poorly understood. To bridge this\ngap, we present a systematic investigation of multi-domain reasoning within the\nRLVR framework, explicitly focusing on three primary domains: mathematical\nreasoning, code generation, and logical puzzle solving. We conduct a\ncomprehensive study comprising four key components: (1) Leveraging the GRPO\nalgorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the\nmodels' in-domain improvements and cross-domain generalization capabilities\nwhen trained on single-domain datasets. (2) Additionally, we examine the\nintricate interactions including mutual enhancements and conflicts that emerge\nduring combined cross-domain training. (3) To further understand the influence\nof SFT on RL, we also analyze and compare performance differences between base\nand instruct models under identical RL configurations. (4) Furthermore, we\ndelve into critical RL training details, systematically exploring the impacts\nof curriculum learning strategies, variations in reward design, and\nlanguage-specific factors. Through extensive experiments, our results offer\nsignificant insights into the dynamics governing domain interactions, revealing\nkey factors influencing both specialized and generalizable reasoning\nperformance. These findings provide valuable guidance for optimizing RL\nmethodologies to foster comprehensive, multi-domain reasoning capabilities in\nLLMs.\n","authors":["Yu Li","Zhuoshi Pan","Honglin Lin","Mengyuan Sun","Conghui He","Lijun Wu"],"pdf_url":"https://arxiv.org/pdf/2507.17512v1.pdf","comment":"27 pages, 24 figures"},{"id":"http://arxiv.org/abs/2507.13508v3","updated":"2025-07-23T13:48:01Z","published":"2025-07-17T19:35:29Z","title":"Fake or Real: The Impostor Hunt in Texts for Space Operations","summary":"  The \"Fake or Real\" competition hosted on Kaggle\n(https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt ) is the\nsecond part of a series of follow-up competitions and hackathons related to the\n\"Assurance for Space Domain AI Applications\" project funded by the European\nSpace Agency (https://assurance-ai.space-codev.org/ ). The competition idea is\nbased on two real-life AI security threats identified within the project --\ndata poisoning and overreliance in Large Language Models. The task is to\ndistinguish between the proper output from LLM and the output generated under\nmalicious modification of the LLM. As this problem was not extensively\nresearched, participants are required to develop new techniques to address this\nissue or adjust already existing ones to this problem's statement.\n","authors":["Agata Kaczmarek","Dawid Płudowski","Piotr Wilczyński","Krzysztof Kotowski","Ramez Shendy","Evridiki Ntagiou","Jakub Nalepa","Artur Janicki","Przemysław Biecek"],"pdf_url":"https://arxiv.org/pdf/2507.13508v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17509v1","updated":"2025-07-23T13:47:38Z","published":"2025-07-23T13:47:38Z","title":"Graph Neural Network Approach to Predicting Magnetization in\n  Quasi-One-Dimensional Ising Systems","summary":"  We present a graph-based deep learning framework for predicting the magnetic\nproperties of quasi-one-dimensional Ising spin systems. The lattice geometry is\nencoded as a graph and processed by a graph neural network (GNN) followed by\nfully connected layers. The model is trained on Monte Carlo simulation data and\naccurately reproduces key features of the magnetization curve, including\nplateaus, critical transition points, and the effects of geometric frustration.\nIt captures both local motifs and global symmetries, demonstrating that GNNs\ncan infer magnetic behavior directly from structural connectivity. The proposed\napproach enables efficient prediction of magnetization without the need for\nadditional Monte Carlo simulations.\n","authors":["V. Slavin","O. Kryvchikov","D. Laptev"],"pdf_url":"https://arxiv.org/pdf/2507.17509v1.pdf","comment":"18 pages, 4 figures"},{"id":"http://arxiv.org/abs/2507.17506v1","updated":"2025-07-23T13:43:29Z","published":"2025-07-23T13:43:29Z","title":"Joint Multi-Target Detection-Tracking in Cognitive Massive MIMO Radar\n  via POMCP","summary":"  This correspondence presents a power-aware cognitive radar framework for\njoint detection and tracking of multiple targets in a massive multiple-input\nmultiple-output (MIMO) radar environment. Building on a previous single-target\nalgorithm based on Partially Observable Monte Carlo Planning (POMCP), we extend\nit to the multi-target case by assigning each target an independent POMCP tree,\nenabling scalable and efficient planning.\n  Departing from uniform power allocation-which is often suboptimal with\nvarying signal-to-noise ratios (SNRs)-our approach predicts each target's\nfuture angular position and expected received power, based on its estimated\nrange and radar cross-section (RCS). These predictions guide adaptive waveform\ndesign via a constrained optimization problem that allocates transmit energy to\nenhance the detectability of weaker or distant targets, while ensuring\nsufficient power for high-SNR targets. The reward function in the underlying\npartially observable Markov decision process (POMDP) is also modified to\nprioritize accurate spatial and power estimation.\n  Simulations involving multiple targets with different SNRs confirm the\neffectiveness of our method. The proposed framework for the cognitive radar\nimproves detection probability for low-SNR targets and achieves more accurate\ntracking compared to approaches using uniform or orthogonal waveforms. These\nresults demonstrate the potential of the POMCP-based framework for adaptive,\nefficient multi-target radar systems.\n","authors":["Imad Bouhou","Stefano Fortunati","Leila Gharsalli","Alexandre Renaux"],"pdf_url":"https://arxiv.org/pdf/2507.17506v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17501v1","updated":"2025-07-23T13:37:23Z","published":"2025-07-23T13:37:23Z","title":"DNT: a Deeply Normalized Transformer that can be trained by Momentum SGD","summary":"  Transformers have become the de facto backbone of modern deep learning, yet\ntheir training typically demands an advanced optimizer with adaptive learning\nrate like AdamW, rather than a momentum SGDW (mSGDW). Previous works show that\nit is mainly due to a heavy-tailed distribution of the gradients. In this\npaper, we introduce a Deeply Normalized Transformer (DNT), which is\nmeticulously engineered to overcome this limitation enabling seamless training\nwith vanilla mSGDW while yielding comparable performance to the Transformers\ntrained via AdamW. To be specific, in DNT, we strategically integrate\nnormalization techniques at proper positions in the Transformers to effectively\nmodulate the Jacobian matrices of each layer, balance the influence of weights,\nactivations, and their interactions, and thus enable the distributions of\ngradients concentrated. We provide both theoretical justifications of the\nnormalization technique used in our DNT and extensive empirical evaluation on\ntwo popular Transformer architectures to validate that: a) DNT outperforms its\ncounterparts (\\ie, ViT and GPT), and b) DNT can be effectively trained with\nvanilla mSGDW.\n","authors":["Xianbiao Qi","Marco Chen","Wenjie Xiao","Jiaquan Ye","Yelin He","Chun-Guang Li","Zhouchen Lin"],"pdf_url":"https://arxiv.org/pdf/2507.17501v1.pdf","comment":"We have introduced a novel architecture, Deeply Normalized\n  Transformer (DNT), which enables efficient training with vanilla momentum\n  SGDW (mSGDW), achieving performance on par with AdamW-optimized Transformers"},{"id":"http://arxiv.org/abs/2303.05263v4","updated":"2025-07-23T13:27:03Z","published":"2023-03-09T13:58:35Z","title":"Fast post-process Bayesian inference with Variational Sparse Bayesian\n  Quadrature","summary":"  In applied Bayesian inference scenarios, users may have access to a large\nnumber of pre-existing model evaluations, for example from maximum-a-posteriori\n(MAP) optimization runs. However, traditional approximate inference techniques\nmake little to no use of this available information. We propose the framework\nof post-process Bayesian inference as a means to obtain a quick posterior\napproximation from existing target density evaluations, with no further model\ncalls. Within this framework, we introduce Variational Sparse Bayesian\nQuadrature (VSBQ), a method for post-process approximate inference for models\nwith black-box and potentially noisy likelihoods. VSBQ reuses existing target\ndensity evaluations to build a sparse Gaussian process (GP) surrogate model of\nthe log posterior density function. Subsequently, we leverage sparse-GP\nBayesian quadrature combined with variational inference to achieve fast\napproximate posterior inference over the surrogate. We validate our method on\nchallenging synthetic scenarios and real-world applications from computational\nneuroscience. The experiments show that VSBQ builds high-quality posterior\napproximations by post-processing existing optimization traces, with no further\nmodel evaluations.\n","authors":["Chengkun Li","Grégoire Clarté","Martin Jørgensen","Luigi Acerbi"],"pdf_url":"https://arxiv.org/pdf/2303.05263v4.pdf","comment":"Accepted for publication in Statistics and Computing"},{"id":"http://arxiv.org/abs/2507.17494v1","updated":"2025-07-23T13:23:43Z","published":"2025-07-23T13:23:43Z","title":"To Trust or Not to Trust: On Calibration in ML-based Resource Allocation\n  for Wireless Networks","summary":"  In next-generation communications and networks, machine learning (ML) models\nare expected to deliver not only accurate predictions but also well-calibrated\nconfidence scores that reflect the true likelihood of correct decisions. This\npaper studies the calibration performance of an ML-based outage predictor\nwithin a single-user, multi-resource allocation framework. We first establish\nkey theoretical properties of this system's outage probability (OP) under\nperfect calibration. Importantly, we show that as the number of resources\ngrows, the OP of a perfectly calibrated predictor approaches the expected\noutput conditioned on it being below the classification threshold. In contrast,\nwhen only one resource is available, the system's OP equals the model's overall\nexpected output. We then derive the OP conditions for a perfectly calibrated\npredictor. These findings guide the choice of the classification threshold to\nachieve a desired OP, helping system designers meet specific reliability\nrequirements. We also demonstrate that post-processing calibration cannot\nimprove the system's minimum achievable OP, as it does not introduce new\ninformation about future channel states. Additionally, we show that\nwell-calibrated models are part of a broader class of predictors that\nnecessarily improve OP. In particular, we establish a monotonicity condition\nthat the accuracy-confidence function must satisfy for such improvement to\noccur. To demonstrate these theoretical properties, we conduct a rigorous\nsimulation-based analysis using post-processing calibration techniques: Platt\nscaling and isotonic regression. As part of this framework, the predictor is\ntrained using an outage loss function specifically designed for this system.\nFurthermore, this analysis is performed on Rayleigh fading channels with\ntemporal correlation captured by Clarke's 2D model, which accounts for receiver\nmobility.\n","authors":["Rashika Raina","Nidhi Simmons","David E. Simmons","Michel Daoud Yacoub","Trung Q. Duong"],"pdf_url":"https://arxiv.org/pdf/2507.17494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09068v2","updated":"2025-07-23T13:06:44Z","published":"2025-07-11T23:07:04Z","title":"Infinite Video Understanding","summary":"  The rapid advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have ushered in remarkable progress in video understanding.\nHowever, a fundamental challenge persists: effectively processing and\ncomprehending video content that extends beyond minutes or hours. While recent\nefforts like Video-XL-2 have demonstrated novel architectural solutions for\nextreme efficiency, and advancements in positional encoding such as HoPE and\nVideoRoPE++ aim to improve spatio-temporal understanding over extensive\ncontexts, current state-of-the-art models still encounter significant\ncomputational and memory constraints when faced with the sheer volume of visual\ntokens from lengthy sequences. Furthermore, maintaining temporal coherence,\ntracking complex events, and preserving fine-grained details over extended\nperiods remain formidable hurdles, despite progress in agentic reasoning\nsystems like Deep Video Discovery. This position paper posits that a logical,\nalbeit ambitious, next frontier for multimedia research is Infinite Video\nUnderstanding -- the capability for models to continuously process, understand,\nand reason about video data of arbitrary, potentially never-ending duration. We\nargue that framing Infinite Video Understanding as a blue-sky research\nobjective provides a vital north star for the multimedia, and the wider AI,\nresearch communities, driving innovation in areas such as streaming\narchitectures, persistent memory mechanisms, hierarchical and adaptive\nrepresentations, event-centric reasoning, and novel evaluation paradigms.\nDrawing inspiration from recent work on long/ultra-long video understanding and\nseveral closely related fields, we outline the core challenges and key research\ndirections towards achieving this transformative capability.\n","authors":["Dell Zhang","Xiangyu Chen","Jixiang Luo","Mengxi Jia","Changzhi Sun","Ruilong Ren","Jingren Liu","Hao Sun","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2507.09068v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.20863v3","updated":"2025-07-23T13:04:46Z","published":"2025-05-27T08:14:58Z","title":"Leveraging Diffusion Models for Parameterized Quantum Circuit Generation","summary":"  Quantum computing holds immense potential, yet its practical success depends\non multiple factors, including advances in quantum circuit design. In this\npaper, we introduce a generative approach based on denoising diffusion models\n(DMs) to synthesize parameterized quantum circuits (PQCs). Extending the recent\ndiffusion model pipeline of F\\\"urrutter et al. [1], our model effectively\nconditions the synthesis process, enabling the simultaneous generation of\ncircuit architectures and their continuous gate parameters. We demonstrate our\napproach in synthesizing PQCs optimized for generating high-fidelity\nGreenberger-Horne-Zeilinger (GHZ) states and achieving high accuracy in quantum\nmachine learning (QML) classification tasks. Our results indicate a strong\ngeneralization across varying gate sets and scaling qubit counts, highlighting\nthe versatility and computational efficiency of diffusion-based methods. This\nwork illustrates the potential of generative models as a powerful tool for\naccelerating and optimizing the design of PQCs, supporting the development of\nmore practical and scalable quantum applications.\n","authors":["Daniel Barta","Darya Martyniuk","Johannes Jung","Adrian Paschke"],"pdf_url":"https://arxiv.org/pdf/2505.20863v3.pdf","comment":"This work has been accepted for presentation at IEEE Quantum Week\n  2025: IEEE International Conference on Quantum Computing and Engineering\n  (QCE)"},{"id":"http://arxiv.org/abs/2501.18965v2","updated":"2025-07-23T13:03:41Z","published":"2025-01-31T08:55:56Z","title":"The Surprising Agreement Between Convex Optimization Theory and\n  Learning-Rate Scheduling for Large Model Training","summary":"  We show that learning-rate schedules for large model training behave\nsurprisingly similar to a performance bound from non-smooth convex optimization\ntheory. We provide a bound for the constant schedule with linear cooldown; in\nparticular, the practical benefit of cooldown is reflected in the bound due to\nthe absence of logarithmic terms. Further, we show that this surprisingly close\nmatch between optimization theory and practice can be exploited for\nlearning-rate tuning: we achieve noticeable improvements for training 124M and\n210M Llama-type models by (i) extending the schedule for continued training\nwith optimal learning-rate, and (ii) transferring the optimal learning-rate\nacross schedules.\n","authors":["Fabian Schaipp","Alexander Hägele","Adrien Taylor","Umut Simsekli","Francis Bach"],"pdf_url":"https://arxiv.org/pdf/2501.18965v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17479v1","updated":"2025-07-23T13:01:19Z","published":"2025-07-23T13:01:19Z","title":"SRMambaV2: Biomimetic Attention for Sparse Point Cloud Upsampling in\n  Autonomous Driving","summary":"  Upsampling LiDAR point clouds in autonomous driving scenarios remains a\nsignificant challenge due to the inherent sparsity and complex 3D structures of\nthe data. Recent studies have attempted to address this problem by converting\nthe complex 3D spatial scenes into 2D image super-resolution tasks. However,\ndue to the sparse and blurry feature representation of range images, accurately\nreconstructing detailed and complex spatial topologies remains a major\ndifficulty. To tackle this, we propose a novel sparse point cloud upsampling\nmethod named SRMambaV2, which enhances the upsampling accuracy in long-range\nsparse regions while preserving the overall geometric reconstruction quality.\nSpecifically, inspired by human driver visual perception, we design a\nbiomimetic 2D selective scanning self-attention (2DSSA) mechanism to model the\nfeature distribution in distant sparse areas. Meanwhile, we introduce a\ndual-branch network architecture to enhance the representation of sparse\nfeatures. In addition, we introduce a progressive adaptive loss (PAL) function\nto further refine the reconstruction of fine-grained details during the\nupsampling process. Experimental results demonstrate that SRMambaV2 achieves\nsuperior performance in both qualitative and quantitative evaluations,\nhighlighting its effectiveness and practical value in automotive sparse point\ncloud upsampling tasks.\n","authors":["Chuang Chen","Xiaolin Qin","Jing Hu","Wenyi Ge"],"pdf_url":"https://arxiv.org/pdf/2507.17479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17472v1","updated":"2025-07-23T12:52:38Z","published":"2025-07-23T12:52:38Z","title":"BGM-HAN: A Hierarchical Attention Network for Accurate and Fair Decision\n  Assessment on Semi-Structured Profiles","summary":"  Human decision-making in high-stakes domains often relies on expertise and\nheuristics, but is vulnerable to hard-to-detect cognitive biases that threaten\nfairness and long-term outcomes. This work presents a novel approach to\nenhancing complex decision-making workflows through the integration of\nhierarchical learning alongside various enhancements. Focusing on university\nadmissions as a representative high-stakes domain, we propose BGM-HAN, an\nenhanced Byte-Pair Encoded, Gated Multi-head Hierarchical Attention Network,\ndesigned to effectively model semi-structured applicant data. BGM-HAN captures\nmulti-level representations that are crucial for nuanced assessment, improving\nboth interpretability and predictive performance. Experimental results on real\nadmissions data demonstrate that our proposed model significantly outperforms\nboth state-of-the-art baselines from traditional machine learning to large\nlanguage models, offering a promising framework for augmenting decision-making\nin domains where structure, context, and fairness matter. Source code is\navailable at: https://github.com/junhua/bgm-han.\n","authors":["Junhua Liu","Roy Ka-Wei Lee","Kwan Hui Lim"],"pdf_url":"https://arxiv.org/pdf/2507.17472v1.pdf","comment":"Accepted at ASONAM 2025"},{"id":"http://arxiv.org/abs/2507.17470v1","updated":"2025-07-23T12:51:03Z","published":"2025-07-23T12:51:03Z","title":"Demonstration of Efficient Predictive Surrogates for Large-scale Quantum\n  Processors","summary":"  The ongoing development of quantum processors is driving breakthroughs in\nscientific discovery. Despite this progress, the formidable cost of fabricating\nlarge-scale quantum processors means they will remain rare for the foreseeable\nfuture, limiting their widespread application. To address this bottleneck, we\nintroduce the concept of predictive surrogates, which are classical learning\nmodels designed to emulate the mean-value behavior of a given quantum processor\nwith provably computational efficiency. In particular, we propose two\npredictive surrogates that can substantially reduce the need for quantum\nprocessor access in diverse practical scenarios. To demonstrate their potential\nin advancing digital quantum simulation, we use these surrogates to emulate a\nquantum processor with up to 20 programmable superconducting qubits, enabling\nefficient pre-training of variational quantum eigensolvers for families of\ntransverse-field Ising models and identification of non-equilibrium Floquet\nsymmetry-protected topological phases. Experimental results reveal that the\npredictive surrogates not only reduce measurement overhead by orders of\nmagnitude, but can also surpass the performance of conventional,\nquantum-resource-intensive approaches. Collectively, these findings establish\npredictive surrogates as a practical pathway to broadening the impact of\nadvanced quantum processors.\n","authors":["Wei-You Liao","Yuxuan Du","Xinbiao Wang","Tian-Ci Tian","Yong Luo","Bo Du","Dacheng Tao","He-Liang Huang"],"pdf_url":"https://arxiv.org/pdf/2507.17470v1.pdf","comment":"53 pages, 15 figures, comments are welcome"},{"id":"http://arxiv.org/abs/2506.07584v3","updated":"2025-07-23T12:45:18Z","published":"2025-06-09T09:27:17Z","title":"MIRA: Medical Time Series Foundation Model for Real-World Health Data","summary":"  A unified foundation model for medical time series -- pretrained on open\naccess and ethics board-approved medical corpora -- offers the potential to\nreduce annotation burdens, minimize model customization, and enable robust\ntransfer across clinical institutions, modalities, and tasks, particularly in\ndata-scarce or privacy-constrained environments. However, existing generalist\ntime series foundation models struggle to handle medical time series data due\nto their inherent challenges, including irregular intervals, heterogeneous\nsampling rates, and frequent missing values. To address these challenges, we\nintroduce MIRA, a unified foundation model specifically designed for medical\ntime series forecasting. MIRA incorporates a Continuous-Time Rotary Positional\nEncoding that enables fine-grained modeling of variable time intervals, a\nfrequency-specific mixture-of-experts layer that routes computation across\nlatent frequency regimes to further promote temporal specialization, and a\nContinuous Dynamics Extrapolation Block based on Neural ODE that models the\ncontinuous trajectory of latent states, enabling accurate forecasting at\narbitrary target timestamps. Pretrained on a large-scale and diverse medical\ncorpus comprising over 454 billion time points collect from publicly available\ndatasets, MIRA achieves reductions in forecasting errors by an average of 10%\nand 7% in out-of-distribution and in-distribution scenarios, respectively, when\ncompared to other zero-shot and fine-tuned baselines. We also introduce a\ncomprehensive benchmark spanning multiple downstream clinical tasks,\nestablishing a foundation for future research in medical time series modeling.\n","authors":["Hao Li","Bowen Deng","Chang Xu","Zhiyuan Feng","Viktor Schlegel","Yu-Hao Huang","Yizheng Sun","Jingyuan Sun","Kailai Yang","Yiyao Yu","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2506.07584v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.19991v2","updated":"2025-07-23T12:31:13Z","published":"2025-04-28T17:09:10Z","title":"Mapping of Weed Management Methods in Orchards using Sentinel-2 and\n  PlanetScope Data","summary":"  Effective weed management is crucial for improving agricultural productivity,\nas weeds compete with crops for vital resources like nutrients and water.\nAccurate maps of weed management methods are essential for policymakers to\nassess farmer practices, evaluate impacts on vegetation health, biodiversity,\nand climate, as well as ensure compliance with policies and subsidies. However,\nmonitoring weed management methods is challenging as they commonly rely on\nground-based field surveys, which are often costly, time-consuming and subject\nto delays. In order to tackle this problem, we leverage earth observation data\nand Machine Learning (ML). Specifically, we developed separate ML models using\nSentinel-2 and PlanetScope satellite time series data, respectively, to\nclassify four distinct weed management methods (Mowing, Tillage,\nChemical-spraying, and No practice) in orchards. The findings demonstrate the\npotential of ML-driven remote sensing to enhance the efficiency and accuracy of\nweed management mapping in orchards.\n","authors":["Ioannis Kontogiorgakis","Iason Tsardanidis","Dimitrios Bormpoudakis","Ilias Tsoumas","Dimitra A. Loka","Christos Noulas","Alexandros Tsitouras","Charalampos Kontoes"],"pdf_url":"https://arxiv.org/pdf/2504.19991v2.pdf","comment":"Accepted for 2025 IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS 2025)"},{"id":"http://arxiv.org/abs/2507.17454v1","updated":"2025-07-23T12:21:26Z","published":"2025-07-23T12:21:26Z","title":"C3RL: Rethinking the Combination of Channel-independence and\n  Channel-mixing from Representation Learning","summary":"  Multivariate time series forecasting has drawn increasing attention due to\nits practical importance. Existing approaches typically adopt either\nchannel-mixing (CM) or channel-independence (CI) strategies. CM strategy can\ncapture inter-variable dependencies but fails to discern variable-specific\ntemporal patterns. CI strategy improves this aspect but fails to fully exploit\ncross-variable dependencies like CM. Hybrid strategies based on feature fusion\noffer limited generalization and interpretability. To address these issues, we\npropose C3RL, a novel representation learning framework that jointly models\nboth CM and CI strategies. Motivated by contrastive learning in computer\nvision, C3RL treats the inputs of the two strategies as transposed views and\nbuilds a siamese network architecture: one strategy serves as the backbone,\nwhile the other complements it. By jointly optimizing contrastive and\nprediction losses with adaptive weighting, C3RL balances representation and\nforecasting performance. Extensive experiments on seven models show that C3RL\nboosts the best-case performance rate to 81.4\\% for models based on CI strategy\nand to 76.3\\% for models based on CM strategy, demonstrating strong\ngeneralization and effectiveness. The code will be available once the paper is\naccepted.\n","authors":["Shusen Ma","Yun-Bo Zhao","Yu Kang"],"pdf_url":"https://arxiv.org/pdf/2507.17454v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17453v1","updated":"2025-07-23T12:20:20Z","published":"2025-07-23T12:20:20Z","title":"Efficient Neural Network Verification via Order Leading Exploration of\n  Branch-and-Bound Trees","summary":"  The vulnerability of neural networks to adversarial perturbations has\nnecessitated formal verification techniques that can rigorously certify the\nquality of neural networks. As the state-of-the-art, branch and bound (BaB) is\na \"divide-and-conquer\" strategy that applies off-the-shelf verifiers to\nsub-problems for which they perform better. While BaB can identify the\nsub-problems that are necessary to be split, it explores the space of these\nsub-problems in a naive \"first-come-first-serve\" manner, thereby suffering from\nan issue of inefficiency to reach a verification conclusion. To bridge this\ngap, we introduce an order over different sub-problems produced by BaB,\nconcerning with their different likelihoods of containing counterexamples.\nBased on this order, we propose a novel verification framework Oliva that\nexplores the sub-problem space by prioritizing those sub-problems that are more\nlikely to find counterexamples, in order to efficiently reach the conclusion of\nthe verification. Even if no counterexample can be found in any sub-problem, it\nonly changes the order of visiting different sub-problem and so will not lead\nto a performance degradation. Specifically, Oliva has two variants, including\n$Oliva^{GR}$, a greedy strategy that always prioritizes the sub-problems that\nare more likely to find counterexamples, and $Oliva^{SA}$, a balanced strategy\ninspired by simulated annealing that gradually shifts from exploration to\nexploitation to locate the globally optimal sub-problems. We experimentally\nevaluate the performance of Oliva on 690 verification problems spanning over 5\nmodels with datasets MNIST and CIFAR10. Compared to the state-of-the-art\napproaches, we demonstrate the speedup of Oliva for up to 25X in MNIST, and up\nto 80X in CIFAR10.\n","authors":["Guanqin Zhang","Kota Fukuda","Zhenya Zhang","H. M. N. Dilum Bandara","Shiping Chen","Jianjun Zhao","Yulei Sui"],"pdf_url":"https://arxiv.org/pdf/2507.17453v1.pdf","comment":"This is an extended version of the ECOOP 2025 paper, with a\n  comparison with DATE 2025 (Figure 7 of RQ1 in Section 5.2), as well as an\n  in-depth discussion of OOPSLA 2025 in the related work (Section 6)"},{"id":"http://arxiv.org/abs/2505.19166v2","updated":"2025-07-23T12:14:57Z","published":"2025-05-25T14:32:24Z","title":"JEDI: The Force of Jensen-Shannon Divergence in Disentangling Diffusion\n  Models","summary":"  We introduce JEDI, a test-time adaptation method that enhances subject\nseparation and compositional alignment in diffusion models without requiring\nretraining or external supervision. JEDI operates by minimizing semantic\nentanglement in attention maps using a novel Jensen-Shannon divergence based\nobjective. To improve efficiency, we leverage adversarial optimization,\nreducing the number of updating steps required. JEDI is model-agnostic and\napplicable to architectures such as Stable Diffusion 1.5 and 3.5, consistently\nimproving prompt alignment and disentanglement in complex scenes. Additionally,\nJEDI provides a lightweight, CLIP-free disentanglement score derived from\ninternal attention distributions, offering a principled benchmark for\ncompositional alignment under test-time conditions. Code and results are\navailable at https://ericbill21.github.io/JEDI/.\n","authors":["Eric Tillmann Bill","Enis Simsar","Thomas Hofmann"],"pdf_url":"https://arxiv.org/pdf/2505.19166v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17450v1","updated":"2025-07-23T12:14:17Z","published":"2025-07-23T12:14:17Z","title":"Persistent Patterns in Eye Movements: A Topological Approach to Emotion\n  Recognition","summary":"  We present a topological pipeline for automated multiclass emotion\nrecognition from eye-tracking data. Delay embeddings of gaze trajectories are\nanalyzed using persistent homology. From the resulting persistence diagrams, we\nextract shape-based features such as mean persistence, maximum persistence, and\nentropy. A random forest classifier trained on these features achieves up to\n$75.6\\%$ accuracy on four emotion classes, which are the quadrants the\nCircumplex Model of Affect. The results demonstrate that persistence diagram\ngeometry effectively encodes discriminative gaze dynamics, suggesting a\npromising topological approach for affective computing and human behavior\nanalysis.\n","authors":["Arsha Niksa","Hooman Zare","Ali Shahrabi","Hanieh Hatami","Mohammadreza Razvan"],"pdf_url":"https://arxiv.org/pdf/2507.17450v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17439v1","updated":"2025-07-23T11:58:54Z","published":"2025-07-23T11:58:54Z","title":"Doubly robust outlier resistant inference on causal treatment effect","summary":"  Outliers can severely distort causal effect estimation in observational\nstudies, yet this issue has received limited attention in the literature. Their\ninfluence is especially pronounced in small sample sizes, where detecting and\nremoving outliers becomes increasingly difficult. Therefore, it is essential to\nestimate treatment effects robustly without excluding these influential data\npoints. To address this, we propose a doubly robust point estimator for the\naverage treatment effect under a contaminated model that includes outliers.\nRobustness in outcome regression is achieved through a robust estimating\nequation, while covariate balancing propensity scores (CBPS) ensure resilience\nin propensity score modeling.\n  To prevent model overfitting due to the inclusion of numerous parameters, we\nincorporate variable selection. All these components are unified under a\npenalized empirical likelihood framework. For confidence interval estimation,\nmost existing approaches rely on asymptotic properties, which may be unreliable\nin finite samples. We derive an optimal finite-sample confidence interval for\nthe average treatment effect using our proposed estimating equation, ensuring\nthat the interval bounds remain unaffected by outliers. Through simulations and\na real-world application involving hypertension data with outliers, we\ndemonstrate that our method consistently outperforms existing approaches in\nboth accuracy and robustness.\n","authors":["Joonsung Kang"],"pdf_url":"https://arxiv.org/pdf/2507.17439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03885v4","updated":"2025-07-23T11:50:49Z","published":"2023-12-06T20:24:05Z","title":"Gathering and Exploiting Higher-Order Information when Training Large\n  Structured Models","summary":"  When training large models, such as neural networks, the full derivatives of\norder 2 and beyond are usually inaccessible, due to their computational cost.\nTherefore, among the second-order optimization methods, it is common to bypass\nthe computation of the Hessian by using first-order information, such as the\ngradient of the parameters (e.g., quasi-Newton methods) or the activations\n(e.g., K-FAC). In this paper, we focus on the exact and explicit computation of\nprojections of the Hessian and higher-order derivatives on well-chosen\nsubspaces relevant for optimization. Namely, for a given partition of the set\nof parameters, we compute tensors that can be seen as \"higher-order derivatives\naccording to the partition\", at a reasonable cost as long as the number of\nsubsets of the partition remains small. Then, we give some examples of how\nthese tensors can be used. First, we show how to compute a learning rate per\nsubset of parameters, which can be used for hyperparameter tuning. Second, we\nshow how to use these tensors at order 2 to construct an optimization method\nthat uses information contained in the Hessian. Third, we show how to use these\ntensors at order 3 (information contained in the third derivative of the loss)\nto regularize this optimization method. The resulting training step has several\ninteresting properties, including: it takes into account long-range\ninteractions between the layers of the trained neural network, which is usually\nnot the case in similar methods (e.g., K-FAC); the trajectory of the\noptimization is invariant under affine layer-wise reparameterization.\n","authors":["Pierre Wolinski"],"pdf_url":"https://arxiv.org/pdf/2312.03885v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17418v1","updated":"2025-07-23T11:21:27Z","published":"2025-07-23T11:21:27Z","title":"Ctx2TrajGen: Traffic Context-Aware Microscale Vehicle Trajectories using\n  Generative Adversarial Imitation Learning","summary":"  Precise modeling of microscopic vehicle trajectories is critical for traffic\nbehavior analysis and autonomous driving systems. We propose Ctx2TrajGen, a\ncontext-aware trajectory generation framework that synthesizes realistic urban\ndriving behaviors using GAIL. Leveraging PPO and WGAN-GP, our model addresses\nnonlinear interdependencies and training instability inherent in microscopic\nsettings. By explicitly conditioning on surrounding vehicles and road geometry,\nCtx2TrajGen generates interaction-aware trajectories aligned with real-world\ncontext. Experiments on the drone-captured DRIFT dataset demonstrate superior\nperformance over existing methods in terms of realism, behavioral diversity,\nand contextual fidelity, offering a robust solution to data scarcity and domain\nshift without simulation.\n","authors":["Joobin Jin","Seokjun Hong","Gyeongseon Baek","Yeeun Kim","Byeongjoon Noh"],"pdf_url":"https://arxiv.org/pdf/2507.17418v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17417v1","updated":"2025-07-23T11:21:21Z","published":"2025-07-23T11:21:21Z","title":"A Comprehensive Evaluation on Quantization Techniques for Large Language\n  Models","summary":"  For large language models (LLMs), post-training quantization (PTQ) can\nsignificantly reduce memory footprint and computational overhead. Model\nquantization is a rapidly evolving research field. Though many papers have\nreported breakthrough performance, they may not conduct experiments on the same\nground since one quantization method usually contains multiple components. In\naddition, analyzing the theoretical connections among existing methods is\ncrucial for in-depth understanding. To bridge these gaps, we conduct an\nextensive review of state-of-the-art methods and perform comprehensive\nevaluations on the same ground to ensure fair comparisons. To our knowledge,\nthis fair and extensive investigation remains critically important yet\nunderexplored. To better understand the theoretical connections, we decouple\nthe published quantization methods into two steps: pre-quantization\ntransformation and quantization error mitigation. We define the former as a\npreprocessing step applied before quantization to reduce the impact of\noutliers, making the data distribution flatter and more suitable for\nquantization. Quantization error mitigation involves techniques that offset the\nerrors introduced during quantization, thereby enhancing model performance. We\nevaluate and analyze the impact of different components of quantization\nmethods. Additionally, we analyze and evaluate the latest MXFP4 data format and\nits performance. Our experimental results demonstrate that optimized rotation\nand scaling yield the best performance for pre-quantization transformation, and\ncombining low-rank compensation with GPTQ occasionally outperforms using GPTQ\nalone for quantization error mitigation. Furthermore, we explore the potential\nof the latest MXFP4 quantization and reveal that the optimal pre-quantization\ntransformation strategy for INT4 does not generalize well to MXFP4, inspiring\nfurther investigation.\n","authors":["Yutong Liu","Cairong Zhao","Guosheng Hu"],"pdf_url":"https://arxiv.org/pdf/2507.17417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.01955v2","updated":"2025-07-23T10:52:38Z","published":"2025-07-02T17:59:07Z","title":"How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation\n  Models on Standard Computer Vision Tasks","summary":"  Multimodal foundation models, such as GPT-4o, have recently made remarkable\nprogress, but it is not clear where exactly these models stand in terms of\nunderstanding vision. In this paper, we benchmark the performance of popular\nmultimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0\nFlash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision\ntasks (semantic segmentation, object detection, image classification, depth and\nsurface normal prediction) using established datasets (e.g., COCO, ImageNet and\nits variants, etc).\n  The main challenges to performing this are: 1) most models are trained to\noutput text and cannot natively express versatile domains, such as segments or\n3D geometry, and 2) many leading models are proprietary and accessible only at\nan API level, i.e., there is no weight access to adapt them. We address these\nchallenges by translating standard vision tasks into equivalent text-promptable\nand API-compatible tasks via prompt chaining to create a standardized\nbenchmarking framework.\n  We observe that 1) the models are not close to the state-of-the-art\nspecialist models at any task. However, 2) they are respectable generalists;\nthis is remarkable as they are presumably trained on primarily image-text-based\ntasks. 3) They perform semantic tasks notably better than geometric ones. 4)\nWhile the prompt-chaining techniques affect performance, better models exhibit\nless sensitivity to prompt variations. 5) GPT-4o performs the best among\nnon-reasoning models, securing the top position in 4 out of 6 tasks, 6)\nreasoning models, e.g. o3, show improvements in geometric tasks, and 7) a\npreliminary analysis of models with native image generation, like the latest\nGPT-4o, shows they exhibit quirks like hallucinations and spatial\nmisalignments.\n","authors":["Rahul Ramachandran","Ali Garjani","Roman Bachmann","Andrei Atanov","Oğuzhan Fatih Kar","Amir Zamir"],"pdf_url":"https://arxiv.org/pdf/2507.01955v2.pdf","comment":"Project page at https://fm-vision-evals.epfl.ch/"},{"id":"http://arxiv.org/abs/2507.17396v1","updated":"2025-07-23T10:46:25Z","published":"2025-07-23T10:46:25Z","title":"Learning from Scratch: Structurally-masked Transformer for Next\n  Generation Lib-free Simulation","summary":"  This paper proposes a neural framework for power and timing prediction of\nmulti-stage data path, distinguishing itself from traditional lib-based\nanalytical methods dependent on driver characterization and load\nsimplifications. To the best of our knowledge, this is the first\nlanguage-based, netlist-aware neural network designed explicitly for standard\ncells. Our approach employs two pre-trained neural models of waveform\nprediction and delay estimation that directly infer transient waveforms and\npropagation delays from SPICE netlists, conditioned on critical physical\nparameters such as load capacitance, input slew, and gate size. This method\naccurately captures both intrinsic and coupling-induced delay effects without\nrequiring simplification or interpolation. For multi-stage timing prediction,\nwe implement a recursive propagation strategy where predicted waveforms from\neach stage feed into subsequent stages, cumulatively capturing delays across\nthe logic chain. This approach ensures precise timing alignment and complete\nwaveform visibility throughout complex signal pathways. The waveform prediction\nutilizes a hybrid CNN-Transformer architecture with netlist-aware node-level\nencoding, addressing traditional Transformers' fixed input dimensionality\nconstraints. Additionally, specialized subnetworks separately handle primary\ndelay estimation and crosstalk correction. Experimental results demonstrate\nSPICE-level accuracy, consistently achieving RMSE below 0.0098 across diverse\nindustrial circuits. The proposed framework provides a scalable, structurally\nadaptable neural alternative to conventional power and timing engines,\ndemonstrating high fidelity to physical circuit behaviors.\n","authors":["Junlang Huang","Hao Chen","Zhong Guan"],"pdf_url":"https://arxiv.org/pdf/2507.17396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17792v1","updated":"2025-07-23T10:35:37Z","published":"2025-07-23T10:35:37Z","title":"Causal Mechanism Estimation in Multi-Sensor Systems Across Multiple\n  Domains","summary":"  To gain deeper insights into a complex sensor system through the lens of\ncausality, we present common and individual causal mechanism estimation\n(CICME), a novel three-step approach to inferring causal mechanisms from\nheterogeneous data collected across multiple domains. By leveraging the\nprinciple of Causal Transfer Learning (CTL), CICME is able to reliably detect\ndomain-invariant causal mechanisms when provided with sufficient samples. The\nidentified common causal mechanisms are further used to guide the estimation of\nthe remaining causal mechanisms in each domain individually. The performance of\nCICME is evaluated on linear Gaussian models under scenarios inspired from a\nmanufacturing process. Building upon existing continuous optimization-based\ncausal discovery methods, we show that CICME leverages the benefits of applying\ncausal discovery on the pooled data and repeatedly on data from individual\ndomains, and it even outperforms both baseline methods under certain scenarios.\n","authors":["Jingyi Yu","Tim Pychynski","Marco F. Huber"],"pdf_url":"https://arxiv.org/pdf/2507.17792v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17791v1","updated":"2025-07-23T10:33:35Z","published":"2025-07-23T10:33:35Z","title":"Helix 1.0: An Open-Source Framework for Reproducible and Interpretable\n  Machine Learning on Tabular Scientific Data","summary":"  Helix is an open-source, extensible, Python-based software framework to\nfacilitate reproducible and interpretable machine learning workflows for\ntabular data. It addresses the growing need for transparent experimental data\nanalytics provenance, ensuring that the entire analytical process -- including\ndecisions around data transformation and methodological choices -- is\ndocumented, accessible, reproducible, and comprehensible to relevant\nstakeholders. The platform comprises modules for standardised data\npreprocessing, visualisation, machine learning model training, evaluation,\ninterpretation, results inspection, and model prediction for unseen data. To\nfurther empower researchers without formal training in data science to derive\nmeaningful and actionable insights, Helix features a user-friendly interface\nthat enables the design of computational experiments, inspection of outcomes,\nincluding a novel interpretation approach to machine learning decisions using\nlinguistic terms all within an integrated environment. Released under the MIT\nlicence, Helix is accessible via GitHub and PyPI, supporting community-driven\ndevelopment and promoting adherence to the FAIR principles.\n","authors":["Eduardo Aguilar-Bejarano","Daniel Lea","Karthikeyan Sivakumar","Jimiama M. Mase","Reza Omidvar","Ruizhe Li","Troy Kettle","James Mitchell-White","Morgan R Alexander","David A Winkler","Grazziela Figueredo"],"pdf_url":"https://arxiv.org/pdf/2507.17791v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2507.17383v1","updated":"2025-07-23T10:26:10Z","published":"2025-07-23T10:26:10Z","title":"Confidence Calibration in Vision-Language-Action Models","summary":"  Trustworthy robot behavior requires not only high levels of task success but\nalso that the robot can reliably quantify how likely it is to succeed. To this\nend, we present the first systematic study of confidence calibration in\nvision-language-action (VLA) foundation models, which map visual observations\nand natural-language instructions to low-level robot motor commands. We begin\nwith extensive benchmarking to understand the critical relationship between\ntask success and calibration error across multiple datasets and VLA variants,\nfinding that task performance and calibration are not in tension. Next, we\nintroduce prompt ensembles for VLAs, a lightweight, Bayesian-inspired algorithm\nthat averages confidence across paraphrased instructions and consistently\nimproves calibration. We further analyze calibration over the task time\nhorizon, showing that confidence is often most reliable after making some\nprogress, suggesting natural points for risk-aware intervention. Finally, we\nreveal differential miscalibration across action dimensions and propose\naction-wise Platt scaling, a method to recalibrate each action dimension\nindependently to produce better confidence estimates. Our aim in this study is\nto begin to develop the tools and conceptual understanding necessary to render\nVLAs both highly performant and highly trustworthy via reliable uncertainty\nquantification.\n","authors":["Thomas P Zollo","Richard Zemel"],"pdf_url":"https://arxiv.org/pdf/2507.17383v1.pdf","comment":"34 pages, 19 figures"},{"id":"http://arxiv.org/abs/2507.17382v1","updated":"2025-07-23T10:25:27Z","published":"2025-07-23T10:25:27Z","title":"Continual Generalized Category Discovery: Learning and Forgetting from a\n  Bayesian Perspective","summary":"  Continual Generalized Category Discovery (C-GCD) faces a critical challenge:\nincrementally learning new classes from unlabeled data streams while preserving\nknowledge of old classes. Existing methods struggle with catastrophic\nforgetting, especially when unlabeled data mixes known and novel categories. We\naddress this by analyzing C-GCD's forgetting dynamics through a Bayesian lens,\nrevealing that covariance misalignment between old and new classes drives\nperformance degradation. Building on this insight, we propose Variational Bayes\nC-GCD (VB-CGCD), a novel framework that integrates variational inference with\ncovariance-aware nearest-class-mean classification. VB-CGCD adaptively aligns\nclass distributions while suppressing pseudo-label noise via stochastic\nvariational updates. Experiments show VB-CGCD surpasses prior art by +15.21%\nwith the overall accuracy in the final session on standard benchmarks. We also\nintroduce a new challenging benchmark with only 10% labeled data and extended\nonline phases, VB-CGCD achieves a 67.86% final accuracy, significantly higher\nthan state-of-the-art (38.55%), demonstrating its robust applicability across\ndiverse scenarios. Code is available at: https://github.com/daihao42/VB-CGCD\n","authors":["Hao Dai","Jagmohan Chauhan"],"pdf_url":"https://arxiv.org/pdf/2507.17382v1.pdf","comment":"20 pages, 6 figures. Forty-second International Conference on Machine\n  Learning. 2025"},{"id":"http://arxiv.org/abs/2312.15234v2","updated":"2025-07-23T10:11:55Z","published":"2023-12-23T11:57:53Z","title":"Towards Efficient Generative Large Language Model Serving: A Survey from\n  Algorithms to Systems","summary":"  In the rapidly evolving landscape of artificial intelligence (AI), generative\nlarge language models (LLMs) stand at the forefront, revolutionizing how we\ninteract with our data. However, the computational intensity and memory\nconsumption of deploying these models present substantial challenges in terms\nof serving efficiency, particularly in scenarios demanding low latency and high\nthroughput. This survey addresses the imperative need for efficient LLM serving\nmethodologies from a machine learning system (MLSys) research perspective,\nstanding at the crux of advanced AI innovations and practical system\noptimizations. We provide in-depth analysis, covering a spectrum of solutions,\nranging from cutting-edge algorithmic modifications to groundbreaking changes\nin system designs. The survey aims to provide a comprehensive understanding of\nthe current state and future directions in efficient LLM serving, offering\nvaluable insights for researchers and practitioners in overcoming the barriers\nof effective LLM deployment, thereby reshaping the future of AI.\n","authors":["Xupeng Miao","Gabriele Oliaro","Zhihao Zhang","Xinhao Cheng","Hongyi Jin","Tianqi Chen","Zhihao Jia"],"pdf_url":"https://arxiv.org/pdf/2312.15234v2.pdf","comment":"ACM Computing Surveys"},{"id":"http://arxiv.org/abs/2507.17368v1","updated":"2025-07-23T10:04:30Z","published":"2025-07-23T10:04:30Z","title":"ViRN: Variational Inference and Distribution Trilateration for\n  Long-Tailed Continual Representation Learning","summary":"  Continual learning (CL) with long-tailed data distributions remains a\ncritical challenge for real-world AI systems, where models must sequentially\nadapt to new classes while retaining knowledge of old ones, despite severe\nclass imbalance. Existing methods struggle to balance stability and plasticity,\noften collapsing under extreme sample scarcity. To address this, we propose\nViRN, a novel CL framework that integrates variational inference (VI) with\ndistributional trilateration for robust long-tailed learning. First, we model\nclass-conditional distributions via a Variational Autoencoder to mitigate bias\ntoward head classes. Second, we reconstruct tail-class distributions via\nWasserstein distance-based neighborhood retrieval and geometric fusion,\nenabling sample-efficient alignment of tail-class representations. Evaluated on\nsix long-tailed classification benchmarks, including speech (e.g., rare\nacoustic events, accents) and image tasks, ViRN achieves a 10.24% average\naccuracy gain over state-of-the-art methods.\n","authors":["Hao Dai","Chong Tang","Jagmohan Chauhan"],"pdf_url":"https://arxiv.org/pdf/2507.17368v1.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2507.10382v2","updated":"2025-07-23T10:02:51Z","published":"2025-07-14T15:23:11Z","title":"Leveraging RAG-LLMs for Urban Mobility Simulation and Analysis","summary":"  With the rise of smart mobility and shared e-mobility services, numerous\nadvanced technologies have been applied to this field. Cloud-based traffic\nsimulation solutions have flourished, offering increasingly realistic\nrepresentations of the evolving mobility landscape. LLMs have emerged as\npioneering tools, providing robust support for various applications, including\nintelligent decision-making, user interaction, and real-time traffic analysis.\nAs user demand for e-mobility continues to grow, delivering comprehensive\nend-to-end solutions has become crucial. In this paper, we present a\ncloud-based, LLM-powered shared e-mobility platform, integrated with a mobile\napplication for personalized route recommendations. The optimization module is\nevaluated based on travel time and cost across different traffic scenarios.\nAdditionally, the LLM-powered RAG framework is evaluated at the schema level\nfor different users, using various evaluation methods. Schema-level RAG with\nXiYanSQL achieves an average execution accuracy of 0.81 on system operator\nqueries and 0.98 on user queries.\n","authors":["Yue Ding","Conor McCarthy","Kevin O'Shea","Mingming Liu"],"pdf_url":"https://arxiv.org/pdf/2507.10382v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.14219v2","updated":"2025-07-23T10:00:49Z","published":"2025-07-16T10:56:24Z","title":"Artificial Intelligence for Green Hydrogen Yield Prediction and Site\n  Suitability using SHAP-Based Composite Index: Focus on Oman","summary":"  As nations seek sustainable alternatives to fossil fuels, green hydrogen has\nemerged as a promising strategic pathway toward decarbonisation, particularly\nin solar-rich arid regions. However, identifying optimal locations for hydrogen\nproduction requires the integration of complex environmental, atmospheric, and\ninfrastructural factors, often compounded by limited availability of direct\nhydrogen yield data. This study presents a novel Artificial Intelligence (AI)\nframework for computing green hydrogen yield and site suitability index using\nmean absolute SHAP (SHapley Additive exPlanations) values. This framework\nconsists of a multi-stage pipeline of unsupervised multi-variable clustering,\nsupervised machine learning classifier and SHAP algorithm. The pipeline trains\non an integrated meteorological, topographic and temporal dataset and the\nresults revealed distinct spatial patterns of suitability and relative\ninfluence of the variables. With model predictive accuracy of 98%, the result\nalso showed that water proximity, elevation and seasonal variation are the most\ninfluential factors determining green hydrogen site suitability in Oman with\nmean absolute shap values of 2.470891, 2.376296 and 1.273216 respectively.\nGiven limited or absence of ground-truth yield data in many countries that have\ngreen hydrogen prospects and ambitions, this study offers an objective and\nreproducible alternative to subjective expert weightings, thus allowing the\ndata to speak for itself and potentially discover novel latent groupings\nwithout pre-imposed assumptions. This study offers industry stakeholders and\npolicymakers a replicable and scalable tool for green hydrogen infrastructure\nplanning and other decision making in data-scarce regions.\n","authors":["Obumneme Zimuzor Nwafor","Mohammed Abdul Majeed Al Hooti"],"pdf_url":"https://arxiv.org/pdf/2507.14219v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17365v1","updated":"2025-07-23T09:58:31Z","published":"2025-07-23T09:58:31Z","title":"DynaSearcher: Dynamic Knowledge Graph Augmented Search Agent via\n  Multi-Reward Reinforcement Learning","summary":"  Multi-step agentic retrieval systems based on large language models (LLMs)\nhave demonstrated remarkable performance in complex information search tasks.\nHowever, these systems still face significant challenges in practical\napplications, particularly in generating factually inconsistent intermediate\nqueries and inefficient search trajectories, which can lead to reasoning\ndeviations or redundant computations. To address these issues, we propose\nDynaSearcher, an innovative search agent enhanced by dynamic knowledge graphs\nand multi-reward reinforcement learning (RL). Specifically, our system\nleverages knowledge graphs as external structured knowledge to guide the search\nprocess by explicitly modeling entity relationships, thereby ensuring factual\nconsistency in intermediate queries and mitigating biases from irrelevant\ninformation. Furthermore, we employ a multi-reward RL framework for\nfine-grained control over training objectives such as retrieval accuracy,\nefficiency, and response quality. This framework promotes the generation of\nhigh-quality intermediate queries and comprehensive final answers, while\ndiscouraging unnecessary exploration and minimizing information omissions or\nredundancy. Experimental results demonstrate that our approach achieves\nstate-of-the-art answer accuracy on six multi-hop question answering datasets,\nmatching frontier LLMs while using only small-scale models and limited\ncomputational resources. Furthermore, our approach demonstrates strong\ngeneralization and robustness across diverse retrieval environments and\nlarger-scale models, highlighting its broad applicability.\n","authors":["Chuzhan Hao","Wenfeng Feng","Yuewei Zhang","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2507.17365v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2507.17788v1","updated":"2025-07-23T09:54:44Z","published":"2025-07-23T09:54:44Z","title":"Adaptive Repetition for Mitigating Position Bias in LLM-Based Ranking","summary":"  When using LLMs to rank items based on given criteria, or evaluate answers,\nthe order of candidate items can influence the model's final decision. This\nsensitivity to item positioning in a LLM's prompt is known as position bias.\nPrior research shows that this bias exists even in large models, though its\nseverity varies across models and tasks. In addition to position bias, LLMs\nalso exhibit varying degrees of low repetition consistency, where repeating the\nLLM call with the same candidate ordering can lead to different rankings. To\naddress both inconsistencies, a common approach is to prompt the model multiple\ntimes with different candidate orderings and aggregate the results via majority\nvoting. However, this repetition strategy, significantly increases\ncomputational costs. Extending prior findings, we observe that both the\ndirection -- favoring either the earlier or later candidate in the prompt --\nand magnitude of position bias across instances vary substantially, even within\na single dataset. This observation highlights the need for a per-instance\nmitigation strategy. To this end, we introduce a dynamic early-stopping method\nthat adaptively determines the number of repetitions required for each\ninstance. Evaluating our approach across three LLMs of varying sizes and on two\ntasks, namely re-ranking and alignment, we demonstrate that transitioning to a\ndynamic repetition strategy reduces the number of LLM calls by an average of\n81%, while preserving the accuracy. Furthermore, we propose a confidence-based\nadaptation to our early-stopping method, reducing LLM calls by an average of\n87% compared to static repetition, with only a slight accuracy trade-off\nrelative to our original early-stopping method.\n","authors":["Ali Vardasbi","Gustavo Penha","Claudia Hauff","Hugues Bouchard"],"pdf_url":"https://arxiv.org/pdf/2507.17788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.19996v2","updated":"2025-07-23T09:50:45Z","published":"2025-04-28T17:16:40Z","title":"Monitoring digestate application on agricultural crops using Sentinel-2\n  Satellite imagery","summary":"  The widespread use of Exogenous Organic Matter in agriculture necessitates\nmonitoring to assess its effects on soil and crop health. This study evaluates\noptical Sentinel-2 satellite imagery for detecting digestate application, a\npractice that enhances soil fertility but poses environmental risks like\nmicroplastic contamination and nitrogen losses. In the first instance,\nSentinel-2 satellite image time series (SITS) analysis of specific indices\n(EOMI, NDVI, EVI) was used to characterize EOM's spectral behavior after\napplication on the soils of four different crop types in Thessaly, Greece.\nFurthermore, Machine Learning (ML) models (namely Random Forest, k-NN, Gradient\nBoosting and a Feed-Forward Neural Network), were used to investigate digestate\npresence detection, achieving F1-scores up to 0.85. The findings highlight the\npotential of combining remote sensing and ML for scalable and cost-effective\nmonitoring of EOM applications, supporting precision agriculture and\nsustainability.\n","authors":["Andreas Kalogeras","Dimitrios Bormpoudakis","Iason Tsardanidis","Dimitra A. Loka","Charalampos Kontoes"],"pdf_url":"https://arxiv.org/pdf/2504.19996v2.pdf","comment":"Accepted for 2025 IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS 2025)"},{"id":"http://arxiv.org/abs/2507.17787v1","updated":"2025-07-23T09:50:17Z","published":"2025-07-23T09:50:17Z","title":"Hyperbolic Deep Learning for Foundation Models: A Survey","summary":"  Foundation models pre-trained on massive datasets, including large language\nmodels (LLMs), vision-language models (VLMs), and large multimodal models, have\ndemonstrated remarkable success in diverse downstream tasks. However, recent\nstudies have shown fundamental limitations of these models: (1) limited\nrepresentational capacity, (2) lower adaptability, and (3) diminishing\nscalability. These shortcomings raise a critical question: is Euclidean\ngeometry truly the optimal inductive bias for all foundation models, or could\nincorporating alternative geometric spaces enable models to better align with\nthe intrinsic structure of real-world data and improve reasoning processes?\nHyperbolic spaces, a class of non-Euclidean manifolds characterized by\nexponential volume growth with respect to distance, offer a mathematically\ngrounded solution. These spaces enable low-distortion embeddings of\nhierarchical structures (e.g., trees, taxonomies) and power-law distributions\nwith substantially fewer dimensions compared to Euclidean counterparts. Recent\nadvances have leveraged these properties to enhance foundation models,\nincluding improving LLMs' complex reasoning ability, VLMs' zero-shot\ngeneralization, and cross-modal semantic alignment, while maintaining parameter\nefficiency. This paper provides a comprehensive review of hyperbolic neural\nnetworks and their recent development for foundation models. We further outline\nkey challenges and research directions to advance the field.\n","authors":["Neil He","Hiren Madhu","Ngoc Bui","Menglin Yang","Rex Ying"],"pdf_url":"https://arxiv.org/pdf/2507.17787v1.pdf","comment":"11 Pages, SIGKDD 2025"},{"id":"http://arxiv.org/abs/2507.17346v1","updated":"2025-07-23T09:22:51Z","published":"2025-07-23T09:22:51Z","title":"DeCo-SGD: Joint Optimization of Delay Staleness and Gradient Compression\n  Ratio for Distributed SGD","summary":"  Distributed machine learning in high end-to-end latency and low, varying\nbandwidth network environments undergoes severe throughput degradation. Due to\nits low communication requirements, distributed SGD (D-SGD) remains the\nmainstream optimizer in such challenging networks, but it still suffers from\nsignificant throughput reduction. To mitigate these limitations, existing\napproaches typically employ gradient compression and delayed aggregation to\nalleviate low bandwidth and high latency, respectively. To address both\nchallenges simultaneously, these strategies are often combined, introducing a\ncomplex three-way trade-off among compression ratio, staleness (delayed\nsynchronization steps), and model convergence rate. To achieve the balance\nunder varying bandwidth conditions, an adaptive policy is required to\ndynamically adjust these parameters. Unfortunately, existing works rely on\nstatic heuristic strategies due to the lack of theoretical guidance, which\nprevents them from achieving this goal. This study fills in this theoretical\ngap by introducing a new theoretical tool, decomposing the joint optimization\nproblem into a traditional convergence rate analysis with multiple analyzable\nnoise terms. We are the first to reveal that staleness exponentially amplifies\nthe negative impact of gradient compression on training performance, filling a\ncritical gap in understanding how compressed and delayed gradients affect\ntraining. Furthermore, by integrating the convergence rate with a network-aware\ntime minimization condition, we propose DeCo-SGD, which dynamically adjusts the\ncompression ratio and staleness based on the real-time network condition and\ntraining task. DeCo-SGD achieves up to 5.07 and 1.37 speed-ups over D-SGD and\nstatic strategy in high-latency and low, varying bandwidth networks,\nrespectively.\n","authors":["Rongwei Lu","Jingyan Jiang","Chunyang Li","Haotian Dong","Xingguang Wei","Delin Cai","Zhi Wang"],"pdf_url":"https://arxiv.org/pdf/2507.17346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17786v1","updated":"2025-07-23T09:14:25Z","published":"2025-07-23T09:14:25Z","title":"Reinforcement Learning for Accelerated Aerodynamic Shape Optimisation","summary":"  We introduce a reinforcement learning (RL) based adaptive optimization\nalgorithm for aerodynamic shape optimization focused on dimensionality\nreduction. The form in which RL is applied here is that of a surrogate-based,\nactor-critic policy evaluation MCMC approach allowing for temporal 'freezing'\nof some of the parameters to be optimized. The goals are to minimize\ncomputational effort, and to use the observed optimization results for\ninterpretation of the discovered extrema in terms of their role in achieving\nthe desired flow-field.\n  By a sequence of local optimized parameter changes around intermediate CFD\nsimulations acting as ground truth, it is possible to speed up the global\noptimization if (a) the local neighbourhoods of the parameters in which the\nchanged parameters must reside are sufficiently large to compete with the\ngrid-sized steps and its large number of simulations, and (b) the estimates of\nthe rewards and costs on these neighbourhoods necessary for a good step-wise\nparameter adaption are sufficiently accurate. We give an example of a simple\nfluid-dynamical problem on which the method allows interpretation in the sense\nof a feature importance scoring.\n","authors":["Florian Sobieczky","Alfredo Lopez","Erika Dudkin","Christopher Lackner","Matthias Hochsteger","Bernhard Scheichl","Helmut Sobieczky"],"pdf_url":"https://arxiv.org/pdf/2507.17786v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17343v1","updated":"2025-07-23T09:12:25Z","published":"2025-07-23T09:12:25Z","title":"Principled Multimodal Representation Learning","summary":"  Multimodal representation learning seeks to create a unified representation\nspace by integrating diverse data modalities to improve multimodal\nunderstanding. Traditional methods often depend on pairwise contrastive\nlearning, which relies on a predefined anchor modality, restricting alignment\nacross all modalities. Recent advances have investigated the simultaneous\nalignment of multiple modalities, yet several challenges remain, such as\nlimitations imposed by fixed anchor points and instability arising from\noptimizing the product of singular values. To address the challenges, in this\npaper, we propose Principled Multimodal Representation Learning (PMRL), a novel\nframework that achieves simultaneous alignment of multiple modalities without\nanchor dependency in a more stable manner. Specifically, grounded in the\ntheoretical insight that full alignment corresponds to a rank-1 Gram matrix,\nPMRL optimizes the dominant singular value of the representation matrix to\nalign modalities along a shared leading direction. We propose a softmax-based\nloss function that treats singular values as logits to prioritize the largest\nsingular value. Besides, instance-wise contrastive regularization on the\nleading eigenvectors maintains inter-instance separability and prevents\nrepresentation collapse. Extensive experiments across diverse tasks demonstrate\nPMRL's superiority compared to baseline methods. The source code will be\npublicly available.\n","authors":["Xiaohao Liu","Xiaobo Xia","See-Kiong Ng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2507.17343v1.pdf","comment":"32 pages, 9 figures, 10 tables"},{"id":"http://arxiv.org/abs/2507.17785v1","updated":"2025-07-23T09:01:53Z","published":"2025-07-23T09:01:53Z","title":"Self-similarity Analysis in Deep Neural Networks","summary":"  Current research has found that some deep neural networks exhibit strong\nhierarchical self-similarity in feature representation or parameter\ndistribution. However, aside from preliminary studies on how the power-law\ndistribution of weights across different training stages affects model\nperformance,there has been no quantitative analysis on how the self-similarity\nof hidden space geometry influences model weight optimization, nor is there a\nclear understanding of the dynamic behavior of internal neurons. Therefore,\nthis paper proposes a complex network modeling method based on the output\nfeatures of hidden-layer neurons to investigate the self-similarity of feature\nnetworks constructed at different hidden layers, and analyzes how adjusting the\ndegree of self-similarity in feature networks can enhance the classification\nperformance of deep neural networks. Validated on three types of networks MLP\narchitectures, convolutional networks, and attention architectures this study\nreveals that the degree of self-similarity exhibited by feature networks varies\nacross different model architectures. Furthermore, embedding constraints on the\nself-similarity of feature networks during the training process can improve the\nperformance of self-similar deep neural networks (MLP architectures and\nattention architectures) by up to 6 percentage points.\n","authors":["Jingyi Ding","Chengwen Qi","Hongfei Wang","Jianshe Wu","Licheng Jiao","Yuwei Guo","Jian Gao"],"pdf_url":"https://arxiv.org/pdf/2507.17785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14644v2","updated":"2025-07-23T08:55:02Z","published":"2025-01-24T17:05:00Z","title":"Optimizing Privacy-Utility Trade-off in Decentralized Learning with\n  Generalized Correlated Noise","summary":"  Decentralized learning enables distributed agents to collaboratively train a\nshared machine learning model without a central server, through local\ncomputation and peer-to-peer communication. Although each agent retains its\ndataset locally, sharing local models can still expose private information\nabout the local training datasets to adversaries. To mitigate privacy attacks,\na common strategy is to inject random artificial noise at each agent before\nexchanging local models between neighbors. However, this often leads to utility\ndegradation due to the negative effects of cumulated artificial noise on the\nlearning algorithm. In this work, we introduce CorN-DSGD, a novel\ncovariance-based framework for generating correlated privacy noise across\nagents, which unifies several state-of-the-art methods as special cases. By\nleveraging network topology and mixing weights, CorN-DSGD optimizes the noise\ncovariance to achieve network-wide noise cancellation. Experimental results\nshow that CorN-DSGD cancels more noise than existing pairwise correlation\nschemes, improving model performance under formal privacy guarantees.\n","authors":["Angelo Rodio","Zheng Chen","Erik G. Larsson"],"pdf_url":"https://arxiv.org/pdf/2501.14644v2.pdf","comment":"6 pages, 5 figures, accepted at IEEE ITW 2025"},{"id":"http://arxiv.org/abs/2507.17328v1","updated":"2025-07-23T08:54:36Z","published":"2025-07-23T08:54:36Z","title":"A Learning-based Domain Decomposition Method","summary":"  Recent developments in mechanical, aerospace, and structural engineering have\ndriven a growing need for efficient ways to model and analyse structures at\nmuch larger and more complex scales than before. While established numerical\nmethods like the Finite Element Method remain reliable, they often struggle\nwith computational cost and scalability when dealing with large and\ngeometrically intricate problems. In recent years, neural network-based methods\nhave shown promise because of their ability to efficiently approximate\nnonlinear mappings. However, most existing neural approaches are still largely\nlimited to simple domains, which makes it difficult to apply to real-world PDEs\ninvolving complex geometries. In this paper, we propose a learning-based domain\ndecomposition method (L-DDM) that addresses this gap. Our approach uses a\nsingle, pre-trained neural operator-originally trained on simple domains-as a\nsurrogate model within a domain decomposition scheme, allowing us to tackle\nlarge and complicated domains efficiently. We provide a general theoretical\nresult on the existence of neural operator approximations in the context of\ndomain decomposition solution of abstract PDEs. We then demonstrate our method\nby accurately approximating solutions to elliptic PDEs with discontinuous\nmicrostructures in complex geometries, using a physics-pretrained neural\noperator (PPNO). Our results show that this approach not only outperforms\ncurrent state-of-the-art methods on these challenging problems, but also offers\nresolution-invariance and strong generalization to microstructural patterns\nunseen during training.\n","authors":["Rui Wu","Nikola Kovachki","Burigede Liu"],"pdf_url":"https://arxiv.org/pdf/2507.17328v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.16450v2","updated":"2025-07-23T08:38:58Z","published":"2025-07-22T10:51:35Z","title":"RIS-aided Latent Space Alignment for Semantic Channel Equalization","summary":"  Semantic communication systems introduce a new paradigm in wireless\ncommunications, focusing on transmitting the intended meaning rather than\nensuring strict bit-level accuracy. These systems often rely on Deep Neural\nNetworks (DNNs) to learn and encode meaning directly from data, enabling more\nefficient communication. However, in multi-user settings where interacting\nagents are trained independently-without shared context or joint\noptimization-divergent latent representations across AI-native devices can lead\nto semantic mismatches, impeding mutual understanding even in the absence of\ntraditional transmission errors. In this work, we address semantic mismatch in\nMultiple-Input Multiple-Output (MIMO) channels by proposing a joint physical\nand semantic channel equalization framework that leverages the presence of\nReconfigurable Intelligent Surfaces (RIS). The semantic equalization is\nimplemented as a sequence of transformations: (i) a pre-equalization stage at\nthe transmitter; (ii) propagation through the RIS-aided channel; and (iii) a\npost-equalization stage at the receiver. We formulate the problem as a\nconstrained Minimum Mean Squared Error (MMSE) optimization and propose two\nsolutions: (i) a linear semantic equalization chain, and (ii) a non-linear\nDNN-based semantic equalizer. Both methods are designed to operate under\nsemantic compression in the latent space and adhere to transmit power\nconstraints. Through extensive evaluations, we show that the proposed joint\nequalization strategies consistently outperform conventional, disjoint\napproaches to physical and semantic channel equalization across a broad range\nof scenarios and wireless channel conditions.\n","authors":["Tomás Hüttebräucker","Mario Edoardo Pandolfo","Simone Fiorellino","Emilio Calvanese Strinati","Paolo Di Lorenzo"],"pdf_url":"https://arxiv.org/pdf/2507.16450v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13844v2","updated":"2025-07-23T08:38:14Z","published":"2025-03-18T02:33:38Z","title":"Towards Detecting Persuasion on Social Media: From Model Development to\n  Insights on Persuasion Strategies","summary":"  Political advertising plays a pivotal role in shaping public opinion and\ninfluencing electoral outcomes, often through subtle persuasive techniques\nembedded in broader propaganda strategies. Detecting these persuasive elements\nis crucial for enhancing voter awareness and ensuring transparency in\ndemocratic processes. This paper presents an integrated approach that bridges\nmodel development and real-world application through two interconnected\nstudies. First, we introduce a lightweight model for persuasive text detection\nthat achieves state-of-the-art performance in Subtask 3 of SemEval 2023 Task 3\nwhile requiring significantly fewer computational resources and training data\nthan existing methods. Second, we demonstrate the model's practical utility by\ncollecting the Australian Federal Election 2022 Facebook Ads (APA22) dataset,\npartially annotating a subset for persuasion, and fine-tuning the model to\nadapt from mainstream news to social media content. We then apply the\nfine-tuned model to label the remainder of the APA22 dataset, revealing\ndistinct patterns in how political campaigns leverage persuasion through\ndifferent funding strategies, word choices, demographic targeting, and temporal\nshifts in persuasion intensity as election day approaches. Our findings not\nonly underscore the necessity of domain-specific modeling for analyzing\npersuasion on social media but also show how uncovering these strategies can\nenhance transparency, inform voters, and promote accountability in digital\ncampaigns.\n","authors":["Elyas Meguellati","Stefano Civelli","Pietro Bernardelle","Shazia Sadiq","Irwin King","Gianluca Demartini"],"pdf_url":"https://arxiv.org/pdf/2503.13844v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17316v1","updated":"2025-07-23T08:30:37Z","published":"2025-07-23T08:30:37Z","title":"Nearly Minimax Discrete Distribution Estimation in Kullback-Leibler\n  Divergence with High Probability","summary":"  We consider the problem of estimating a discrete distribution $p$ with\nsupport of size $K$ and provide both upper and lower bounds with high\nprobability in KL divergence. We prove that in the worst case, for any\nestimator $\\widehat{p}$, with probability at least $\\delta$, $\\text{KL}(p \\|\n\\widehat{p}) \\geq C\\max\\{K,\\ln(K)\\ln(1/\\delta) \\}/n $, where $n$ is the sample\nsize and $C > 0$ is a constant. We introduce a computationally efficient\nestimator $p^{\\text{OTB}}$, based on Online to Batch conversion and suffix\naveraging, and show that with probability at least $1 - \\delta$ $\\text{KL}(p \\|\n\\widehat{p}) \\leq C(K\\log(\\log(K)) + \\ln(K)\\ln(1/\\delta)) /n$.\n  Furthermore, we also show that with sufficiently many observations relative\nto $\\log(1/\\delta)$, the maximum likelihood estimator $\\bar{p}$ guarantees that\nwith probability at least $1-\\delta$ $$\n  1/6 \\chi^2(\\bar{p}\\|p) \\leq 1/4 \\chi^2(p\\|\\bar{p}) \\leq \\text{KL}(p|\\bar{p})\n\\leq C(K + \\log(1/\\delta))/n\\,, $$ where $\\chi^2$ denotes the\n$\\chi^2$-divergence.\n","authors":["Dirk van der Hoeven","Julia Olkhovskaia","Tim van Erven"],"pdf_url":"https://arxiv.org/pdf/2507.17316v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17309v1","updated":"2025-07-23T08:23:34Z","published":"2025-07-23T08:23:34Z","title":"Confounded Causal Imitation Learning with Instrumental Variables","summary":"  Imitation learning from demonstrations usually suffers from the confounding\neffects of unmeasured variables (i.e., unmeasured confounders) on the states\nand actions. If ignoring them, a biased estimation of the policy would be\nentailed. To break up this confounding gap, in this paper, we take the best of\nthe strong power of instrumental variables (IV) and propose a Confounded Causal\nImitation Learning (C2L) model. This model accommodates confounders that\ninfluence actions across multiple timesteps, rather than being restricted to\nimmediate temporal dependencies. We develop a two-stage imitation learning\nframework for valid IV identification and policy optimization. In particular,\nin the first stage, we construct a testing criterion based on the defined\npseudo-variable, with which we achieve identifying a valid IV for the C2L\nmodels. Such a criterion entails the sufficient and necessary identifiability\nconditions for IV validity. In the second stage, with the identified IV, we\npropose two candidate policy learning approaches: one is based on a simulator,\nwhile the other is offline. Extensive experiments verified the effectiveness of\nidentifying the valid IV as well as learning the policy.\n","authors":["Yan Zeng","Shenglan Nie","Feng Xie","Libo Huang","Peng Wu","Zhi Geng"],"pdf_url":"https://arxiv.org/pdf/2507.17309v1.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2507.17307v1","updated":"2025-07-23T08:14:36Z","published":"2025-07-23T08:14:36Z","title":"R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning","summary":"  Chain-of-thought (CoT) reasoning enhances the problem-solving capabilities of\nlarge language models by encouraging step-by-step intermediate reasoning during\ninference. While effective, CoT introduces substantial computational overhead\ndue to its reliance on autoregressive decoding over long token sequences.\nExisting acceleration strategies either reduce sequence length through early\nstopping or compressive reward designs, or improve decoding speed via\nspeculative decoding with smaller models. However, speculative decoding suffers\nfrom limited speedup when the agreement between small and large models is low,\nand fails to exploit the potential advantages of small models in producing\nconcise intermediate reasoning. In this paper, we present R-Stitch, a\ntoken-level, confidence-based hybrid decoding framework that accelerates CoT\ninference by switching between a small language model (SLM) and a large\nlanguage model (LLM) along the reasoning trajectory. R-Stitch uses the SLM to\ngenerate tokens by default and delegates to the LLM only when the SLM's\nconfidence falls below a threshold. This design avoids full-sequence rollback\nand selectively invokes the LLM on uncertain steps, preserving both efficiency\nand answer quality. R-Stitch is model-agnostic, training-free, and compatible\nwith standard decoding pipelines. Experiments on math reasoning benchmarks\ndemonstrate that R-Stitch achieves up to 85\\% reduction in inference latency\nwith negligible accuracy drop, highlighting its practical effectiveness in\naccelerating CoT reasoning.\n","authors":["Zhuokun Chen","Zeren Chen","Jiahao He","Mingkui Tan","Jianfei Cai","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2507.17307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12233v2","updated":"2025-07-23T08:07:08Z","published":"2025-07-16T13:47:20Z","title":"Universal Fourier Neural Operators for Micromechanics","summary":"  Solving cell problems in homogenization is hard, and available deep-learning\nframeworks fail to match the speed and generality of traditional computational\nframeworks. More to the point, it is generally unclear what to expect of\nmachine-learning approaches, let alone single out which approaches are\npromising. In the work at hand, we advocate Fourier Neural Operators (FNOs) for\nmicromechanics, empowering them by insights from computational micromechanics\nmethods based on the fast Fourier transform (FFT). We construct an FNO\nsurrogate mimicking the basic scheme foundational for FFT-based methods and\nshow that the resulting operator predicts solutions to cell problems with\narbitrary stiffness distribution only subject to a material-contrast constraint\nup to a desired accuracy. In particular, there are no restrictions on the\nmaterial symmetry like isotropy, on the number of phases and on the geometry of\nthe interfaces between materials. Also, the provided fidelity is sharp and\nuniform, providing explicit guarantees leveraging our physical empowerment of\nFNOs. To show the desired universal approximation property, we construct an FNO\nexplicitly that requires no training to begin with. Still, the obtained neural\noperator complies with the same memory requirements as the basic scheme and\ncomes with runtimes proportional to classical FFT solvers. In particular,\nlarge-scale problems with more than 100 million voxels are readily handled. The\ngoal of this work is to underline the potential of FNOs for solving\nmicromechanical problems, linking FFT-based methods to FNOs. This connection is\nexpected to provide a fruitful exchange between both worlds.\n","authors":["Binh Huy Nguyen","Matti Schneider"],"pdf_url":"https://arxiv.org/pdf/2507.12233v2.pdf","comment":"48 pages, 13 figures"},{"id":"http://arxiv.org/abs/2507.03038v2","updated":"2025-07-23T08:06:29Z","published":"2025-07-03T05:49:18Z","title":"Cautious Next Token Prediction","summary":"  Next token prediction paradigm has been prevailing for autoregressive models\nin the era of LLMs. The current default sampling choice for popular LLMs is\ntemperature scaling together with nucleus sampling to balance diversity and\ncoherence. Nevertheless, such approach leads to inferior performance in various\nNLP tasks when the model is not certain about testing questions. To this end,\nwe propose a brand new training-free decoding strategy, dubbed as Cautious Next\nToken Prediction (CNTP). In the decoding process, if the model has\ncomparatively high prediction entropy at a certain step, we sample multiple\ntrials starting from the step independently and stop when encountering any\npunctuation. Then we select the trial with the lowest perplexity score viewed\nas the most probable and reliable trial path given the model's capacity. The\ntrial number is negatively correlated with the prediction confidence, i.e., the\nless confident the model is, the more trials it should sample. This is\nconsistent with human beings' behaviour: when feeling uncertain or unconfident,\none tends to think more creatively, exploring multiple thinking paths, to\ncautiously select the path one feels most confident about. Extensive\nexperiments on both LLMs and MLLMs show that our proposed CNTP approach\noutperforms existing standard decoding strategies consistently by a clear\nmargin. Moreover, the integration of CNTP with self consistency can further\nimprove over vanilla self consistency. We believe our proposed CNTP has the\npotential to become one of the default choices for LLM decoding. Code is\navailable at https://github.com/wyzjack/CNTP.\n","authors":["Yizhou Wang","Lingzhi Zhang","Yue Bai","Mang Tik Chiu","Zhengmian Hu","Mingyuan Zhang","Qihua Dong","Yu Yin","Sohrab Amirghodsi","Yun Fu"],"pdf_url":"https://arxiv.org/pdf/2507.03038v2.pdf","comment":"ACL 2025"},{"id":"http://arxiv.org/abs/2410.02846v2","updated":"2025-07-23T08:01:02Z","published":"2024-10-03T15:10:55Z","title":"A Spatio-Temporal Machine Learning Model for Mortgage Credit Risk:\n  Default Probabilities and Loan Portfolios","summary":"  We introduce a novel machine learning model for credit risk by combining\ntree-boosting with a latent spatio-temporal Gaussian process model accounting\nfor frailty correlation. This allows for modeling non-linearities and\ninteractions among predictor variables in a flexible data-driven manner and for\naccounting for spatio-temporal variation that is not explained by observable\npredictor variables. We also show how estimation and prediction can be done in\na computationally efficient manner. In an application to a large U.S. mortgage\ncredit risk data set, we find that both predictive default probabilities for\nindividual loans and predictive loan portfolio loss distributions obtained with\nour novel approach are more accurate compared to conventional independent\nlinear hazard models and also linear spatio-temporal models. Using\ninterpretability tools for machine learning models, we find that the likely\nreasons for this outperformance are strong interaction and non-linear effects\nin the predictor variables and the presence of spatio-temporal frailty effects.\n","authors":["Pascal Kündig","Fabio Sigrist"],"pdf_url":"https://arxiv.org/pdf/2410.02846v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17297v1","updated":"2025-07-23T07:58:28Z","published":"2025-07-23T07:58:28Z","title":"On Temporal Guidance and Iterative Refinement in Audio Source Separation","summary":"  Spatial semantic segmentation of sound scenes (S5) involves the accurate\nidentification of active sound classes and the precise separation of their\nsources from complex acoustic mixtures. Conventional systems rely on a\ntwo-stage pipeline - audio tagging followed by label-conditioned source\nseparation - but are often constrained by the absence of fine-grained temporal\ninformation critical for effective separation. In this work, we address this\nlimitation by introducing a novel approach for S5 that enhances the synergy\nbetween the event detection and source separation stages. Our key contributions\nare threefold. First, we fine-tune a pre-trained Transformer to detect active\nsound classes. Second, we utilize a separate instance of this fine-tuned\nTransformer to perform sound event detection (SED), providing the separation\nmodule with detailed, time-varying guidance. Third, we implement an iterative\nrefinement mechanism that progressively enhances separation quality by\nrecursively reusing the separator's output from previous iterations. These\nadvancements lead to significant improvements in both audio tagging and source\nseparation performance, as demonstrated by our system's second-place finish in\nTask 4 of the DCASE Challenge 2025. Our implementation and model checkpoints\nare available in our GitHub repository: https://github.com/theMoro/dcase25task4 .\n","authors":["Tobias Morocutti","Jonathan Greif","Paul Primus","Florian Schmid","Gerhard Widmer"],"pdf_url":"https://arxiv.org/pdf/2507.17297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17294v1","updated":"2025-07-23T07:54:10Z","published":"2025-07-23T07:54:10Z","title":"VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level\n  Tactile Feedback","summary":"  Tactile feedback is generally recognized to be crucial for effective\ninteraction with the physical world. However, state-of-the-art\nVision-Language-Action (VLA) models lack the ability to interpret and use\ntactile signals, limiting their effectiveness in contact-rich tasks.\nIncorporating tactile feedback into these systems is challenging due to the\nabsence of large multi-modal datasets. We present VLA-Touch, an approach that\nenhances generalist robot policies with tactile sensing \\emph{without\nfine-tuning} the base VLA. Our method introduces two key innovations: (1) a\npipeline that leverages a pretrained tactile-language model that provides\nsemantic tactile feedback for high-level task planning, and (2) a\ndiffusion-based controller that refines VLA-generated actions with tactile\nsignals for contact-rich manipulation. Through real-world experiments, we\ndemonstrate that our dual-level integration of tactile feedback improves task\nplanning efficiency while enhancing execution precision. Code is open-sourced\nat \\href{https://github.com/jxbi1010/VLA-Touch}{this URL}.\n","authors":["Jianxin Bi","Kevin Yuchen Ma","Ce Hao","Mike Zheng Shou","Harold Soh"],"pdf_url":"https://arxiv.org/pdf/2507.17294v1.pdf","comment":"19 pages, 5 figures"},{"id":"http://arxiv.org/abs/2507.17293v1","updated":"2025-07-23T07:53:56Z","published":"2025-07-23T07:53:56Z","title":"Data Virtualization for Machine Learning","summary":"  Nowadays, machine learning (ML) teams have multiple concurrent ML workflows\nfor different applications. Each workflow typically involves many experiments,\niterations, and collaborative activities and commonly takes months and\nsometimes years from initial data wrangling to model deployment.\nOrganizationally, there is a large amount of intermediate data to be stored,\nprocessed, and maintained. \\emph{Data virtualization} becomes a critical\ntechnology in an infrastructure to serve ML workflows. In this paper, we\npresent the design and implementation of a data virtualization service,\nfocusing on its service architecture and service operations. The infrastructure\ncurrently supports six ML applications, each with more than one ML workflow.\nThe data virtualization service allows the number of applications and workflows\nto grow in the coming years.\n","authors":["Saiful Khan","Joyraj Chakraborty","Philip Beaucamp","Niraj Bhujel","Min Chen"],"pdf_url":"https://arxiv.org/pdf/2507.17293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17285v1","updated":"2025-07-23T07:45:20Z","published":"2025-07-23T07:45:20Z","title":"Decentralized Federated Learning of Probabilistic Generative Classifiers","summary":"  Federated learning is a paradigm of increasing relevance in real world\napplications, aimed at building a global model across a network of\nheterogeneous users without requiring the sharing of private data. We focus on\nmodel learning over decentralized architectures, where users collaborate\ndirectly to update the global model without relying on a central server. In\nthis context, the current paper proposes a novel approach to collaboratively\nlearn probabilistic generative classifiers with a parametric form. The\nframework is composed by a communication network over a set of local nodes,\neach of one having its own local data, and a local updating rule. The proposal\ninvolves sharing local statistics with neighboring nodes, where each node\naggregates the neighbors' information and iteratively learns its own local\nclassifier, which progressively converges to a global model. Extensive\nexperiments demonstrate that the algorithm consistently converges to a globally\ncompetitive model across a wide range of network topologies, network sizes,\nlocal dataset sizes, and extreme non-i.i.d. data distributions.\n","authors":["Aritz Pérez","Carlos Echegoyen","Guzmán Santafé"],"pdf_url":"https://arxiv.org/pdf/2507.17285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01670v2","updated":"2025-07-23T07:39:55Z","published":"2025-02-01T17:03:45Z","title":"Hardware-Efficient Photonic Tensor Core: Accelerating Deep Neural\n  Networks with Structured Compression","summary":"  The rapid growth in computing demands, particularly driven by artificial\nintelligence applications, has begun to exceed the capabilities of traditional\nelectronic hardware. Optical computing offers a promising alternative due to\nits parallelism, high computational speed, and low power consumption. However,\nexisting photonic integrated circuits are constrained by large footprints,\ncostly electro-optical interfaces, and complex control mechanisms, limiting the\npractical scalability of optical neural networks (ONNs). To address these\nlimitations, we introduce a block-circulant photonic tensor core for a\nstructure-compressed optical neural network (StrC-ONN) architecture. The\nstructured compression technique substantially reduces both model complexity\nand hardware resources without sacrificing the versatility of neural networks,\nand achieves accuracy comparable to uncompressed models. Additionally, we\npropose a hardware-aware training framework to compensate for on-chip\nnonidealities to improve model robustness and accuracy. Experimental validation\nthrough image processing and classification tasks demonstrates that our\nStrC-ONN achieves a reduction in trainable parameters of up to 74.91%,while\nstill maintaining competitive accuracy levels. Performance analyses further\nindicate that this hardware-software co-design approach is expected to yield a\n3.56 times improvement in power efficiency. By reducing both hardware\nrequirements and control complexity across multiple dimensions, this work\nexplores a new pathway toward practical and scalable ONNs, highlighting a\npromising route to address future computational efficiency challenges.\n","authors":["Shupeng Ning","Hanqing Zhu","Chenghao Feng","Jiaqi Gu","David Z. Pan","Ray T. Chen"],"pdf_url":"https://arxiv.org/pdf/2502.01670v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.22334v2","updated":"2025-07-23T07:37:08Z","published":"2025-05-28T13:21:38Z","title":"Advancing Multimodal Reasoning via Reinforcement Learning with Cold\n  Start","summary":"  Recent advancements in large language models (LLMs) have demonstrated\nimpressive chain-of-thought reasoning capabilities, with reinforcement learning\n(RL) playing a crucial role in this progress. While \"aha moment\"\npatterns--where models exhibit self-correction through reflection--are often\nattributed to emergent properties from RL, we first demonstrate that these\npatterns exist in multimodal LLMs (MLLMs) prior to RL training but may not\nnecessarily correlate with improved reasoning performance. Building on these\ninsights, we present a comprehensive study on enhancing multimodal reasoning\nthrough a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start\nwith structured chain-of-thought reasoning patterns, followed by (2)\nreinforcement learning via GRPO to further refine these capabilities. Our\nextensive experiments show that this combined approach consistently outperforms\nboth SFT-only and RL-only methods across challenging multimodal reasoning\nbenchmarks. The resulting models achieve state-of-the-art performance among\nopen-source MLLMs at both 3B and 7B scales, with our 7B model showing\nsubstantial improvements over base models (e.g., 66.3 %$\\rightarrow$73.4 % on\nMathVista, 62.9 %$\\rightarrow$70.4 % on We-Math) and our 3B model achieving\nperformance competitive with several 7B models. Overall, this work provides\npractical guidance for building advanced multimodal reasoning models. Our code\nis available at https://github.com/waltonfuture/RL-with-Cold-Start.\n","authors":["Lai Wei","Yuting Li","Kaipeng Zheng","Chen Wang","Yue Wang","Linghe Kong","Lichao Sun","Weiran Huang"],"pdf_url":"https://arxiv.org/pdf/2505.22334v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17275v1","updated":"2025-07-23T07:25:04Z","published":"2025-07-23T07:25:04Z","title":"Prolonging Tool Life: Learning Skillful Use of General-purpose Tools\n  through Lifespan-guided Reinforcement Learning","summary":"  In inaccessible environments with uncertain task demands, robots often rely\non general-purpose tools that lack predefined usage strategies. These tools are\nnot tailored for particular operations, making their longevity highly sensitive\nto how they are used. This creates a fundamental challenge: how can a robot\nlearn a tool-use policy that both completes the task and prolongs the tool's\nlifespan? In this work, we address this challenge by introducing a\nreinforcement learning (RL) framework that incorporates tool lifespan as a\nfactor during policy optimization. Our framework leverages Finite Element\nAnalysis (FEA) and Miner's Rule to estimate Remaining Useful Life (RUL) based\non accumulated stress, and integrates the RUL into the RL reward to guide\npolicy learning toward lifespan-guided behavior. To handle the fact that RUL\ncan only be estimated after task execution, we introduce an Adaptive Reward\nNormalization (ARN) mechanism that dynamically adjusts reward scaling based on\nestimated RULs, ensuring stable learning signals. We validate our method across\nsimulated and real-world tool use tasks, including Object-Moving and\nDoor-Opening with multiple general-purpose tools. The learned policies\nconsistently prolong tool lifespan (up to 8.01x in simulation) and transfer\neffectively to real-world settings, demonstrating the practical value of\nlearning lifespan-guided tool use strategies.\n","authors":["Po-Yen Wu","Cheng-Yu Kuo","Yuki Kadokawa","Takamitsu Matsubara"],"pdf_url":"https://arxiv.org/pdf/2507.17275v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2507.17273v1","updated":"2025-07-23T07:18:55Z","published":"2025-07-23T07:18:55Z","title":"Leveraging Knowledge Graphs and LLM Reasoning to Identify Operational\n  Bottlenecks for Warehouse Planning Assistance","summary":"  Analyzing large, complex output datasets from Discrete Event Simulations\n(DES) of warehouse operations to identify bottlenecks and inefficiencies is a\ncritical yet challenging task, often demanding significant manual effort or\nspecialized analytical tools. Our framework integrates Knowledge Graphs (KGs)\nand Large Language Model (LLM)-based agents to analyze complex Discrete Event\nSimulation (DES) output data from warehouse operations. It transforms raw DES\ndata into a semantically rich KG, capturing relationships between simulation\nevents and entities. An LLM-based agent uses iterative reasoning, generating\ninterdependent sub-questions. For each sub-question, it creates Cypher queries\nfor KG interaction, extracts information, and self-reflects to correct errors.\nThis adaptive, iterative, and self-correcting process identifies operational\nissues mimicking human analysis. Our DES approach for warehouse bottleneck\nidentification, tested with equipment breakdowns and process irregularities,\noutperforms baseline methods. For operational questions, it achieves\nnear-perfect pass rates in pinpointing inefficiencies. For complex\ninvestigative questions, we demonstrate its superior diagnostic ability to\nuncover subtle, interconnected issues. This work bridges simulation modeling\nand AI (KG+LLM), offering a more intuitive method for actionable insights,\nreducing time-to-insight, and enabling automated warehouse inefficiency\nevaluation and diagnosis.\n","authors":["Rishi Parekh","Saisubramaniam Gopalakrishnan","Zishan Ahmad","Anirudh Deodhar"],"pdf_url":"https://arxiv.org/pdf/2507.17273v1.pdf","comment":"12 pages, 2 figures"}]},"2025-07-22T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2507.17085v1","updated":"2025-07-22T23:58:30Z","published":"2025-07-22T23:58:30Z","title":"Deformable Cluster Manipulation via Whole-Arm Policy Learning","summary":"  Manipulating clusters of deformable objects presents a substantial challenge\nwith widespread applicability, but requires contact-rich whole-arm\ninteractions. A potential solution must address the limited capacity for\nrealistic model synthesis, high uncertainty in perception, and the lack of\nefficient spatial abstractions, among others. We propose a novel framework for\nlearning model-free policies integrating two modalities: 3D point clouds and\nproprioceptive touch indicators, emphasising manipulation with full body\ncontact awareness, going beyond traditional end-effector modes. Our\nreinforcement learning framework leverages a distributional state\nrepresentation, aided by kernel mean embeddings, to achieve improved training\nefficiency and real-time inference. Furthermore, we propose a novel\ncontext-agnostic occlusion heuristic to clear deformables from a target region\nfor exposure tasks. We deploy the framework in a power line clearance scenario\nand observe that the agent generates creative strategies leveraging multiple\narm links for de-occlusion. Finally, we perform zero-shot sim-to-real policy\ntransfer, allowing the arm to clear real branches with unknown occlusion\npatterns, unseen topology, and uncertain dynamics.\n","authors":["Jayadeep Jacob","Wenzheng Zhang","Houston Warren","Paulo Borges","Tirthankar Bandyopadhyay","Fabio Ramos"],"pdf_url":"https://arxiv.org/pdf/2507.17085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07661v2","updated":"2025-07-22T23:55:25Z","published":"2024-06-11T19:06:41Z","title":"ROADWork Dataset: Learning to Recognize, Observe, Analyze and Drive\n  Through Work Zones","summary":"  Perceiving and autonomously navigating through work zones is a challenging\nand underexplored problem. Open datasets for this long-tailed scenario are\nscarce. We propose the ROADWork dataset to learn to recognize, observe,\nanalyze, and drive through work zones. State-of-the-art foundation models fail\nwhen applied to work zones. Fine-tuning models on our dataset significantly\nimproves perception and navigation in work zones. With ROADWork dataset, we\ndiscover new work zone images with higher precision (+32.5%) at a much higher\nrate (12.8$\\times$) around the world. Open-vocabulary methods fail too, whereas\nfine-tuned detectors improve performance (+32.2 AP). Vision-Language Models\n(VLMs) struggle to describe work zones, but fine-tuning substantially improves\nperformance (+36.7 SPICE).\n  Beyond fine-tuning, we show the value of simple techniques. Video label\npropagation provides additional gains (+2.6 AP) for instance segmentation.\nWhile reading work zone signs, composing a detector and text spotter via\ncrop-scaling improves performance +14.2% 1-NED). Composing work zone detections\nto provide context further reduces hallucinations (+3.9 SPICE) in VLMs. We\npredict navigational goals and compute drivable paths from work zone videos.\nIncorporating road work semantics ensures 53.6% goals have angular error (AE) <\n0.5 (+9.9 %) and 75.3% pathways have AE < 0.5 (+8.1 %).\n","authors":["Anurag Ghosh","Shen Zheng","Robert Tamburo","Khiem Vuong","Juan Alvarez-Padilla","Hailiang Zhu","Michael Cardei","Nicholas Dunn","Christoph Mertz","Srinivasa G. Narasimhan"],"pdf_url":"https://arxiv.org/pdf/2406.07661v2.pdf","comment":"ICCV 2025 Accepted Paper"},{"id":"http://arxiv.org/abs/2507.09822v4","updated":"2025-07-22T23:25:21Z","published":"2025-07-13T23:06:46Z","title":"Active Probing with Multimodal Predictions for Motion Planning","summary":"  Navigation in dynamic environments requires autonomous systems to reason\nabout uncertainties in the behavior of other agents. In this paper, we\nintroduce a unified framework that combines trajectory planning with multimodal\npredictions and active probing to enhance decision-making under uncertainty. We\ndevelop a novel risk metric that seamlessly integrates multimodal prediction\nuncertainties through mixture models. When these uncertainties follow a\nGaussian mixture distribution, we prove that our risk metric admits a\nclosed-form solution, and is always finite, thus ensuring analytical\ntractability. To reduce prediction ambiguity, we incorporate an active probing\nmechanism that strategically selects actions to improve its estimates of\nbehavioral parameters of other agents, while simultaneously handling multimodal\nuncertainties. We extensively evaluate our framework in autonomous navigation\nscenarios using the MetaDrive simulation environment. Results demonstrate that\nour active probing approach successfully navigates complex traffic scenarios\nwith uncertain predictions. Additionally, our framework shows robust\nperformance across diverse traffic agent behavior models, indicating its broad\napplicability to real-world autonomous navigation challenges. Code and videos\nare available at\nhttps://darshangm.github.io/papers/active-probing-multimodal-predictions/.\n","authors":["Darshan Gadginmath","Farhad Nawaz","Minjun Sung","Faizan M Tariq","Sangjae Bae","David Isele","Fabio Pasqualetti","Jovin D'sa"],"pdf_url":"https://arxiv.org/pdf/2507.09822v4.pdf","comment":"To appear at IROS '25. 8 pages. 3 tables. 6 figures. Project page:\n  https://darshangm.github.io/papers/active-probing-multimodal-predictions/"},{"id":"http://arxiv.org/abs/2507.17055v1","updated":"2025-07-22T22:31:11Z","published":"2025-07-22T22:31:11Z","title":"Shared Control of Holonomic Wheelchairs through Reinforcement Learning","summary":"  Smart electric wheelchairs can improve user experience by supporting the\ndriver with shared control. State-of-the-art work showed the potential of\nshared control in improving safety in navigation for non-holonomic robots.\nHowever, for holonomic systems, current approaches often lead to unintuitive\nbehavior for the user and fail to utilize the full potential of omnidirectional\ndriving. Therefore, we propose a reinforcement learning-based method, which\ntakes a 2D user input and outputs a 3D motion while ensuring user comfort and\nreducing cognitive load on the driver. Our approach is trained in Isaac Gym and\ntested in simulation in Gazebo. We compare different RL agent architectures and\nreward functions based on metrics considering cognitive load and user comfort.\nWe show that our method ensures collision-free navigation while smartly\norienting the wheelchair and showing better or competitive smoothness compared\nto a previous non-learning-based method. We further perform a sim-to-real\ntransfer and demonstrate, to the best of our knowledge, the first real-world\nimplementation of RL-based shared control for an omnidirectional mobility\nplatform.\n","authors":["Jannis Bähler","Diego Paez-Granados","Jorge Peña-Queralta"],"pdf_url":"https://arxiv.org/pdf/2507.17055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17049v1","updated":"2025-07-22T22:15:59Z","published":"2025-07-22T22:15:59Z","title":"Evaluating Uncertainty and Quality of Visual Language Action-enabled\n  Robots","summary":"  Visual Language Action (VLA) models are a multi-modal class of Artificial\nIntelligence (AI) systems that integrate visual perception, natural language\nunderstanding, and action planning to enable agents to interpret their\nenvironment, comprehend instructions, and perform embodied tasks autonomously.\nRecently, significant progress has been made to advance this field. These kinds\nof models are typically evaluated through task success rates, which fail to\ncapture the quality of task execution and the mode's confidence in its\ndecisions. In this paper, we propose eight uncertainty metrics and five quality\nmetrics specifically designed for VLA models for robotic manipulation tasks. We\nassess their effectiveness through a large-scale empirical study involving 908\nsuccessful task executions from three state-of-the-art VLA models across four\nrepresentative robotic manipulation tasks. Human domain experts manually\nlabeled task quality, allowing us to analyze the correlation between our\nproposed metrics and expert judgments. The results reveal that several metrics\nshow moderate to strong correlation with human assessments, highlighting their\nutility for evaluating task quality and model confidence. Furthermore, we found\nthat some of the metrics can discriminate between high-, medium-, and\nlow-quality executions from unsuccessful tasks, which can be interesting when\ntest oracles are not available. Our findings challenge the adequacy of current\nevaluation practices that rely solely on binary success rates and pave the way\nfor improved real-time monitoring and adaptive enhancement of VLA-enabled\nrobotic systems.\n","authors":["Pablo Valle","Chengjie Lu","Shaukat Ali","Aitor Arrieta"],"pdf_url":"https://arxiv.org/pdf/2507.17049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08791v2","updated":"2025-07-22T21:17:17Z","published":"2025-02-12T21:07:10Z","title":"VL-Explore: Zero-shot Vision-Language Exploration and Target Discovery\n  by Mobile Robots","summary":"  Vision-language navigation (VLN) has emerged as a promising paradigm,\nenabling mobile robots to perform zero-shot inference and execute tasks without\nspecific pre-programming. However, current systems often separate map\nexploration and path planning, with exploration relying on inefficient\nalgorithms due to limited (partially observed) environmental information. In\nthis paper, we present a novel navigation pipeline named \"VL-Explore\" for\nsimultaneous exploration and target discovery in unknown environments,\nleveraging the capabilities of a vision-language model named CLIP. Our approach\nrequires only monocular vision and operates without any prior map or knowledge\nabout the target. For comprehensive evaluations, we designed a functional\nprototype of a UGV (unmanned ground vehicle) system named \"Open Rover\", a\ncustomized platform for general-purpose VLN tasks. We integrated and deployed\nthe VL-Explore pipeline on Open Rover to evaluate its throughput, obstacle\navoidance capability, and trajectory performance across various real-world\nscenarios. Experimental results demonstrate that VL-Explore consistently\noutperforms traditional map-traversal algorithms and achieves performance\ncomparable to path-planning methods that depend on prior map and target\nknowledge. Notably, VL-Explore offers real-time active navigation without\nrequiring pre-captured candidate images or pre-built node graphs, addressing\nkey limitations of existing VLN pipelines.\n","authors":["Yuxuan Zhang","Adnan Abdullah","Sanjeev J. Koppal","Md Jahidul Islam"],"pdf_url":"https://arxiv.org/pdf/2502.08791v2.pdf","comment":"V2, includes suppl as appendix"},{"id":"http://arxiv.org/abs/2410.17491v3","updated":"2025-07-22T20:45:12Z","published":"2024-10-23T01:11:29Z","title":"X-MOBILITY: End-To-End Generalizable Navigation via World Modeling","summary":"  General-purpose navigation in challenging environments remains a significant\nproblem in robotics, with current state-of-the-art approaches facing myriad\nlimitations. Classical approaches struggle with cluttered settings and require\nextensive tuning, while learning-based methods face difficulties generalizing\nto out-of-distribution environments. This paper introduces X-Mobility, an\nend-to-end generalizable navigation model that overcomes existing challenges by\nleveraging three key ideas. First, X-Mobility employs an auto-regressive world\nmodeling architecture with a latent state space to capture world dynamics.\nSecond, a diverse set of multi-head decoders enables the model to learn a rich\nstate representation that correlates strongly with effective navigation skills.\nThird, by decoupling world modeling from action policy, our architecture can\ntrain effectively on a variety of data sources, both with and without expert\npolicies: off-policy data allows the model to learn world dynamics, while\non-policy data with supervisory control enables optimal action policy learning.\nThrough extensive experiments, we demonstrate that X-Mobility not only\ngeneralizes effectively but also surpasses current state-of-the-art navigation\napproaches. Additionally, X-Mobility also achieves zero-shot Sim2Real\ntransferability and shows strong potential for cross-embodiment generalization.\n","authors":["Wei Liu","Huihua Zhao","Chenran Li","Joydeep Biswas","Billy Okal","Pulkit Goyal","Yan Chang","Soha Pouya"],"pdf_url":"https://arxiv.org/pdf/2410.17491v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.16988v1","updated":"2025-07-22T19:52:05Z","published":"2025-07-22T19:52:05Z","title":"RAPTAR: Radar Radiation Pattern Acquisition through Automated\n  Collaborative Robotics","summary":"  Accurate characterization of modern on-chip antennas remains challenging, as\ncurrent probe-station techniques offer limited angular coverage, rely on\nbespoke hardware, and require frequent manual alignment. This research\nintroduces RAPTAR (Radiation Pattern Acquisition through Robotic Automation), a\nportable, state-of-the-art, and autonomous system based on collaborative\nrobotics. RAPTAR enables 3D radiation-pattern measurement of integrated radar\nmodules without dedicated anechoic facilities. The system is designed to\naddress the challenges of testing radar modules mounted in diverse real-world\nconfigurations, including vehicles, UAVs, AR/VR headsets, and biomedical\ndevices, where traditional measurement setups are impractical. A\n7-degree-of-freedom Franka cobot holds the receiver probe and performs\ncollision-free manipulation across a hemispherical spatial domain, guided by\nreal-time motion planning and calibration accuracy with RMS error below 0.9 mm.\nThe system achieves an angular resolution upto 2.5 degree and integrates\nseamlessly with RF instrumentation for near- and far-field power measurements.\nExperimental scans of a 60 GHz radar module show a mean absolute error of less\nthan 2 dB compared to full-wave electromagnetic simulations ground truth.\nBenchmarking against baseline method demonstrates 36.5% lower mean absolute\nerror, highlighting RAPTAR accuracy and repeatability.\n","authors":["Maaz Qureshi","Mohammad Omid Bagheri","Abdelrahman Elbadrawy","William Melek","George Shaker"],"pdf_url":"https://arxiv.org/pdf/2507.16988v1.pdf","comment":"8 Pages, IEEE Journal"},{"id":"http://arxiv.org/abs/2507.16983v1","updated":"2025-07-22T19:47:04Z","published":"2025-07-22T19:47:04Z","title":"Hierarchical Reinforcement Learning Framework for Adaptive Walking\n  Control Using General Value Functions of Lower-Limb Sensor Signals","summary":"  Rehabilitation technology is a natural setting to study the shared learning\nand decision-making of human and machine agents. In this work, we explore the\nuse of Hierarchical Reinforcement Learning (HRL) to develop adaptive control\nstrategies for lower-limb exoskeletons, aiming to enhance mobility and autonomy\nfor individuals with motor impairments. Inspired by prominent models of\nbiological sensorimotor processing, our investigated HRL approach breaks down\nthe complex task of exoskeleton control adaptation into a higher-level\nframework for terrain strategy adaptation and a lower-level framework for\nproviding predictive information; this latter element is implemented via the\ncontinual learning of general value functions (GVFs). GVFs generated temporal\nabstractions of future signal values from multiple wearable lower-limb sensors,\nincluding electromyography, pressure insoles, and goniometers. We investigated\ntwo methods for incorporating actual and predicted sensor signals into a policy\nnetwork with the intent to improve the decision-making capacity of the control\nsystem of a lower-limb exoskeleton during ambulation across varied terrains. As\na key result, we found that the addition of predictions made from GVFs\nincreased overall network accuracy. Terrain-specific performance increases were\nseen while walking on even ground, uneven ground, up and down ramps, and turns,\nterrains that are often misclassified without predictive information. This\nsuggests that predictive information can aid decision-making during\nuncertainty, e.g., on terrains that have a high chance of being misclassified.\nThis work, therefore, contributes new insights into the nuances of HRL and the\nfuture development of exoskeletons to facilitate safe transitioning and\ntraversing across different walking environments.\n","authors":["Sonny T. Jones","Grange M. Simpson","Patrick M. Pilarski","Ashley N. Dalrymple"],"pdf_url":"https://arxiv.org/pdf/2507.16983v1.pdf","comment":"5 pages, 3 figures, accepted at the 6th Multi-disciplinary Conference\n  on Reinforcement Learning and Decision Making (RLDM2025), June 11-14, 2025"},{"id":"http://arxiv.org/abs/2507.16941v1","updated":"2025-07-22T18:24:23Z","published":"2025-07-22T18:24:23Z","title":"Multi-agent Reinforcement Learning for Robotized Coral Reef Sample\n  Collection","summary":"  This paper presents a reinforcement learning (RL) environment for developing\nan autonomous underwater robotic coral sampling agent, a crucial coral reef\nconservation and research task. Using software-in-the-loop (SIL) and\nhardware-in-the-loop (HIL), an RL-trained artificial intelligence (AI)\ncontroller is developed using a digital twin (DT) in simulation and\nsubsequently verified in physical experiments. An underwater motion capture\n(MOCAP) system provides real-time 3D position and orientation feedback during\nverification testing for precise synchronization between the digital and\nphysical domains. A key novelty of this approach is the combined use of a\ngeneral-purpose game engine for simulation, deep RL, and real-time underwater\nmotion capture for an effective zero-shot sim-to-real strategy.\n","authors":["Daniel Correa","Tero Kaarlela","Jose Fuentes","Paulo Padrao","Alain Duran","Leonardo Bobadilla"],"pdf_url":"https://arxiv.org/pdf/2507.16941v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18489v2","updated":"2025-07-22T18:02:57Z","published":"2024-10-24T07:24:11Z","title":"LLM as a code generator in Agile Model Driven Development","summary":"  Leveraging Large Language Models (LLM) like GPT4 in the auto generation of\ncode represents a significant advancement, yet it is not without its\nchallenges. The ambiguity inherent in natural language descriptions of software\nposes substantial obstacles to generating deployable, structured artifacts.\nThis research champions Model Driven Development (MDD) as a viable strategy to\novercome these challenges, proposing an Agile Model Driven Development (AMDD)\napproach that employs GPT4 as a code generator. This approach enhances the\nflexibility and scalability of the code auto generation process and offers\nagility that allows seamless adaptation to changes in models or deployment\nenvironments. We illustrate this by modeling a multi agent Unmanned Vehicle\nFleet (UVF) system using the Unified Modeling Language (UML), significantly\nreducing model ambiguity by integrating the Object Constraint Language (OCL)\nfor code structure meta modeling, and the FIPA ontology language for\ncommunication semantics meta modeling. Applying GPT4 auto generation\ncapabilities yields Java and Python code that is compatible with the JADE and\nPADE frameworks, respectively. Our thorough evaluation of the auto generated\ncode verifies its alignment with expected behaviors and identifies enhancements\nin agent interactions. Structurally, we assessed the complexity of code derived\nfrom a model constrained solely by OCL meta models, against that influenced by\nboth OCL and FIPA ontology meta models. The results indicate that the ontology\nconstrained meta model produces inherently more complex code, yet its\ncyclomatic complexity remains within manageable levels, suggesting that\nadditional meta model constraints can be incorporated without exceeding the\nhigh risk threshold for complexity.\n","authors":["Ahmed R. Sadik","Sebastian Brulin","Markus Olhofer"],"pdf_url":"https://arxiv.org/pdf/2410.18489v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.16815v1","updated":"2025-07-22T17:59:46Z","published":"2025-07-22T17:59:46Z","title":"ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent\n  Planning","summary":"  Vision-language-action (VLA) reasoning tasks require agents to interpret\nmultimodal instructions, perform long-horizon planning, and act adaptively in\ndynamic environments. Existing approaches typically train VLA models in an\nend-to-end fashion, directly mapping inputs to actions without explicit\nreasoning, which hinders their ability to plan over multiple steps or adapt to\ncomplex task variations. In this paper, we propose ThinkAct, a dual-system\nframework that bridges high-level reasoning with low-level action execution via\nreinforced visual latent planning. ThinkAct trains a multimodal LLM to generate\nembodied reasoning plans guided by reinforcing action-aligned visual rewards\nbased on goal completion and trajectory consistency. These reasoning plans are\ncompressed into a visual plan latent that conditions a downstream action model\nfor robust action execution on target environments. Extensive experiments on\nembodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct\nenables few-shot adaptation, long-horizon planning, and self-correction\nbehaviors in complex embodied AI tasks.\n","authors":["Chi-Pin Huang","Yueh-Hua Wu","Min-Hung Chen","Yu-Chiang Frank Wang","Fu-En Yang"],"pdf_url":"https://arxiv.org/pdf/2507.16815v1.pdf","comment":"Project page: https://jasper0314-huang.github.io/thinkact-vla/"},{"id":"http://arxiv.org/abs/2501.06348v3","updated":"2025-07-22T17:44:37Z","published":"2025-01-10T21:20:11Z","title":"Why Automate This? Exploring Correlations between Desire for Robotic\n  Automation, Invested Time and Well-Being","summary":"  Understanding the motivations underlying the human inclination to automate\ntasks is vital to developing truly helpful robots integrated into daily life.\nAccordingly, we ask: are individuals more inclined to automate chores based on\nthe time they consume or the feelings experienced while performing them? This\nstudy explores these preferences and whether they vary across different social\ngroups (i.e., gender category and income level). Leveraging data from the\nBEHAVIOR-1K dataset, the American Time-Use Survey, and the American Time-Use\nSurvey Well-Being Module, we investigate the relationship between the desire\nfor automation, time spent on daily activities, and their associated feelings -\nHappiness, Meaningfulness, Sadness, Painfulness, Stressfulness, or Tiredness.\nOur key findings show that, despite common assumptions, time spent does not\nstrongly relate to the desire for automation for the general population. For\nthe feelings analyzed, only happiness and pain are key indicators. Significant\ndifferences by gender and economic level also emerged: Women prefer to automate\nstressful activities, whereas men prefer to automate those that make them\nunhappy; mid-income individuals prioritize automating less enjoyable and\nmeaningful activities, while low and high-income show no significant\ncorrelations. We hope our research helps motivate technologies to develop\nrobots that match the priorities of potential users, moving domestic robotics\ntoward more socially relevant solutions. We open-source all the data, including\nan online tool that enables the community to replicate our analysis and explore\nadditional trends at https://robin-lab.cs.utexas.edu/why-automate-this/.\n","authors":["Ruchira Ray","Leona Pang","Sanjana Srivastava","Li Fei-Fei","Samantha Shorey","Roberto Martín-Martín"],"pdf_url":"https://arxiv.org/pdf/2501.06348v3.pdf","comment":"20 pages, 14 figures"},{"id":"http://arxiv.org/abs/2507.16713v1","updated":"2025-07-22T15:48:49Z","published":"2025-07-22T15:48:49Z","title":"Experience is the Best Teacher: Grounding VLMs for Robotics through\n  Self-Generated Memory","summary":"  Vision-language models (VLMs) have been widely adopted in robotics to enable\nautonomous planning. However, grounding VLMs, originally trained on internet\ndata, to diverse real-world robots remains a challenge. This paper presents\nExpTeach, a framework that grounds VLMs to physical robots by building a\nself-generated memory of real-world experiences. In ExpTeach, the VLM\nautonomously plans actions, verifies outcomes, reflects on failures, and adapts\nrobot behaviors in a closed loop. The self-generated experiences during this\nprocess are then summarized into a long-term memory, enabling retrieval of\nlearned knowledge to guide future tasks via retrieval-augmented generation\n(RAG). Additionally, ExpTeach enhances the spatial understanding of VLMs with\nan on-demand image annotation module. In experiments, we show that reflection\nimproves success rates from 36% to 84% on four challenging robotic tasks and\nobserve the emergence of intelligent object interactions, including creative\ntool use. Across extensive tests on 12 real-world scenarios (including eight\nunseen ones), we find that grounding with long-term memory boosts single-trial\nsuccess rates from 22% to 80%, demonstrating the effectiveness and\ngeneralizability of ExpTeach.\n","authors":["Guowei Lan","Kaixian Qu","René Zurbrügg","Changan Chen","Christopher E. Mower","Haitham Bou-Ammar","Marco Hutter"],"pdf_url":"https://arxiv.org/pdf/2507.16713v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.15493v2","updated":"2025-07-22T15:04:37Z","published":"2025-07-21T10:54:13Z","title":"GR-3 Technical Report","summary":"  We report our recent progress towards building generalist robot policies, the\ndevelopment of GR-3. GR-3 is a large-scale vision-language-action (VLA) model.\nIt showcases exceptional capabilities in generalizing to novel objects,\nenvironments, and instructions involving abstract concepts. Furthermore, it can\nbe efficiently fine-tuned with minimal human trajectory data, enabling rapid\nand cost-effective adaptation to new settings. GR-3 also excels in handling\nlong-horizon and dexterous tasks, including those requiring bi-manual\nmanipulation and mobile movement, showcasing robust and reliable performance.\nThese capabilities are achieved through a multi-faceted training recipe that\nincludes co-training with web-scale vision-language data, efficient fine-tuning\nfrom human trajectory data collected via VR devices, and effective imitation\nlearning with robot trajectory data. In addition, we introduce ByteMini, a\nversatile bi-manual mobile robot designed with exceptional flexibility and\nreliability, capable of accomplishing a wide range of tasks when integrated\nwith GR-3. Through extensive real-world experiments, we show GR-3 surpasses the\nstate-of-the-art baseline method, $\\pi_0$, on a wide variety of challenging\ntasks. We hope GR-3 can serve as a step towards building generalist robots\ncapable of assisting humans in daily life.\n","authors":["Chilam Cheang","Sijin Chen","Zhongren Cui","Yingdong Hu","Liqun Huang","Tao Kong","Hang Li","Yifeng Li","Yuxiao Liu","Xiao Ma","Hao Niu","Wenxuan Ou","Wanli Peng","Zeyu Ren","Haixin Shi","Jiawen Tian","Hongtao Wu","Xin Xiao","Yuyang Xiao","Jiafeng Xu","Yichu Yang"],"pdf_url":"https://arxiv.org/pdf/2507.15493v2.pdf","comment":"Tech report. Authors are listed in alphabetical order. Project page:\n  https://seed.bytedance.com/GR3/"},{"id":"http://arxiv.org/abs/2507.16645v1","updated":"2025-07-22T14:42:49Z","published":"2025-07-22T14:42:49Z","title":"Morpheus: A Neural-driven Animatronic Face with Hybrid Actuation and\n  Diverse Emotion Control","summary":"  Previous animatronic faces struggle to express emotions effectively due to\nhardware and software limitations. On the hardware side, earlier approaches\neither use rigid-driven mechanisms, which provide precise control but are\ndifficult to design within constrained spaces, or tendon-driven mechanisms,\nwhich are more space-efficient but challenging to control. In contrast, we\npropose a hybrid actuation approach that combines the best of both worlds. The\neyes and mouth-key areas for emotional expression-are controlled using rigid\nmechanisms for precise movement, while the nose and cheek, which convey subtle\nfacial microexpressions, are driven by strings. This design allows us to build\na compact yet versatile hardware platform capable of expressing a wide range of\nemotions. On the algorithmic side, our method introduces a self-modeling\nnetwork that maps motor actions to facial landmarks, allowing us to\nautomatically establish the relationship between blendshape coefficients for\ndifferent facial expressions and the corresponding motor control signals\nthrough gradient backpropagation. We then train a neural network to map speech\ninput to corresponding blendshape controls. With our method, we can generate\ndistinct emotional expressions such as happiness, fear, disgust, and anger,\nfrom any given sentence, each with nuanced, emotion-specific control signals-a\nfeature that has not been demonstrated in earlier systems. We release the\nhardware design and code at https://github.com/ZZongzheng0918/Morpheus-Hardware\nand https://github.com/ZZongzheng0918/Morpheus-Software.\n","authors":["Zongzheng Zhang","Jiawen Yang","Ziqiao Peng","Meng Yang","Jianzhu Ma","Lin Cheng","Huazhe Xu","Hang Zhao","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2507.16645v1.pdf","comment":"Accepted to RSS 2025, Project Page:\n  https://jiawenyang-ch.github.io/Morpheus-Hardware-Design/"},{"id":"http://arxiv.org/abs/2409.17731v2","updated":"2025-07-22T14:41:43Z","published":"2024-09-26T11:01:00Z","title":"Robust Ladder Climbing with a Quadrupedal Robot","summary":"  Quadruped robots are proliferating in industrial environments where they\ncarry sensor payloads and serve as autonomous inspection platforms. Despite the\nadvantages of legged robots over their wheeled counterparts on rough and uneven\nterrain, they are still unable to reliably negotiate a ubiquitous feature of\nindustrial infrastructure: ladders. Inability to traverse ladders prevents\nquadrupeds from inspecting dangerous locations, puts humans in harm's way, and\nreduces industrial site productivity. In this paper, we learn quadrupedal\nladder climbing via a reinforcement learning-based control policy and a\ncomplementary hooked end effector. We evaluate the robustness in simulation\nacross different ladder inclinations, rung geometries, and inter-rung spacings.\nOn hardware, we demonstrate zero-shot transfer with an overall 90% success rate\nat ladder angles ranging from 70{\\deg} to 90{\\deg}, consistent climbing\nperformance during unmodeled perturbations, and climbing speeds 232x faster\nthan the state of the art. This work expands the scope of industrial quadruped\nrobot applications beyond inspection on nominal terrains to challenging\ninfrastructural features in the environment, highlighting synergies between\nrobot morphology and control policy when performing complex skills. More\ninformation can be found at the project website:\nhttps://sites.google.com/leggedrobotics.com/climbingladders.\n","authors":["Dylan Vogel","Robert Baines","Joseph Church","Julian Lotzer","Karl Werner","Marco Hutter"],"pdf_url":"https://arxiv.org/pdf/2409.17731v2.pdf","comment":"Project website:\n  https://sites.google.com/leggedrobotics.com/climbingladders"},{"id":"http://arxiv.org/abs/2507.16621v1","updated":"2025-07-22T14:15:28Z","published":"2025-07-22T14:15:28Z","title":"A Target-based Multi-LiDAR Multi-Camera Extrinsic Calibration System","summary":"  Extrinsic Calibration represents the cornerstone of autonomous driving. Its\naccuracy plays a crucial role in the perception pipeline, as any errors can\nhave implications for the safety of the vehicle. Modern sensor systems collect\ndifferent types of data from the environment, making it harder to align the\ndata. To this end, we propose a target-based extrinsic calibration system\ntailored for a multi-LiDAR and multi-camera sensor suite. This system enables\ncross-calibration between LiDARs and cameras with limited prior knowledge using\na custom ChArUco board and a tailored nonlinear optimization method. We test\nthe system with real-world data gathered in a warehouse. Results demonstrated\nthe effectiveness of the proposed method, highlighting the feasibility of a\nunique pipeline tailored for various types of sensors.\n","authors":["Lorenzo Gentilini","Pierpaolo Serio","Valentina Donzella","Lorenzo Pollini"],"pdf_url":"https://arxiv.org/pdf/2507.16621v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07714v2","updated":"2025-07-22T13:39:28Z","published":"2025-07-10T12:52:19Z","title":"Adaptive Gaussian Mixture Models-based Anomaly Detection for\n  under-constrained Cable-Driven Parallel Robots","summary":"  Cable-Driven Parallel Robots (CDPRs) are increasingly used for load\nmanipulation tasks involving predefined toolpaths with intermediate stops. At\neach stop, where the platform maintains a fixed pose and the motors keep the\ncables under tension, the system must evaluate whether it is safe to proceed by\ndetecting anomalies that could compromise performance (e.g., wind gusts or\ncable impacts). This paper investigates whether anomalies can be detected using\nonly motor torque data, without additional sensors. It introduces an adaptive,\nunsupervised outlier detection algorithm based on Gaussian Mixture Models\n(GMMs) to identify anomalies from torque signals. The method starts with a\nbrief calibration period, just a few seconds, during which a GMM is fit on\nknown anomaly-free data. Real-time torque measurements are then evaluated using\nMahalanobis distance from the GMM, with statistically derived thresholds\ntriggering anomaly flags. Model parameters are periodically updated using the\nlatest segments identified as anomaly-free to adapt to changing conditions.\nValidation includes 14 long-duration test sessions simulating varied wind\nintensities. The proposed method achieves a 100% true positive rate and 95.4%\naverage true negative rate, with 1-second detection latency. Comparative\nevaluation against power threshold and non-adaptive GMM methods indicates\nhigher robustness to drift and environmental variation.\n","authors":["Julio Garrido","Javier Vales","Diego Silva-Muñiz","Enrique Riveiro","Pablo López-Matencio","Josué Rivera-Andrade"],"pdf_url":"https://arxiv.org/pdf/2507.07714v2.pdf","comment":"14 pages, 8 figures, 1 table"},{"id":"http://arxiv.org/abs/2503.02581v2","updated":"2025-07-22T13:20:58Z","published":"2025-03-04T13:04:46Z","title":"Unveiling the Potential of Segment Anything Model 2 for RGB-Thermal\n  Semantic Segmentation with Language Guidance","summary":"  The perception capability of robotic systems relies on the richness of the\ndataset. Although Segment Anything Model 2 (SAM2), trained on large datasets,\ndemonstrates strong perception potential in perception tasks, its inherent\ntraining paradigm prevents it from being suitable for RGB-T tasks. To address\nthese challenges, we propose SHIFNet, a novel SAM2-driven Hybrid Interaction\nParadigm that unlocks the potential of SAM2 with linguistic guidance for\nefficient RGB-Thermal perception. Our framework consists of two key components:\n(1) Semantic-Aware Cross-modal Fusion (SACF) module that dynamically balances\nmodality contributions through text-guided affinity learning, overcoming SAM2's\ninherent RGB bias; (2) Heterogeneous Prompting Decoder (HPD) that enhances\nglobal semantic information through a semantic enhancement module and then\ncombined with category embeddings to amplify cross-modal semantic consistency.\nWith 32.27M trainable parameters, SHIFNet achieves state-of-the-art\nsegmentation performance on public benchmarks, reaching 89.8% on PST900 and\n67.8% on FMB, respectively. The framework facilitates the adaptation of\npre-trained large models to RGB-T segmentation tasks, effectively mitigating\nthe high costs associated with data collection while endowing robotic systems\nwith comprehensive perception capabilities. The source code will be made\npublicly available at https://github.com/iAsakiT3T/SHIFNet.\n","authors":["Jiayi Zhao","Fei Teng","Kai Luo","Guoqiang Zhao","Zhiyong Li","Xu Zheng","Kailun Yang"],"pdf_url":"https://arxiv.org/pdf/2503.02581v2.pdf","comment":"Accepted to IROS 2025. The source code will be made publicly\n  available at https://github.com/iAsakiT3T/SHIFNet"},{"id":"http://arxiv.org/abs/2507.16481v1","updated":"2025-07-22T11:36:45Z","published":"2025-07-22T11:36:45Z","title":"Guided Reinforcement Learning for Omnidirectional 3D Jumping in\n  Quadruped Robots","summary":"  Jumping poses a significant challenge for quadruped robots, despite being\ncrucial for many operational scenarios. While optimisation methods exist for\ncontrolling such motions, they are often time-consuming and demand extensive\nknowledge of robot and terrain parameters, making them less robust in\nreal-world scenarios. Reinforcement learning (RL) is emerging as a viable\nalternative, yet conventional end-to-end approaches lack efficiency in terms of\nsample complexity, requiring extensive training in simulations, and\npredictability of the final motion, which makes it difficult to certify the\nsafety of the final motion. To overcome these limitations, this paper\nintroduces a novel guided reinforcement learning approach that leverages\nphysical intuition for efficient and explainable jumping, by combining B\\'ezier\ncurves with a Uniformly Accelerated Rectilinear Motion (UARM) model. Extensive\nsimulation and experimental results clearly demonstrate the advantages of our\napproach over existing alternatives.\n","authors":["Riccardo Bussola","Michele Focchi","Giulio Turrisi","Claudio Semini","Luigi Palopoli"],"pdf_url":"https://arxiv.org/pdf/2507.16481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.16480v1","updated":"2025-07-22T11:36:08Z","published":"2025-07-22T11:36:08Z","title":"Designing for Difference: How Human Characteristics Shape Perceptions of\n  Collaborative Robots","summary":"  The development of assistive robots for social collaboration raises critical\nquestions about responsible and inclusive design, especially when interacting\nwith individuals from protected groups such as those with disabilities or\nadvanced age. Currently, research is scarce on how participants assess varying\nrobot behaviors in combination with diverse human needs, likely since\nparticipants have limited real-world experience with advanced domestic robots.\nIn the current study, we aim to address this gap while using methods that\nenable participants to assess robot behavior, as well as methods that support\nmeaningful reflection despite limited experience. In an online study, 112\nparticipants (from both experimental and control groups) evaluated 7 videos\nfrom a total of 28 variations of human-robot collaboration types. The\nexperimental group first completed a cognitive-affective mapping (CAM) exercise\non human-robot collaboration before providing their ratings. Although CAM\nreflection did not significantly affect overall ratings, it led to more\npronounced assessments for certain combinations of robot behavior and human\ncondition. Most importantly, the type of human-robot collaboration influences\nthe assessment. Antisocial robot behavior was consistently rated as the lowest,\nwhile collaboration with aged individuals elicited more sensitive evaluations.\nScenarios involving object handovers were viewed more positively than those\nwithout them. These findings suggest that both human characteristics and\ninteraction paradigms influence the perceived acceptability of collaborative\nrobots, underscoring the importance of prosocial design. They also highlight\nthe potential of reflective methods, such as CAM, to elicit nuanced feedback,\nsupporting the development of user-centered and socially responsible robotic\nsystems tailored to diverse populations.\n","authors":["Sabrina Livanec","Laura Londoño","Michael Gorki","Adrian Röfer","Abhinav Valada","Andrea Kiesel"],"pdf_url":"https://arxiv.org/pdf/2507.16480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.16458v1","updated":"2025-07-22T11:00:31Z","published":"2025-07-22T11:00:31Z","title":"Distributed Oscillatory Guidance for Formation Flight of Fixed-Wing\n  Drones","summary":"  The autonomous formation flight of fixed-wing drones is hard when the\ncoordination requires the actuation over their speeds since they are critically\nbounded and aircraft are mostly designed to fly at a nominal airspeed. This\npaper proposes an algorithm to achieve formation flights of fixed-wing drones\nwithout requiring any actuation over their speed. In particular, we guide all\nthe drones to travel over specific paths, e.g., parallel straight lines, and we\nsuperpose an oscillatory behavior onto the guiding vector field that drives the\ndrones to the paths. This oscillation enables control over the average velocity\nalong the path, thereby facilitating inter-drone coordination. Each drone\nadjusts its oscillation amplitude distributively in a closed-loop manner by\ncommunicating with neighboring agents in an undirected and connected graph. A\nnovel consensus algorithm is introduced, leveraging a non-negative, asymmetric\nsaturation function. This unconventional saturation is justified since negative\namplitudes do not make drones travel backward or have a negative velocity along\nthe path. Rigorous theoretical analysis of the algorithm is complemented by\nvalidation through numerical simulations and a real-world formation flight.\n","authors":["Yang Xu","Jesús Bautista","José Hinojosa","Héctor García de Marina"],"pdf_url":"https://arxiv.org/pdf/2507.16458v1.pdf","comment":"Yang Xu and Jes\\'us Bautista contributed equally to this work. In the\n  proceedings of the IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025"},{"id":"http://arxiv.org/abs/2507.06605v2","updated":"2025-07-22T10:36:35Z","published":"2025-07-09T07:24:18Z","title":"Growing Trees with an Agent: Accelerating RRTs with Learned, Multi-Step\n  Episodic Exploration","summary":"  Classical sampling-based motion planners like the RRTs suffer from\ninefficiencies, particularly in cluttered or high-dimensional spaces, due to\ntheir reliance on undirected, random sampling. This paper introduces the\nEpisodic RRT, a novel hybrid planning framework that replaces the primitive of\na random point with a learned, multi-step \"exploratory episode\" generated by a\nDeep Reinforcement Learning agent. By making the DRL agent the engine of\nexploration, ERRT transforms the search process from a diffuse, volumetric\nexpansion into a directed, branch-like growth. This paradigm shift yields key\nadvantages: it counters the curse of dimensionality with focused exploration,\nminimizes expensive collision checks by proactively proposing locally valid\npaths, and improves connectivity by generating inherently connected path\nsegments. We demonstrate through extensive empirical evaluation across 2D, 3D,\nand 6D environments that ERRT and its variants consistently and significantly\noutperform their classical counterparts without any GPU acceleration. In a\nchallenging 6D robotic arm scenario, ERRT achieves a 98% success rate compared\nto 19% for RRT, is up to 107x faster, reduces collision checks by over 99.6%,\nand finds initial paths that are nearly 50% shorter. Furthermore, its\nasymptotically optimal variant, ERRT*, demonstrates vastly superior anytime\nperformance, refining solutions to near-optimality up to 29x faster than\nstandard RRT* in 3D environments. Code:\nhttps://xinyuwuu.github.io/Episodic_RRT/.\n","authors":["Xinyu Wu"],"pdf_url":"https://arxiv.org/pdf/2507.06605v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.16398v1","updated":"2025-07-22T09:48:57Z","published":"2025-07-22T09:48:57Z","title":"AI or Human? Understanding Perceptions of Embodied Robots with LLMs","summary":"  The pursuit of artificial intelligence has long been associated to the the\nchallenge of effectively measuring intelligence. Even if the Turing Test was\nintroduced as a means of assessing a system intelligence, its relevance and\napplication within the field of human-robot interaction remain largely\nunderexplored. This study investigates the perception of intelligence in\nembodied robots by performing a Turing Test within a robotic platform. A total\nof 34 participants were tasked with distinguishing between AI- and\nhuman-operated robots while engaging in two interactive tasks: an information\nretrieval and a package handover. These tasks assessed the robot perception and\nnavigation abilities under both static and dynamic conditions. Results indicate\nthat participants were unable to reliably differentiate between AI- and\nhuman-controlled robots beyond chance levels. Furthermore, analysis of\nparticipant responses reveals key factors influencing the perception of\nartificial versus human intelligence in embodied robotic systems. These\nfindings provide insights into the design of future interactive robots and\ncontribute to the ongoing discourse on intelligence assessment in AI-driven\nsystems.\n","authors":["Lavinia Hriscu","Alberto Sanfeliu","Anais Garrell"],"pdf_url":"https://arxiv.org/pdf/2507.16398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.16382v1","updated":"2025-07-22T09:26:00Z","published":"2025-07-22T09:26:00Z","title":"Application of LLM Guided Reinforcement Learning in Formation Control\n  with Collision Avoidance","summary":"  Multi-Agent Systems (MAS) excel at accomplishing complex objectives through\nthe collaborative efforts of individual agents. Among the methodologies\nemployed in MAS, Multi-Agent Reinforcement Learning (MARL) stands out as one of\nthe most efficacious algorithms. However, when confronted with the complex\nobjective of Formation Control with Collision Avoidance (FCCA): designing an\neffective reward function that facilitates swift convergence of the policy\nnetwork to an optimal solution. In this paper, we introduce a novel framework\nthat aims to overcome this challenge. By giving large language models (LLMs) on\nthe prioritization of tasks and the observable information available to each\nagent, our framework generates reward functions that can be dynamically\nadjusted online based on evaluation outcomes by employing more advanced\nevaluation metrics rather than the rewards themselves. This mechanism enables\nthe MAS to simultaneously achieve formation control and obstacle avoidance in\ndynamic environments with enhanced efficiency, requiring fewer iterations to\nreach superior performance levels. Our empirical studies, conducted in both\nsimulation and real-world settings, validate the practicality and effectiveness\nof our proposed approach.\n","authors":["Chenhao Yao","Zike Yuan","Xiaoxu Liu","Chi Zhu"],"pdf_url":"https://arxiv.org/pdf/2507.16382v1.pdf","comment":"Accepted by IROS 2025"},{"id":"http://arxiv.org/abs/2507.16369v1","updated":"2025-07-22T09:08:28Z","published":"2025-07-22T09:08:28Z","title":"Humanoid Robot Whole-body Geometric Calibration with Embedded Sensors\n  and a Single Plane","summary":"  Whole-body geometric calibration of humanoid robots using classical robot\ncalibration methods is a timeconsuming and experimentally burdensome task.\nHowever, despite its significance for accurate control and simulation, it is\noften overlooked in the humanoid robotics community. To address this issue, we\npropose a novel practical method that utilizes a single plane, embedded force\nsensors, and an admittance controller to calibrate the whole-body kinematics of\nhumanoids without requiring manual intervention. Given the complexity of\nhumanoid robots, it is crucial to generate and determine a minimal set of\noptimal calibration postures. To do so, we propose a new algorithm called IROC\n(Information Ranking algorithm for selecting Optimal Calibration postures).\nIROC requires a pool of feasible candidate postures to build a normalized\nweighted information matrix for each posture. Then, contrary to other\nalgorithms from the literature, IROC will determine the minimal number of\noptimal postures that are to be played onto a robot for its calibration. Both\nIROC and the single-plane calibration method were experimentally validated on a\nTALOS humanoid robot. The total whole-body kinematics chain was calibrated\nusing solely 31 optimal postures with 3-point contacts on a table by the robot\ngripper. In a cross-validation experiment, the average root-mean-square (RMS)\nerror was reduced by a factor of 2.3 compared to the manufacturer's model.\n","authors":["Thanh D V Nguyen","Vincent Bonnet","Pierre Fernbach","David Daney","Florent Lamiraux"],"pdf_url":"https://arxiv.org/pdf/2507.16369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01092v2","updated":"2025-07-22T08:51:33Z","published":"2025-03-03T01:34:56Z","title":"One-Shot Affordance Grounding of Deformable Objects in Egocentric\n  Organizing Scenes","summary":"  Deformable object manipulation in robotics presents significant challenges\ndue to uncertainties in component properties, diverse configurations, visual\ninterference, and ambiguous prompts. These factors complicate both perception\nand control tasks. To address these challenges, we propose a novel method for\nOne-Shot Affordance Grounding of Deformable Objects (OS-AGDO) in egocentric\norganizing scenes, enabling robots to recognize previously unseen deformable\nobjects with varying colors and shapes using minimal samples. Specifically, we\nfirst introduce the Deformable Object Semantic Enhancement Module (DefoSEM),\nwhich enhances hierarchical understanding of the internal structure and\nimproves the ability to accurately identify local features, even under\nconditions of weak component information. Next, we propose the ORB-Enhanced\nKeypoint Fusion Module (OEKFM), which optimizes feature extraction of key\ncomponents by leveraging geometric constraints and improves adaptability to\ndiversity and visual interference. Additionally, we propose an\ninstance-conditional prompt based on image data and task context, which\neffectively mitigates the issue of region ambiguity caused by prompt words. To\nvalidate these methods, we construct a diverse real-world dataset, AGDDO15,\nwhich includes 15 common types of deformable objects and their associated\norganizational actions. Experimental results demonstrate that our approach\nsignificantly outperforms state-of-the-art methods, achieving improvements of\n6.2%, 3.2%, and 2.9% in KLD, SIM, and NSS metrics, respectively, while\nexhibiting high generalization performance. Source code and benchmark dataset\nare made publicly available at https://github.com/Dikay1/OS-AGDO.\n","authors":["Wanjun Jia","Fan Yang","Mengfei Duan","Xianchi Chen","Yinxi Wang","Yiming Jiang","Wenrui Chen","Kailun Yang","Zhiyong Li"],"pdf_url":"https://arxiv.org/pdf/2503.01092v2.pdf","comment":"Accepted to IROS 2025. Source code and benchmark dataset will be\n  publicly available at https://github.com/Dikay1/OS-AGDO"},{"id":"http://arxiv.org/abs/2507.16874v1","updated":"2025-07-22T08:32:55Z","published":"2025-07-22T08:32:55Z","title":"Budget Allocation Policies for Real-Time Multi-Agent Path Finding","summary":"  Multi-Agent Pathfinding (MAPF) is the problem of finding paths for a set of\nagents such that each agent reaches its desired destination while avoiding\ncollisions with the other agents. Many MAPF solvers are designed to run\noffline, that is, first generate paths for all agents and then execute them.\nReal-Time MAPF (RT-MAPF) embodies a realistic MAPF setup in which one cannot\nwait until a complete path for each agent has been found before they start to\nmove. Instead, planning and execution are interleaved, where the agents must\ncommit to a fixed number of steps in a constant amount of computation time,\nreferred to as the planning budget. Existing solutions to RT-MAPF iteratively\ncall windowed versions of MAPF algorithms in every planning period, without\nexplicitly considering the size of the planning budget. We address this gap and\nexplore different policies for allocating the planning budget in windowed\nversions of standard MAPF algorithms, namely Prioritized Planning (PrP) and\nMAPF-LNS2. Our exploration shows that the baseline approach in which all agents\ndraw from a shared planning budget pool is ineffective in over-constrained\nsituations. Instead, policies that distribute the planning budget over the\nagents are able to solve more problems with a smaller makespan.\n","authors":["Raz Beck","Roni Stern"],"pdf_url":"https://arxiv.org/pdf/2507.16874v1.pdf","comment":"8 pages, 2 figures, 3 tables"},{"id":"http://arxiv.org/abs/2507.16335v1","updated":"2025-07-22T08:19:09Z","published":"2025-07-22T08:19:09Z","title":"Topology Optimization of Leg Structures for Construction Robots Based on\n  Variable Density Method","summary":"  In complex terrain construction environments, there are high demands for\nrobots to achieve both high payload capacity and mobility flexibility. As the\nkey load-bearing component, the optimization of robotic leg structures is of\nparticular importance. Therefore, this study focuses on the optimization of leg\nstructures for construction robots, proposing a topology optimization strategy\nbased on the SIMP (Solid Isotropic Microstructures with Penalization) variable\ndensity method along with a structural re-design approach. The design\nperformance is comprehensively validated through finite element analysis using\nANSYS. First, static and modal analyses are conducted to evaluate the\nrationality of the initial design. Then, topology optimization using the\nSIMP-based variable density method is applied to the femur section, which\naccounts for the largest proportion of the leg's weight. Based on iterative\ncalculations, the femur undergoes secondary structural reconstruction. After\noptimization, the mass of the femur is reduced by 19.45\\%, and the overall leg\nmass decreases by 7.92\\%, achieving the goal of lightweight design. Finally,\nstatic and modal analyses are conducted on the reconstructed leg. The results\ndemonstrate that the optimized leg still meets structural performance\nrequirements, validating the feasibility of lightweight design. This research\nprovides robust theoretical and technical support for lightweight construction\nrobot design and lays a foundation for their efficient operation in complex\nconstruction environments.\n","authors":["Xiao Liu","Xianlong Yang","Weijun Wang","Wei Feng"],"pdf_url":"https://arxiv.org/pdf/2507.16335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.16328v1","updated":"2025-07-22T08:10:12Z","published":"2025-07-22T08:10:12Z","title":"Design and Dimensional Optimization of Legged Structures for\n  Construction Robots","summary":"  Faced with complex and unstructured construction environments, wheeled and\ntracked robots exhibit significant limitations in terrain adaptability and\nflexibility, making it difficult to meet the requirements of autonomous\noperation. Inspired by ants in nature, this paper proposes a leg configuration\ndesign and optimization method tailored for construction scenarios, aiming to\nenhance the autonomous mobility of construction robots. This paper analyzes the\nfull operational motion performance of the leg during both swing and stance\nphases. First, based on kinematic modeling and multi-dimensional workspace\nanalysis, the concept of an \"improved workspace\" is introduced, and graphical\nmethods are used to optimize the leg dimensions during the swing phase.\nFurthermore, a new concept of \"average manipulability\" is introduced based on\nthe velocity Jacobian matrix, and numerical solutions are applied to obtain the\nleg segment ratio that maximizes manipulability. To overcome the difficulties\nassociated with traditional analytical methods, virtual prototype simulations\nare conducted in ADAMS to explore the relationship between the robot body's\noptimal flexibility and leg segment proportions. In summary, the leg segment\nproportions with the best comprehensive motion performance are obtained. This\nstudy presents the first multi-dimensional quantitative evaluation framework\nfor leg motion performance tailored for construction environments, providing a\nstructural design foundation for legged construction robots to achieve\nautonomous mobility in complex terrains.\n","authors":["Xiao Liu","Xianlong Yang","Weijun Wang","Wei Feng"],"pdf_url":"https://arxiv.org/pdf/2507.16328v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.16306v1","updated":"2025-07-22T07:44:08Z","published":"2025-07-22T07:44:08Z","title":"COMPASS: Cooperative Multi-Agent Persistent Monitoring using\n  Spatio-Temporal Attention Network","summary":"  Persistent monitoring of dynamic targets is essential in real-world\napplications such as disaster response, environmental sensing, and wildlife\nconservation, where mobile agents must continuously gather information under\nuncertainty. We propose COMPASS, a multi-agent reinforcement learning (MARL)\nframework that enables decentralized agents to persistently monitor multiple\nmoving targets efficiently. We model the environment as a graph, where nodes\nrepresent spatial locations and edges capture topological proximity, allowing\nagents to reason over structured layouts and revisit informative regions as\nneeded. Each agent independently selects actions based on a shared\nspatio-temporal attention network that we design to integrate historical\nobservations and spatial context. We model target dynamics using Gaussian\nProcesses (GPs), which support principled belief updates and enable\nuncertainty-aware planning. We train COMPASS using centralized value estimation\nand decentralized policy execution under an adaptive reward setting. Our\nextensive experiments demonstrate that COMPASS consistently outperforms strong\nbaselines in uncertainty reduction, target coverage, and coordination\nefficiency across dynamic multi-target scenarios.\n","authors":["Xingjian Zhang","Yizhuo Wang","Guillaume Sartoretti"],"pdf_url":"https://arxiv.org/pdf/2507.16306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.16305v1","updated":"2025-07-22T07:43:54Z","published":"2025-07-22T07:43:54Z","title":"Trajectory Planning of a Curtain Wall Installation Robot Based on\n  Biomimetic Mechanisms","summary":"  As the robotics market rapidly evolves, energy consumption has become a\ncritical issue, particularly restricting the application of construction\nrobots. To tackle this challenge, our study innovatively draws inspiration from\nthe mechanics of human upper limb movements during weight lifting, proposing a\nbio-inspired trajectory planning framework that incorporates human energy\nconversion principles. By collecting motion trajectories and electromyography\n(EMG) signals during dumbbell curls, we construct an anthropomorphic trajectory\nplanning that integrates human force exertion patterns and energy consumption\npatterns. Utilizing the Particle Swarm Optimization (PSO) algorithm, we achieve\ndynamic load distribution for robotic arm trajectory planning based on\nhuman-like movement features. In practical application, these bio-inspired\nmovement characteristics are applied to curtain wall installation tasks,\nvalidating the correctness and superiority of our trajectory planning method.\nSimulation results demonstrate a 48.4% reduction in energy consumption through\nintelligent conversion between kinetic and potential energy. This approach\nprovides new insights and theoretical support for optimizing energy use in\ncurtain wall installation robots during actual handling tasks.\n","authors":["Xiao Liu","Weijun Wang","Tianlun Huang","Zhiyong Wang","Wei Feng"],"pdf_url":"https://arxiv.org/pdf/2507.16305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07477v3","updated":"2025-07-22T07:30:12Z","published":"2024-12-10T12:50:25Z","title":"Progressive-Resolution Policy Distillation: Leveraging Coarse-Resolution\n  Simulations for Time-Efficient Fine-Resolution Policy Learning","summary":"  In earthwork and construction, excavators often encounter large rocks mixed\nwith various soil conditions, requiring skilled operators. This paper presents\na framework for achieving autonomous excavation using reinforcement learning\n(RL) through a rock excavation simulator. In the simulation, resolution can be\ndefined by the particle size/number in the whole soil space. Fine-resolution\nsimulations closely mimic real-world behavior but demand significant\ncalculation time and challenging sample collection, while coarse-resolution\nsimulations enable faster sample collection but deviate from real-world\nbehavior. To combine the advantages of both resolutions, we explore using\npolicies developed in coarse-resolution simulations for pre-training in\nfine-resolution simulations. To this end, we propose a novel policy learning\nframework called Progressive-Resolution Policy Distillation (PRPD), which\nprogressively transfers policies through some middle-resolution simulations\nwith conservative policy transfer to avoid domain gaps that could lead to\npolicy transfer failure. Validation in a rock excavation simulator and nine\nreal-world rock environments demonstrated that PRPD reduced sampling time to\nless than 1/7 while maintaining task success rates comparable to those achieved\nthrough policy learning in a fine-resolution simulation.\n","authors":["Yuki Kadokawa","Hirotaka Tahara","Takamitsu Matsubara"],"pdf_url":"https://arxiv.org/pdf/2412.07477v3.pdf","comment":"accepted for IEEE Transactions on Automation Science and Engineering\n  (T-ASE)"},{"id":"http://arxiv.org/abs/2409.01559v2","updated":"2025-07-22T07:28:38Z","published":"2024-09-03T02:55:21Z","title":"PR2: A Physics- and Photo-realistic Humanoid Testbed with Pilot Study in\n  Competition","summary":"  This paper presents the development of a Physics-realistic and\nPhoto-realistic humanoid robot testbed, PR2, to facilitate collaborative\nresearch between Embodied Artificial Intelligence (Embodied AI) and robotics.\nPR2 offers high-quality scene rendering and robot dynamic simulation, enabling\n(i) the creation of diverse scenes using various digital assets, (ii) the\nintegration of advanced perception or foundation models, and (iii) the\nimplementation of planning and control algorithms for dynamic humanoid robot\nbehaviors based on environmental feedback. The beta version of PR2 has been\ndeployed for the simulation track of a nationwide full-size humanoid robot\ncompetition for college students, attracting 137 teams and over 400\nparticipants within four months. This competition covered traditional tasks in\nbipedal walking, as well as novel challenges in loco-manipulation and\nlanguage-instruction-based object search, marking a first for public college\nrobotics competitions. A retrospective analysis of the competition suggests\nthat future events should emphasize the integration of locomotion with\nmanipulation and perception. By making the PR2 testbed publicly available at\nhttps://github.com/pr2-humanoid/PR2-Platform, we aim to further advance\neducation and training in humanoid robotics. Video demonstration:\nhttps://pr2-humanoid.github.io/\n","authors":["Hangxin Liu","Qi Xie","Zeyu Zhang","Tao Yuan","Song Wang","Zaijin Wang","Xiaokun Leng","Lining Sun","Jingwen Zhang","Zhicheng He","Yao Su"],"pdf_url":"https://arxiv.org/pdf/2409.01559v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.04451v2","updated":"2025-07-22T06:53:25Z","published":"2025-04-06T11:42:34Z","title":"eKalibr-Stereo: Continuous-Time Spatiotemporal Calibration for\n  Event-Based Stereo Visual Systems","summary":"  The bioinspired event camera, distinguished by its exceptional temporal\nresolution, high dynamic range, and low power consumption, has been extensively\nstudied in recent years for motion estimation, robotic perception, and object\ndetection. In ego-motion estimation, the stereo event camera setup is commonly\nadopted due to its direct scale perception and depth recovery. For optimal\nstereo visual fusion, accurate spatiotemporal (extrinsic and temporal)\ncalibration is required. Considering that few stereo visual calibrators\norienting to event cameras exist, based on our previous work eKalibr (an event\ncamera intrinsic calibrator), we propose eKalibr-Stereo for accurate\nspatiotemporal calibration of event-based stereo visual systems. To improve the\ncontinuity of grid pattern tracking, building upon the grid pattern recognition\nmethod in eKalibr, an additional motion prior-based tracking module is designed\nin eKalibr-Stereo to track incomplete grid patterns. Based on tracked grid\npatterns, a two-step initialization procedure is performed to recover initial\nguesses of piece-wise B-splines and spatiotemporal parameters, followed by a\ncontinuous-time batch bundle adjustment to refine the initialized states to\noptimal ones. The results of extensive real-world experiments show that\neKalibr-Stereo can achieve accurate event-based stereo spatiotemporal\ncalibration. The implementation of eKalibr-Stereo is open-sourced at\n(https://github.com/Unsigned-Long/eKalibr) to benefit the research community.\n","authors":["Shuolong Chen","Xingxing Li","Liu Yuan"],"pdf_url":"https://arxiv.org/pdf/2504.04451v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.16269v1","updated":"2025-07-22T06:37:10Z","published":"2025-07-22T06:37:10Z","title":"Improved Wake-Up Time For Euclidean Freeze-Tag Problem","summary":"  The Freeze-Tag Problem (FTP) involves activating a set of initially asleep\nrobots as quickly as possible, starting from a single awake robot. Once\nactivated, a robot can assist in waking up other robots. Each active robot\nmoves at unit speed. The objective is to minimize the makespan, i.e., the time\nrequired to activate the last robot. A key performance measure is the wake-up\nratio, defined as the maximum time needed to activate any number of robots in\nany primary positions. This work focuses on the geometric (Euclidean) version\nof FTP in $\\mathbb{R}^d$ under the $\\ell_p$ norm, where the initial distance\nbetween each asleep robot and the single active robot is at most 1. For\n$(\\mathbb{R}^2, \\ell_2)$, we improve the previous upper bound of 4.62 ([7],\nCCCG 2024) to 4.31. Note that it is known that 3.82 is a lower bound for the\nwake-up ratio. In $\\mathbb{R}^3$, we propose a new strategy that achieves a\nwake-up ratio of 12 for $(\\mathbb{R}^3, \\ell_1)$ and 12.76 for $(\\mathbb{R}^3,\n\\ell_2)$, improving upon the previous bounds of 13 and $13\\sqrt{3}$,\nrespectively, reported in [2].\n","authors":["Sharareh Alipour","Arash Ahadi","Kajal Baghestani"],"pdf_url":"https://arxiv.org/pdf/2507.16269v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14247v3","updated":"2025-07-22T06:30:03Z","published":"2025-03-18T13:35:49Z","title":"GeoFlow-SLAM: A Robust Tightly-Coupled RGBD-Inertial and Legged Odometry\n  Fusion SLAM for Dynamic Legged Robotics","summary":"  This paper presents GeoFlow-SLAM, a robust and effective Tightly-Coupled\nRGBD-inertial SLAM for legged robotics undergoing aggressive and high-frequency\nmotions.By integrating geometric consistency, legged odometry constraints, and\ndual-stream optical flow (GeoFlow), our method addresses three critical\nchallenges:feature matching and pose initialization failures during fast\nlocomotion and visual feature scarcity in texture-less scenes.Specifically, in\nrapid motion scenarios, feature matching is notably enhanced by leveraging\ndual-stream optical flow, which combines prior map points and poses.\nAdditionally, we propose a robust pose initialization method for fast\nlocomotion and IMU error in legged robots, integrating IMU/Legged odometry,\ninter-frame Perspective-n-Point (PnP), and Generalized Iterative Closest Point\n(GICP). Furthermore, a novel optimization framework that tightly couples\ndepth-to-map and GICP geometric constraints is first introduced to improve the\nrobustness and accuracy in long-duration, visually texture-less environments.\nThe proposed algorithms achieve state-of-the-art (SOTA) on collected legged\nrobots and open-source datasets. To further promote research and development,\nthe open-source datasets and code will be made publicly available at\nhttps://github.com/HorizonRobotics/GeoFlowSlam\n","authors":["Tingyang Xiao","Xiaolin Zhou","Liu Liu","Wei Sui","Wei Feng","Jiaxiong Qiu","Xinjie Wang","Zhizhong Su"],"pdf_url":"https://arxiv.org/pdf/2503.14247v3.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2507.16259v1","updated":"2025-07-22T06:17:11Z","published":"2025-07-22T06:17:11Z","title":"Physics-aware Truck and Drone Delivery Planning Using Optimization &\n  Machine Learning","summary":"  Combining an energy-efficient drone with a high-capacity truck for last-mile\npackage delivery can benefit operators and customers by reducing delivery times\nand environmental impact. However, directly integrating drone flight dynamics\ninto the combinatorially hard truck route planning problem is challenging.\nSimplified models that ignore drone flight physics can lead to suboptimal\ndelivery plans. We propose an integrated formulation for the joint problem of\ntruck route and drone trajectory planning and a new end-to-end solution\napproach that combines optimization and machine learning to generate\nhigh-quality solutions in practical online runtimes. Our solution method trains\nneural network predictors based on offline solutions to the drone trajectory\noptimization problem instances to approximate drone flight times, and uses\nthese approximations to optimize the overall truck-and-drone delivery plan by\naugmenting an existing order-first-split-second heuristic. Our method\nexplicitly incorporates key kinematics and energy equations in drone trajectory\noptimization, and thereby outperforms state-of-the-art benchmarks that ignore\ndrone flight physics. Extensive experimentation using synthetic datasets and\nreal-world case studies shows that the integration of drone trajectories into\npackage delivery planning substantially improves system performance in terms of\ntour duration and drone energy consumption. Our modeling and computational\nframework can help delivery planners achieve annual savings worth millions of\ndollars while also benefiting the environment.\n","authors":["Yineng Sun","Armin Fügenschuh","Vikrant Vaze"],"pdf_url":"https://arxiv.org/pdf/2507.16259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.07989v2","updated":"2025-07-22T05:35:16Z","published":"2025-03-11T02:47:21Z","title":"Bio-Skin: A Cost-Effective Thermostatic Tactile Sensor with Multi-Modal\n  Force and Temperature Detection","summary":"  Tactile sensors can significantly enhance the perception of humanoid robotics\nsystems by providing contact information that facilitates human-like\ninteractions. However, existing commercial tactile sensors focus on improving\nthe resolution and sensitivity of single-modal detection with high-cost\ncomponents and densely integrated design, incurring complex manufacturing\nprocesses and unaffordable prices. In this work, we present Bio-Skin, a\ncost-effective multi-modal tactile sensor that utilizes single-axis Hall-effect\nsensors for planar normal force measurement and bar-shape piezo resistors for\n2D shear force measurement. A thermistor coupling with a heating wire is\nintegrated into a silicone body to achieve temperature sensation and\nthermostatic function analogous to human skin. We also present a\ncross-reference framework to validate the two modalities of the force sensing\nsignal, improving the sensing fidelity in a complex electromagnetic\nenvironment. Bio-Skin has a multi-layer design, and each layer is manufactured\nsequentially and subsequently integrated, thereby offering a fast production\npathway. After calibration, Bio-Skin demonstrates performance metrics-including\nsignal-to-range ratio, sampling rate, and measurement range-comparable to\ncurrent commercial products, with one-tenth of the cost. The sensor's\nreal-world performance is evaluated using an Allegro hand in object grasping\ntasks, while its temperature regulation functionality was assessed in a\nmaterial detection task.\n","authors":["Haoran Guo","Haoyang Wang","Zhengxiong Li","Lingfeng Tao"],"pdf_url":"https://arxiv.org/pdf/2503.07989v2.pdf","comment":"This work has been accepted by IROS2025"},{"id":"http://arxiv.org/abs/2507.16233v1","updated":"2025-07-22T05:07:11Z","published":"2025-07-22T05:07:11Z","title":"GFM-Planner: Perception-Aware Trajectory Planning with Geometric Feature\n  Metric","summary":"  Like humans who rely on landmarks for orientation, autonomous robots depend\non feature-rich environments for accurate localization. In this paper, we\npropose the GFM-Planner, a perception-aware trajectory planning framework based\non the geometric feature metric, which enhances LiDAR localization accuracy by\nguiding the robot to avoid degraded areas. First, we derive the Geometric\nFeature Metric (GFM) from the fundamental LiDAR localization problem. Next, we\ndesign a 2D grid-based Metric Encoding Map (MEM) to efficiently store GFM\nvalues across the environment. A constant-time decoding algorithm is further\nproposed to retrieve GFM values for arbitrary poses from the MEM. Finally, we\ndevelop a perception-aware trajectory planning algorithm that improves LiDAR\nlocalization capabilities by guiding the robot in selecting trajectories\nthrough feature-rich areas. Both simulation and real-world experiments\ndemonstrate that our approach enables the robot to actively select trajectories\nthat significantly enhance LiDAR localization accuracy.\n","authors":["Yue Lin","Xiaoxuan Zhang","Yang Liu","Dong Wang","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2507.16233v1.pdf","comment":"Accepted by IROS 2025"},{"id":"http://arxiv.org/abs/2503.10706v2","updated":"2025-07-22T03:13:07Z","published":"2025-03-12T16:35:51Z","title":"SciFi-Benchmark: Leveraging Science Fiction To Improve Robot Behavior","summary":"  Given the recent rate of progress in artificial intelligence (AI) and\nrobotics, a tantalizing question is emerging: would robots controlled by\nemerging AI systems be strongly aligned with human values? In this work, we\npropose a scalable way to probe this question by generating a benchmark\nspanning the key moments in 824 major pieces of science fiction literature\n(movies, tv, novels and scientific books) where an agent (AI or robot) made\ncritical decisions (good or bad). We use a state-of-the-art LLM's recollection\nof each key moment to generate questions in similar situations, the decisions\nmade by the agent, and alternative decisions it could have made (good or bad).\nWe then measure an approximation of how well models align with human values on\na set of human-voted answers. We also generate rules that can be automatically\nimproved via an amendment process in order to generate the first Sci-Fi\ninspired constitutions for promoting ethical behavior in AIs and robots in the\nreal world. Our first finding is that modern LLMs paired with constitutions\nturn out to be well-aligned with human values (95.8%), contrary to unsettling\ndecisions typically made in Sci-Fi (only 21.2% alignment). Secondly, we find\nthat generated constitutions substantially increase alignment compared to the\nbase model (79.4% to 95.8%), and show resilience to an adversarial prompt\nsetting (23.3% to 92.3%). Additionally, we find that those constitutions are\namong the top performers on the ASIMOV Benchmark which is derived from\nreal-world images and hospital injury reports. Sci-Fi-inspired constitutions\nare thus highly aligned and applicable in real-world situations. We release\nSciFi-Benchmark: a large-scale dataset to advance robot ethics and safety\nresearch. It comprises 9,056 questions and 53,384 answers generated through a\nnovel LLM-introspection process, in addition to a smaller human-labeled\nevaluation set.\n","authors":["Pierre Sermanet","Anirudha Majumdar","Vikas Sindhwani"],"pdf_url":"https://arxiv.org/pdf/2503.10706v2.pdf","comment":"Minor improvements over previous version"},{"id":"http://arxiv.org/abs/2409.06521v3","updated":"2025-07-22T03:00:33Z","published":"2024-09-10T13:56:08Z","title":"Asymptotically Optimal Lazy Lifelong Sampling-based Algorithm for\n  Efficient Motion Planning in Dynamic Environments","summary":"  The paper introduces an asymptotically optimal lifelong sampling-based path\nplanning algorithm that combines the merits of lifelong planning algorithms and\nlazy search algorithms for rapid replanning in dynamic environments where edge\nevaluation is expensive. By evaluating only sub-path candidates for the optimal\nsolution, the algorithm saves considerable evaluation time and thereby reduces\nthe overall planning cost. It employs a novel informed rewiring cascade to\nefficiently repair the search tree when the underlying search graph changes.\nTheoretical analysis indicates that the proposed algorithm converges to the\noptimal solution as long as sufficient planning time is given. Planning results\non robotic systems with $\\mathbb{SE}(3)$ and $\\mathbb{R}^7$ state spaces in\nchallenging environments highlight the superior performance of the proposed\nalgorithm over various state-of-the-art sampling-based planners in both static\nand dynamic motion planning tasks. The experiment of planning for a Turtlebot 4\noperating in a dynamic environment with several moving pedestrians further\nverifies the feasibility and advantages of the proposed algorithm.\n","authors":["Lu Huang","Jingwen Yu","Jiankun Wang","Xingjian Jing"],"pdf_url":"https://arxiv.org/pdf/2409.06521v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14208v2","updated":"2025-07-22T02:57:55Z","published":"2024-12-17T07:48:41Z","title":"Beacon: A Naturalistic Driving Dataset During Blackouts for Benchmarking\n  Traffic Reconstruction and Control","summary":"  Extreme weather and infrastructure vulnerabilities pose significant\nchallenges to urban mobility, particularly at intersections where signals\nbecome inoperative. To address this growing concern, we introduce Beacon, a\nnaturalistic driving dataset capturing traffic dynamics during blackouts at two\nmajor intersections in Memphis, TN, USA. The dataset provides detailed traffic\nmovements, including timesteps, origin, and destination lanes for each vehicle\nover four hours of peak periods. We analyze traffic demand, vehicle\ntrajectories, and density across different scenarios, demonstrating\nhigh-fidelity reconstruction under unsignalized, signalized, and mixed traffic\nconditions. We find that integrating robot vehicles (RVs) into traffic flow can\nsubstantially reduce intersection delays, with wait time improvements of up to\n82.6%. However, this enhanced traffic efficiency comes with varying\nenvironmental impacts, as decreased vehicle idling may lead to higher overall\nCO2 emissions. To the best of our knowledge, Beacon is the first publicly\navailable traffic dataset for naturalistic driving behaviors during blackouts\nat intersections.\n","authors":["Supriya Sarker","Iftekharul Islam","Bibek Poudel","Weizi Li"],"pdf_url":"https://arxiv.org/pdf/2412.14208v2.pdf","comment":"IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS), 2025"},{"id":"http://arxiv.org/abs/2505.01966v2","updated":"2025-07-22T02:50:39Z","published":"2025-05-04T02:35:18Z","title":"A Goal-Oriented Reinforcement Learning-Based Path Planning Algorithm for\n  Modular Self-Reconfigurable Satellites","summary":"  Modular self-reconfigurable satellites refer to satellite clusters composed\nof individual modular units capable of altering their configurations. The\nconfiguration changes enable the execution of diverse tasks and mission\nobjectives. Existing path planning algorithms for reconfiguration often suffer\nfrom high computational complexity, poor generalization capability, and limited\nsupport for diverse target configurations. To address these challenges, this\npaper proposes a goal-oriented reinforcement learning-based path planning\nalgorithm. This algorithm is the first to address the challenge that previous\nreinforcement learning methods failed to overcome, namely handling multiple\ntarget configurations. Moreover, techniques such as Hindsight Experience Replay\nand Invalid Action Masking are incorporated to overcome the significant\nobstacles posed by sparse rewards and invalid actions. Based on these designs,\nour model achieves a 95% and 73% success rate in reaching arbitrary target\nconfigurations in a modular satellite cluster composed of four and six units,\nrespectively.\n","authors":["Bofei Liu","Dong Ye","Zunhao Yao","Zhaowei Sun"],"pdf_url":"https://arxiv.org/pdf/2505.01966v2.pdf","comment":"6 pages, 7 figures"},{"id":"http://arxiv.org/abs/2507.16175v1","updated":"2025-07-22T02:43:25Z","published":"2025-07-22T02:43:25Z","title":"Scanning Bot: Efficient Scan Planning using Panoramic Cameras","summary":"  Panoramic RGB-D cameras are known for their ability to produce high quality\n3D scene reconstructions. However, operating these cameras involves manually\nselecting viewpoints and physically transporting the camera, making the\ngeneration of a 3D model time consuming and tedious. Additionally, the process\ncan be challenging for novice users due to spatial constraints, such as\nensuring sufficient feature overlap between viewpoint frames. To address these\nchallenges, we propose a fully autonomous scan planning that generates an\nefficient tour plan for environment scanning, ensuring collision-free\nnavigation and adequate overlap between viewpoints within the plan. Extensive\nexperiments conducted in both synthetic and real-world environments validate\nthe performance of our planner against state-of-the-art view planners. In\nparticular, our method achieved an average scan coverage of 99 percent in the\nreal-world experiment, with our approach being up to 3 times faster than\nstate-of-the-art planners in total scan time.\n","authors":["Euijeong Lee","Kyung Min Han","Young J. Kim"],"pdf_url":"https://arxiv.org/pdf/2507.16175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03707v2","updated":"2025-07-22T02:32:33Z","published":"2025-03-05T17:58:16Z","title":"Curating Demonstrations using Online Experience","summary":"  Many robot demonstration datasets contain heterogeneous demonstrations of\nvarying quality. This heterogeneity may benefit policy pre-training, but can\nhinder robot performance when used with a final imitation learning objective.\nIn particular, some strategies in the data may be less reliable than others or\nmay be underrepresented in the data, leading to poor performance when such\nstrategies are sampled at test time. Moreover, such unreliable or\nunderrepresented strategies can be difficult even for people to discern, and\nsifting through demonstration datasets is time-consuming and costly. On the\nother hand, policy performance when trained on such demonstrations can reflect\nthe reliability of different strategies. We thus propose for robots to\nself-curate based on online robot experience (Demo-SCORE). More specifically,\nwe train and cross-validate a classifier to discern successful policy roll-outs\nfrom unsuccessful ones and use the classifier to filter heterogeneous\ndemonstration datasets. Our experiments in simulation and the real world show\nthat Demo-SCORE can effectively identify suboptimal demonstrations without\nmanual curation. Notably, Demo-SCORE achieves over 15-35% higher absolute\nsuccess rate in the resulting policy compared to the base policy trained with\nall original demonstrations.\n","authors":["Annie S. Chen","Alec M. Lessing","Yuejiang Liu","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2503.03707v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01059v3","updated":"2025-07-22T02:12:32Z","published":"2023-11-02T08:22:28Z","title":"Adapt On-the-Go: Behavior Modulation for Single-Life Robot Deployment","summary":"  To succeed in the real world, robots must cope with situations that differ\nfrom those seen during training. We study the problem of adapting on-the-fly to\nsuch novel scenarios during deployment, by drawing upon a diverse repertoire of\npreviouslylearned behaviors. Our approach, RObust Autonomous Modulation (ROAM),\nintroduces a mechanism based on the perceived value of pre-trained behaviors to\nselect and adapt pre-trained behaviors to the situation at hand. Crucially,\nthis adaptation process all happens within a single episode at test time,\nwithout any human supervision. We demonstrate that ROAM enables a robot to\nadapt rapidly to changes in dynamics both in simulation and on a real Go1\nquadruped, even successfully moving forward with roller skates on its feet. Our\napproach adapts over 2x as efficiently compared to existing methods when facing\na variety of out-of-distribution situations during deployment by effectively\nchoosing and adapting relevant behaviors on-the-fly.\n","authors":["Annie S. Chen","Govind Chada","Laura Smith","Archit Sharma","Zipeng Fu","Sergey Levine","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2311.01059v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11551v2","updated":"2025-07-22T01:45:07Z","published":"2024-07-16T09:57:50Z","title":"Human-Machine Shared Control Approach for the Takeover of Cooperative\n  Adaptive Cruise Control","summary":"  Cooperative Adaptive Cruise Control (CACC) often requires human takeover for\ntasks such as exiting a freeway. Direct human takeover can pose significant\nrisks, especially given the close-following strategy employed by CACC, which\nmight cause drivers to feel unsafe and execute hard braking, potentially\nleading to collisions. This research aims to develop a CACC takeover controller\nthat ensures a smooth transition from automated to human control. The proposed\nCACC takeover maneuver employs an indirect human-machine shared control\napproach, modeled as a Stackelberg competition where the machine acts as the\nleader and the human as the follower. The machine guides the human to respond\nin a manner that aligns with the machine's expectations, aiding in maintaining\nfollowing stability. Additionally, the human reaction function is integrated\ninto the machine's predictive control system, moving beyond a simple\n\"prediction-planning\" pipeline to enhance planning optimality. The controller\nhas been verified to i) enable a smooth takeover maneuver of CACC; ii) ensure\nstring stability in the condition that the platoon has less than 6 CAVs and\nhuman control authority is less than 40%; iii) enhance both perceived and\nactual safety through machine interventions; and iv) reduce the impact on\nupstream traffic by up to 60%.\n","authors":["Haoran Wang","Zhexi Lian","Zhenning Li","Jiawei Wang","Arno Eichberger","Jia Hu","Yongyu Chen","Yongji Gao"],"pdf_url":"https://arxiv.org/pdf/2407.11551v2.pdf","comment":"IEEE Transactions on Intelligent Transportation Systems (2025)"},{"id":"http://arxiv.org/abs/2507.16139v1","updated":"2025-07-22T01:13:45Z","published":"2025-07-22T01:13:45Z","title":"Equivariant Goal Conditioned Contrastive Reinforcement Learning","summary":"  Contrastive Reinforcement Learning (CRL) provides a promising framework for\nextracting useful structured representations from unlabeled interactions. By\npulling together state-action pairs and their corresponding future states,\nwhile pushing apart negative pairs, CRL enables learning nontrivial policies\nwithout manually designed rewards. In this work, we propose Equivariant CRL\n(ECRL), which further structures the latent space using equivariant\nconstraints. By leveraging inherent symmetries in goal-conditioned manipulation\ntasks, our method improves both sample efficiency and spatial generalization.\nSpecifically, we formally define Goal-Conditioned Group-Invariant MDPs to\ncharacterize rotation-symmetric robotic manipulation tasks, and build on this\nby introducing a novel rotation-invariant critic representation paired with a\nrotation-equivariant actor for Contrastive RL. Our approach consistently\noutperforms strong baselines across a range of simulated tasks in both\nstate-based and image-based settings. Finally, we extend our method to the\noffline RL setting, demonstrating its effectiveness across multiple tasks.\n","authors":["Arsh Tangri","Nichols Crawford Taylor","Haojie Huang","Robert Platt"],"pdf_url":"https://arxiv.org/pdf/2507.16139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.16124v1","updated":"2025-07-22T00:36:59Z","published":"2025-07-22T00:36:59Z","title":"Benchmarking LLM Privacy Recognition for Social Robot Decision Making","summary":"  Social robots are embodied agents that interact with people while following\nhuman communication norms. These robots interact using verbal and non-verbal\ncues, and share the physical environments of people. While social robots have\npreviously utilized rule-based systems or probabilistic models for user\ninteraction, the rapid evolution of large language models (LLMs) presents new\nopportunities to develop LLM-empowered social robots for enhanced human-robot\ninteraction. To fully realize these capabilities, however, robots need to\ncollect data such as audio, fine-grained images, video, and locations. As a\nresult, LLMs often process sensitive personal information, particularly within\nhome environments. Given the tension between utility and privacy risks,\nevaluating how current LLMs manage sensitive data is critical. Specifically, we\naim to explore the extent to which out-of-the-box LLMs are privacy-aware in the\ncontext of household social robots. In this study, we present a set of\nprivacy-relevant scenarios crafted through the lens of Contextual Integrity\n(CI). We first survey users' privacy preferences regarding in-home social robot\nbehaviors and then examine how their privacy orientation affects their choices\nof these behaviors (N = 450). We then provide the same set of scenarios and\nquestions to state-of-the-art LLMs (N = 10) and find that the agreement between\nhumans and LLMs is low. To further investigate the capabilities of LLMs as a\npotential privacy controller, we implement four additional prompting strategies\nand compare their results. Finally, we discuss the implications and potential\nof AI privacy awareness in human-robot interaction.\n","authors":["Dakota Sullivan","Shirley Zhang","Jennica Li","Heather Kirkorian","Bilge Mutlu","Kassem Fawaz"],"pdf_url":"https://arxiv.org/pdf/2507.16124v1.pdf","comment":"18 pages, 7 figures. Dakota Sullivan and Shirley Zhang contributed\n  equally to this work"},{"id":"http://arxiv.org/abs/2507.16121v1","updated":"2025-07-22T00:28:58Z","published":"2025-07-22T00:28:58Z","title":"DWSFormer: A Lightweight Inertial Odometry Network for Complex Motion\n  Modeling","summary":"  Inertial odometry (IO) directly estimates the position of a carrier from\ninertial sensor measurements and serves as a core technology for the widespread\ndeployment of consumer grade localization systems. While existing IO methods\ncan accurately reconstruct simple and near linear motion trajectories, they\noften fail to account for drift errors caused by complex motion patterns such\nas turning. This limitation significantly degrades localization accuracy and\nrestricts the applicability of IO systems in real world scenarios. To address\nthese challenges, we propose a lightweight IO framework. Specifically, inertial\ndata is projected into a high dimensional implicit nonlinear feature space\nusing the Star Operation method, enabling the extraction of complex motion\nfeatures that are typically overlooked. We further introduce a collaborative\nattention mechanism that jointly models global motion dynamics across both\nchannel and temporal dimensions. In addition, we design Multi Scale Gated\nConvolution Units to capture fine grained dynamic variations throughout the\nmotion process, thereby enhancing the model's ability to learn rich and\nexpressive motion representations. Extensive experiments demonstrate that our\nproposed method consistently outperforms SOTA baselines across six widely used\ninertial datasets. Compared to baseline models on the RoNIN dataset, it\nachieves reductions in ATE ranging from 2.26% to 65.78%, thereby establishing a\nnew benchmark in the field.\n","authors":["Shanshan Zhang","Qi Zhang","Siyue Wang","Tianshui Wen","Ziheng Zhou","Lingxiang Zheng","Yu Yang"],"pdf_url":"https://arxiv.org/pdf/2507.16121v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.16865v1","updated":"2025-07-22T00:24:06Z","published":"2025-07-22T00:24:06Z","title":"ResKACNNet: A Residual ChebyKAN Network for Inertial Odometry","summary":"  Inertial Measurement Unit (IMU) has become a key technology for achieving\nlow-cost and precise positioning. However, traditional CNN-based inertial\npositioning methods struggle to capture the nonlinear motion characteristics\nand long-term dependencies in IMU data. To address this limitation, we propose\na novel inertial positioning network with a generic backbone called\nResChebyKAN, which leverages the nonlinear approximation capabilities of\nChebyshev polynomials to model complex motion patterns. Additionally, we\nintroduce an Efficient Kernel-based Self-Attention (EKSA) module to effectively\ncapture contextual information and enhance long-term dependency modeling.\nExperimental results on public datasets (e.g., RIDI, RoNIN, RNIN-VIO, OxIOD,\nIMUNet, and TLIO) demonstrate that our method reduces the absolute trajectory\nerror by 3.79% to 42.32% compared to existing benchmark methods. Furthermore,\nwe release a preprocessed dataset and empirically show that removing the\ngravity component from acceleration data significantly improves inertial\npositioning performance.\n","authors":["Shanshan Zhang","Tianshui Wen","Siyue Wang","Qi Zhang","Ziheng Zhou","Huiru Zheng","Lingxiang Zheng","Yu Yang"],"pdf_url":"https://arxiv.org/pdf/2507.16865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.16120v1","updated":"2025-07-22T00:18:54Z","published":"2025-07-22T00:18:54Z","title":"FTIN: Frequency-Time Integration Network for Inertial Odometry","summary":"  In recent years, machine learning has achieved significant advancements in\ninertial odometry. However, most existing inertial odometry methods primarily\nrely on CNNs in the time domain. These methods often struggle to capture\nlong-term dependency in inertial measurement unit data, thereby constraining\nthe potential for further improvements in localization accuracy. To address\nthese issues, we propose a novel network architecture that integrates both\nfrequency-domain and time-domain information. Specifically, we leverage the\nglobal view and energy compaction properties of frequency-domain learning to\neffectively model long-term dependency and reduce redundancy in IMU data.\nAdditionally, we introduce a Scalar LSTM to capture sequential dependencies in\nthe time domain, enabling cross-domain information fusion and providing a\nstable and reliable reference for localization. Experimental evaluations on\nmultiple public datasets (e.g., RIDI, RoNIN, OxIOD, RNIN, TLIO, and IMUNet)\ndemonstrate the effectiveness of the proposed frequency-time domain fusion\nstrategy. Notably, on the RoNIN dataset, our method achieves a 43.0% reduction\nin absolute trajectory error and a 13.1% reduction in relative trajectory error\ncompared to RoNIN ResNet.\n","authors":["Shanshan Zhang","Qi Zhang","Siyue Wang","Tianshui Wen","Ziheng Zhou","Lingxiang Zheng","Yu Yang"],"pdf_url":"https://arxiv.org/pdf/2507.16120v1.pdf","comment":null}]},"2025-07-21T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2409.12190v2","updated":"2025-07-21T23:13:56Z","published":"2024-09-18T17:59:29Z","title":"Bundle Adjustment in the Eager Mode","summary":"  Bundle adjustment (BA) is a critical technique in various robotic\napplications such as simultaneous localization and mapping (SLAM), augmented\nreality (AR), and photogrammetry. BA optimizes parameters such as camera poses\nand 3D landmarks to align them with observations. With the growing importance\nof deep learning in perception systems, there is an increasing need to\nintegrate BA with deep learning frameworks for enhanced reliability and\nperformance. However, widely-used C++-based BA libraries, such as GTSAM,\ng$^2$o, and Ceres, lack native integration with modern deep learning libraries\nlike PyTorch. This limitation affects their flexibility, adaptability, ease of\ndebugging, and overall implementation efficiency. To address this gap, we\nintroduce an eager-mode BA library seamlessly integrated with PyTorch with high\nefficiency. Our approach includes GPU-accelerated, differentiable, and sparse\noperations designed for \\nth{2}-order optimization, Lie group and Lie algebra\noperations, and linear solvers. Our eager-mode BA on GPU demonstrates\nsubstantial runtime efficiency, achieving an average speedup of 18.5$\\times$,\n22$\\times$, and 23$\\times$ compared to GTSAM, g$^2$o, and Ceres, respectively.\nThe source code will be available at https://github.com/sair-lab/bae.\n","authors":["Zitong Zhan","Huan Xu","Zihang Fang","Xinpeng Wei","Yaoyu Hu","Chen Wang"],"pdf_url":"https://arxiv.org/pdf/2409.12190v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.16059v1","updated":"2025-07-21T20:52:56Z","published":"2025-07-21T20:52:56Z","title":"Therapist-Exoskeleton-Patient Interaction: An Immersive Gait Therapy","summary":"  Following a stroke, individuals often experience mobility and balance\nimpairments due to lower-limb weakness and loss of independent joint control.\nGait recovery is a key goal of rehabilitation, traditionally achieved through\nhigh-intensity therapist-led training. However, manual assistance can be\nphysically demanding and limits the therapist's ability to interact with\nmultiple joints simultaneously. Robotic exoskeletons offer multi-joint support,\nreduce therapist strain, and provide objective feedback, but current control\nstrategies often limit therapist involvement and adaptability.\n  We present a novel gait rehabilitation paradigm based on physical\nHuman-Robot-Human Interaction (pHRHI), where both the therapist and the\npost-stroke individual wear lower-limb exoskeletons virtually connected at the\nhips and knees via spring-damper elements. This enables bidirectional\ninteraction, allowing the therapist to guide movement and receive haptic\nfeedback. In a study with eight chronic stroke patients, pHRHI training\noutperformed conventional therapist-guided treadmill walking, leading to\nincreased joint range of motion, step metrics, muscle activation, and\nmotivation. These results highlight pHRHI's potential to combine robotic\nprecision with therapist intuition for improved rehabilitation outcomes.\n","authors":["Emek Barış Küçüktabak","Matthew R. Short","Lorenzo Vianello","Daniel Ludvig","Levi Hargrove","Kevin Lynch","Jose Pons"],"pdf_url":"https://arxiv.org/pdf/2507.16059v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.16034v1","updated":"2025-07-21T19:53:40Z","published":"2025-07-21T19:53:40Z","title":"Improved Semantic Segmentation from Ultra-Low-Resolution RGB Images\n  Applied to Privacy-Preserving Object-Goal Navigation","summary":"  User privacy in mobile robotics has become a critical concern. Existing\nmethods typically prioritize either the performance of downstream robotic tasks\nor privacy protection, with the latter often constraining the effectiveness of\ntask execution. To jointly address both objectives, we study semantic-based\nrobot navigation in an ultra-low-resolution setting to preserve visual privacy.\nA key challenge in such scenarios is recovering semantic segmentation from\nultra-low-resolution RGB images. In this work, we introduce a novel fully\njoint-learning method that integrates an agglomerative feature extractor and a\nsegmentation-aware discriminator to solve ultra-low-resolution semantic\nsegmentation, thereby enabling privacy-preserving, semantic object-goal\nnavigation. Our method outperforms different baselines on ultra-low-resolution\nsemantic segmentation and our improved segmentation results increase the\nsuccess rate of the semantic object-goal navigation in a real-world\nprivacy-constrained scenario.\n","authors":["Xuying Huang","Sicong Pan","Olga Zatsarynna","Juergen Gall","Maren Bennewitz"],"pdf_url":"https://arxiv.org/pdf/2507.16034v1.pdf","comment":"Submitted to RA-L"},{"id":"http://arxiv.org/abs/2507.06574v3","updated":"2025-07-21T18:39:04Z","published":"2025-07-09T06:05:52Z","title":"AI Space Cortex: An Experimental System for Future Era Space Exploration","summary":"  Our Robust, Explainable Autonomy for Scientific Icy Moon Operations (REASIMO)\neffort contributes to NASA's Concepts for Ocean worlds Life Detection\nTechnology (COLDTech) program, which explores science platform technologies for\nocean worlds such as Europa and Enceladus. Ocean world missions pose\nsignificant operational challenges. These include long communication lags,\nlimited power, and lifetime limitations caused by radiation damage and hostile\nconditions. Given these operational limitations, onboard autonomy will be vital\nfor future Ocean world missions. Besides the management of nominal lander\noperations, onboard autonomy must react appropriately in the event of\nanomalies. Traditional spacecraft rely on a transition into 'safe-mode' in\nwhich non-essential components and subsystems are powered off to preserve\nsafety and maintain communication with Earth. For a severely time-limited Ocean\nworld mission, resolutions to these anomalies that can be executed without\nEarth-in-the-loop communication and associated delays are paramount for\ncompletion of the mission objectives and science goals. To address these\nchallenges, the REASIMO effort aims to demonstrate a robust level of\nAI-assisted autonomy for such missions, including the ability to detect and\nrecover from anomalies, and to perform missions based on pre-trained behaviors\nrather than hard-coded, predetermined logic like all prior space missions. We\ndeveloped an AI-assisted, personality-driven, intelligent framework for control\nof an Ocean world mission by combining a mix of advanced technologies. To\ndemonstrate the capabilities of the framework, we perform tests of autonomous\nsampling operations on a lander-manipulator testbed at the NASA Jet Propulsion\nLaboratory, approximating possible surface conditions such a mission might\nencounter.\n","authors":["Thomas Touma","Ersin Daş","Erica Tevere","Martin Feather","Ksenia Kolcio","Maurice Prather","Alberto Candela","Ashish Goel","Erik Kramer","Hari Nayar","Lorraine Fesq","Joel W. Burdick"],"pdf_url":"https://arxiv.org/pdf/2507.06574v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.16000v1","updated":"2025-07-21T18:38:36Z","published":"2025-07-21T18:38:36Z","title":"A Comprehensive Evaluation of LiDAR Odometry Techniques","summary":"  Light Detection and Ranging (LiDAR) sensors have become the sensor of choice\nfor many robotic state estimation tasks. Because of this, in recent years there\nhas been significant work done to fine the most accurate method to perform\nstate estimation using these sensors. In each of these prior works, an\nexplosion of possible technique combinations has occurred, with each work\ncomparing LiDAR Odometry (LO) \"pipelines\" to prior \"pipelines\". Unfortunately,\nlittle work up to this point has performed the significant amount of ablation\nstudies comparing the various building-blocks of a LO pipeline. In this work,\nwe summarize the various techniques that go into defining a LO pipeline and\nempirically evaluate these LO components on an expansive number of datasets\nacross environments, LiDAR types, and vehicle motions. Finally, we make\nempirically-backed recommendations for the design of future LO pipelines to\nprovide the most accurate and reliable performance.\n","authors":["Easton Potokar","Michael Kaess"],"pdf_url":"https://arxiv.org/pdf/2507.16000v1.pdf","comment":"Accepted to IROS 2025"},{"id":"http://arxiv.org/abs/2507.15975v1","updated":"2025-07-21T18:10:23Z","published":"2025-07-21T18:10:23Z","title":"Fast Task Planning with Neuro-Symbolic Relaxation","summary":"  Real-world task planning requires long-horizon reasoning over large sets of\nentities with complex relationships and attributes, leading to a combinatorial\nexplosion for classical symbolic planners. To prune the search space, recent\nmethods prioritize searching on a simplified task only containing a few\n\"important\" entities predicted by a neural network. However, such a simple\nneuro-symbolic (NeSy) integration risks omitting critical entities and wasting\nresources on unsolvable simplified tasks. To enable Fast and reliable planning,\nwe introduce a NeSy relaxation strategy (Flax), combining neural importance\nprediction with symbolic expansion. Specifically, we first learn a graph neural\nnetwork to predict entity importance to create a simplified task and solve it\nwith a symbolic planner. Then, we solve a rule-relaxed task to obtain a quick\nrough plan, and reintegrate all referenced entities into the simplified task to\nrecover any overlooked but essential elements. Finally, we apply complementary\nrules to refine the updated task, keeping it both reliable and compact.\nExtensive experiments are conducted on both synthetic and real-world maze\nnavigation benchmarks where a robot must traverse through a maze and interact\nwith movable objects. The results show that Flax boosts the average success\nrate by 20.82% and cuts mean wall-clock planning time by 17.65% compared with\nthe state-of-the-art NeSy baseline. We expect that Flax offers a practical path\ntoward fast, scalable, long-horizon task planning in complex environments.\n","authors":["Qiwei Du","Bowen Li","Yi Du","Shaoshu Su","Taimeng Fu","Zitong Zhan","Zhipeng Zhao","Chen Wang"],"pdf_url":"https://arxiv.org/pdf/2507.15975v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2507.15833v1","updated":"2025-07-21T17:44:10Z","published":"2025-07-21T17:44:10Z","title":"Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and\n  Foveated Vision Transformers","summary":"  Human vision is a highly active process driven by gaze, which directs\nattention and fixation to task-relevant regions and dramatically reduces visual\nprocessing. In contrast, robot learning systems typically rely on passive,\nuniform processing of raw camera images. In this work, we explore how\nincorporating human-like active gaze into robotic policies can enhance both\nefficiency and performance. We build on recent advances in foveated image\nprocessing and apply them to an Active Vision robot system that emulates both\nhuman head movement and eye tracking. Extending prior work on the AV-ALOHA\nrobot simulation platform, we introduce a framework for simultaneously\ncollecting eye-tracking data and robot demonstrations from a human operator as\nwell as a simulation benchmark and dataset for training robot policies that\nincorporate human gaze. Given the widespread use of Vision Transformers (ViTs)\nin robot learning, we integrate gaze information into ViTs using a foveated\npatch tokenization scheme inspired by recent work in image segmentation.\nCompared to uniform patch tokenization, this significantly reduces the number\nof tokens-and thus computation-without sacrificing visual fidelity near regions\nof interest. We also explore two approaches to gaze imitation and prediction\nfrom human data. The first is a two-stage model that predicts gaze to guide\nfoveation and action; the second integrates gaze into the action space,\nallowing the policy to jointly predict gaze and actions end-to-end. Our results\nshow that our method for foveated robot vision not only drastically reduces\ncomputational overhead, but also improves performance for high precision tasks\nand robustness to unseen distractors. Together, these findings suggest that\nhuman-inspired visual processing offers a useful inductive bias for robotic\nvision systems. https://ian-chuang.github.io/gaze-av-aloha/\n","authors":["Ian Chuang","Andrew Lee","Dechen Gao","Jinyu Zou","Iman Soltani"],"pdf_url":"https://arxiv.org/pdf/2507.15833v1.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2506.12251v2","updated":"2025-07-21T17:22:35Z","published":"2025-06-13T21:56:52Z","title":"Efficient Multi-Camera Tokenization with Triplanes for End-to-End\n  Driving","summary":"  Autoregressive Transformers are increasingly being deployed as end-to-end\nrobot and autonomous vehicle (AV) policy architectures, owing to their\nscalability and potential to leverage internet-scale pretraining for\ngeneralization. Accordingly, tokenizing sensor data efficiently is paramount to\nensuring the real-time feasibility of such architectures on embedded hardware.\nTo this end, we present an efficient triplane-based multi-camera tokenization\nstrategy that leverages recent advances in 3D neural reconstruction and\nrendering to produce sensor tokens that are agnostic to the number of input\ncameras and their resolution, while explicitly accounting for their geometry\naround an AV. Experiments on a large-scale AV dataset and state-of-the-art\nneural simulator demonstrate that our approach yields significant savings over\ncurrent image patch-based tokenization strategies, producing up to 72% fewer\ntokens, resulting in up to 50% faster policy inference while achieving the same\nopen-loop motion planning accuracy and improved offroad rates in closed-loop\ndriving simulations.\n","authors":["Boris Ivanovic","Cristiano Saltori","Yurong You","Yan Wang","Wenjie Luo","Marco Pavone"],"pdf_url":"https://arxiv.org/pdf/2506.12251v2.pdf","comment":"12 pages, 10 figures, 5 tables"},{"id":"http://arxiv.org/abs/2507.15782v1","updated":"2025-07-21T16:37:50Z","published":"2025-07-21T16:37:50Z","title":"Interleaved LLM and Motion Planning for Generalized Multi-Object\n  Collection in Large Scene Graphs","summary":"  Household robots have been a longstanding research topic, but they still lack\nhuman-like intelligence, particularly in manipulating open-set objects and\nnavigating large environments efficiently and accurately. To push this\nboundary, we consider a generalized multi-object collection problem in large\nscene graphs, where the robot needs to pick up and place multiple objects\nacross multiple locations in a long mission of multiple human commands. This\nproblem is extremely challenging since it requires long-horizon planning in a\nvast action-state space under high uncertainties. To this end, we propose a\nnovel interleaved LLM and motion planning algorithm Inter-LLM. By designing a\nmultimodal action cost similarity function, our algorithm can both reflect the\nhistory and look into the future to optimize plans, striking a good balance of\nquality and efficiency. Simulation experiments demonstrate that compared with\nlatest works, our algorithm improves the overall mission performance by 30% in\nterms of fulfilling human commands, maximizing mission success rates, and\nminimizing mission costs.\n","authors":["Ruochu Yang","Yu Zhou","Fumin Zhang","Mengxue Hou"],"pdf_url":"https://arxiv.org/pdf/2507.15782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08033v3","updated":"2025-07-21T16:17:09Z","published":"2025-02-12T00:26:01Z","title":"Predictive Planner for Autonomous Driving with Consistency Models","summary":"  Trajectory prediction and planning are essential for autonomous vehicles to\nnavigate safely and efficiently in dynamic environments. Traditional approaches\noften treat them separately, limiting the ability for interactive planning.\nWhile recent diffusion-based generative models have shown promise in\nmulti-agent trajectory generation, their slow sampling is less suitable for\nhigh-frequency planning tasks. In this paper, we leverage the consistency model\nto build a predictive planner that samples from a joint distribution of ego and\nsurrounding agents, conditioned on the ego vehicle's navigational goal. Trained\non real-world human driving datasets, our consistency model generates\nhigher-quality trajectories with fewer sampling steps than standard diffusion\nmodels, making it more suitable for real-time deployment. To enforce multiple\nplanning constraints simultaneously on the ego trajectory, a novel online\nguided sampling approach inspired by the Alternating Direction Method of\nMultipliers (ADMM) is introduced. Evaluated on the Waymo Open Motion Dataset\n(WOMD), our method enables proactive behavior such as nudging and yielding, and\nalso demonstrates smoother, safer, and more efficient trajectories and\nsatisfaction of multiple constraints under a limited computational budget.\n","authors":["Anjian Li","Sangjae Bae","David Isele","Ryne Beeson","Faizan M. Tariq"],"pdf_url":"https://arxiv.org/pdf/2502.08033v3.pdf","comment":"Accepted at the 28th IEEE International Conference on Intelligent\n  Transportation Systems (ITSC) 2025"},{"id":"http://arxiv.org/abs/2501.02184v3","updated":"2025-07-21T15:58:40Z","published":"2025-01-04T04:35:05Z","title":"Model-Free and Real-Time Bioinspired Unicycle-Based Source Seeking:\n  Differential Wheeled Robotic Experiments","summary":"  Many autonomous robots aimed at source-seeking are studied, and their\ncontrols designed, using unicycle modeling and formulation. This is true not\nonly for model-based controllers, but also for model-free, real-time control\nmethods such as extremum seeking control (ESC). In this paper, we propose a\nunicycle-based ESC design applicable to differential wheeled robots that: (1)\nis very simple design, based on one simple control-affine law, and without\nstate integrators; (2) attenuates oscillations known to persist in ESC designs\n(i.e., fully stop at the source); and (3) operates in a model-free, real-time\nsetting, tolerating environmental/sensor noise. We provide simulation and\nreal-world robotic experimental results for fixed and moving light source\nseeking by a differential wheeled robot using our proposed design. Results\nindicate clear advantages of our proposed design when compared to the\nliterature, including attenuation of undesired oscillations, improved\nconvergence speed, and better handling of noise.\n","authors":["Ahmed A. Elgohary","Sameh A. Eisa","Shivam Bajpai"],"pdf_url":"https://arxiv.org/pdf/2501.02184v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.15729v1","updated":"2025-07-21T15:38:25Z","published":"2025-07-21T15:38:25Z","title":"Gaze-supported Large Language Model Framework for Bi-directional\n  Human-Robot Interaction","summary":"  The rapid development of Large Language Models (LLMs) creates an exciting\npotential for flexible, general knowledge-driven Human-Robot Interaction (HRI)\nsystems for assistive robots. Existing HRI systems demonstrate great progress\nin interpreting and following user instructions, action generation, and robot\ntask solving. On the other hand, bi-directional, multi-modal, and context-aware\nsupport of the user in collaborative tasks still remains an open challenge. In\nthis paper, we present a gaze- and speech-informed interface to the assistive\nrobot, which is able to perceive the working environment from multiple vision\ninputs and support the dynamic user in their tasks. Our system is designed to\nbe modular and transferable to adapt to diverse tasks and robots, and it is\ncapable of real-time use of language-based interaction state representation and\nfast on board perception modules. Its development was supported by multiple\npublic dissemination events, contributing important considerations for improved\nrobustness and user experience. Furthermore, in two lab studies, we compare the\nperformance and user ratings of our system with those of a traditional scripted\nHRI pipeline. Our findings indicate that an LLM-based approach enhances\nadaptability and marginally improves user engagement and task execution metrics\nbut may produce redundant output, while a scripted pipeline is well suited for\nmore straightforward tasks.\n","authors":["Jens V. Rüppel","Andrey Rudenko","Tim Schreiter","Martin Magnusson","Achim J. Lilienthal"],"pdf_url":"https://arxiv.org/pdf/2507.15729v1.pdf","comment":"This paper has been accepted to the 34th IEEE International\n  Conference on Robot and Human Interactive Communication (RO-MAN), which will\n  be held in Eindhoven, Netherlands on August 25-29, 2025. Copyright 2025 IEEE.\n  Personal use of this material is permitted. Permission from IEEE must be\n  obtained for all other uses"},{"id":"http://arxiv.org/abs/2507.15716v1","updated":"2025-07-21T15:27:23Z","published":"2025-07-21T15:27:23Z","title":"DiffPF: Differentiable Particle Filtering with Generative Sampling via\n  Conditional Diffusion Models","summary":"  This paper proposes DiffPF, a differentiable particle filter that leverages\ndiffusion models for state estimation in dynamic systems. Unlike conventional\ndifferentiable particle filters, which require importance weighting and\ntypically rely on predefined or low-capacity proposal distributions. DiffPF\nlearns a flexible posterior sampler by conditioning a diffusion model on\npredicted particles and the current observation. This enables accurate,\nequally-weighted sampling from complex, high-dimensional, and multimodal\nfiltering distributions. We evaluate DiffPF across a range of scenarios,\nincluding both unimodal and highly multimodal distributions, and test it on\nsimulated as well as real-world tasks, where it consistently outperforms\nexisting filtering baselines. In particular, DiffPF achieves an 82.8%\nimprovement in estimation accuracy on a highly multimodal global localization\nbenchmark, and a 26% improvement on the real-world KITTI visual odometry\nbenchmark, compared to state-of-the-art differentiable filters. To the best of\nour knowledge, DiffPF is the first method to integrate conditional diffusion\nmodels into particle filtering, enabling high-quality posterior sampling that\nproduces more informative particles and significantly improves state\nestimation.\n","authors":["Ziyu Wan","Lin Zhao"],"pdf_url":"https://arxiv.org/pdf/2507.15716v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.15710v1","updated":"2025-07-21T15:17:59Z","published":"2025-07-21T15:17:59Z","title":"Selective Densification for Rapid Motion Planning in High Dimensions\n  with Narrow Passages","summary":"  Sampling-based algorithms are widely used for motion planning in\nhigh-dimensional configuration spaces. However, due to low sampling efficiency,\ntheir performance often diminishes in complex configuration spaces with narrow\ncorridors. Existing approaches address this issue using handcrafted or learned\nheuristics to guide sampling toward useful regions. Unfortunately, these\nstrategies often lack generalizability to various problems or require extensive\nprior training. In this paper, we propose a simple yet efficient sampling-based\nplanning framework along with its bidirectional version that overcomes these\nissues by integrating different levels of planning granularity. Our approach\nprobes configuration spaces with uniform random samples at varying resolutions\nand explores these multi-resolution samples online with a bias towards sparse\nsamples when traveling large free configuration spaces. By seamlessly\ntransitioning between sparse and dense samples, our approach can navigate\ncomplex configuration spaces while maintaining planning speed and completeness.\nThe simulation results demonstrate that our approach outperforms several\nstate-of-the-art sampling-based planners in $\\mathbb{SE}(2)$, $\\mathbb{SE}(3)$,\nand $\\mathbb{R}^{14}$ with challenging terrains. Furthermore, experiments\nconducted with the Franka Emika Panda robot operating in a constrained\nworkspace provide additional evidence of the superiority of the proposed\nmethod.\n","authors":["Lu Huang","Lingxiao Meng","Jiankun Wang","Xingjian Jing"],"pdf_url":"https://arxiv.org/pdf/2507.15710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06950v4","updated":"2025-07-21T15:06:04Z","published":"2023-09-13T13:33:03Z","title":"3D Active Metric-Semantic SLAM","summary":"  In this letter, we address the problem of exploration and metric-semantic\nmapping of multi-floor GPS-denied indoor environments using Size Weight and\nPower (SWaP) constrained aerial robots. Most previous work in exploration\nassumes that robot localization is solved. However, neglecting the state\nuncertainty of the agent can ultimately lead to cascading errors both in the\nresulting map and in the state of the agent itself. Furthermore, actions that\nreduce localization errors may be at direct odds with the exploration task. We\npropose a framework that balances the efficiency of exploration with actions\nthat reduce the state uncertainty of the agent. In particular, our algorithmic\napproach for active metric-semantic SLAM is built upon sparse information\nabstracted from raw problem data, to make it suitable for SWaP-constrained\nrobots. Furthermore, we integrate this framework within a fully autonomous\naerial robotic system that achieves autonomous exploration in cluttered, 3D\nenvironments. From extensive real-world experiments, we showed that by\nincluding Semantic Loop Closure (SLC), we can reduce the robot pose estimation\nerrors by over 90% in translation and approximately 75% in yaw, and the\nuncertainties in pose estimates and semantic maps by over 70% and 65%,\nrespectively. Although discussed in the context of indoor multi-floor\nexploration, our system can be used for various other applications, such as\ninfrastructure inspection and precision agriculture where reliable GPS data may\nnot be available.\n","authors":["Yuezhan Tao","Xu Liu","Igor Spasojevic","Saurav Agarwal","Vijay Kumar"],"pdf_url":"https://arxiv.org/pdf/2309.06950v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.15693v1","updated":"2025-07-21T15:02:01Z","published":"2025-07-21T15:02:01Z","title":"Strong, Accurate, and Low-Cost Robot Manipulator","summary":"  This paper presents Forte, a fully 3D-printable, 6-DoF robotic arm designed\nto achieve near industrial-grade performance - 0.63 kg payload, 0.467 m reach,\nand sub-millimeter repeatability - at a material cost under $215. As an\naccessible robot for broad applications across classroom education to AI\nexperiments, Forte pushes forward the performance limitations of existing\nlow-cost educational arms. We introduce a cost-effective mechanical design that\ncombines capstan-based cable drives, timing belts, simple tensioning\nmechanisms, and lightweight 3D-printed structures, along with topology\noptimization for structural stiffness. Through careful drivetrain engineering,\nwe minimize backlash and maintain control fidelity without relying on\nhigh-power electronics or expensive manufacturing processes. Experimental\nvalidation demonstrates that Forte achieves high repeatability and load\ncapacity, offering a compelling robotic platform for both classroom instruction\nand advanced robotics research.\n","authors":["Georges Chebly","Spencer Little","Nisal Perera","Aliya Abedeen","Ken Suzuki","Donghyun Kim"],"pdf_url":"https://arxiv.org/pdf/2507.15693v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.15677v1","updated":"2025-07-21T14:42:29Z","published":"2025-07-21T14:42:29Z","title":"Data-Driven MPC with Data Selection for Flexible Cable-Driven Robotic\n  Arms","summary":"  Flexible cable-driven robotic arms (FCRAs) offer dexterous and compliant\nmotion. Still, the inherent properties of cables, such as resilience,\nhysteresis, and friction, often lead to particular difficulties in modeling and\ncontrol. This paper proposes a model predictive control (MPC) method that\nrelies exclusively on input-output data, without a physical model, to improve\nthe control accuracy of FCRAs. First, we develop an implicit model based on\ninput-output data and integrate it into an MPC optimization framework. Second,\na data selection algorithm (DSA) is introduced to filter the data that best\ncharacterize the system, thereby reducing the solution time per step to\napproximately 4 ms, which is an improvement of nearly 80%. Lastly, the\ninfluence of hyperparameters on tracking error is investigated through\nsimulation. The proposed method has been validated on a real FCRA platform,\nincluding five-point positioning accuracy tests, a five-point response tracking\ntest, and trajectory tracking for letter drawing. The results demonstrate that\nthe average positioning accuracy is approximately 2.070 mm. Moreover, compared\nto the PID method with an average tracking error of 1.418{\\deg}, the proposed\nmethod achieves an average tracking error of 0.541{\\deg}.\n","authors":["Huayue Liang","Yanbo Chen","Hongyang Cheng","Yanzhao Yu","Shoujie Li","Junbo Tan","Xueqian Wang","Long Zeng"],"pdf_url":"https://arxiv.org/pdf/2507.15677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.15649v1","updated":"2025-07-21T14:14:04Z","published":"2025-07-21T14:14:04Z","title":"EMP: Executable Motion Prior for Humanoid Robot Standing Upper-body\n  Motion Imitation","summary":"  To support humanoid robots in performing manipulation tasks, it is essential\nto study stable standing while accommodating upper-body motions. However, the\nlimited controllable range of humanoid robots in a standing position affects\nthe stability of the entire body. Thus we introduce a reinforcement learning\nbased framework for humanoid robots to imitate human upper-body motions while\nmaintaining overall stability. Our approach begins with designing a retargeting\nnetwork that generates a large-scale upper-body motion dataset for training the\nreinforcement learning (RL) policy, which enables the humanoid robot to track\nupper-body motion targets, employing domain randomization for enhanced\nrobustness. To avoid exceeding the robot's execution capability and ensure\nsafety and stability, we propose an Executable Motion Prior (EMP) module, which\nadjusts the input target movements based on the robot's current state. This\nadjustment improves standing stability while minimizing changes to motion\namplitude. We evaluate our framework through simulation and real-world tests,\ndemonstrating its practical applicability.\n","authors":["Haocheng Xu","Haodong Zhang","Zhenghan Chen","Rong Xiong"],"pdf_url":"https://arxiv.org/pdf/2507.15649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01474v2","updated":"2025-07-21T14:00:54Z","published":"2025-03-03T12:29:48Z","title":"Interactive Navigation for Legged Manipulators with Learned Arm-Pushing\n  Controller","summary":"  Interactive navigation is crucial in scenarios where proactively interacting\nwith objects can yield shorter paths, thus significantly improving traversal\nefficiency. Existing methods primarily focus on using the robot body to\nrelocate large obstacles (which could be comparable to the size of a robot).\nHowever, they prove ineffective in narrow or constrained spaces where the\nrobot's dimensions restrict its manipulation capabilities. This paper\nintroduces a novel interactive navigation framework for legged manipulators,\nfeaturing an active arm-pushing mechanism that enables the robot to reposition\nmovable obstacles in space-constrained environments. To this end, we develop a\nreinforcement learning-based arm-pushing controller with a two-stage reward\nstrategy for large-object manipulation. Specifically, this strategy first\ndirects the manipulator to a designated pushing zone to achieve a kinematically\nfeasible contact configuration. Then, the end effector is guided to maintain\nits position at appropriate contact points for stable object displacement while\npreventing toppling. The simulations validate the robustness of the arm-pushing\ncontroller, showing that the two-stage reward strategy improves policy\nconvergence and long-term performance. Real-world experiments further\ndemonstrate the effectiveness of the proposed navigation framework, which\nachieves shorter paths and reduced traversal time. The open-source project can\nbe found at\nhttps://github.com/Zhihaibi/Interactive-Navigation-for-legged-manipulator.git.\n","authors":["Zhihai Bi","Kai Chen","Chunxin Zheng","Yulin Li","Haoang Li","Jun Ma"],"pdf_url":"https://arxiv.org/pdf/2503.01474v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.15608v1","updated":"2025-07-21T13:33:24Z","published":"2025-07-21T13:33:24Z","title":"Optimizing Force Signals from Human Demonstrations of In-Contact Motions","summary":"  For non-robot-programming experts, kinesthetic guiding can be an intuitive\ninput method, as robot programming of in-contact tasks is becoming more\nprominent. However, imprecise and noisy input signals from human demonstrations\npose problems when reproducing motions directly or using the signal as input\nfor machine learning methods. This paper explores optimizing force signals to\ncorrespond better to the human intention of the demonstrated signal. We compare\ndifferent signal filtering methods and propose a peak detection method for\ndealing with first-contact deviations in the signal. The evaluation of these\nmethods considers a specialized error criterion between the input and the\nhuman-intended signal. In addition, we analyze the critical parameters'\ninfluence on the filtering methods. The quality for an individual motion could\nbe increased by up to \\SI{20}{\\percent} concerning the error criterion. The\nproposed contribution can improve the usability of robot programming and the\ninteraction between humans and robots.\n","authors":["Johannes Hartwig","Fabian Viessmann","Dominik Henrich"],"pdf_url":"https://arxiv.org/pdf/2507.15608v1.pdf","comment":"Accepted for publication in Annals of Scientific Society for\n  Assembly, Handling and Industrial Robotics 2024 (to appear)"},{"id":"http://arxiv.org/abs/2507.15607v1","updated":"2025-07-21T13:31:02Z","published":"2025-07-21T13:31:02Z","title":"A Universal Vehicle-Trailer Navigation System with Neural Kinematics and\n  Online Residual Learning","summary":"  Autonomous navigation of vehicle-trailer systems is crucial in environments\nlike airports, supermarkets, and concert venues, where various types of\ntrailers are needed to navigate with different payloads and conditions.\nHowever, accurately modeling such systems remains challenging, especially for\ntrailers with castor wheels. In this work, we propose a novel universal\nvehicle-trailer navigation system that integrates a hybrid nominal kinematic\nmodel--combining classical nonholonomic constraints for vehicles and neural\nnetwork-based trailer kinematics--with a lightweight online residual learning\nmodule to correct real-time modeling discrepancies and disturbances.\nAdditionally, we develop a model predictive control framework with a weighted\nmodel combination strategy that improves long-horizon prediction accuracy and\nensures safer motion planning. Our approach is validated through extensive\nreal-world experiments involving multiple trailer types and varying payload\nconditions, demonstrating robust performance without manual tuning or\ntrailer-specific calibration.\n","authors":["Yanbo Chen","Yunzhe Tan","Yaojia Wang","Zhengzhe Xu","Junbo Tan","Xueqian Wang"],"pdf_url":"https://arxiv.org/pdf/2507.15607v1.pdf","comment":"8 pages, 10 figures"},{"id":"http://arxiv.org/abs/2507.15604v1","updated":"2025-07-21T13:27:04Z","published":"2025-07-21T13:27:04Z","title":"Estimation of Payload Inertial Parameters from Human Demonstrations by\n  Hand Guiding","summary":"  As the availability of cobots increases, it is essential to address the needs\nof users with little to no programming knowledge to operate such systems\nefficiently. Programming concepts often use intuitive interaction modalities,\nsuch as hand guiding, to address this. When programming in-contact motions,\nsuch frameworks require knowledge of the robot tool's payload inertial\nparameters (PIP) in addition to the demonstrated velocities and forces to\nensure effective hybrid motion-force control. This paper aims to enable\nnon-expert users to program in-contact motions more efficiently by eliminating\nthe need for a dedicated PIP calibration, thereby enabling flexible robot tool\nchanges. Since demonstrated tasks generally also contain motions with\nnon-contact, our approach uses these parts to estimate the robot's PIP using\nestablished estimation techniques. The results show that the estimation of the\npayload's mass is accurate, whereas the center of mass and the inertia tensor\nare affected by noise and a lack of excitation. Overall, these findings show\nthe feasibility of PIP estimation during hand guiding but also highlight the\nneed for sufficient payload accelerations for an accurate estimation.\n","authors":["Johannes Hartwig","Philipp Lienhardt","Dominik Henrich"],"pdf_url":"https://arxiv.org/pdf/2507.15604v1.pdf","comment":"Accepted for publication in Annals of Scientific Society for\n  Assembly, Handling and Industrial Robotics 2025 (to appear)"},{"id":"http://arxiv.org/abs/2507.15597v1","updated":"2025-07-21T13:19:09Z","published":"2025-07-21T13:19:09Z","title":"Being-H0: Vision-Language-Action Pretraining from Large-Scale Human\n  Videos","summary":"  We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained\non large-scale human videos. Existing VLAs struggle with complex manipulation\ntasks requiring high dexterity and generalize poorly to novel scenarios and\ntasks, primarily due to their reliance on synthetic data with significant\nsim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To\naddress this data bottleneck, we propose leveraging human hands as a foundation\nmanipulator, capitalizing on the rich dexterity and scalability present in web\ndata. Our approach centers on physical instruction tuning, a novel training\nparadigm that combines large-scale VLA pretraining from human videos, physical\nspace alignment for 3D reasoning, and post-training adaptation for robotic\ntasks. Additionally, we introduce a part-level motion tokenization method which\nachieves millimeter-level reconstruction accuracy to model precise hand\ntrajectories for action learning. To support our proposed paradigm, we further\ndevelop a comprehensive data curation pipeline that integrates heterogeneous\nsources -- including motion capture, VR, and RGB-only videos -- into a\nlarge-scale dataset with millions of motion-based instructional instances. We\nempirically show the excellence of Being-H0 in hand motion generation and\ninstruction following, and it also scales well with model and data sizes.\nImportantly, we observe the expected gains of Being-H0 in real-world robotic\nmanipulation as physical instruction tuning is applied. More details are\navailable at https://beingbeyond.github.io/Being-H0.\n","authors":["Hao Luo","Yicheng Feng","Wanpeng Zhang","Sipeng Zheng","Ye Wang","Haoqi Yuan","Jiazheng Liu","Chaoyi Xu","Qin Jin","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2507.15597v1.pdf","comment":"37 pages"},{"id":"http://arxiv.org/abs/2507.15594v1","updated":"2025-07-21T13:17:44Z","published":"2025-07-21T13:17:44Z","title":"Improving Functional Reliability of Near-Field Monitoring for Emergency\n  Braking in Autonomous Vehicles","summary":"  Autonomous vehicles require reliable hazard detection. However, primary\nsensor systems may miss near-field obstacles, resulting in safety risks.\nAlthough a dedicated fast-reacting near-field monitoring system can mitigate\nthis, it typically suffers from false positives. To mitigate these, in this\npaper, we introduce three monitoring strategies based on dynamic spatial\nproperties, relevant object sizes, and motion-aware prediction. In experiments\nin a validated simulation, we compare the initial monitoring strategy against\nthe proposed improvements. The results demonstrate that the proposed strategies\ncan significantly improve the reliability of near-field monitoring systems.\n","authors":["Junnan Pan","Prodromos Sotiriadis","Vladislav Nenchev","Ferdinand Englberger"],"pdf_url":"https://arxiv.org/pdf/2507.15594v1.pdf","comment":"6 pages, 3 figures, conference paper"},{"id":"http://arxiv.org/abs/2505.13982v2","updated":"2025-07-21T13:10:44Z","published":"2025-05-20T06:29:20Z","title":"Adaptive Visuo-Tactile Fusion with Predictive Force Attention for\n  Dexterous Manipulation","summary":"  Effectively utilizing multi-sensory data is important for robots to\ngeneralize across diverse tasks. However, the heterogeneous nature of these\nmodalities makes fusion challenging. Existing methods propose strategies to\nobtain comprehensively fused features but often ignore the fact that each\nmodality requires different levels of attention at different manipulation\nstages. To address this, we propose a force-guided attention fusion module that\nadaptively adjusts the weights of visual and tactile features without human\nlabeling. We also introduce a self-supervised future force prediction auxiliary\ntask to reinforce the tactile modality, improve data imbalance, and encourage\nproper adjustment. Our method achieves an average success rate of 93% across\nthree fine-grained, contactrich tasks in real-world experiments. Further\nanalysis shows that our policy appropriately adjusts attention to each modality\nat different manipulation stages. The videos can be viewed at\nhttps://adaptac-dex.github.io/.\n","authors":["Jinzhou Li","Tianhao Wu","Jiyao Zhang","Zeyuan Chen","Haotian Jin","Mingdong Wu","Yujun Shen","Yaodong Yang","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2505.13982v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00091v4","updated":"2025-07-21T12:45:28Z","published":"2025-04-30T18:02:45Z","title":"CoordField: Coordination Field for Agentic UAV Task Allocation In\n  Low-altitude Urban Scenarios","summary":"  With the increasing demand for heterogeneous Unmanned Aerial Vehicle (UAV)\nswarms to perform complex tasks in urban environments, system design now faces\nmajor challenges, including efficient semantic understanding, flexible task\nplanning, and the ability to dynamically adjust coordination strategies in\nresponse to evolving environmental conditions and continuously changing task\nrequirements. To address the limitations of existing methods, this paper\nproposes CoordField, a coordination field agent system for coordinating\nheterogeneous drone swarms in complex urban scenarios. In this system, large\nlanguage models (LLMs) is responsible for interpreting high-level human\ninstructions and converting them into executable commands for the UAV swarms,\nsuch as patrol and target tracking. Subsequently, a Coordination field\nmechanism is proposed to guide UAV motion and task selection, enabling\ndecentralized and adaptive allocation of emergent tasks. A total of 50 rounds\nof comparative testing were conducted across different models in a 2D\nsimulation space to evaluate their performance. Experimental results\ndemonstrate that the proposed system achieves superior performance in terms of\ntask coverage, response time, and adaptability to dynamic changes.\n","authors":["Tengchao Zhang","Yonglin Tian","Fei Lin","Jun Huang","Patrik P. Süli","Qinghua Ni","Rui Qin","Xiao Wang","Fei-Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2505.00091v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07872v2","updated":"2025-07-21T12:23:53Z","published":"2025-07-10T15:55:05Z","title":"Improving AEBS Validation Through Objective Intervention Classification\n  Leveraging the Prediction Divergence Principle","summary":"  The safety validation of automatic emergency braking system (AEBS) requires\naccurately distinguishing between false positive (FP) and true positive (TP)\nsystem activations. While simulations allow straightforward differentiation by\ncomparing scenarios with and without interventions, analyzing activations from\nopen-loop resimulations - such as those from field operational testing (FOT) -\nis more complex. This complexity arises from scenario parameter uncertainty and\nthe influence of driver interventions in the recorded data. Human labeling is\nfrequently used to address these challenges, relying on subjective assessments\nof intervention necessity or situational criticality, potentially introducing\nbiases and limitations. This work proposes a rule-based classification approach\nleveraging the Prediction Divergence Principle (PDP) to address those issues.\nApplied to a simplified AEBS, the proposed method reveals key strengths,\nlimitations, and system requirements for effective implementation. The findings\nsuggest that combining this approach with human labeling may enhance the\ntransparency and consistency of classification, thereby improving the overall\nvalidation process. While the rule set for classification derived in this work\nadopts a conservative approach, the paper outlines future directions for\nrefinement and broader applicability. Finally, this work highlights the\npotential of such methods to complement existing practices, paving the way for\nmore reliable and reproducible AEBS validation frameworks.\n","authors":["Daniel Betschinske","Steven Peters"],"pdf_url":"https://arxiv.org/pdf/2507.07872v2.pdf","comment":"This work has been accepted for publication at the 2025 IEEE\n  International Automated Vehicle Validation Conference (IAVVC)"},{"id":"http://arxiv.org/abs/2507.15499v1","updated":"2025-07-21T11:04:58Z","published":"2025-07-21T11:04:58Z","title":"CLEVER: Stream-based Active Learning for Robust Semantic Perception from\n  Human Instructions","summary":"  We propose CLEVER, an active learning system for robust semantic perception\nwith Deep Neural Networks (DNNs). For data arriving in streams, our system\nseeks human support when encountering failures and adapts DNNs online based on\nhuman instructions. In this way, CLEVER can eventually accomplish the given\nsemantic perception tasks. Our main contribution is the design of a system that\nmeets several desiderata of realizing the aforementioned capabilities. The key\nenabler herein is our Bayesian formulation that encodes domain knowledge\nthrough priors. Empirically, we not only motivate CLEVER's design but further\ndemonstrate its capabilities with a user validation study as well as\nexperiments on humanoid and deformable objects. To our knowledge, we are the\nfirst to realize stream-based active learning on a real robot, providing\nevidence that the robustness of the DNN-based semantic perception can be\nimproved in practice. The project website can be accessed at\nhttps://sites.google.com/view/thecleversystem.\n","authors":["Jongseok Lee","Timo Birr","Rudolph Triebel","Tamim Asfour"],"pdf_url":"https://arxiv.org/pdf/2507.15499v1.pdf","comment":"8 pages. Accepted to IEEE RAL"},{"id":"http://arxiv.org/abs/2507.15496v1","updated":"2025-07-21T10:58:10Z","published":"2025-07-21T10:58:10Z","title":"Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point\n  Clouds and Images","summary":"  Odometry is a critical task for autonomous systems for self-localization and\nnavigation. We propose a novel LiDAR-Visual odometry framework that integrates\nLiDAR point clouds and images for accurate and robust pose estimation. Our\nmethod utilizes a dense-depth map estimated from point clouds and images\nthrough depth completion, and incorporates a multi-scale feature extraction\nnetwork with attention mechanisms, enabling adaptive depth-aware\nrepresentations. Furthermore, we leverage dense depth information to refine\nflow estimation and mitigate errors in occlusion-prone regions. Our\nhierarchical pose refinement module optimizes motion estimation progressively,\nensuring robust predictions against dynamic environments and scale ambiguities.\nComprehensive experiments on the KITTI odometry benchmark demonstrate that our\napproach achieves similar or superior accuracy and robustness compared to\nstate-of-the-art visual and LiDAR odometry methods.\n","authors":["JunYing Huang","Ao Xu","DongSun Yong","KeRen Li","YuanFeng Wang","Qi Qin"],"pdf_url":"https://arxiv.org/pdf/2507.15496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.15484v1","updated":"2025-07-21T10:40:28Z","published":"2025-07-21T10:40:28Z","title":"Robots for Kiwifruit Harvesting and Pollination","summary":"  This research was a part of a project that developed mobile robots that\nperformed targeted pollen spraying and automated harvesting in pergola\nstructured kiwifruit orchards. Multiple kiwifruit detachment mechanisms were\ndesigned and field testing of one of the concepts showed that the mechanism\ncould reliably pick kiwifruit. Furthermore, this kiwifruit detachment mechanism\nwas able to reach over 80 percent of fruit in the cluttered kiwifruit canopy,\nwhereas the previous state of the art mechanism was only able to reach less\nthan 70 percent of the fruit. Artificial pollination was performed by detecting\nflowers and then spraying pollen in solution onto the detected flowers from a\nline of sprayers on a boom, while driving at up to 1.4 ms-1. In addition, the\nheight of the canopy was measured and the spray boom was moved up and down to\nkeep the boom close enough to the flowers for the spray to reach the flowers,\nwhile minimising collisions with the canopy. Mobile robot navigation was\nperformed using a 2D lidar in apple orchards and vineyards. Lidar navigation in\nkiwifruit orchards was more challenging because the pergola structure only\nprovides a small amount of data for the direction of rows, compared to the\namount of data from the overhead canopy, the undulating ground and other\nobjects in the orchards. Multiple methods are presented here for extracting\nstructure defining features from 3D lidar data in kiwifruit orchards. In\naddition, a 3D lidar navigation system -- which performed row following, row\nend detection and row end turns -- was tested for over 30 km of autonomous\ndriving in kiwifruit orchards. Computer vision algorithms for row detection and\nrow following were also tested. The computer vision algorithm worked as well as\nthe 3D lidar row following method in testing.\n","authors":["Jamie Bell"],"pdf_url":"https://arxiv.org/pdf/2507.15484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.15478v1","updated":"2025-07-21T10:33:31Z","published":"2025-07-21T10:33:31Z","title":"The Constitutional Controller: Doubt-Calibrated Steering of Compliant\n  Agents","summary":"  Ensuring reliable and rule-compliant behavior of autonomous agents in\nuncertain environments remains a fundamental challenge in modern robotics. Our\nwork shows how neuro-symbolic systems, which integrate probabilistic, symbolic\nwhite-box reasoning models with deep learning methods, offer a powerful\nsolution to this challenge. This enables the simultaneous consideration of\nexplicit rules and neural models trained on noisy data, combining the strength\nof structured reasoning with flexible representations. To this end, we\nintroduce the Constitutional Controller (CoCo), a novel framework designed to\nenhance the safety and reliability of agents by reasoning over deep\nprobabilistic logic programs representing constraints such as those found in\nshared traffic spaces. Furthermore, we propose the concept of self-doubt,\nimplemented as a probability density conditioned on doubt features such as\ntravel velocity, employed sensors, or health factors. In a real-world aerial\nmobility study, we demonstrate CoCo's advantages for intelligent autonomous\nsystems to learn appropriate doubts and navigate complex and uncertain\nenvironments safely and compliantly.\n","authors":["Simon Kohaut","Felix Divo","Navid Hamid","Benedict Flade","Julian Eggert","Devendra Singh Dhami","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2507.15478v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.15474v1","updated":"2025-07-21T10:25:52Z","published":"2025-07-21T10:25:52Z","title":"All-UWB SLAM Using UWB Radar and UWB AOA","summary":"  There has been a growing interest in autonomous systems designed to operate\nin adverse conditions (e.g. smoke, dust), where the visible light spectrum\nfails. In this context, Ultra-wideband (UWB) radar is capable of penetrating\nthrough such challenging environmental conditions due to the lower frequency\ncomponents within its broad bandwidth. Therefore, UWB radar has emerged as a\npotential sensing technology for Simultaneous Localization and Mapping (SLAM)\nin vision-denied environments where optical sensors (e.g. LiDAR, Camera) are\nprone to failure. Existing approaches involving UWB radar as the primary\nexteroceptive sensor generally extract features in the environment, which are\nlater initialized as landmarks in a map. However, these methods are constrained\nby the number of distinguishable features in the environment. Hence, this paper\nproposes a novel method incorporating UWB Angle of Arrival (AOA) measurements\ninto UWB radar-based SLAM systems to improve the accuracy and scalability of\nSLAM in feature-deficient environments. The AOA measurements are obtained using\nUWB anchor-tag units which are dynamically deployed by the robot in featureless\nareas during mapping of the environment. This paper thoroughly discusses\nprevailing constraints associated with UWB AOA measurement units and presents\nsolutions to overcome them. Our experimental results show that integrating UWB\nAOA units with UWB radar enables SLAM in vision-denied feature-deficient\nenvironments.\n","authors":["Charith Premachandra","Achala Athukorala","U-Xuan Tan"],"pdf_url":"https://arxiv.org/pdf/2507.15474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.15469v1","updated":"2025-07-21T10:21:42Z","published":"2025-07-21T10:21:42Z","title":"The Emergence of Deep Reinforcement Learning for Path Planning","summary":"  The increasing demand for autonomous systems in complex and dynamic\nenvironments has driven significant research into intelligent path planning\nmethodologies. For decades, graph-based search algorithms, linear programming\ntechniques, and evolutionary computation methods have served as foundational\napproaches in this domain. Recently, deep reinforcement learning (DRL) has\nemerged as a powerful method for enabling autonomous agents to learn optimal\nnavigation strategies through interaction with their environments. This survey\nprovides a comprehensive overview of traditional approaches as well as the\nrecent advancements in DRL applied to path planning tasks, focusing on\nautonomous vehicles, drones, and robotic platforms. Key algorithms across both\nconventional and learning-based paradigms are categorized, with their\ninnovations and practical implementations highlighted. This is followed by a\nthorough discussion of their respective strengths and limitations in terms of\ncomputational efficiency, scalability, adaptability, and robustness. The survey\nconcludes by identifying key open challenges and outlining promising avenues\nfor future research. Special attention is given to hybrid approaches that\nintegrate DRL with classical planning techniques to leverage the benefits of\nboth learning-based adaptability and deterministic reliability, offering\npromising directions for robust and resilient autonomous navigation.\n","authors":["Thanh Thi Nguyen","Saeid Nahavandi","Imran Razzak","Dung Nguyen","Nhat Truong Pham","Quoc Viet Hung Nguyen"],"pdf_url":"https://arxiv.org/pdf/2507.15469v1.pdf","comment":"Accepted for publication in the Proceedings of the 2025 IEEE\n  International Conference on Systems, Man, and Cybernetics (SMC)"},{"id":"http://arxiv.org/abs/2403.15333v4","updated":"2025-07-21T10:11:39Z","published":"2024-03-22T16:39:13Z","title":"Gesture-Controlled Aerial Robot Formation for Human-Swarm Interaction in\n  Safety Monitoring Applications","summary":"  This paper presents a formation control approach for contactless\ngesture-based Human-Swarm Interaction (HSI) between a team of multi-rotor\nUnmanned Aerial Vehicles (UAVs) and a human worker. The approach is designed to\nmonitor the safety of human workers, particularly those operating at heights.\nIn the proposed dynamic formation scheme, one UAV acts as the formation leader,\nequipped with sensors for detecting human workers and recognizing gestures. The\nfollower UAVs maintain a predetermined formation relative to the worker's\nposition, providing additional perspectives of the monitored scene. Hand\ngestures enable the human worker to specify movement and action commands for\nthe UAV team and to initiate other mission-related tasks without requiring\nadditional communication channels or specific markers. Combined with a novel\nunified human detection and tracking algorithm, a human position estimation\nmethod, and a gesture detection pipeline, the proposed approach represents the\nfirst instance of an HSI system incorporating all these modules onboard\nreal-world UAVs. Simulations and field experiments involving three UAVs and a\nhuman worker in a mock-up scenario demonstrate the effectiveness and\nresponsiveness of the proposed approach.\n","authors":["Vít Krátký","Giuseppe Silano","Matouš Vrba","Christos Papaioannidis","Ioannis Mademlis","Robert Pěnička","Ioannis Pitas","Martin Saska"],"pdf_url":"https://arxiv.org/pdf/2403.15333v4.pdf","comment":"2025 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2412.18347v3","updated":"2025-07-21T10:09:59Z","published":"2024-12-24T11:14:55Z","title":"The Constitutional Filter: Bayesian Estimation of Compliant Agents","summary":"  Predicting agents impacted by legal policies, physical limitations, and\noperational preferences is inherently difficult. In recent years,\nneuro-symbolic methods have emerged, integrating machine learning and symbolic\nreasoning models into end-to-end learnable systems. Hereby, a promising avenue\nfor expressing high-level constraints over multi-modal input data in robotics\nhas opened up. This work introduces an approach for Bayesian estimation of\nagents expected to comply with a human-interpretable neuro-symbolic model we\ncall its Constitution. Hence, we present the Constitutional Filter (CoFi),\nleading to improved tracking of agents by leveraging expert knowledge,\nincorporating deep learning architectures, and accounting for environmental\nuncertainties. CoFi extends the general, recursive Bayesian estimation setting,\nensuring compatibility with a vast landscape of established techniques such as\nParticle Filters. To underpin the advantages of CoFi, we evaluate its\nperformance on real-world marine traffic data. Beyond improved performance, we\nshow how CoFi can learn to trust and adapt to the level of compliance of an\nagent, recovering baseline performance even if the assumed Constitution clashes\nwith reality.\n","authors":["Simon Kohaut","Felix Divo","Benedict Flade","Devendra Singh Dhami","Julian Eggert","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2412.18347v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01655v2","updated":"2025-07-21T10:01:15Z","published":"2024-08-03T03:53:05Z","title":"Stimulating Imagination: Towards General-purpose \"Something Something\n  Placement\"","summary":"  General-purpose object placement is a fundamental capability of an\nintelligent generalist robot: being capable of rearranging objects following\nprecise human instructions even in novel environments. This work is dedicated\nto achieving general-purpose object placement with ``something something''\ninstructions. Specifically, we break the entire process down into three parts,\nincluding object localization, goal imagination and robot control, and propose\na method named SPORT. SPORT leverages a pre-trained large vision model for\nbroad semantic reasoning about objects, and learns a diffusion-based pose\nestimator to ensure physically-realistic results in 3D space. Only object types\n(movable or reference) are communicated between these two parts, which brings\ntwo benefits. One is that we can fully leverage the powerful ability of\nopen-set object recognition and localization since no specific fine-tuning is\nneeded for the robotic scenario. Moreover, the diffusion-based estimator only\nneed to ``imagine\" the object poses after the placement, while no necessity for\ntheir semantic information. Thus the training burden is greatly reduced and no\nmassive training is required. The training data for the goal pose estimation is\ncollected in simulation and annotated by using GPT-4. Experimental results\ndemonstrate the effectiveness of our approach. SPORT can not only generate\npromising 3D goal poses for unseen simulated objects, but also be seamlessly\napplied to real-world settings.\n","authors":["Jianyang Wu","Jie Gu","Xiaokang Ma","Fangzhou Qiu","Chu Tang","Jingmin Chen"],"pdf_url":"https://arxiv.org/pdf/2408.01655v2.pdf","comment":"7 pages, accepted to the 2025 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS 2025)"},{"id":"http://arxiv.org/abs/2507.15444v1","updated":"2025-07-21T09:53:42Z","published":"2025-07-21T09:53:42Z","title":"Low-Latency Event-Based Velocimetry for Quadrotor Control in a Narrow\n  Pipe","summary":"  Autonomous quadrotor flight in confined spaces such as pipes and tunnels\npresents significant challenges due to unsteady, self-induced aerodynamic\ndisturbances. Very recent advances have enabled flight in such conditions, but\nthey either rely on constant motion through the pipe to mitigate airflow\nrecirculation effects or suffer from limited stability during hovering. In this\nwork, we present the first closed-loop control system for quadrotors for\nhovering in narrow pipes that leverages real-time flow field measurements. We\ndevelop a low-latency, event-based smoke velocimetry method that estimates\nlocal airflow at high temporal resolution. This flow information is used by a\ndisturbance estimator based on a recurrent convolutional neural network, which\ninfers force and torque disturbances in real time. The estimated disturbances\nare integrated into a learning-based controller trained via reinforcement\nlearning. The flow-feedback control proves particularly effective during\nlateral translation maneuvers in the pipe cross-section. There, the real-time\ndisturbance information enables the controller to effectively counteract\ntransient aerodynamic effects, thereby preventing collisions with the pipe\nwall. To the best of our knowledge, this work represents the first\ndemonstration of an aerial robot with closed-loop control informed by real-time\nflow field measurements. This opens new directions for research on flight in\naerodynamically complex environments. In addition, our work also sheds light on\nthe characteristic flow structures that emerge during flight in narrow,\ncircular pipes, providing new insights at the intersection of robotics and\nfluid dynamics.\n","authors":["Leonard Bauersfeld","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2507.15444v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2507.16853v1","updated":"2025-07-21T09:37:05Z","published":"2025-07-21T09:37:05Z","title":"MobileUse: A GUI Agent with Hierarchical Reflection for Autonomous\n  Mobile Operation","summary":"  Recent advances in Multimodal Large Language Models (MLLMs) have enabled the\ndevelopment of mobile agents that can understand visual inputs and follow user\ninstructions, unlocking new possibilities for automating complex tasks on\nmobile devices. However, applying these models to real-world mobile scenarios\nremains a significant challenge due to the long-horizon task execution,\ndifficulty in error recovery, and the cold-start problem in unfamiliar\nenvironments. To address these challenges, we propose MobileUse, a GUI agent\ndesigned for robust and adaptive mobile task execution. To improve resilience\nin long-horizon tasks and dynamic environments, we introduce a hierarchical\nreflection architecture that enables the agent to self-monitor, detect, and\nrecover from errors across multiple temporal scales-ranging from individual\nactions to overall task completion-while maintaining efficiency through a\nreflection-on-demand strategy. To tackle cold-start issues, we further\nintroduce a proactive exploration module, which enriches the agent's\nunderstanding of the environment through self-planned exploration. Evaluations\non AndroidWorld and AndroidLab benchmarks demonstrate that MobileUse\nestablishes new state-of-the-art performance, achieving success rates of 62.9%\nand 44.2%, respectively. To facilitate real-world applications, we release an\nout-of-the-box toolkit for automated task execution on physical mobile devices,\nwhich is available at https://github.com/MadeAgents/mobile-use.\n","authors":["Ning Li","Xiangmou Qu","Jiamu Zhou","Jun Wang","Muning Wen","Kounianhua Du","Xingyu Lou","Qiuying Peng","Jun Wang","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.16853v1.pdf","comment":"A technical report on a GUI agent based on multi-agent systems"},{"id":"http://arxiv.org/abs/2504.07623v2","updated":"2025-07-21T09:26:40Z","published":"2025-04-10T10:13:20Z","title":"Joint Travel Route Optimization Framework for Platooning","summary":"  Platooning represents an advanced driving technology designed to assist\ndrivers in traffic convoys of varying lengths, enhancing road safety, reducing\ndriver fatigue, and improving fuel efficiency. Sophisticated automated driving\nassistance systems have facilitated this innovation. Recent advancements in\nplatooning emphasize cooperative mechanisms within both centralized and\ndecentralized architectures enabled by vehicular communication technologies.\nThis study introduces a cooperative route planning optimization framework aimed\nat promoting the adoption of platooning through a centralized platoon formation\nstrategy at the system level. This approach is envisioned as a transitional\nphase from individual (ego) driving to fully collaborative driving.\nAdditionally, this research formulates and incorporates travel cost metrics\nrelated to fuel consumption, driver fatigue, and travel time, considering\nregulatory constraints on consecutive driving durations. The performance of\nthese cost metrics has been evaluated using Dijkstra's and A* shortest path\nalgorithms within a network graph framework. The results indicate that the\nproposed architecture achieves an average cost improvement of 14 % compared to\nindividual route planning for long road trips.\n","authors":["Akif Adas","Stefano Arrigoni","Mattia Brambilla","Monica Barbara Nicoli","Edoardo Sabbioni"],"pdf_url":"https://arxiv.org/pdf/2504.07623v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15009v2","updated":"2025-07-21T07:33:02Z","published":"2025-06-17T22:36:36Z","title":"Six-DoF Hand-Based Teleoperation for Omnidirectional Aerial Robots","summary":"  Omnidirectional aerial robots offer full 6-DoF independent control over\nposition and orientation, making them popular for aerial manipulation. Although\nadvancements in robotic autonomy, human operation remains essential in complex\naerial environments. Existing teleoperation approaches for multirotors fail to\nfully leverage the additional DoFs provided by omnidirectional rotation.\nAdditionally, the dexterity of human fingers should be exploited for more\nengaged interaction. In this work, we propose an aerial teleoperation system\nthat brings the rotational flexibility of human hands into the unbounded aerial\nworkspace. Our system includes two motion-tracking marker sets--one on the\nshoulder and one on the hand--along with a data glove to capture hand gestures.\nUsing these inputs, we design four interaction modes for different tasks,\nincluding Spherical Mode and Cartesian Mode for long-range moving, Operation\nMode for precise manipulation, as well as Locking Mode for temporary pauses,\nwhere the hand gestures are utilized for seamless mode switching. We evaluate\nour system on a vertically mounted valve-turning task in the real world,\ndemonstrating how each mode contributes to effective aerial manipulation. This\ninteraction framework bridges human dexterity with aerial robotics, paving the\nway for enhanced aerial teleoperation in unstructured environments.\n","authors":["Jinjie Li","Jiaxuan Li","Kotaro Kaneko","Haokun Liu","Liming Shu","Moju Zhao"],"pdf_url":"https://arxiv.org/pdf/2506.15009v2.pdf","comment":"7 pages, 10 figures. This work has been accepted to IROS 2025. The\n  video is released in https://youtu.be/n0IQEnjPzrw?si=Zp3kb3ss-D_AySOE"},{"id":"http://arxiv.org/abs/2503.11145v2","updated":"2025-07-21T06:54:03Z","published":"2025-03-14T07:25:26Z","title":"Leveraging Semantic Graphs for Efficient and Robust LiDAR SLAM","summary":"  Accurate and robust simultaneous localization and mapping (SLAM) is crucial\nfor autonomous mobile systems, typically achieved by leveraging the geometric\nfeatures of the environment. Incorporating semantics provides a richer scene\nrepresentation that not only enhances localization accuracy in SLAM but also\nenables advanced cognitive functionalities for downstream navigation and\nplanning tasks. Existing point-wise semantic LiDAR SLAM methods often suffer\nfrom poor efficiency and generalization, making them less robust in diverse\nreal-world scenarios. In this paper, we propose a semantic graph-enhanced SLAM\nframework, named SG-SLAM, which effectively leverages the geometric, semantic,\nand topological characteristics inherent in environmental structures. The\nsemantic graph serves as a fundamental component that facilitates critical\nfunctionalities of SLAM, including robust relocalization during odometry\nfailures, accurate loop closing, and semantic graph map construction. Our\nmethod employs a dual-threaded architecture, with one thread dedicated to\nonline odometry and relocalization, while the other handles loop closure, pose\ngraph optimization, and map update. This design enables our method to operate\nin real time and generate globally consistent semantic graph maps and point\ncloud maps. We extensively evaluate our method across the KITTI, MulRAN, and\nApollo datasets, and the results demonstrate its superiority compared to\nstate-of-the-art methods. Our method has been released at\nhttps://github.com/nubot-nudt/SG-SLAM.\n","authors":["Neng Wang","Huimin Lu","Zhiqiang Zheng","Hesheng Wang","Yun-Hui Liu","Xieyuanli Chen"],"pdf_url":"https://arxiv.org/pdf/2503.11145v2.pdf","comment":"8 pages, 4 figures,Accpted for IROS 2025"},{"id":"http://arxiv.org/abs/2507.15293v1","updated":"2025-07-21T06:48:45Z","published":"2025-07-21T06:48:45Z","title":"RepILN: Reparameterized Inertial Localization Network","summary":"  Inertial localization is regarded as a promising positioning solution for\nconsumer-grade IoT devices due to its cost-effectiveness and independence from\nexternal infrastructure. However, data-driven inertial localization methods\noften rely on increasingly complex network architectures to improve accuracy,\nwhich challenges the limited computational resources of IoT devices. Moreover,\nthese methods frequently overlook the importance of modeling long-term\ndependencies in inertial measurements - a critical factor for accurate\ntrajectory reconstruction - thereby limiting localization performance. To\naddress these challenges, we propose a reparameterized inertial localization\nnetwork that uses a multi-branch structure during training to enhance feature\nextraction. At inference time, this structure is transformed into an equivalent\nsingle-path architecture to improve parameter efficiency. To further capture\nlong-term dependencies in motion trajectories, we introduce a temporal-scale\nsparse attention mechanism that selectively emphasizes key trajectory segments\nwhile suppressing noise. Additionally, a gated convolutional unit is\nincorporated to effectively integrate long-range dependencies with local\nfine-grained features. Extensive experiments on public benchmarks demonstrate\nthat our method achieves a favorable trade-off between accuracy and model\ncompactness. For example, on the RoNIN dataset, our approach reduces the\nAbsolute Trajectory Error (ATE) by 2.59% compared to RoNIN-ResNet while\nreducing the number of parameters by 3.86%.\n","authors":["Shanshan Zhang","Tianshui Wen","Siyue Wang","Qi Zhang","Ziheng Zhou","Lingxiang Zheng","Yu Yang"],"pdf_url":"https://arxiv.org/pdf/2507.15293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.15266v1","updated":"2025-07-21T06:06:27Z","published":"2025-07-21T06:06:27Z","title":"VLM-UDMC: VLM-Enhanced Unified Decision-Making and Motion Control for\n  Urban Autonomous Driving","summary":"  Scene understanding and risk-aware attentions are crucial for human drivers\nto make safe and effective driving decisions. To imitate this cognitive ability\nin urban autonomous driving while ensuring the transparency and\ninterpretability, we propose a vision-language model (VLM)-enhanced unified\ndecision-making and motion control framework, named VLM-UDMC. This framework\nincorporates scene reasoning and risk-aware insights into an upper-level slow\nsystem, which dynamically reconfigures the optimal motion planning for the\ndownstream fast system. The reconfiguration is based on real-time environmental\nchanges, which are encoded through context-aware potential functions. More\nspecifically, the upper-level slow system employs a two-step reasoning policy\nwith Retrieval-Augmented Generation (RAG), leveraging foundation models to\nprocess multimodal inputs and retrieve contextual knowledge, thereby generating\nrisk-aware insights. Meanwhile, a lightweight multi-kernel decomposed LSTM\nprovides real-time trajectory predictions for heterogeneous traffic\nparticipants by extracting smoother trend representations for short-horizon\ntrajectory prediction. The effectiveness of the proposed VLM-UDMC framework is\nverified via both simulations and real-world experiments with a full-size\nautonomous vehicle. It is demonstrated that the presented VLM-UDMC effectively\nleverages scene understanding and attention decomposition for rational driving\ndecisions, thus improving the overall urban driving performance. Our\nopen-source project is available at https://github.com/henryhcliu/vlmudmc.git.\n","authors":["Haichao Liu","Haoren Guo","Pei Liu","Benshan Ma","Yuxiang Zhang","Jun Ma","Tong Heng Lee"],"pdf_url":"https://arxiv.org/pdf/2507.15266v1.pdf","comment":"14 pages, 12 figures"},{"id":"http://arxiv.org/abs/2507.04790v2","updated":"2025-07-21T05:46:21Z","published":"2025-07-07T09:11:45Z","title":"Interaction-Merged Motion Planning: Effectively Leveraging Diverse\n  Motion Datasets for Robust Planning","summary":"  Motion planning is a crucial component of autonomous robot driving. While\nvarious trajectory datasets exist, effectively utilizing them for a target\ndomain remains challenging due to differences in agent interactions and\nenvironmental characteristics. Conventional approaches, such as domain\nadaptation or ensemble learning, leverage multiple source datasets but suffer\nfrom domain imbalance, catastrophic forgetting, and high computational costs.\nTo address these challenges, we propose Interaction-Merged Motion Planning\n(IMMP), a novel approach that leverages parameter checkpoints trained on\ndifferent domains during adaptation to the target domain. IMMP follows a\ntwo-step process: pre-merging to capture agent behaviors and interactions,\nsufficiently extracting diverse information from the source domain, followed by\nmerging to construct an adaptable model that efficiently transfers diverse\ninteractions to the target domain. Our method is evaluated on various planning\nbenchmarks and models, demonstrating superior performance compared to\nconventional approaches.\n","authors":["Giwon Lee","Wooseong Jeong","Daehee Park","Jaewoo Jeong","Kuk-Jin Yoon"],"pdf_url":"https://arxiv.org/pdf/2507.04790v2.pdf","comment":"Accepted at ICCV 2025"},{"id":"http://arxiv.org/abs/2507.15189v1","updated":"2025-07-21T02:22:39Z","published":"2025-07-21T02:22:39Z","title":"CHADET: Cross-Hierarchical-Attention for Depth-Completion Using\n  Unsupervised Lightweight Transformer","summary":"  Depth information which specifies the distance between objects and current\nposition of the robot is essential for many robot tasks such as navigation.\nRecently, researchers have proposed depth completion frameworks to provide\ndense depth maps that offer comprehensive information about the surrounding\nenvironment. However, existing methods show significant trade-offs between\ncomputational efficiency and accuracy during inference. The substantial memory\nand computational requirements make them unsuitable for real-time applications,\nhighlighting the need to improve the completeness and accuracy of depth\ninformation while improving processing speed to enhance robot performance in\nvarious tasks. To address these challenges, in this paper, we propose\nCHADET(cross-hierarchical-attention depth-completion transformer), a\nlightweight depth-completion network that can generate accurate dense depth\nmaps from RGB images and sparse depth points. For each pair, its feature is\nextracted from the depthwise blocks and passed to the equally lightweight\ntransformer-based decoder. In the decoder, we utilize the novel\ncross-hierarchical-attention module that refines the image features from the\ndepth information. Our approach improves the quality and reduces memory usage\nof the depth map prediction, as validated in both KITTI, NYUv2, and VOID\ndatasets.\n","authors":["Kevin Christiansen Marsim","Jinwoo Jeon","Yeeun Kim","Myeongwoo Jeong","Hyun Myung"],"pdf_url":"https://arxiv.org/pdf/2507.15189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12911v2","updated":"2025-07-21T01:01:29Z","published":"2025-07-17T08:58:24Z","title":"LaViPlan : Language-Guided Visual Path Planning with RLVR","summary":"  Out-of-distribution (OOD) scenarios in autonomous driving refer to situations\nthat deviate from the training domain, often leading to unexpected and\npotentially hazardous behavior from planners that lack prior exposure to such\ncases. Recently, Vision-Language Models (VLMs) have been introduced into\nautonomous driving research for their promising generalization capabilities in\nOOD settings. Early studies demonstrated that VLMs could recognize OOD\nscenarios and generate user-level decisions such as \"go straight\" or \"turn\nright.\" However, a new challenge has emerged due to the misalignment between\nthe VLM's high-level decisions or visual reasoning expressed in language, and\nthe low-level predicted trajectories interpreted as actions. In this paper, we\npropose LaViPlan, a framework that leverages Reinforcement Learning with\nVerifiable Rewards (RLVR) to optimize VLMs using planning-oriented metrics.\nThis approach addresses the vision-language-action misalignment observed in\nexisting VLMs fine-tuned via supervised learning, which can recognize driving\nscenarios but often produce context-unaware decisions. Experimental results\ndemonstrate that our method improves situational awareness and decision-making\nunder OOD conditions, highlighting its potential to mitigate the misalignment\nissue. This work introduces a promising post-training paradigm for VLM agents\nin the context of autonomous driving.\n","authors":["Hayeon Oh"],"pdf_url":"https://arxiv.org/pdf/2507.12911v2.pdf","comment":"11 pages, 6 figures"}]},"2025-07-20T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2305.10442v3","updated":"2025-07-20T23:36:32Z","published":"2023-05-13T20:06:53Z","title":"CBAGAN-RRT: Convolutional Block Attention Generative Adversarial Network\n  for Sampling-Based Path Planning","summary":"  Sampling-based path planning algorithms play an important role in autonomous\nrobotics. However, a common problem among these algorithms is that the initial\npath generated is not optimal, and the convergence is too slow for real-world\napplications. In this paper, we propose a novel image-based learning algorithm\nusing a Convolutional Block Attention Generative Adversarial Network\n(CBAGAN-RRT) with a combination of spatial and channel attention and a novel\nloss function to design the heuristics, find a better optimal path, and improve\nthe convergence of the algorithm, both concerning time and speed. The\nprobability distribution of the paths generated from our GAN model is used to\nguide the sampling process for the RRT algorithm. We demonstrate that our\nalgorithm outperforms the previous state-of-the-art algorithms using both the\nimage quality generation metrics, like IOU Score, Dice Score, FID score, and\npath planning metrics like time cost and the number of nodes. Ablation studies\nshow the effectiveness of various components in our network architecture. The\nadvantage of our approach is that we can avoid the complicated preprocessing in\nthe state space, our model can be generalized to complex environments like\nthose containing turns and narrow passages without loss of accuracy, and our\nmodel can be easily integrated with other sampling-based path planning\nalgorithms.\n","authors":["Abhinav Sagar","Sai Teja Gilukara"],"pdf_url":"https://arxiv.org/pdf/2305.10442v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.15155v1","updated":"2025-07-20T23:27:44Z","published":"2025-07-20T23:27:44Z","title":"Learning-Based Modeling of a Magnetically Steerable Soft Suction Device\n  for Endoscopic Endonasal Interventions","summary":"  This letter introduces a novel learning-based modeling framework for a\nmagnetically steerable soft suction device designed for endoscopic endonasal\nbrain tumor resection. The device is miniaturized (4 mm outer diameter, 2 mm\ninner diameter, 40 mm length), 3D printed using biocompatible SIL 30 material,\nand integrates embedded Fiber Bragg Grating (FBG) sensors for real-time shape\nfeedback. Shape reconstruction is represented using four Bezier control points,\nenabling a compact and smooth model of the device's deformation. A data-driven\nmodel was trained on 5,097 experimental samples covering a range of magnetic\nfield magnitudes (0-14 mT), actuation frequencies (0.2-1.0 Hz), and vertical\ntip distances (90-100 mm), using both Neural Network (NN) and Random Forest\n(RF) architectures. The RF model outperformed the NN across all metrics,\nachieving a mean root mean square error of 0.087 mm in control point prediction\nand a mean shape reconstruction error of 0.064 mm. Feature importance analysis\nfurther revealed that magnetic field components predominantly influence distal\ncontrol points, while frequency and distance affect the base configuration.\nThis learning-based approach effectively models the complex nonlinear behavior\nof hyperelastic soft robots under magnetic actuation without relying on\nsimplified physical assumptions. By enabling sub-millimeter shape prediction\naccuracy and real-time inference, this work represents an advancement toward\nthe intelligent control of magnetically actuated soft robotic tools in\nminimally invasive neurosurgery.\n","authors":["Majid Roshanfar","Alex Zhang","Changyan He","Amir Hooshiar","Dale J. Podolsky","Thomas Looi","Eric Diller"],"pdf_url":"https://arxiv.org/pdf/2507.15155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17262v3","updated":"2025-07-20T22:44:23Z","published":"2024-09-25T18:20:05Z","title":"CROSS-GAiT: Cross-Attention-Based Multimodal Representation Fusion for\n  Parametric Gait Adaptation in Complex Terrains","summary":"  We present CROSS-GAiT, a novel algorithm for quadruped robots that uses Cross\nAttention to fuse terrain representations derived from visual and time-series\ninputs; including linear accelerations, angular velocities, and joint efforts.\nThese fused representations are used to continuously adjust two critical gait\nparameters (step height and hip splay), enabling adaptive gaits that respond\ndynamically to varying terrain conditions. To generate terrain representations,\nwe process visual inputs through a masked Vision Transformer (ViT) encoder and\ntime-series data through a dilated causal convolutional encoder. The Cross\nAttention mechanism then selects and integrates the most relevant features from\neach modality, combining terrain characteristics with robot dynamics for\ninformed gait adaptation. This fused representation allows CROSS-GAiT to\ncontinuously adjust gait parameters in response to unpredictable terrain\nconditions in real-time. We train CROSS-GAiT on a diverse set of terrains\nincluding asphalt, concrete, brick pavements, grass, dense vegetation, pebbles,\ngravel, and sand and validate its generalization ability on unseen\nenvironments. Our hardware implementation on the Ghost Robotics Vision 60\ndemonstrates superior performance in challenging terrains, such as high-density\nvegetation, unstable surfaces, sandbanks, and deformable substrates. We observe\nat least a 7.04% reduction in IMU energy density and a 27.3% reduction in total\njoint effort, which directly correlates with increased stability and reduced\nenergy usage when compared to state-of-the-art methods. Furthermore, CROSS-GAiT\ndemonstrates at least a 64.5% increase in success rate and a 4.91% reduction in\ntime to reach the goal in four complex scenarios. Additionally, the learned\nrepresentations perform 4.48% better than the state-of-the-art on a terrain\nclassification task.\n","authors":["Gershom Seneviratne","Kasun Weerakoon","Mohamed Elnoor","Vignesh Rajgopal","Harshavarthan Varatharajan","Mohamed Khalid M Jaffar","Jason Pusey","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2409.17262v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.05351v2","updated":"2025-07-20T21:32:24Z","published":"2025-04-06T16:51:36Z","title":"Design and Characterization of a Micro-Vibration Adhesion System","summary":"  In recent years, miniature wall-climbing robots have attracted widespread\nattention due to their significant potential in equipment inspection and\nin-situ repair applications. Traditional wall-climbing systems typically rely\non electromagnetic, electrostatic, vacuum suction, or van der Waals forces for\ncontrollable adhesion. However, these conventional methods impose limitations\nwhen striving for both a compact design and high-speed mobility. This paper\nproposes a novel Vibration-Based Adhesion (VBA) technique, which utilizes a\nflexible disk vibrating near a surface to generate a strong and controllable\nattractive force without direct contact. By employing an electric motor as the\nvibration source, the constructed VBA system was experimentally evaluated,\nachieving an adhesion-to-weight ratio exceeding 51 times. The experimental\nresults demonstrate that this adhesion mechanism not only provides a high\nnormal force but also maintains minimal shear force, making it particularly\nsuitable for high-speed movement and heavy load applications in miniature\nwall-climbing robots.\n","authors":["Siqian Li","Xi Wang","Jung-Che Chang","Xin Dong"],"pdf_url":"https://arxiv.org/pdf/2504.05351v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.15106v1","updated":"2025-07-20T20:02:50Z","published":"2025-07-20T20:02:50Z","title":"From Kicking to Causality: Simulating Infant Agency Detection with a\n  Robust Intrinsic Reward","summary":"  While human infants robustly discover their own causal efficacy, standard\nreinforcement learning agents remain brittle, as their reliance on\ncorrelation-based rewards fails in noisy, ecologically valid scenarios. To\naddress this, we introduce the Causal Action Influence Score (CAIS), a novel\nintrinsic reward rooted in causal inference. CAIS quantifies an action's\ninfluence by measuring the 1-Wasserstein distance between the learned\ndistribution of sensory outcomes conditional on that action, $p(h|a)$, and the\nbaseline outcome distribution, $p(h)$. This divergence provides a robust reward\nthat isolates the agent's causal impact from confounding environmental noise.\nWe test our approach in a simulated infant-mobile environment where\ncorrelation-based perceptual rewards fail completely when the mobile is\nsubjected to external forces. In stark contrast, CAIS enables the agent to\nfilter this noise, identify its influence, and learn the correct policy.\nFurthermore, the high-quality predictive model learned for CAIS allows our\nagent, when augmented with a surprise signal, to successfully reproduce the\n\"extinction burst\" phenomenon. We conclude that explicitly inferring causality\nis a crucial mechanism for developing a robust sense of agency, offering a\npsychologically plausible framework for more adaptive autonomous systems.\n","authors":["Xia Xu","Jochen Triesch"],"pdf_url":"https://arxiv.org/pdf/2507.15106v1.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2507.16846v1","updated":"2025-07-20T19:19:38Z","published":"2025-07-20T19:19:38Z","title":"Analytical Formulation of Autonomous Vehicle Freeway Merging Control\n  with State-Dependent Discharge Rates","summary":"  The core of the freeway merging control problem lies in dynamic queue\npropagation and dissipation linked to merging vehicle behavior. Traditionally,\nqueuing is modeled through demand-supply interactions with time varying demand\nand fixed capacity. However, field observations show flow rates decrease during\ncongestion at freeway merges due to the impact of intersecting traffic, a\nfactor overlooked in fundamental diagrams. This manuscript introduces an\nanalytical approach to characterize and control the dynamic multi-stage merging\nof autonomous vehicles, prioritizing traffic efficiency and safety. For the\nfirst time, the effective discharge rate at the merging point, reduced by the\nmulti-stage dynamic merging process, is analytically derived using a closed\nform formulation. Leveraging this expression, performance metrics such as queue\nlength and traffic delay are derived as the first objective. Additionally, a\ncrash risk function is established to quantitatively assess potential\ncollisions during the merging process, serving as the second objective.\nFinally, the problem is formulated as a dynamic programming model to jointly\nminimize delay and crash risk, with the merging location and speed as decision\nvariables. Given the terminal state, the ramp vehicle merging task is\nformulated as a recursive optimization problem, employing backward induction to\nfind the minimum cost solution. Numerical experiments using the NGSIM dataset\nvalidate the derived effective discharge rate. The results indicate that the\nproposed model outperforms two benchmark algorithms, leading to a more\nefficient and safer merging process.\n","authors":["Qing Tang","Xianbiao Hu"],"pdf_url":"https://arxiv.org/pdf/2507.16846v1.pdf","comment":"Accepted for publication in IEEE Transactions on Intelligent\n  Transportation Systems (2025) as a regular paper (minor revision approved)"},{"id":"http://arxiv.org/abs/2507.15089v1","updated":"2025-07-20T19:02:15Z","published":"2025-07-20T19:02:15Z","title":"Visual Place Recognition for Large-Scale UAV Applications","summary":"  Visual Place Recognition (vPR) plays a crucial role in Unmanned Aerial\nVehicle (UAV) navigation, enabling robust localization across diverse\nenvironments. Despite significant advancements, aerial vPR faces unique\nchallenges due to the limited availability of large-scale, high-altitude\ndatasets, which limits model generalization, along with the inherent rotational\nambiguity in UAV imagery. To address these challenges, we introduce LASED, a\nlarge-scale aerial dataset with approximately one million images,\nsystematically sampled from 170,000 unique locations throughout Estonia over a\ndecade, offering extensive geographic and temporal diversity. Its structured\ndesign ensures clear place separation significantly enhancing model training\nfor aerial scenarios. Furthermore, we propose the integration of steerable\nConvolutional Neural Networks (CNNs) to explicitly handle rotational variance,\nleveraging their inherent rotational equivariance to produce robust,\norientation-invariant feature representations. Our extensive benchmarking\ndemonstrates that models trained on LASED achieve significantly higher recall\ncompared to those trained on smaller, less diverse datasets, highlighting the\nbenefits of extensive geographic coverage and temporal diversity. Moreover,\nsteerable CNNs effectively address rotational ambiguity inherent in aerial\nimagery, consistently outperforming conventional convolutional architectures,\nachieving on average 12\\% recall improvement over the best-performing\nnon-steerable network. By combining structured, large-scale datasets with\nrotation-equivariant neural networks, our approach significantly enhances model\nrobustness and generalization for aerial vPR.\n","authors":["Ioannis Tsampikos Papapetros","Ioannis Kansizoglou","Antonios Gasteratos"],"pdf_url":"https://arxiv.org/pdf/2507.15089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.15088v1","updated":"2025-07-20T19:02:10Z","published":"2025-07-20T19:02:10Z","title":"Search-Based Autonomous Vehicle Motion Planning Using Game Theory","summary":"  In this paper, we propose a search-based interactive motion planning scheme\nfor autonomous vehicles (AVs), using a game-theoretic approach. In contrast to\ntraditional search-based approaches, the newly developed approach considers\nother road users (e.g. drivers and pedestrians) as intelligent agents rather\nthan static obstacles. This leads to the generation of a more realistic path\nfor the AV. Due to the low computational time, the proposed motion planning\nscheme is implementable in real-time applications. The performance of the\ndeveloped motion planning scheme is compared with existing motion planning\ntechniques and validated through experiments using WATonoBus, an electrical\nall-weather autonomous shuttle bus.\n","authors":["Pouya Panahandeh","Mohammad Pirani","Baris Fidan","Amir Khajepour"],"pdf_url":"https://arxiv.org/pdf/2507.15088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.15062v1","updated":"2025-07-20T17:53:59Z","published":"2025-07-20T17:53:59Z","title":"Touch in the Wild: Learning Fine-Grained Manipulation with a Portable\n  Visuo-Tactile Gripper","summary":"  Handheld grippers are increasingly used to collect human demonstrations due\nto their ease of deployment and versatility. However, most existing designs\nlack tactile sensing, despite the critical role of tactile feedback in precise\nmanipulation. We present a portable, lightweight gripper with integrated\ntactile sensors that enables synchronized collection of visual and tactile data\nin diverse, real-world, and in-the-wild settings. Building on this hardware, we\npropose a cross-modal representation learning framework that integrates visual\nand tactile signals while preserving their distinct characteristics. The\nlearning procedure allows the emergence of interpretable representations that\nconsistently focus on contacting regions relevant for physical interactions.\nWhen used for downstream manipulation tasks, these representations enable more\nefficient and effective policy learning, supporting precise robotic\nmanipulation based on multimodal feedback. We validate our approach on\nfine-grained tasks such as test tube insertion and pipette-based fluid\ntransfer, demonstrating improved accuracy and robustness under external\ndisturbances. Our project page is available at\nhttps://binghao-huang.github.io/touch_in_the_wild/ .\n","authors":["Xinyue Zhu","Binghao Huang","Yunzhu Li"],"pdf_url":"https://arxiv.org/pdf/2507.15062v1.pdf","comment":"More videos can be found on our\n  website:https://binghao-huang.github.io/touch_in_the_wild/"},{"id":"http://arxiv.org/abs/2507.15036v1","updated":"2025-07-20T16:37:37Z","published":"2025-07-20T16:37:37Z","title":"EBA-AI: Ethics-Guided Bias-Aware AI for Efficient Underwater Image\n  Enhancement and Coral Reef Monitoring","summary":"  Underwater image enhancement is vital for marine conservation, particularly\ncoral reef monitoring. However, AI-based enhancement models often face dataset\nbias, high computational costs, and lack of transparency, leading to potential\nmisinterpretations. This paper introduces EBA-AI, an ethics-guided bias-aware\nAI framework to address these challenges. EBA-AI leverages CLIP embeddings to\ndetect and mitigate dataset bias, ensuring balanced representation across\nvaried underwater environments. It also integrates adaptive processing to\noptimize energy efficiency, significantly reducing GPU usage while maintaining\ncompetitive enhancement quality. Experiments on LSUI400, Oceanex, and UIEB100\nshow that while PSNR drops by a controlled 1.0 dB, computational savings enable\nreal-time feasibility for large-scale marine monitoring. Additionally,\nuncertainty estimation and explainability techniques enhance trust in AI-driven\nenvironmental decisions. Comparisons with CycleGAN, FunIEGAN, RAUNENet,\nWaterNet, UGAN, PUGAN, and UTUIE validate EBA-AI's effectiveness in balancing\nefficiency, fairness, and interpretability in underwater image processing. By\naddressing key limitations of AI-driven enhancement, this work contributes to\nsustainable, bias-aware, and computationally efficient marine conservation\nefforts. For interactive visualizations, animations, source code, and access to\nthe preprint, visit: https://lyessaadsaoud.github.io/EBA-AI/\n","authors":["Lyes Saad Saoud","Irfan Hussain"],"pdf_url":"https://arxiv.org/pdf/2507.15036v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.15022v1","updated":"2025-07-20T16:06:31Z","published":"2025-07-20T16:06:31Z","title":"CPED-NCBFs: A Conformal Prediction for Expert Demonstration-based Neural\n  Control Barrier Functions","summary":"  Among the promising approaches to enforce safety in control systems, learning\nControl Barrier Functions (CBFs) from expert demonstrations has emerged as an\neffective strategy. However, a critical challenge remains: verifying that the\nlearned CBFs truly enforce safety across the entire state space. This is\nespecially difficult when CBF is represented using neural networks (NCBFs).\nSeveral existing verification techniques attempt to address this problem\nincluding SMT-based solvers, mixed-integer programming (MIP), and interval or\nbound-propagation methods but these approaches often introduce loose,\nconservative bounds. To overcome these limitations, in this work we use\nCPED-NCBFs a split-conformal prediction based verification strategy to verify\nthe learned NCBF from the expert demonstrations. We further validate our method\non point mass systems and unicycle models to demonstrate the effectiveness of\nthe proposed theory.\n","authors":["Sumeadh MS","Kevin Dsouza","Ravi Prakash"],"pdf_url":"https://arxiv.org/pdf/2507.15022v1.pdf","comment":"6pages, 4figures, Submitted to the prestigious Indian Control\n  Conference (ICC), 2025"},{"id":"http://arxiv.org/abs/2507.14975v1","updated":"2025-07-20T14:15:39Z","published":"2025-07-20T14:15:39Z","title":"FCRF: Flexible Constructivism Reflection for Long-Horizon Robotic Task\n  Planning with Large Language Models","summary":"  Autonomous error correction is critical for domestic robots to achieve\nreliable execution of complex long-horizon tasks. Prior work has explored\nself-reflection in Large Language Models (LLMs) for task planning error\ncorrection; however, existing methods are constrained by inflexible\nself-reflection mechanisms that limit their effectiveness. Motivated by these\nlimitations and inspired by human cognitive adaptation, we propose the Flexible\nConstructivism Reflection Framework (FCRF), a novel Mentor-Actor architecture\nthat enables LLMs to perform flexible self-reflection based on task difficulty,\nwhile constructively integrating historical valuable experience with failure\nlessons. We evaluated FCRF on diverse domestic tasks through simulation in\nAlfWorld and physical deployment in the real-world environment. Experimental\nresults demonstrate that FCRF significantly improves overall performance and\nself-reflection flexibility in complex long-horizon robotic tasks.\n","authors":["Yufan Song","Jiatao Zhang","Zeng Gu","Qingmiao Liang","Tuocheng Hu","Wei Song","Shiqiang Zhu"],"pdf_url":"https://arxiv.org/pdf/2507.14975v1.pdf","comment":"8 pages, 6 figures, IROS 2025"},{"id":"http://arxiv.org/abs/2507.14967v1","updated":"2025-07-20T13:53:35Z","published":"2025-07-20T13:53:35Z","title":"Heterogeneous object manipulation on nonlinear soft surface through\n  linear controller","summary":"  Manipulation surfaces indirectly control and reposition objects by actively\nmodifying their shape or properties rather than directly gripping objects.\nThese surfaces, equipped with dense actuator arrays, generate dynamic\ndeformations. However, a high-density actuator array introduces considerable\ncomplexity due to increased degrees of freedom (DOF), complicating control\ntasks. High DOF restrict the implementation and utilization of manipulation\nsurfaces in real-world applications as the maintenance and control of such\nsystems exponentially increase with array/surface size. Learning-based control\napproaches may ease the control complexity, but they require extensive training\nsamples and struggle to generalize for heterogeneous objects. In this study, we\nintroduce a simple, precise and robust PID-based linear close-loop feedback\ncontrol strategy for heterogeneous object manipulation on MANTA-RAY\n(Manipulation with Adaptive Non-rigid Textile Actuation with Reduced Actuation\ndensity). Our approach employs a geometric transformation-driven PID\ncontroller, directly mapping tilt angle control outputs(1D/2D) to actuator\ncommands to eliminate the need for extensive black-box training. We validate\nthe proposed method through simulations and experiments on a physical system,\nsuccessfully manipulating objects with diverse geometries, weights and\ntextures, including fragile objects like eggs and apples. The outcomes\ndemonstrate that our approach is highly generalized and offers a practical and\nreliable solution for object manipulation on soft robotic manipulation,\nfacilitating real-world implementation without prohibitive training demands.\n","authors":["Pratik Ingle","Kasper Støy","Andres Faiña"],"pdf_url":"https://arxiv.org/pdf/2507.14967v1.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2507.14931v1","updated":"2025-07-20T11:58:04Z","published":"2025-07-20T11:58:04Z","title":"Designing Robots with, not for: A Co-Design Framework for Empowering\n  Interactions in Forensic Psychiatry","summary":"  Forensic mental health care involves the treatment of individuals with severe\nmental disorders who have committed violent offences. These settings are often\ncharacterized by high levels of bureaucracy, risk avoidance, and restricted\nautonomy. Patients frequently experience a profound loss of control over their\nlives, leading to heightened psychological stress-sometimes resulting in\nisolation as a safety measure. In this study, we explore how co-design can be\nused to collaboratively develop a companion robot that helps monitor and\nregulate stress while maintaining tracking of the patients' interaction\nbehaviours for long-term intervention. We conducted four co-design workshops in\na forensic psychiatric clinic with patients, caregivers, and therapists. Our\nprocess began with the presentation of an initial speculative prototype to\ntherapists, enabling reflection on shared concerns, ethical risks, and\ndesirable features. This was followed by a creative ideation session with\npatients, a third workshop focused on defining desired functions and emotional\nresponses, and we are planning a final prototype demo to gather direct patient\nfeedback. Our findings emphasize the importance of empowering patients in the\ndesign process and adapting proposals based on their current emotional state.\nThe goal was to empower the patient in the design process and ensure each\npatient's voice was heard.\n","authors":["Qiaoqiao Ren","Remko Proesmans","Arend Pissens","Lara Dehandschutter","William Denecker","Lotte Rouckhout","Joke Carrette","Peter Vanhopplinus","Tony Belpaeme","Francis wyffels"],"pdf_url":"https://arxiv.org/pdf/2507.14931v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.14929v1","updated":"2025-07-20T11:56:59Z","published":"2025-07-20T11:56:59Z","title":"Digital twin and extended reality for teleoperation of the electric\n  vehicle battery disassembly","summary":"  Disassembling and sorting Electric Vehicle Batteries (EVBs) supports a\nsustainable transition to electric vehicles by enabling a closed-loop supply\nchain. Currently, the manual disassembly process exposes workers to hazards,\nincluding electrocution and toxic chemicals. We propose a teleoperated system\nfor the safe disassembly and sorting of EVBs. A human-in-the-loop can create\nand save disassembly sequences for unknown EVB types, enabling future\nautomation. An RGB camera aligns the physical and digital twins of the EVB, and\nthe digital twin of the robot is based on the Robot Operating System (ROS)\nmiddleware. This hybrid approach combines teleoperation and automation to\nimprove safety, adaptability, and efficiency in EVB disassembly and sorting.\nThe economic contribution is realized by reducing labor dependency and\nincreasing throughput in battery recycling. An online pilot study was set up to\nevaluate the usability of the presented approach, and the results demonstrate\nthe potential as a user-friendly solution.\n","authors":["Tero Kaarlela","Sami Salo","Jose Outeiro"],"pdf_url":"https://arxiv.org/pdf/2507.14929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.14914v1","updated":"2025-07-20T11:00:18Z","published":"2025-07-20T11:00:18Z","title":"One Step Beyond: Feedthrough & Placement-Aware Rectilinear Floorplanner","summary":"  Floorplanning determines the shapes and locations of modules on a chip canvas\nand plays a critical role in optimizing the chip's Power, Performance, and Area\n(PPA) metrics. However, existing floorplanning approaches often fail to\nintegrate with subsequent physical design stages, leading to suboptimal\nin-module component placement and excessive inter-module feedthrough. To tackle\nthis challenge, we propose Flora, a three-stage feedthrough and placement aware\nrectilinear floorplanner. In the first stage, Flora employs wiremask and\nposition mask techniques to achieve coarse-grained optimization of HPWL and\nfeedthrough. In the second stage, under the constraint of a fixed outline,\nFlora achieves a zero-whitespace layout by locally resizing module shapes,\nthereby performing fine-grained optimization of feedthrough and improving\ncomponent placement. In the third stage, Flora utilizes a fast tree\nsearch-based method to efficiently place components-including macros and\nstandard cells-within each module, subsequently adjusting module boundaries\nbased on the placement results to enable cross-stage optimization. Experimental\nresults show that Flora outperforms recent state-of-the-art floorplanning\napproaches, achieving an average reduction of 6% in HPWL, 5.16% in FTpin,\n29.15% in FTmod, and a 14% improvement in component placement performance.\n","authors":["Zhexuan Xu","Jie Wang","Siyuan Xu","Zijie Geng","Mingxuan Yuan","Feng Wu"],"pdf_url":"https://arxiv.org/pdf/2507.14914v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.14903v1","updated":"2025-07-20T10:27:44Z","published":"2025-07-20T10:27:44Z","title":"CoMoCAVs: Cohesive Decision-Guided Motion Planning for Connected and\n  Autonomous Vehicles with Multi-Policy Reinforcement Learning","summary":"  Autonomous driving demands reliable and efficient solutions to closely\nrelated problems such as decision-making and motion planning. In this work,\ndecision-making refers specifically to highway lane selection, while motion\nplanning involves generating control commands (such as speed and steering) to\nreach the chosen lane. In the context of Connected Autonomous Vehicles (CAVs),\nachieving both flexible and safe lane selection alongside precise trajectory\nexecution remains a significant challenge. This paper proposes a framework\ncalled Cohesive Decision-Guided Motion Planning (CDGMP), which tightly\nintegrates decision-making and motion planning using a Mixture of Experts (MoE)\ninspired architecture combined with multi-policy reinforcement learning. By\ncoordinating multiple specialized sub-networks through a gating mechanism, the\nmethod decomposes the complex driving task into modular components. Each\nsub-network focuses on a specific aspect of driving, improving efficiency by\nactivating only the most relevant modules during inference. This design also\nenhances safety through modular specialization. CDGMP improves the adaptability\nand robustness of CAVs across diverse traffic scenarios, offering a scalable\nsolution to real-world autonomy challenges. The architectural principles behind\nCDGMP, especially the use of MoE, also provide a strong foundation for other\nhigh-dimensional decision and control tasks. Simulation results (available at\nhttps://youtu.be/_-4OXNHV0UY) demonstrate reliable performance in both lane\nselection and motion planning.\n","authors":["Pan Hu"],"pdf_url":"https://arxiv.org/pdf/2507.14903v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2503.19457v2","updated":"2025-07-20T09:32:50Z","published":"2025-03-25T08:46:50Z","title":"G-DexGrasp: Generalizable Dexterous Grasping Synthesis Via Part-Aware\n  Prior Retrieval and Prior-Assisted Generation","summary":"  Recent advances in dexterous grasping synthesis have demonstrated significant\nprogress in producing reasonable and plausible grasps for many task purposes.\nBut it remains challenging to generalize to unseen object categories and\ndiverse task instructions. In this paper, we propose G-DexGrasp, a\nretrieval-augmented generation approach that can produce high-quality dexterous\nhand configurations for unseen object categories and language-based task\ninstructions. The key is to retrieve generalizable grasping priors, including\nthe fine-grained contact part and the affordance-related distribution of\nrelevant grasping instances, for the following synthesis pipeline.\nSpecifically, the fine-grained contact part and affordance act as generalizable\nguidance to infer reasonable grasping configurations for unseen objects with a\ngenerative model, while the relevant grasping distribution plays as\nregularization to guarantee the plausibility of synthesized grasps during the\nsubsequent refinement optimization. Our comparison experiments validate the\neffectiveness of our key designs for generalization and demonstrate the\nremarkable performance against the existing approaches. Project page:\nhttps://g-dexgrasp.github.io/\n","authors":["Juntao Jian","Xiuping Liu","Zixuan Chen","Manyi Li","Jian Liu","Ruizhen Hu"],"pdf_url":"https://arxiv.org/pdf/2503.19457v2.pdf","comment":"Accepted by ICCV 2025"},{"id":"http://arxiv.org/abs/2507.14850v1","updated":"2025-07-20T07:43:18Z","published":"2025-07-20T07:43:18Z","title":"Hierarchical Multi-Agent Reinforcement Learning with Control Barrier\n  Functions for Safety-Critical Autonomous Systems","summary":"  We address the problem of safe policy learning in multi-agent safety-critical\nautonomous systems. In such systems, it is necessary for each agent to meet the\nsafety requirements at all times while also cooperating with other agents to\naccomplish the task. Toward this end, we propose a safe Hierarchical\nMulti-Agent Reinforcement Learning (HMARL) approach based on Control Barrier\nFunctions (CBFs). Our proposed hierarchical approach decomposes the overall\nreinforcement learning problem into two levels learning joint cooperative\nbehavior at the higher level and learning safe individual behavior at the lower\nor agent level conditioned on the high-level policy. Specifically, we propose a\nskill-based HMARL-CBF algorithm in which the higher level problem involves\nlearning a joint policy over the skills for all the agents and the lower-level\nproblem involves learning policies to execute the skills safely with CBFs. We\nvalidate our approach on challenging environment scenarios whereby a large\nnumber of agents have to safely navigate through conflicting road networks.\nCompared with existing state of the art methods, our approach significantly\nimproves the safety achieving near perfect (within 5%) success/safety rate\nwhile also improving performance across all the environments.\n","authors":["H. M. Sabbir Ahmad","Ehsan Sabouni","Alexander Wasilkoff","Param Budhraja","Zijian Guo","Songyuan Zhang","Chuchu Fan","Christos Cassandras","Wenchao Li"],"pdf_url":"https://arxiv.org/pdf/2507.14850v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14537v4","updated":"2025-07-20T07:23:58Z","published":"2025-03-17T13:56:03Z","title":"Learning-based 3D Reconstruction in Autonomous Driving: A Comprehensive\n  Survey","summary":"  Learning-based 3D reconstruction has emerged as a transformative technique in\nautonomous driving, enabling precise modeling of both dynamic and static\nenvironments through advanced neural representations. Despite data\naugmentation, 3D reconstruction inspires pioneering solution for vital tasks in\nthe field of autonomous driving, such as scene understanding and closed-loop\nsimulation. We investigates the details of 3D reconstruction and conducts a\nmulti-perspective, in-depth analysis of recent advancements. Specifically, we\nfirst provide a systematic introduction of preliminaries, including data\nmodalities, benchmarks and technical preliminaries of learning-based 3D\nreconstruction, facilitating instant identification of suitable methods\naccording to sensor suites. Then, we systematically review learning-based 3D\nreconstruction methods in autonomous driving, categorizing approaches by\nsubtasks and conducting multi-dimensional analysis and summary to establish a\ncomprehensive technical reference. The development trends and existing\nchallenges are summarized in the context of learning-based 3D reconstruction in\nautonomous driving. We hope that our review will inspire future researches.\n","authors":["Liewen Liao","Weihao Yan","Ming Yang","Songan Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.14537v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01483v2","updated":"2025-07-20T07:13:34Z","published":"2024-01-03T01:15:55Z","title":"To Lead or to Follow? Adaptive Robot Task Planning in Human-Robot\n  Collaboration","summary":"  Adaptive task planning is fundamental to ensuring effective and seamless\nhuman-robot collaboration. This paper introduces a robot task planning\nframework that takes into account both human leading/following preferences and\nperformance, specifically focusing on task allocation and scheduling in\ncollaborative settings. We present a proactive task allocation approach with\nthree primary objectives: enhancing team performance, incorporating human\npreferences, and upholding a positive human perception of the robot and the\ncollaborative experience. Through a user study, involving an autonomous mobile\nmanipulator robot working alongside participants in a collaborative scenario,\nwe confirm that the task planning framework successfully attains all three\nintended goals, thereby contributing to the advancement of adaptive task\nplanning in human-robot collaboration. This paper mainly focuses on the first\ntwo objectives, and we discuss the third objective, participants' perception of\nthe robot, tasks, and collaboration in a companion paper.\n","authors":["Ali Noormohammadi-Asl","Stephen L. Smith","Kerstin Dautenhahn"],"pdf_url":"https://arxiv.org/pdf/2401.01483v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.06535v2","updated":"2025-07-20T06:51:18Z","published":"2025-06-06T21:06:00Z","title":"MapleGrasp: Mask-guided Feature Pooling for Language-driven Efficient\n  Robotic Grasping","summary":"  Robotic manipulation of unseen objects via natural language commands remains\nchallenging. Language driven robotic grasping (LDRG) predicts stable grasp\nposes from natural language queries and RGB-D images. We propose MapleGrasp, a\nnovel framework that leverages mask-guided feature pooling for efficient\nvision-language driven grasping. Our two-stage training first predicts\nsegmentation masks from CLIP-based vision-language features. The second stage\npools features within these masks to generate pixel-level grasp predictions,\nimproving efficiency, and reducing computation. Incorporating mask pooling\nresults in a 7% improvement over prior approaches on the OCID-VLG benchmark.\nFurthermore, we introduce RefGraspNet, an open-source dataset eight times\nlarger than existing alternatives, significantly enhancing model generalization\nfor open-vocabulary grasping. MapleGrasp scores a strong grasping accuracy of\n89\\% when compared with competing methods in the RefGraspNet benchmark. Our\nmethod achieves comparable performance to larger Vision-Language-Action models\non the LIBERO benchmark, and shows significantly better generalization to\nunseen tasks. Real-world experiments on a Franka arm demonstrate 73% success\nrate with unseen objects, surpassing competitive baselines by 11%. Code will be\nreleased after publication.\n","authors":["Vineet Bhat","Naman Patel","Prashanth Krishnamurthy","Ramesh Karri","Farshad Khorrami"],"pdf_url":"https://arxiv.org/pdf/2506.06535v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04020v3","updated":"2025-07-20T06:21:15Z","published":"2024-12-05T09:56:24Z","title":"How Cars Move: Analyzing Driving Dynamics for Safer Urban Traffic","summary":"  Understanding the spatial dynamics of cars within urban systems is essential\nfor optimizing infrastructure management and resource allocation. Recent\nempirical approaches for analyzing traffic patterns have gained traction due to\ntheir applicability to city-scale policy development. However, conventional\nmethodologies often rely on fragmented grid-based techniques, which may\noverlook critical interdependencies among spatial elements and temporal\ncontinuity. These limitations can compromise analytical effectiveness in\ncomplex urban environments. To address these challenges, we propose\nPriorMotion, a data integration framework designed to systematically uncover\nmovement patterns through driving dynamics analysis. Our approach combines\nmulti-scale empirical observations with customized analytical tools to capture\nevolving spatial-temporal trends in urban traffic. Comprehensive evaluations\ndemonstrate that PriorMotion significantly enhances analytical outcomes,\nincluding increased accuracy in traffic pattern analysis, improved adaptability\nto heterogeneous data environments, and reduced long-term projection errors.\nValidation confirms its effectiveness for urban infrastructure management\napplications requiring precise characterization of complex spatial-temporal\ninteractions.\n","authors":["Kangan Qian","Jinyu Miao","Xinyu Jiao","Ziang Luo","Zheng Fu","Yining Shi","Yunlong Wang","Kun Jiang","Diange Yang"],"pdf_url":"https://arxiv.org/pdf/2412.04020v3.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2507.08224v2","updated":"2025-07-20T05:22:14Z","published":"2025-07-11T00:17:08Z","title":"Making VLMs More Robot-Friendly: Self-Critical Distillation of Low-Level\n  Procedural Reasoning","summary":"  Large language models (LLMs) have shown promise in robotic procedural\nplanning, yet their human-centric reasoning often omits the low-level, grounded\ndetails needed for robotic execution. Vision-language models (VLMs) offer a\npath toward more perceptually grounded plans, but current methods either rely\non expensive, large-scale models or are constrained to narrow simulation\nsettings. We introduce SelfReVision, a lightweight and scalable\nself-improvement framework for vision-language procedural planning.\nSelfReVision enables small VLMs to iteratively critique, revise, and verify\ntheir own plans-without external supervision or teacher models-drawing\ninspiration from chain-of-thought prompting and self-instruct paradigms.\nThrough this self-distillation loop, models generate higher-quality,\nexecution-ready plans that can be used both at inference and for continued\nfine-tuning. Using models varying from 3B to 72B, our results show that\nSelfReVision not only boosts performance over weak base VLMs but also\noutperforms models 100X the size, yielding improved control in downstream\nembodied tasks.\n","authors":["Chan Young Park","Jillian Fisher","Marius Memmel","Dipika Khullar","Seoho Yun","Abhishek Gupta","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2507.08224v2.pdf","comment":"Code Available: https://github.com/chan0park/SelfReVision"},{"id":"http://arxiv.org/abs/2507.14820v1","updated":"2025-07-20T04:35:31Z","published":"2025-07-20T04:35:31Z","title":"KGN-Pro: Keypoint-Based Grasp Prediction through Probabilistic 2D-3D\n  Correspondence Learning","summary":"  High-level robotic manipulation tasks demand flexible 6-DoF grasp estimation\nto serve as a basic function. Previous approaches either directly generate\ngrasps from point-cloud data, suffering from challenges with small objects and\nsensor noise, or infer 3D information from RGB images, which introduces\nexpensive annotation requirements and discretization issues. Recent methods\nmitigate some challenges by retaining a 2D representation to estimate grasp\nkeypoints and applying Perspective-n-Point (PnP) algorithms to compute 6-DoF\nposes. However, these methods are limited by their non-differentiable nature\nand reliance solely on 2D supervision, which hinders the full exploitation of\nrich 3D information. In this work, we present KGN-Pro, a novel grasping network\nthat preserves the efficiency and fine-grained object grasping of previous KGNs\nwhile integrating direct 3D optimization through probabilistic PnP layers.\nKGN-Pro encodes paired RGB-D images to generate Keypoint Map, and further\noutputs a 2D confidence map to weight keypoint contributions during\nre-projection error minimization. By modeling the weighted sum of squared\nre-projection errors probabilistically, the network effectively transmits 3D\nsupervision to its 2D keypoint predictions, enabling end-to-end learning.\nExperiments on both simulated and real-world platforms demonstrate that KGN-Pro\noutperforms existing methods in terms of grasp cover rate and success rate.\n","authors":["Bingran Chen","Baorun Li","Jian Yang","Yong Liu","Guangyao Zhai"],"pdf_url":"https://arxiv.org/pdf/2507.14820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.14809v1","updated":"2025-07-20T03:57:18Z","published":"2025-07-20T03:57:18Z","title":"Light Future: Multimodal Action Frame Prediction via InstructPix2Pix","summary":"  Predicting future motion trajectories is a critical capability across domains\nsuch as robotics, autonomous systems, and human activity forecasting, enabling\nsafer and more intelligent decision-making. This paper proposes a novel,\nefficient, and lightweight approach for robot action prediction, offering\nsignificantly reduced computational cost and inference latency compared to\nconventional video prediction models. Importantly, it pioneers the adaptation\nof the InstructPix2Pix model for forecasting future visual frames in robotic\ntasks, extending its utility beyond static image editing. We implement a deep\nlearning-based visual prediction framework that forecasts what a robot will\nobserve 100 frames (10 seconds) into the future, given a current image and a\ntextual instruction. We repurpose and fine-tune the InstructPix2Pix model to\naccept both visual and textual inputs, enabling multimodal future frame\nprediction. Experiments on the RoboTWin dataset (generated based on real-world\nscenarios) demonstrate that our method achieves superior SSIM and PSNR compared\nto state-of-the-art baselines in robot action prediction tasks. Unlike\nconventional video prediction models that require multiple input frames, heavy\ncomputation, and slow inference latency, our approach only needs a single image\nand a text prompt as input. This lightweight design enables faster inference,\nreduced GPU demands, and flexible multimodal control, particularly valuable for\napplications like robotics and sports motion trajectory analytics, where motion\ntrajectory precision is prioritized over visual fidelity.\n","authors":["Zesen Zhong","Duomin Zhang","Yijia Li"],"pdf_url":"https://arxiv.org/pdf/2507.14809v1.pdf","comment":"9 pages including appendix, 5 tables, 8 figures, to be submitted to\n  WACV 2026"},{"id":"http://arxiv.org/abs/2403.09971v3","updated":"2025-07-20T03:04:27Z","published":"2024-03-15T02:28:26Z","title":"Advancing Object Goal Navigation Through LLM-enhanced Object Affinities\n  Transfer","summary":"  Object-goal navigation requires mobile robots to efficiently locate targets\nwith visual and spatial information, yet existing methods struggle with\ngeneralization in unseen environments. Heuristic approaches with naive metrics\nfail in complex layouts, while graph-based and learning-based methods suffer\nfrom environmental biases and limited generalization. Although Large Language\nModels (LLMs) as planners or agents offer a rich knowledge base, they are\ncost-inefficient and lack targeted historical experience. To address these\nchallenges, we propose the LLM-enhanced Object Affinities Transfer (LOAT)\nframework, integrating LLM-derived semantics with learning-based approaches to\nleverage experiential object affinities for better generalization in unseen\nsettings. LOAT employs a dual-module strategy: one module accesses LLMs' vast\nknowledge, and the other applies learned object semantic relationships,\ndynamically fusing these sources based on context. Evaluations in AI2-THOR and\nHabitat simulators show significant improvements in navigation success and\nefficiency, and real-world deployment demonstrates the zero-shot ability of\nLOAT to enhance object-goal navigation systems.\n","authors":["Mengying Lin","Shugao Liu","Dingxi Zhang","Yaran Chen","Zhaoran Wang","Haoran Li","Dongbin Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.09971v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.06513v4","updated":"2025-07-20T02:35:10Z","published":"2025-04-09T01:23:44Z","title":"Safe Navigation in Uncertain Crowded Environments Using Risk Adaptive\n  CVaR Barrier Functions","summary":"  Robot navigation in dynamic, crowded environments poses a significant\nchallenge due to the inherent uncertainties in the obstacle model. In this\nwork, we propose a risk-adaptive approach based on the Conditional\nValue-at-Risk Barrier Function (CVaR-BF), where the risk level is automatically\nadjusted to accept the minimum necessary risk, achieving a good performance in\nterms of safety and optimization feasibility under uncertainty. Additionally,\nwe introduce a dynamic zone-based barrier function which characterizes the\ncollision likelihood by evaluating the relative state between the robot and the\nobstacle. By integrating risk adaptation with this new function, our approach\nadaptively expands the safety margin, enabling the robot to proactively avoid\nobstacles in highly dynamic environments. Comparisons and ablation studies\ndemonstrate that our method outperforms existing social navigation approaches,\nand validate the effectiveness of our proposed framework.\n","authors":["Xinyi Wang","Taekyung Kim","Bardh Hoxha","Georgios Fainekos","Dimitra Panagou"],"pdf_url":"https://arxiv.org/pdf/2504.06513v4.pdf","comment":"2025 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS). Project page: {https://lawliet9666.github.io/cvarbf/}"}]},"2025-07-19T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2507.14731v1","updated":"2025-07-19T19:40:53Z","published":"2025-07-19T19:40:53Z","title":"X-Nav: Learning End-to-End Cross-Embodiment Navigation for Mobile Robots","summary":"  Existing navigation methods are primarily designed for specific robot\nembodiments, limiting their generalizability across diverse robot platforms. In\nthis paper, we introduce X-Nav, a novel framework for end-to-end\ncross-embodiment navigation where a single unified policy can be deployed\nacross various embodiments for both wheeled and quadrupedal robots. X-Nav\nconsists of two learning stages: 1) multiple expert policies are trained using\ndeep reinforcement learning with privileged observations on a wide range of\nrandomly generated robot embodiments; and 2) a single general policy is\ndistilled from the expert policies via navigation action chunking with\ntransformer (Nav-ACT). The general policy directly maps visual and\nproprioceptive observations to low-level control commands, enabling\ngeneralization to novel robot embodiments. Simulated experiments demonstrated\nthat X-Nav achieved zero-shot transfer to both unseen embodiments and\nphotorealistic environments. A scalability study showed that the performance of\nX-Nav improves when trained with an increasing number of randomly generated\nembodiments. An ablation study confirmed the design choices of X-Nav.\nFurthermore, real-world experiments were conducted to validate the\ngeneralizability of X-Nav in real-world environments.\n","authors":["Haitong Wang","Aaron Hao Tan","Angus Fung","Goldie Nejat"],"pdf_url":"https://arxiv.org/pdf/2507.14731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.14721v1","updated":"2025-07-19T18:49:47Z","published":"2025-07-19T18:49:47Z","title":"Leveraging Extrinsic Dexterity for Occluded Grasping on Grasp\n  Constraining Walls","summary":"  This study addresses the problem of occluded grasping, where primary grasp\nconfigurations of an object are not available due to occlusion with\nenvironment. Simple parallel grippers often struggle with such tasks due to\nlimited dexterity and actuation constraints. Prior works have explored object\npose reorientation such as pivoting by utilizing extrinsic contacts between an\nobject and an environment feature like a wall, to make the object graspable.\nHowever, such works often assume the presence of a short wall, and this\nassumption may not always hold in real-world scenarios. If the wall available\nfor interaction is too large or too tall, the robot may still fail to grasp the\nobject even after pivoting, and the robot must combine different types of\nactions to grasp. To address this, we propose a hierarchical reinforcement\nlearning (RL) framework. We use Q-learning to train a high-level policy that\nselects the type of action expected to yield the highest reward. The selected\nlow-level skill then samples a specific robot action in continuous space. To\nguide the robot to an appropriate location for executing the selected action,\nwe adopt a Conditional Variational Autoencoder (CVAE). We condition the CVAE on\nthe object point cloud and the skill ID, enabling it to infer a suitable\nlocation based on the object geometry and the selected skill. To promote\ngeneralization, we apply domain randomization during the training of low-level\nskills. The RL policy is trained entirely in simulation with a box-like object\nand deployed to six objects in real world. We conduct experiments to evaluate\nour method and demonstrate both its generalizability and robust sim-to-real\ntransfer performance with promising success rates.\n","authors":["Keita Kobashi","Masayoshi Tomizuka"],"pdf_url":"https://arxiv.org/pdf/2507.14721v1.pdf","comment":"7 pages, 7 figures"},{"id":"http://arxiv.org/abs/2409.10283v4","updated":"2025-07-19T18:48:48Z","published":"2024-09-16T13:44:50Z","title":"ASMA: An Adaptive Safety Margin Algorithm for Vision-Language Drone\n  Navigation via Scene-Aware Control Barrier Functions","summary":"  In the rapidly evolving field of vision-language navigation (VLN), ensuring\nsafety for physical agents remains an open challenge. For a human-in-the-loop\nlanguage-operated drone to navigate safely, it must understand natural language\ncommands, perceive the environment, and simultaneously avoid hazards in real\ntime. Control Barrier Functions (CBFs) are formal methods that enforce safe\noperating conditions. Model Predictive Control (MPC) is an optimization\nframework that plans a sequence of future actions over a prediction horizon,\nensuring smooth trajectory tracking while obeying constraints. In this work, we\nconsider a VLN-operated drone platform and enhance its safety by formulating a\nnovel scene-aware CBF that leverages ego-centric observations from a camera\nwhich has both Red-Green-Blue as well as Depth (RGB-D) channels. A CBF-less\nbaseline system uses a Vision-Language Encoder with cross-modal attention to\nconvert commands into an ordered sequence of landmarks. An object detection\nmodel identifies and verifies these landmarks in the captured images to\ngenerate a planned path. To further enhance safety, an Adaptive Safety Margin\nAlgorithm (ASMA) is proposed. ASMA tracks moving objects and performs\nscene-aware CBF evaluation on-the-fly, which serves as an additional constraint\nwithin the MPC framework. By continuously identifying potentially risky\nobservations, the system performs prediction in real time about unsafe\nconditions and proactively adjusts its control actions to maintain safe\nnavigation throughout the trajectory. Deployed on a Parrot Bebop2 quadrotor in\nthe Gazebo environment using the Robot Operating System (ROS), ASMA achieves\n64%-67% increase in success rates with only a slight increase (1.4%-5.8%) in\ntrajectory lengths compared to the baseline CBF-less VLN.\n","authors":["Sourav Sanyal","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2409.10283v4.pdf","comment":"Accepted for publication in IEEE Robotics and Automation Letters\n  (RA-L)"},{"id":"http://arxiv.org/abs/2506.18448v2","updated":"2025-07-19T18:26:00Z","published":"2025-06-23T09:34:50Z","title":"GraspMAS: Zero-Shot Language-driven Grasp Detection with Multi-Agent\n  System","summary":"  Language-driven grasp detection has the potential to revolutionize\nhuman-robot interaction by allowing robots to understand and execute grasping\ntasks based on natural language commands. However, existing approaches face two\nkey challenges. First, they often struggle to interpret complex text\ninstructions or operate ineffectively in densely cluttered environments.\nSecond, most methods require a training or finetuning step to adapt to new\ndomains, limiting their generation in real-world applications. In this paper,\nwe introduce GraspMAS, a new multi-agent system framework for language-driven\ngrasp detection. GraspMAS is designed to reason through ambiguities and improve\ndecision-making in real-world scenarios. Our framework consists of three\nspecialized agents: Planner, responsible for strategizing complex queries;\nCoder, which generates and executes source code; and Observer, which evaluates\nthe outcomes and provides feedback. Intensive experiments on two large-scale\ndatasets demonstrate that our GraspMAS significantly outperforms existing\nbaselines. Additionally, robot experiments conducted in both simulation and\nreal-world settings further validate the effectiveness of our approach. Our\nproject page is available at https://zquang2202.github.io/GraspMAS\n","authors":["Quang Nguyen","Tri Le","Huy Nguyen","Thieu Vo","Tung D. Ta","Baoru Huang","Minh N. Vu","Anh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2506.18448v2.pdf","comment":"Accepted to IROS 2025. Webpage:\n  https://zquang2202.github.io/GraspMAS/"},{"id":"http://arxiv.org/abs/2507.16842v1","updated":"2025-07-19T18:04:12Z","published":"2025-07-19T18:04:12Z","title":"Sensor-Space Based Robust Kinematic Control of Redundant Soft\n  Manipulator by Learning","summary":"  The intrinsic compliance and high degree of freedom (DoF) of redundant soft\nmanipulators facilitate safe interaction and flexible task execution. However,\neffective kinematic control remains highly challenging, as it must handle\ndeformations caused by unknown external loads and avoid actuator saturation due\nto improper null-space regulation - particularly in confined environments. In\nthis paper, we propose a Sensor-Space Imitation Learning Kinematic Control\n(SS-ILKC) framework to enable robust kinematic control under actuator\nsaturation and restrictive environmental constraints. We employ a dual-learning\nstrategy: a multi-goal sensor-space control framework based on reinforcement\nlearning principle is trained in simulation to develop robust control policies\nfor open spaces, while a generative adversarial imitation learning approach\nenables effective policy learning from sparse expert demonstrations for\nconfined spaces. To enable zero-shot real-world deployment, a pre-processed\nsim-to-real transfer mechanism is proposed to mitigate the\nsimulation-to-reality gap and accurately characterize actuator saturation\nlimits. Experimental results demonstrate that our method can effectively\ncontrol a pneumatically actuated soft manipulator, achieving precise\npath-following and object manipulation in confined environments under unknown\nloading conditions.\n","authors":["Yinan Meng","Kun Qian","Jiong Yang","Renbo Su","Zhenhong Li","Charlie C. L. Wang"],"pdf_url":"https://arxiv.org/pdf/2507.16842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.20435v3","updated":"2025-07-19T17:51:48Z","published":"2024-09-30T15:53:46Z","title":"ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\n  Detection During Robotic Proximity Operations in Lunar Orbit","summary":"  NASA's forthcoming Lunar Gateway space station, which will be uncrewed most\nof the time, will need to operate with an unprecedented level of autonomy.\nEnhancing autonomy on the Gateway presents several unique challenges, one of\nwhich is to equip the Canadarm3, the Gateway's external robotic system, with\nthe capability to perform worksite monitoring. Monitoring will involve using\nthe arm's inspection cameras to detect any anomalies within the operating\nenvironment, a task complicated by the widely-varying lighting conditions in\nspace. In this paper, we introduce the visual anomaly detection and\nlocalization task for space applications and establish a benchmark with our\nnovel synthetic dataset called ALLO (for Anomaly Localization in Lunar Orbit).\nWe develop a complete data generation pipeline to create ALLO, which we use to\nevaluate the performance of state-of-the-art visual anomaly detection\nalgorithms. Given the low tolerance for risk during space operations and the\nlack of relevant data, we emphasize the need for novel, robust, and accurate\nanomaly detection methods to handle the challenging visual conditions found in\nlunar orbit and beyond.\n","authors":["Selina Leveugle","Chang Won Lee","Svetlana Stolpner","Chris Langley","Paul Grouchy","Steven Waslander","Jonathan Kelly"],"pdf_url":"https://arxiv.org/pdf/2409.20435v3.pdf","comment":"Submitted to International Conference on Robotics and Automation\n  (ICRA'25), Atlanta, USA, May 19-23, 2025"},{"id":"http://arxiv.org/abs/2507.16841v1","updated":"2025-07-19T17:39:12Z","published":"2025-07-19T17:39:12Z","title":"AquaChat: An LLM-Guided ROV Framework for Adaptive Inspection of\n  Aquaculture Net Pens","summary":"  Inspection of aquaculture net pens is essential for maintaining the\nstructural integrity, biosecurity, and operational efficiency of fish farming\nsystems. Traditional inspection approaches rely on pre-programmed missions or\nmanual control, offering limited adaptability to dynamic underwater conditions\nand user-specific demands. In this study, we propose AquaChat, a novel Remotely\nOperated Vehicle (ROV) framework that integrates Large Language Models (LLMs)\nfor intelligent and adaptive net pen inspection. The system features a\nmulti-layered architecture: (1) a high-level planning layer that interprets\nnatural language user commands using an LLM to generate symbolic task plans;\n(2) a mid-level task manager that translates plans into ROV control sequences;\nand (3) a low-level motion control layer that executes navigation and\ninspection tasks with precision. Real-time feedback and event-triggered\nreplanning enhance robustness in challenging aquaculture environments. The\nframework is validated through experiments in both simulated and controlled\naquatic environments representative of aquaculture net pens. Results\ndemonstrate improved task flexibility, inspection accuracy, and operational\nefficiency. AquaChat illustrates the potential of integrating language-based AI\nwith marine robotics to enable intelligent, user-interactive inspection systems\nfor sustainable aquaculture operations.\n","authors":["Waseem Akram","Muhayy Ud Din","Abdelhaleem Saad","Irfan Hussain"],"pdf_url":"https://arxiv.org/pdf/2507.16841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.14700v1","updated":"2025-07-19T17:26:16Z","published":"2025-07-19T17:26:16Z","title":"Corridor-based Adaptive Control Barrier and Lyapunov Functions for Safe\n  Mobile Robot Navigation","summary":"  Safe navigation in unknown and cluttered environments remains a challenging\nproblem in robotics. Model Predictive Contour Control (MPCC) has shown promise\nfor performant obstacle avoidance by enabling precise and agile trajectory\ntracking, however, existing methods lack formal safety assurances. To address\nthis issue, we propose a general Control Lyapunov Function (CLF) and Control\nBarrier Function (CBF) enabled MPCC framework that enforces safety constraints\nderived from a free-space corridor around the planned trajectory. To enhance\nfeasibility, we dynamically adapt the CBF parameters at runtime using a Soft\nActor-Critic (SAC) policy. The approach is validated with extensive simulations\nand an experiment on mobile robot navigation in unknown cluttered environments.\n","authors":["Nicholas Mohammad","Nicola Bezzo"],"pdf_url":"https://arxiv.org/pdf/2507.14700v1.pdf","comment":"To be presented in the 64th IEEE Conference on Decision and Control\n  (CDC 25)"},{"id":"http://arxiv.org/abs/2507.09309v2","updated":"2025-07-19T17:13:29Z","published":"2025-07-12T14:54:46Z","title":"Informed Hybrid Zonotope-based Motion Planning Algorithm","summary":"  Optimal path planning in nonconvex free spaces is notoriously challenging, as\nformulating such problems as mixed-integer linear programs (MILPs) is NP-hard.\nWe propose HZ-MP, an informed Hybrid Zonotope-based Motion Planner, as an\nalternative approach that decomposes the obstacle-free space and performs\nlow-dimensional face sampling guided by an ellipsotope heuristic, enabling\nfocused exploration along promising transit regions. This structured\nexploration eliminates the excessive, unreachable sampling that degrades\nexisting informed planners such as AIT* and EIT* in narrow gaps or boxed-goal\nscenarios. We prove that HZ-MP is probabilistically complete and asymptotically\noptimal. It converges to near-optimal trajectories in finite time and scales to\nhigh-dimensional cluttered scenes.\n","authors":["Peng Xie","Johannes Betz","Amr Alanwar"],"pdf_url":"https://arxiv.org/pdf/2507.09309v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.14694v1","updated":"2025-07-19T17:02:07Z","published":"2025-07-19T17:02:07Z","title":"Uncertainty-aware Probabilistic 3D Human Motion Forecasting via\n  Invertible Networks","summary":"  3D human motion forecasting aims to enable autonomous applications.\nEstimating uncertainty for each prediction (i.e., confidence based on\nprobability density or quantile) is essential for safety-critical contexts like\nhuman-robot collaboration to minimize risks. However, existing diverse motion\nforecasting approaches struggle with uncertainty quantification due to implicit\nprobabilistic representations hindering uncertainty modeling. We propose\nProbHMI, which introduces invertible networks to parameterize poses in a\ndisentangled latent space, enabling probabilistic dynamics modeling. A\nforecasting module then explicitly predicts future latent distributions,\nallowing effective uncertainty quantification. Evaluated on benchmarks, ProbHMI\nachieves strong performance for both deterministic and diverse prediction while\nvalidating uncertainty calibration, critical for risk-aware decision making.\n","authors":["Yue Ma","Kanglei Zhou","Fuyang Yu","Frederick W. B. Li","Xiaohui Liang"],"pdf_url":"https://arxiv.org/pdf/2507.14694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17249v6","updated":"2025-07-19T16:37:29Z","published":"2024-06-25T03:34:02Z","title":"SlideSLAM: Sparse, Lightweight, Decentralized Metric-Semantic SLAM for\n  Multi-Robot Navigation","summary":"  This paper develops a real-time decentralized metric-semantic SLAM algorithm\nthat enables a heterogeneous robot team to collaboratively construct\nobject-based metric-semantic maps. The proposed framework integrates a\ndata-driven front-end for instance segmentation from either RGBD cameras or\nLiDARs and a custom back-end for optimizing robot trajectories and object\nlandmarks in the map. To allow multiple robots to merge their information, we\ndesign semantics-driven place recognition algorithms that leverage the\ninformativeness and viewpoint invariance of the object-level metric-semantic\nmap for inter-robot loop closure detection. A communication module is designed\nto track each robot's observations and those of other robots whenever\ncommunication links are available. The framework supports real-time,\ndecentralized operation onboard the robots and has been integrated with three\ntypes of aerial and ground platforms. We validate its effectiveness through\nexperiments in both indoor and outdoor environments, as well as benchmarks on\npublic datasets and comparisons with existing methods. The framework is\nopen-sourced and suitable for both single-agent and multi-robot real-time\nmetric-semantic SLAM applications. The code is available at:\nhttps://github.com/KumarRobotics/SLIDE_SLAM.\n","authors":["Xu Liu","Jiuzhou Lei","Ankit Prabhu","Yuezhan Tao","Igor Spasojevic","Pratik Chaudhari","Nikolay Atanasov","Vijay Kumar"],"pdf_url":"https://arxiv.org/pdf/2406.17249v6.pdf","comment":"Xu Liu, Jiuzhou Lei, and Ankit Prabhu contributed equally to this\n  work"},{"id":"http://arxiv.org/abs/2503.02600v2","updated":"2025-07-19T15:21:11Z","published":"2025-03-04T13:20:42Z","title":"Resource-Efficient Affordance Grounding with Complementary Depth and\n  Semantic Prompts","summary":"  Affordance refers to the functional properties that an agent perceives and\nutilizes from its environment, and is key perceptual information required for\nrobots to perform actions. This information is rich and multimodal in nature.\nExisting multimodal affordance methods face limitations in extracting useful\ninformation, mainly due to simple structural designs, basic fusion methods, and\nlarge model parameters, making it difficult to meet the performance\nrequirements for practical deployment. To address these issues, this paper\nproposes the BiT-Align image-depth-text affordance mapping framework. The\nframework includes a Bypass Prompt Module (BPM) and a Text Feature Guidance\n(TFG) attention selection mechanism. BPM integrates the auxiliary modality\ndepth image directly as a prompt to the primary modality RGB image, embedding\nit into the primary modality encoder without introducing additional encoders.\nThis reduces the model's parameter count and effectively improves functional\nregion localization accuracy. The TFG mechanism guides the selection and\nenhancement of attention heads in the image encoder using textual features,\nimproving the understanding of affordance characteristics. Experimental results\ndemonstrate that the proposed method achieves significant performance\nimprovements on public AGD20K and HICO-IIF datasets. On the AGD20K dataset,\ncompared with the current state-of-the-art method, we achieve a 6.0%\nimprovement in the KLD metric, while reducing model parameters by 88.8%,\ndemonstrating practical application values. The source code will be made\npublicly available at https://github.com/DAWDSE/BiT-Align.\n","authors":["Yizhou Huang","Fan Yang","Guoliang Zhu","Gen Li","Hao Shi","Yukun Zuo","Wenrui Chen","Zhiyong Li","Kailun Yang"],"pdf_url":"https://arxiv.org/pdf/2503.02600v2.pdf","comment":"Accepted to IROS 2025. The source code will be made publicly\n  available at https://github.com/DAWDSE/BiT-Align"},{"id":"http://arxiv.org/abs/2407.00614v2","updated":"2025-07-19T14:17:38Z","published":"2024-06-30T07:42:57Z","title":"Learning Granularity-Aware Affordances from Human-Object Interaction for\n  Tool-Based Functional Dexterous Grasping","summary":"  To enable robots to use tools, the initial step is teaching robots to employ\ndexterous gestures for touching specific areas precisely where tasks are\nperformed. Affordance features of objects serve as a bridge in the functional\ninteraction between agents and objects. However, leveraging these affordance\ncues to help robots achieve functional tool grasping remains unresolved. To\naddress this, we propose a granularity-aware affordance feature extraction\nmethod for locating functional affordance areas and predicting dexterous coarse\ngestures. We study the intrinsic mechanisms of human tool use. On one hand, we\nuse fine-grained affordance features of object-functional finger contact areas\nto locate functional affordance regions. On the other hand, we use highly\nactivated coarse-grained affordance features in hand-object interaction regions\nto predict grasp gestures. Additionally, we introduce a model-based\npost-processing module that transforms affordance localization and gesture\nprediction into executable robotic actions. This forms GAAF-Dex, a complete\nframework that learns Granularity-Aware Affordances from human-object\ninteraction to enable tool-based functional grasping with dexterous hands.\nUnlike fully-supervised methods that require extensive data annotation, we\nemploy a weakly supervised approach to extract relevant cues from exocentric\n(Exo) images of hand-object interactions to supervise feature extraction in\negocentric (Ego) images. To support this approach, we have constructed a\nsmall-scale dataset, Functional Affordance Hand-object Interaction Dataset\n(FAH), which includes nearly 6K images of functional hand-object interaction\nExo images and Ego images. Extensive experiments on the dataset demonstrate\nthat our method outperforms state-of-the-art methods. The source code and the\nestablished dataset are available at https://github.com/yangfan293/GAAF-DEX.\n","authors":["Fan Yang","Wenrui Chen","Kailun Yang","Haoran Lin","Dongsheng Luo","Conghui Tang","Zhiyong Li","Yaonan Wang"],"pdf_url":"https://arxiv.org/pdf/2407.00614v2.pdf","comment":"Accepted to IEEE Transactions on Neural Networks and Learning Systems\n  (TNNLS). The source code and the established dataset are available at\n  https://github.com/yangfan293/GAAF-DEX"},{"id":"http://arxiv.org/abs/2507.14605v1","updated":"2025-07-19T13:06:39Z","published":"2025-07-19T13:06:39Z","title":"Koopman Operator Based Linear Model Predictive Control for 2D Quadruped\n  Trotting, Bounding, and Gait Transition","summary":"  Online optimal control of quadrupedal robots would enable them to plan their\nmovement in novel scenarios. Linear Model Predictive Control (LMPC) has emerged\nas a practical approach for real-time control. In LMPC, an optimization problem\nwith a quadratic cost and linear constraints is formulated over a finite\nhorizon and solved on the fly. However, LMPC relies on linearizing the\nequations of motion (EOM), which may lead to poor solution quality. In this\npaper, we use Koopman operator theory and the Extended Dynamic Mode\nDecomposition (EDMD) to create a linear model of the system in high dimensional\nspace, thus retaining the nonlinearity of the EOM. We model the aerial phase\nand ground contact phases using different linear models. Then, using LMPC, we\ndemonstrate bounding, trotting, and bound-to-trot and trot-to-bound gait\ntransitions in level and rough terrains. The main novelty is the use of Koopman\noperator theory to create hybrid models of a quadrupedal system and demonstrate\nthe online generation of multiple gaits and gaits transitions.\n","authors":["Chun-Ming Yang","Pranav A. Bhounsule"],"pdf_url":"https://arxiv.org/pdf/2507.14605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.14582v1","updated":"2025-07-19T11:53:24Z","published":"2025-07-19T11:53:24Z","title":"BT-TL-DMPs: A Novel Robot TAMP Framework Combining Behavior Tree,\n  Temporal Logic and Dynamical Movement Primitives","summary":"  In the field of Learning from Demonstration (LfD), enabling robots to\ngeneralize learned manipulation skills to novel scenarios for long-horizon\ntasks remains challenging. Specifically, it is still difficult for robots to\nadapt the learned skills to new environments with different task and motion\nrequirements, especially in long-horizon, multi-stage scenarios with intricate\nconstraints. This paper proposes a novel hierarchical framework, called\nBT-TL-DMPs, that integrates Behavior Tree (BT), Temporal Logic (TL), and\nDynamical Movement Primitives (DMPs) to address this problem. Within this\nframework, Signal Temporal Logic (STL) is employed to formally specify complex,\nlong-horizon task requirements and constraints. These STL specifications are\nsystematically transformed to generate reactive and modular BTs for high-level\ndecision-making task structure. An STL-constrained DMP optimization method is\nproposed to optimize the DMP forcing term, allowing the learned motion\nprimitives to adapt flexibly while satisfying intricate spatiotemporal\nrequirements and, crucially, preserving the essential dynamics learned from\ndemonstrations. The framework is validated through simulations demonstrating\ngeneralization capabilities under various STL constraints and real-world\nexperiments on several long-horizon robotic manipulation tasks. The results\ndemonstrate that the proposed framework effectively bridges the symbolic-motion\ngap, enabling more reliable and generalizable autonomous manipulation for\ncomplex robotic tasks.\n","authors":["Zezhi Liu","Shizhen Wu","Hanqian Luo","Deyun Qin","Yongchun Fang"],"pdf_url":"https://arxiv.org/pdf/2507.14582v1.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2503.12419v3","updated":"2025-07-19T09:02:46Z","published":"2025-03-16T09:08:02Z","title":"EgoEvGesture: Gesture Recognition Based on Egocentric Event Camera","summary":"  Egocentric gesture recognition is a pivotal technology for enhancing natural\nhuman-computer interaction, yet traditional RGB-based solutions suffer from\nmotion blur and illumination variations in dynamic scenarios. While event\ncameras show distinct advantages in handling high dynamic range with ultra-low\npower consumption, existing RGB-based architectures face inherent limitations\nin processing asynchronous event streams due to their synchronous frame-based\nnature. Moreover, from an egocentric perspective, event cameras record data\nthat includes events generated by both head movements and hand gestures,\nthereby increasing the complexity of gesture recognition. To address this, we\npropose a novel network architecture specifically designed for event data\nprocessing, incorporating (1) a lightweight CNN with asymmetric depthwise\nconvolutions to reduce parameters while preserving spatiotemporal features, (2)\na plug-and-play state-space model as context block that decouples head movement\nnoise from gesture dynamics, and (3) a parameter-free Bins-Temporal Shift\nModule (BTSM) that shifts features along bins and temporal dimensions to fuse\nsparse events efficiently. We further establish the EgoEvGesture dataset, the\nfirst large-scale dataset for egocentric gesture recognition using event\ncameras. Experimental results demonstrate that our method achieves 62.7%\naccuracy tested on unseen subjects with only 7M parameters, 3.1% higher than\nstate-of-the-art approaches. Notable misclassifications in freestyle motions\nstem from high inter-personal variability and unseen test patterns differing\nfrom training data. Moreover, our approach achieved a remarkable accuracy of\n97.0% on the DVS128 Gesture, demonstrating the effectiveness and generalization\ncapability of our method on public datasets. The dataset and models are made\navailable at https://github.com/3190105222/EgoEv_Gesture.\n","authors":["Luming Wang","Hao Shi","Xiaoting Yin","Kailun Yang","Kaiwei Wang","Jian Bai"],"pdf_url":"https://arxiv.org/pdf/2503.12419v3.pdf","comment":"Accepted to SMC 2025. The dataset and models are made available at\n  https://github.com/3190105222/EgoEv_Gesture"},{"id":"http://arxiv.org/abs/2507.14538v1","updated":"2025-07-19T08:51:59Z","published":"2025-07-19T08:51:59Z","title":"A 21-DOF Humanoid Dexterous Hand with Hybrid SMA-Motor Actuation: CYJ\n  Hand-0","summary":"  CYJ Hand-0 is a 21-DOF humanoid dexterous hand featuring a hybrid\ntendon-driven actuation system that combines shape memory alloys (SMAs) and DC\nmotors. The hand employs high-strength fishing line as artificial tendons and\nuses a fully 3D-printed AlSi10Mg metal frame designed to replicate the skeletal\nand tendon-muscle structure of the human hand. A linear motor-driven module\ncontrols finger flexion, while an SMA-based module enables finger extension and\nlateral abduction. These modules are integrated into a compact hybrid actuation\nunit mounted on a custom rear support structure. Mechanical and kinematic\nexperiments, conducted under an Arduino Mega 2560-based control system,\nvalidate the effectiveness of the design and demonstrate its biomimetic\ndexterity.\n","authors":["Jin Chai","Xiang Yao","Mengfan Hou","Yanghong Li","Erbao Dong"],"pdf_url":"https://arxiv.org/pdf/2507.14538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.14500v1","updated":"2025-07-19T06:11:09Z","published":"2025-07-19T06:11:09Z","title":"Motion Segmentation and Egomotion Estimation from Event-Based Normal\n  Flow","summary":"  This paper introduces a robust framework for motion segmentation and\negomotion estimation using event-based normal flow, tailored specifically for\nneuromorphic vision sensors. In contrast to traditional methods that rely\nheavily on optical flow or explicit depth estimation, our approach exploits the\nsparse, high-temporal-resolution event data and incorporates geometric\nconstraints between normal flow, scene structure, and inertial measurements.\nThe proposed optimization-based pipeline iteratively performs event\nover-segmentation, isolates independently moving objects via residual analysis,\nand refines segmentations using hierarchical clustering informed by motion\nsimilarity and temporal consistency. Experimental results on the EVIMO2v2\ndataset validate that our method achieves accurate segmentation and\ntranslational motion estimation without requiring full optical flow\ncomputation. This approach demonstrates significant advantages at object\nboundaries and offers considerable potential for scalable, real-time robotic\nand navigation applications.\n","authors":["Zhiyuan Hua","Dehao Yuan","Cornelia Fermüller"],"pdf_url":"https://arxiv.org/pdf/2507.14500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02247v5","updated":"2025-07-19T03:44:28Z","published":"2025-03-04T03:51:36Z","title":"WMNav: Integrating Vision-Language Models into World Models for Object\n  Goal Navigation","summary":"  Object Goal Navigation-requiring an agent to locate a specific object in an\nunseen environment-remains a core challenge in embodied AI. Although recent\nprogress in Vision-Language Model (VLM)-based agents has demonstrated promising\nperception and decision-making abilities through prompting, none has yet\nestablished a fully modular world model design that reduces risky and costly\ninteractions with the environment by predicting the future state of the world.\nWe introduce WMNav, a novel World Model-based Navigation framework powered by\nVision-Language Models (VLMs). It predicts possible outcomes of decisions and\nbuilds memories to provide feedback to the policy module. To retain the\npredicted state of the environment, WMNav proposes the online maintained\nCuriosity Value Map as part of the world model memory to provide dynamic\nconfiguration for navigation policy. By decomposing according to a human-like\nthinking process, WMNav effectively alleviates the impact of model\nhallucination by making decisions based on the feedback difference between the\nworld model plan and observation. To further boost efficiency, we implement a\ntwo-stage action proposer strategy: broad exploration followed by precise\nlocalization. Extensive evaluation on HM3D and MP3D validates WMNav surpasses\nexisting zero-shot benchmarks in both success rate and exploration efficiency\n(absolute improvement: +3.2% SR and +3.2% SPL on HM3D, +13.5% SR and +1.1% SPL\non MP3D). Project page: https://b0b8k1ng.github.io/WMNav/.\n","authors":["Dujun Nie","Xianda Guo","Yiqun Duan","Ruijun Zhang","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2503.02247v5.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2409.08493v3","updated":"2025-07-19T03:34:13Z","published":"2024-09-13T02:37:28Z","title":"Intelligent LiDAR Navigation: Leveraging External Information and\n  Semantic Maps with LLM as Copilot","summary":"  Traditional robot navigation systems primarily utilize occupancy grid maps\nand laser-based sensing technologies, as demonstrated by the popular move_base\npackage in ROS. Unlike robots, humans navigate not only through spatial\nawareness and physical distances but also by integrating external information,\nsuch as elevator maintenance updates from public notification boards and\nexperiential knowledge, like the need for special access through certain doors.\nWith the development of Large Language Models (LLMs), which possesses text\nunderstanding and intelligence close to human performance, there is now an\nopportunity to infuse robot navigation systems with a level of understanding\nakin to human cognition. In this study, we propose using osmAG (Area Graph in\nOpensStreetMap textual format), an innovative semantic topometric hierarchical\nmap representation, to bridge the gap between the capabilities of ROS move_base\nand the contextual understanding offered by LLMs. Our methodology employs LLMs\nas an actual copilot in robot navigation, enabling the integration of a broader\nrange of informational inputs while maintaining the robustness of traditional\nrobotic navigation systems. Our code, demo, map, experiment results can be\naccessed at\nhttps://github.com/xiexiexiaoxiexie/Intelligent-LiDAR-Navigation-LLM-as-Copilot.\n","authors":["Fujing Xie","Jiajie Zhang","Sören Schwertfeger"],"pdf_url":"https://arxiv.org/pdf/2409.08493v3.pdf","comment":"Accepted at IROS 2025"},{"id":"http://arxiv.org/abs/2505.03233v2","updated":"2025-07-19T03:33:23Z","published":"2025-05-06T06:59:28Z","title":"GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale\n  Synthetic Action Data","summary":"  Embodied foundation models are gaining increasing attention for their\nzero-shot generalization, scalability, and adaptability to new tasks through\nfew-shot post-training. However, existing models rely heavily on real-world\ndata, which is costly and labor-intensive to collect. Synthetic data offers a\ncost-effective alternative, yet its potential remains largely underexplored. To\nbridge this gap, we explore the feasibility of training Vision-Language-Action\nmodels entirely with large-scale synthetic action data. We curate SynGrasp-1B,\na billion-frame robotic grasping dataset generated in simulation with\nphotorealistic rendering and extensive domain randomization. Building on this,\nwe present GraspVLA, a VLA model pretrained on large-scale synthetic action\ndata as a foundational model for grasping tasks. GraspVLA integrates\nautoregressive perception tasks and flow-matching-based action generation into\na unified Chain-of-Thought process, enabling joint training on synthetic action\ndata and Internet semantics data. This design helps mitigate sim-to-real gaps\nand facilitates the transfer of learned actions to a broader range of\nInternet-covered objects, achieving open-vocabulary generalization in grasping.\nExtensive evaluations across real-world and simulation benchmarks demonstrate\nGraspVLA's advanced zero-shot generalizability and few-shot adaptability to\nspecific human preferences. We will release SynGrasp-1B dataset and pre-trained\nweights to benefit the community.\n","authors":["Shengliang Deng","Mi Yan","Songlin Wei","Haixin Ma","Yuxin Yang","Jiayi Chen","Zhiqi Zhang","Taoyu Yang","Xuheng Zhang","Heming Cui","Zhizheng Zhang","He Wang"],"pdf_url":"https://arxiv.org/pdf/2505.03233v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.14455v1","updated":"2025-07-19T03:00:52Z","published":"2025-07-19T03:00:52Z","title":"Koopman Operator Based Time-Delay Embeddings and State History Augmented\n  LQR for Periodic Hybrid Systems: Bouncing Pendulum and Bipedal Walking","summary":"  Time-delay embedding is a technique that uses snapshots of state history over\ntime to build a linear state space model of a nonlinear smooth system. We\ndemonstrate that periodic non-smooth or hybrid system can also be modeled as a\nlinear state space system using this approach as long as its behavior is\nconsistent in modes and timings. We extended time-delay embeddings to generate\na linear model of two periodic hybrid systems: the bouncing pendulum and the\nsimplest walker with control inputs. This leads to a novel state history\naugmented linear quadratic regulator (LQR) which uses current and past state\nhistory for feedback control.\n","authors":["Chun-Ming Yang","Pranav A. Bhounsule"],"pdf_url":"https://arxiv.org/pdf/2507.14455v1.pdf","comment":null}]},"2025-07-18T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2507.14412v1","updated":"2025-07-18T23:36:06Z","published":"2025-07-18T23:36:06Z","title":"Personalized Socially Assistive Robots With End-to-End Speech-Language\n  Models For Well-Being Support","summary":"  Socially assistive robots (SARs) have shown great potential for supplementing\nwell-being support. However, prior studies have found that existing dialogue\npipelines for SARs remain limited in real-time latency, back-channeling, and\npersonalized speech dialogue. Toward addressing these limitations, we propose\nusing integrated end-to-end speech-language models (SLMs) with SARs. This work\n1) evaluated the usability of an SLM-enabled SAR dialogue system through a\nsmall user study, and 2) identified remaining limitations through study user\nfeedback to inform future improvements. We conducted a small within-participant\nuser study with university students (N = 11) whose results showed that\nparticipants perceived an SLM-enabled SAR system as capable of providing\nempathetic feedback, natural turn-taking, back-channeling, and adaptive\nresponses. We also found that participants reported the robot's nonverbal\nbehaviors as lacking variability and synchronization with conversation, and the\nSLM's verbal feedback as generic and repetitive. These findings highlighted the\nneed for real-time robot movement synchronized with conversation, improved\nprompting or fine-tuning to generate outputs better aligned with mental health\npractices, and more expressive, adaptive vocal generation.\n","authors":["Mengxue Fu","Zhonghao Shi","Minyu Huang","Siqi Liu","Mina Kian","Yirui Song","Maja J. Matarić"],"pdf_url":"https://arxiv.org/pdf/2507.14412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00306v3","updated":"2025-07-18T22:46:57Z","published":"2025-05-01T04:58:50Z","title":"J-PARSE: Jacobian-based Projection Algorithm for Resolving Singularities\n  Effectively in Inverse Kinematic Control of Serial Manipulators","summary":"  J-PARSE is a method for smooth first-order inverse kinematic control of a\nserial manipulator near kinematic singularities. The commanded end-effector\nvelocity is interpreted component-wise, according to the available mobility in\neach dimension of the task space. First, a substitute \"Safety\" Jacobian matrix\nis created, keeping the aspect ratio of the manipulability ellipsoid above a\nthreshold value. The desired motion is then projected onto non-singular and\nsingular directions, and the latter projection scaled down by a factor informed\nby the threshold value. A right-inverse of the non-singular Safety Jacobian is\napplied to the modified command. In the absence of joint limits and collisions,\nthis ensures smooth transition into and out of low-rank poses, guaranteeing\nasymptotic stability for target poses within the workspace, and stability for\nthose outside. Velocity control with J-PARSE is benchmarked against the\nLeast-Squares and Damped Least-Squares inversions of the Jacobian, and shows\nhigh accuracy in reaching and leaving singular target poses. By expanding the\navailable workspace of manipulators, the method finds applications in servoing,\nteleoperation, and learning. Videos and code are available at\nhttps://jparse-manip.github.io/.\n","authors":["Shivani Guptasarma","Matthew Strong","Honghao Zhen","Monroe Kennedy III"],"pdf_url":"https://arxiv.org/pdf/2505.00306v3.pdf","comment":"18 pages, 25 figures. v1: Fig. 1 replaced with faster-loading\n  version. v2: Website at https://jparse-manip.github.io/"},{"id":"http://arxiv.org/abs/2408.16370v5","updated":"2025-07-18T22:32:58Z","published":"2024-08-29T09:28:05Z","title":"LSTP-Nav: Lightweight Spatiotemporal Policy for Map-free Multi-agent\n  Navigation with LiDAR","summary":"  Safe and efficient multi-agent navigation in dynamic environments remains\ninherently challenging, particularly when real-time decision-making is required\non resource-constrained platforms. Ensuring collision-free trajectories while\nadapting to uncertainties without relying on pre-built maps further complicates\nreal-world deployment. To address these challenges, we propose LSTP-Nav, a\nlightweight end-to-end policy for multi-agent navigation that enables map-free\ncollision avoidance in complex environments by directly mapping raw LiDAR point\nclouds to motion commands. At the core of this framework lies LSTP-Net, an\nefficient network that processes raw LiDAR data using a GRU architecture,\nenhanced with attention mechanisms to dynamically focus on critical\nenvironmental features while minimizing computational overhead. Additionally, a\nnovel HS reward optimizes collision avoidance by incorporating angular\nvelocity, prioritizing obstacles along the predicted heading, and enhancing\ntraining stability. To narrow the sim-to-real gap, we develop\nPhysReplay-Simlab, a physics-realistic multi-agent simulator, employs localized\nreplay to mine near-failure experiences. Relying solely on LiDA, LSTP-Nav\nachieves efficient zero-shot sim-to-real transfer on a CPU-only robotic\nplatform, enabling robust navigation in dynamic environments while maintaining\ncomputation frequencies above 40 Hz. Extensive experiments demonstrate that\nLSTP-Nav outperforms baselines with a 9.58% higher success rate and a 12.30%\nlower collision rate, underscoring its practicality and robustness for\nreal-world applications.\n","authors":["Xingrong Diao","Zhirui Sun","Jianwei Peng","Jiankun Wang"],"pdf_url":"https://arxiv.org/pdf/2408.16370v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10559v2","updated":"2025-07-18T20:14:08Z","published":"2024-03-14T06:51:26Z","title":"Generative Models and Connected and Automated Vehicles: A Survey in\n  Exploring the Intersection of Transportation and AI","summary":"  This report investigates the history and impact of Generative Models and\nConnected and Automated Vehicles (CAVs), two groundbreaking forces pushing\nprogress in technology and transportation. By focusing on the application of\ngenerative models within the context of CAVs, the study aims to unravel how\nthis integration could enhance predictive modeling, simulation accuracy, and\ndecision-making processes in autonomous vehicles. This thesis discusses the\nbenefits and challenges of integrating generative models and CAV technology in\ntransportation. It aims to highlight the progress made, the remaining\nobstacles, and the potential for advancements in safety and innovation.\n","authors":["Bo Shu","Yiting Zhang","Dong Shu"],"pdf_url":"https://arxiv.org/pdf/2403.10559v2.pdf","comment":"9 pages, 2 figures"},{"id":"http://arxiv.org/abs/2507.16839v1","updated":"2025-07-18T19:40:44Z","published":"2025-07-18T19:40:44Z","title":"Summarizing Normative Driving Behavior From Large-Scale NDS Datasets for\n  Vehicle System Development","summary":"  This paper presents a methodology to process large-scale naturalistic driving\nstudies (NDS) to describe the driving behavior for five vehicle metrics,\nincluding speed, speeding, lane keeping, following distance, and headway,\ncontextualized by roadway characteristics, vehicle classes, and driver\ndemographics. Such descriptions of normative driving behaviors can aid in the\ndevelopment of vehicle safety and intelligent transportation systems. The\nmethodology is demonstrated using data from the Second Strategic Highway\nResearch Program (SHRP 2) NDS, which includes over 34 million miles of driving\nacross more than 3,400 drivers. Summaries of each driving metric were generated\nusing vehicle, GPS, and forward radar data. Additionally, interactive online\nanalytics tools were developed to visualize and compare driving behavior across\ngroups through dynamic data selection and grouping. For example, among drivers\non 65-mph roads for the SHRP 2 NDS, females aged 16-19 exceeded the speed limit\nby 7.5 to 15 mph slightly more often than their male counterparts, and younger\ndrivers maintained headways under 1.5 seconds more frequently than older\ndrivers. This work supports better vehicle systems and safer infrastructure by\nquantifying normative driving behaviors and offers a methodology for analyzing\nNDS datasets for cross group comparisons.\n","authors":["Gregory Beale","Gibran Ali"],"pdf_url":"https://arxiv.org/pdf/2507.16839v1.pdf","comment":"Accepted to the 2025 IEEE International Conference on Intelligent\n  Transportation Systems (ITSC 2025)"},{"id":"http://arxiv.org/abs/2507.14274v1","updated":"2025-07-18T17:48:01Z","published":"2025-07-18T17:48:01Z","title":"A Recursive Lie-Group Formulation for the Second-Order Time Derivatives\n  of the Inverse Dynamics of parallel Kinematic Manipulators","summary":"  Series elastic actuators (SEA) were introduced for serial robotic arms. Their\nmodel-based trajectory tracking control requires the second time derivatives of\nthe inverse dynamics solution, for which algorithms were proposed. Trajectory\ncontrol of parallel kinematics manipulators (PKM) equipped with SEAs has not\nyet been pursued. Key element for this is the computationally efficient\nevaluation of the second time derivative of the inverse dynamics solution. This\nhas not been presented in the literature, and is addressed in the present paper\nfor the first time. The special topology of PKM is exploited reusing the\nrecursive algorithms for evaluating the inverse dynamics of serial robots. A\nLie group formulation is used and all relations are derived within this\nframework. Numerical results are presented for a 6-DOF Gough-Stewart platform\n(as part of an exoskeleton), and for a planar PKM when a flatness-based control\nscheme is applied.\n","authors":["Andreas Mueller","Shivesh Kumar","Thomas Kordik"],"pdf_url":"https://arxiv.org/pdf/2507.14274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.14099v1","updated":"2025-07-18T17:25:54Z","published":"2025-07-18T17:25:54Z","title":"Context-Aware Behavior Learning with Heuristic Motion Memory for\n  Underwater Manipulation","summary":"  Autonomous motion planning is critical for efficient and safe underwater\nmanipulation in dynamic marine environments. Current motion planning methods\noften fail to effectively utilize prior motion experiences and adapt to\nreal-time uncertainties inherent in underwater settings. In this paper, we\nintroduce an Adaptive Heuristic Motion Planner framework that integrates a\nHeuristic Motion Space (HMS) with Bayesian Networks to enhance motion planning\nfor autonomous underwater manipulation. Our approach employs the Probabilistic\nRoadmap (PRM) algorithm within HMS to optimize paths by minimizing a composite\ncost function that accounts for distance, uncertainty, energy consumption, and\nexecution time. By leveraging HMS, our framework significantly reduces the\nsearch space, thereby boosting computational performance and enabling real-time\nplanning capabilities. Bayesian Networks are utilized to dynamically update\nuncertainty estimates based on real-time sensor data and environmental\nconditions, thereby refining the joint probability of path success. Through\nextensive simulations and real-world test scenarios, we showcase the advantages\nof our method in terms of enhanced performance and robustness. This\nprobabilistic approach significantly advances the capability of autonomous\nunderwater robots, ensuring optimized motion planning in the face of dynamic\nmarine challenges.\n","authors":["Markus Buchholz","Ignacio Carlucho","Michele Grimaldi","Maria Koskinopoulou","Yvan R. Petillot"],"pdf_url":"https://arxiv.org/pdf/2507.14099v1.pdf","comment":"Accepted at 2025 IEEE International Conference on Intelligent Robots\n  and Systems (IROS)"},{"id":"http://arxiv.org/abs/2507.05169v2","updated":"2025-07-18T16:48:16Z","published":"2025-07-07T16:23:46Z","title":"Critiques of World Models","summary":"  World Model, the supposed algorithmic surrogate of the real-world environment\nwhich biological agents experience with and act upon, has been an emerging\ntopic in recent years because of the rising needs to develop virtual agents\nwith artificial (general) intelligence. There has been much debate on what a\nworld model really is, how to build it, how to use it, and how to evaluate it.\nIn this essay, starting from the imagination in the famed Sci-Fi classic Dune,\nand drawing inspiration from the concept of \"hypothetical thinking\" in\npsychology literature, we offer critiques of several schools of thoughts on\nworld modeling, and argue the primary goal of a world model to be simulating\nall actionable possibilities of the real world for purposeful reasoning and\nacting. Building on the critiques, we propose a new architecture for a\ngeneral-purpose world model, based on hierarchical, multi-level, and mixed\ncontinuous/discrete representations, and a generative and self-supervision\nlearning framework, with an outlook of a Physical, Agentic, and Nested (PAN)\nAGI system enabled by such a model.\n","authors":["Eric Xing","Mingkai Deng","Jinyu Hou","Zhiting Hu"],"pdf_url":"https://arxiv.org/pdf/2507.05169v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16411v2","updated":"2025-07-18T16:46:30Z","published":"2024-10-21T18:27:48Z","title":"The Duality of Generative AI and Reinforcement Learning in Robotics: A\n  Review","summary":"  Recently, generative AI and reinforcement learning (RL) have been redefining\nwhat is possible for AI agents that take information flows as input and produce\nintelligent behavior. As a result, we are seeing similar advancements in\nembodied AI and robotics for control policy generation. Our review paper\nexamines the integration of generative AI models with RL to advance robotics.\nOur primary focus is on the duality between generative AI and RL for robotics\ndownstream tasks. Specifically, we investigate: (1) The role of prominent\ngenerative AI tools as modular priors for multi-modal input fusion in RL tasks.\n(2) How RL can train, fine-tune and distill generative models for policy\ngeneration, such as VLA models, similarly to RL applications in large language\nmodels. We then propose a new taxonomy based on a considerable amount of\nselected papers.\n  Lastly, we identify open challenges accounting for model scalability,\nadaptation and grounding, giving recommendations and insights on future\nresearch directions. We reflect on which generative AI models best fit the RL\ntasks and why. On the other side, we reflect on important issues inherent to\nRL-enhanced generative policies, such as safety concerns and failure modes, and\nwhat are the limitations of current methods. A curated collection of relevant\nresearch papers is maintained on our GitHub repository, serving as a resource\nfor ongoing research and development in this field:\nhttps://github.com/clmoro/Robotics-RL-FMs-Integration.\n","authors":["Angelo Moroncelli","Vishal Soni","Marco Forgione","Dario Piga","Blerina Spahiu","Loris Roveda"],"pdf_url":"https://arxiv.org/pdf/2410.16411v2.pdf","comment":"Submitted for publication to Information Fusion"},{"id":"http://arxiv.org/abs/2507.14061v1","updated":"2025-07-18T16:41:35Z","published":"2025-07-18T16:41:35Z","title":"MorphIt: Flexible Spherical Approximation of Robot Morphology for\n  Representation-driven Adaptation","summary":"  What if a robot could rethink its own morphological representation to better\nmeet the demands of diverse tasks? Most robotic systems today treat their\nphysical form as a fixed constraint rather than an adaptive resource, forcing\nthe same rigid geometric representation to serve applications with vastly\ndifferent computational and precision requirements. We introduce MorphIt, a\nnovel algorithm for approximating robot morphology using spherical primitives\nthat balances geometric accuracy with computational efficiency. Unlike existing\napproaches that rely on either labor-intensive manual specification or\ninflexible computational methods, MorphIt implements an automatic\ngradient-based optimization framework with tunable parameters that provides\nexplicit control over the physical fidelity versus computational cost tradeoff.\nQuantitative evaluations demonstrate that MorphIt outperforms baseline\napproaches (Variational Sphere Set Approximation and Adaptive Medial-Axis\nApproximation) across multiple metrics, achieving better mesh approximation\nwith fewer spheres and reduced computational overhead. Our experiments show\nenhanced robot capabilities in collision detection accuracy, contact-rich\ninteraction simulation, and navigation through confined spaces. By dynamically\nadapting geometric representations to task requirements, robots can now exploit\ntheir physical embodiment as an active resource rather than an inflexible\nparameter, opening new frontiers for manipulation in environments where\nphysical form must continuously balance precision with computational\ntractability.\n","authors":["Nataliya Nechyporenko","Yutong Zhang","Sean Campbell","Alessandro Roncone"],"pdf_url":"https://arxiv.org/pdf/2507.14061v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.14059v1","updated":"2025-07-18T16:40:58Z","published":"2025-07-18T16:40:58Z","title":"Design of a Modular Mobile Inspection and Maintenance Robot for an\n  Orbital Servicing Hub","summary":"  The use of autonomous robots in space is an essential part of the \"New Space\"\ncommercial ecosystem of assembly and re-use of space hardware components in\nEarth orbit and beyond. The STARFAB project aims to create a ground\ndemonstration of an orbital automated warehouse as a hub for sustainable\ncommercial operations and servicing. A critical part of this fully-autonomous\nrobotic facility will be the capability to monitor, inspect, and assess the\ncondition of both the components stored in the warehouse, and the STARFAB\nfacility itself. This paper introduces ongoing work on the STARFAB Mobile\nInspection Module (MIM). The MIM uses Standard Interconnects (SI) so that it\ncan be carried by Walking Manipulators (WM) as an independently-mobile robot,\nand multiple MIMs can be stored and retrieved as needed for operations on\nSTARFAB. The MIM carries high-resolution cameras, a 3D profilometer, and a\nthermal imaging sensor, with the capability to add other modular sensors. A\ngrasping tool and torque wrench are stored within the modular body for use by\nan attached WM for maintenance operations. Implementation and testing is still\nongoing at the time of writing. This paper details the concept of operations\nfor the MIM as an on-orbit autonomous inspection and maintenance system, the\nmechanical and electronic design of the MIM, and the sensors package used for\nnon-destructive testing.\n","authors":["Tianyuan Wang","Mark A Post","Mathieu Deremetz"],"pdf_url":"https://arxiv.org/pdf/2507.14059v1.pdf","comment":"In proceedings of the Towards Autonomous Robotic Systems 2025\n  conference (TAROS 2025), York, UK 6 pages, one page of references, 6 figures"},{"id":"http://arxiv.org/abs/2507.14049v1","updated":"2025-07-18T16:15:09Z","published":"2025-07-18T16:15:09Z","title":"EdgeVLA: Efficient Vision-Language-Action Models","summary":"  Vision-Language Models (VLMs) have emerged as a promising approach to address\nthe data scarcity challenge in robotics, enabling the development of\ngeneralizable visuomotor control policies. While models like OpenVLA showcase\nthe potential of this paradigm, deploying large-scale VLMs on\nresource-constrained mobile manipulation systems remains a significant hurdle.\nThis paper introduces Edge VLA (EVLA), a novel approach designed to\nsignificantly enhance the inference speed of Vision-Language-Action (VLA)\nmodels. EVLA maintains the representational power of these models while\nenabling real-time performance on edge devices. We achieve this through two key\ninnovations: 1) Eliminating the autoregressive requirement for end-effector\nposition prediction, leading to a 7x speedup in inference, and 2) Leveraging\nthe efficiency of Small Language Models (SLMs), demonstrating comparable\ntraining performance to larger models with significantly reduced computational\ndemands. Our early results demonstrate that EVLA achieves comparable training\ncharacteristics to OpenVLA while offering substantial gains in inference speed\nand memory efficiency. We release our model checkpoints and training\n\\href{https://github.com/kscalelabs/evla }{codebase} to foster further\nresearch.\n","authors":["Paweł Budzianowski","Wesley Maa","Matthew Freed","Jingxiang Mo","Winston Hsiao","Aaron Xie","Tomasz Młoduchowski","Viraj Tipnis","Benjamin Bolte"],"pdf_url":"https://arxiv.org/pdf/2507.14049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.14043v1","updated":"2025-07-18T16:11:35Z","published":"2025-07-18T16:11:35Z","title":"A multi-strategy improved snake optimizer for three-dimensional UAV path\n  planning and engineering problems","summary":"  Metaheuristic algorithms have gained widespread application across various\nfields owing to their ability to generate diverse solutions. One such algorithm\nis the Snake Optimizer (SO), a progressive optimization approach. However, SO\nsuffers from the issues of slow convergence speed and susceptibility to local\noptima. In light of these shortcomings, we propose a novel Multi-strategy\nImproved Snake Optimizer (MISO). Firstly, we propose a new adaptive random\ndisturbance strategy based on sine function to alleviate the risk of getting\ntrapped in a local optimum. Secondly, we introduce adaptive Levy flight\nstrategy based on scale factor and leader and endow the male snake leader with\nflight capability, which makes it easier for the algorithm to leap out of the\nlocal optimum and find the global optimum. More importantly, we put forward a\nposition update strategy combining elite leadership and Brownian motion,\neffectively accelerating the convergence speed while ensuring precision.\nFinally, to demonstrate the performance of MISO, we utilize 30 CEC2017 test\nfunctions and the CEC2022 test suite, comparing it with 11 popular algorithms\nacross different dimensions to validate its effectiveness. Moreover, Unmanned\nAerial Vehicle (UAV) has been widely used in various fields due to its\nadvantages of low cost, high mobility and easy operation. However, the UAV path\nplanning problem is crucial for flight safety and efficiency, and there are\nstill challenges in establishing and optimizing the path model. Therefore, we\napply MISO to the UAV 3D path planning problem as well as 6 engineering design\nproblems to assess its feasibility in practical applications. The experimental\nresults demonstrate that MISO exceeds other competitive algorithms in terms of\nsolution quality and stability, establishing its strong potential for\napplication.\n","authors":["Genliang Li","Yaxin Cui","Jinyu Su"],"pdf_url":"https://arxiv.org/pdf/2507.14043v1.pdf","comment":"59 pages, 22 figures"},{"id":"http://arxiv.org/abs/2503.06776v2","updated":"2025-07-18T15:22:33Z","published":"2025-03-09T21:03:53Z","title":"Chance-constrained Linear Quadratic Gaussian Games for Multi-robot\n  Interaction under Uncertainty","summary":"  We address safe multi-robot interaction under uncertainty. In particular, we\nformulate a chance-constrained linear quadratic Gaussian game with coupling\nconstraints and system uncertainties. We find a tractable reformulation of the\ngame and propose a dual ascent algorithm. We prove that the algorithm converges\nto a feedback generalized Nash equilibrium of the reformulated game, ensuring\nthe satisfaction of the chance constraints. We test our method in driving\nsimulations and real-world robot experiments. Our method ensures safety under\nuncertainty and generates less conservative trajectories than single-agent\nmodel predictive control.\n","authors":["Kai Ren","Giulio Salizzoni","Mustafa Emre Gürsoy","Maryam Kamgarpour"],"pdf_url":"https://arxiv.org/pdf/2503.06776v2.pdf","comment":"Published in IEEE Control Systems Letters"},{"id":"http://arxiv.org/abs/2507.14011v1","updated":"2025-07-18T15:22:33Z","published":"2025-07-18T15:22:33Z","title":"Conceptual and Design Principles for a Self-Referential Algorithm\n  Mimicking Neuronal Assembly Functions","summary":"  This article proposes a method to formalise models of cognitive processes\ngrounded in experience, considering experience from the perspective of a living\nsystem and not from that of an observer of the living system. The perspective\nof a living system is defined by the need of the system to preserve the vital\nequilibria. The method is based on an algorithmic schema that we call\nEnvironment Generative Operator (EGO) and uses a self-referential language\ndeveloped for this purpose which we call E-language. EGO simulates cognitive\nprocesses as operations on neuron assemblies as understood by Hebb. In this\narticle we present an EGO prototype (EGO-P) which has already been implemented\nand tested.\n","authors":["Paolo Totaro","Alberto Mangiante"],"pdf_url":"https://arxiv.org/pdf/2507.14011v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05223v2","updated":"2025-07-18T15:16:10Z","published":"2025-05-08T13:16:37Z","title":"Multi-Objective Reinforcement Learning for Adaptable Personalized\n  Autonomous Driving","summary":"  Human drivers exhibit individual preferences regarding driving style.\nAdapting autonomous vehicles to these preferences is essential for user trust\nand satisfaction. However, existing end-to-end driving approaches often rely on\npredefined driving styles or require continuous user feedback for adaptation,\nlimiting their ability to support dynamic, context-dependent preferences. We\npropose a novel approach using multi-objective reinforcement learning (MORL)\nwith preference-driven optimization for end-to-end autonomous driving that\nenables runtime adaptation to driving style preferences. Preferences are\nencoded as continuous weight vectors to modulate behavior along interpretable\nstyle objectives$\\unicode{x2013}$including efficiency, comfort, speed, and\naggressiveness$\\unicode{x2013}$without requiring policy retraining. Our\nsingle-policy agent integrates vision-based perception in complex mixed-traffic\nscenarios and is evaluated in diverse urban environments using the CARLA\nsimulator. Experimental results demonstrate that the agent dynamically adapts\nits driving behavior according to changing preferences while maintaining\nperformance in terms of collision avoidance and route completion.\n","authors":["Hendrik Surmann","Jorge de Heuvel","Maren Bennewitz"],"pdf_url":"https://arxiv.org/pdf/2505.05223v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13970v1","updated":"2025-07-18T14:32:45Z","published":"2025-07-18T14:32:45Z","title":"A segmented robot grasping perception neural network for edge AI","summary":"  Robotic grasping, the ability of robots to reliably secure and manipulate\nobjects of varying shapes, sizes and orientations, is a complex task that\nrequires precise perception and control. Deep neural networks have shown\nremarkable success in grasp synthesis by learning rich and abstract\nrepresentations of objects. When deployed at the edge, these models can enable\nlow-latency, low-power inference, making real-time grasping feasible in\nresource-constrained environments. This work implements Heatmap-Guided Grasp\nDetection, an end-to-end framework for the detection of 6-Dof grasp poses, on\nthe GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware\ntechniques, including input dimensionality reduction, model partitioning, and\nquantisation. Experimental evaluation on the GraspNet-1Billion benchmark\nvalidates the feasibility of fully on-chip inference, highlighting the\npotential of low-power MCUs for real-time, autonomous manipulation.\n","authors":["Casper Bröcheler","Thomas Vroom","Derrick Timmermans","Alan van den Akker","Guangzhi Tang","Charalampos S. Kouzinopoulos","Rico Möckel"],"pdf_url":"https://arxiv.org/pdf/2507.13970v1.pdf","comment":"Accepted by SMC 2025"},{"id":"http://arxiv.org/abs/2507.13969v1","updated":"2025-07-18T14:32:29Z","published":"2025-07-18T14:32:29Z","title":"A Minimalist Controller for Autonomously Self-Aggregating Robotic\n  Swarms: Enabling Compact Formations in Multitasking Scenarios","summary":"  The deployment of simple emergent behaviors in swarm robotics has been\nwell-rehearsed in the literature. A recent study has shown how self-aggregation\nis possible in a multitask approach -- where multiple self-aggregation task\ninstances occur concurrently in the same environment. The multitask approach\nposes new challenges, in special, how the dynamic of each group impacts the\nperformance of others. So far, the multitask self-aggregation of groups of\nrobots suffers from generating a circular formation -- that is not fully\ncompact -- or is not fully autonomous. In this paper, we present a multitask\nself-aggregation where groups of homogeneous robots sort themselves into\ndifferent compact clusters, relying solely on a line-of-sight sensor. Our\nmultitask self-aggregation behavior was able to scale well and achieve a\ncompact formation. We report scalability results from a series of simulation\ntrials with different configurations in the number of groups and the number of\nrobots per group. We were able to improve the multitask self-aggregation\nbehavior performance in terms of the compactness of the clusters, keeping the\nproportion of clustered robots found in other studies.\n","authors":["Maria Eduarda Silva de Macedo","Ana Paula Chiarelli de Souza","Roberto Silvio Ubertino Rosso Jr.","Yuri Kaszubowski Lopes"],"pdf_url":"https://arxiv.org/pdf/2507.13969v1.pdf","comment":"7 pages total (6 pages of content + 1 page of references). Short\n  paper manuscript submitted to TAROS 2025"},{"id":"http://arxiv.org/abs/2507.13940v1","updated":"2025-07-18T14:12:56Z","published":"2025-07-18T14:12:56Z","title":"NeHMO: Neural Hamilton-Jacobi Reachability Learning for Decentralized\n  Safe Multi-Agent Motion Planning","summary":"  Safe Multi-Agent Motion Planning (MAMP) is a significant challenge in\nrobotics. Despite substantial advancements, existing methods often face a\ndilemma. Decentralized algorithms typically rely on predicting the behavior of\nother agents, sharing contracts, or maintaining communication for safety, while\ncentralized approaches struggle with scalability and real-time decision-making.\nTo address these challenges, we introduce Neural Hamilton-Jacobi Reachability\nLearning (HJR) for Decentralized Multi-Agent Motion Planning. Our method\nprovides scalable neural HJR modeling to tackle high-dimensional configuration\nspaces and capture worst-case collision and safety constraints between agents.\nWe further propose a decentralized trajectory optimization framework that\nincorporates the learned HJR solutions to solve MAMP tasks in real-time. We\ndemonstrate that our method is both scalable and data-efficient, enabling the\nsolution of MAMP problems in higher-dimensional scenarios with complex\ncollision constraints. Our approach generalizes across various dynamical\nsystems, including a 12-dimensional dual-arm setup, and outperforms a range of\nstate-of-the-art techniques in successfully addressing challenging MAMP tasks.\nVideo demonstrations are available at https://youtu.be/IZiePX0p1Mc.\n","authors":["Qingyi Chen","Ahmed H. Qureshi"],"pdf_url":"https://arxiv.org/pdf/2507.13940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11887v2","updated":"2025-07-18T13:58:39Z","published":"2025-02-17T15:13:41Z","title":"Stonefish: Supporting Machine Learning Research in Marine Robotics","summary":"  Simulations are highly valuable in marine robotics, offering a cost-effective\nand controlled environment for testing in the challenging conditions of\nunderwater and surface operations. Given the high costs and logistical\ndifficulties of real-world trials, simulators capable of capturing the\noperational conditions of subsea environments have become key in developing and\nrefining algorithms for remotely-operated and autonomous underwater vehicles.\nThis paper highlights recent enhancements to the Stonefish simulator, an\nadvanced open-source platform supporting development and testing of marine\nrobotics solutions. Key updates include a suite of additional sensors, such as\nan event-based camera, a thermal camera, and an optical flow camera, as well\nas, visual light communication, support for tethered operations, improved\nthruster modelling, more flexible hydrodynamics, and enhanced sonar accuracy.\nThese developments and an automated annotation tool significantly bolster\nStonefish's role in marine robotics research, especially in the field of\nmachine learning, where training data with a known ground truth is hard or\nimpossible to collect.\n","authors":["Michele Grimaldi","Patryk Cieslak","Eduardo Ochoa","Vibhav Bharti","Hayat Rajani","Ignacio Carlucho","Maria Koskinopoulou","Yvan R. Petillot","Nuno Gracias"],"pdf_url":"https://arxiv.org/pdf/2502.11887v2.pdf","comment":"2025 IEEE/RSJ International Conference on Robotics and Automation\n  (ICRA)"},{"id":"http://arxiv.org/abs/2507.13903v1","updated":"2025-07-18T13:32:17Z","published":"2025-07-18T13:32:17Z","title":"AeroThrow: An Autonomous Aerial Throwing System for Precise Payload\n  Delivery","summary":"  Autonomous aerial systems play an increasingly vital role in a wide range of\napplications, particularly for transport and delivery tasks in complex\nenvironments. In airdrop missions, these platforms face the dual challenges of\nabrupt control mode switching and inherent system delays along with control\nerrors. To address these issues, this paper presents an autonomous airdrop\nsystem based on an aerial manipulator (AM). The introduction of additional\nactuated degrees of freedom enables active compensation for UAV tracking\nerrors. By imposing smooth and continuous constraints on the parabolic landing\npoint, the proposed approach generates aerial throwing trajectories that are\nless sensitive to the timing of payload release. A hierarchical disturbance\ncompensation strategy is incorporated into the Nonlinear Model Predictive\nControl (NMPC) framework to mitigate the effects of sudden changes in system\nparameters, while the predictive capabilities of NMPC are further exploited to\nimprove the precision of aerial throwing. Both simulation and real-world\nexperimental results demonstrate that the proposed system achieves greater\nagility and precision in airdrop missions.\n","authors":["Ziliang Li","Hongming Chen","Yiyang Lin","Biyu Ye","Ximin Lyu"],"pdf_url":"https://arxiv.org/pdf/2507.13903v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13888v1","updated":"2025-07-18T13:05:47Z","published":"2025-07-18T13:05:47Z","title":"Fixed time convergence guarantees for Higher Order Control Barrier\n  Functions","summary":"  We present a novel method for designing higher-order Control Barrier\nFunctions (CBFs) that guarantee convergence to a safe set within a\nuser-specified finite. Traditional Higher Order CBFs (HOCBFs) ensure asymptotic\nsafety but lack mechanisms for fixed-time convergence, which is critical in\ntime-sensitive and safety-critical applications such as autonomous navigation.\nIn contrast, our approach imposes a structured differential constraint using\nrepeated roots in the characteristic polynomial, enabling closed-form\npolynomial solutions with exact convergence at a prescribed time. We derive\nconditions on the barrier function and its derivatives that ensure forward\ninvariance and fixed-time reachability, and we provide an explicit formulation\nfor second-order systems. Our method is evaluated on three robotic systems - a\npoint-mass model, a unicycle, and a bicycle model and benchmarked against\nexisting HOCBF approaches. Results demonstrate that our formulation reliably\nenforces convergence within the desired time, even when traditional methods\nfail. This work provides a tractable and robust framework for real-time control\nwith provable finite-time safety guarantees.\n","authors":["Janani S K","Shishir Kolathaya"],"pdf_url":"https://arxiv.org/pdf/2507.13888v1.pdf","comment":"6 PAGES, 2 FIGURES"},{"id":"http://arxiv.org/abs/2507.13872v1","updated":"2025-07-18T12:50:47Z","published":"2025-07-18T12:50:47Z","title":"Safe and Performant Controller Synthesis using Gradient-based Model\n  Predictive Control and Control Barrier Functions","summary":"  Ensuring both performance and safety is critical for autonomous systems\noperating in real-world environments. While safety filters such as Control\nBarrier Functions (CBFs) enforce constraints by modifying nominal controllers\nin real time, they can become overly conservative when the nominal policy lacks\nsafety awareness. Conversely, solving State-Constrained Optimal Control\nProblems (SC-OCPs) via dynamic programming offers formal guarantees but is\nintractable in high-dimensional systems. In this work, we propose a novel\ntwo-stage framework that combines gradient-based Model Predictive Control (MPC)\nwith CBF-based safety filtering for co-optimizing safety and performance. In\nthe first stage, we relax safety constraints as penalties in the cost function,\nenabling fast optimization via gradient-based methods. This step improves\nscalability and avoids feasibility issues associated with hard constraints. In\nthe second stage, we modify the resulting controller using a CBF-based\nQuadratic Program (CBF-QP), which enforces hard safety constraints with minimal\ndeviation from the reference. Our approach yields controllers that are both\nperformant and provably safe. We validate the proposed framework on two case\nstudies, showcasing its ability to synthesize scalable, safe, and\nhigh-performance controllers for complex, high-dimensional autonomous systems.\n","authors":["Aditya Singh","Aastha Mishra","Manan Tayal","Shishir Kolathaya","Pushpak Jagtap"],"pdf_url":"https://arxiv.org/pdf/2507.13872v1.pdf","comment":"6 Pages, 2 Figures. The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2507.13871v1","updated":"2025-07-18T12:50:27Z","published":"2025-07-18T12:50:27Z","title":"Safety Certification in the Latent space using Control Barrier Functions\n  and World Models","summary":"  Synthesising safe controllers from visual data typically requires extensive\nsupervised labelling of safety-critical data, which is often impractical in\nreal-world settings. Recent advances in world models enable reliable prediction\nin latent spaces, opening new avenues for scalable and data-efficient safe\ncontrol. In this work, we introduce a semi-supervised framework that leverages\ncontrol barrier certificates (CBCs) learned in the latent space of a world\nmodel to synthesise safe visuomotor policies. Our approach jointly learns a\nneural barrier function and a safe controller using limited labelled data,\nwhile exploiting the predictive power of modern vision transformers for latent\ndynamics modelling.\n","authors":["Mehul Anand","Shishir Kolathaya"],"pdf_url":"https://arxiv.org/pdf/2507.13871v1.pdf","comment":"6 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:2409.12616"},{"id":"http://arxiv.org/abs/2503.12170v2","updated":"2025-07-18T12:26:43Z","published":"2025-03-15T15:23:35Z","title":"DiffAD: A Unified Diffusion Modeling Approach for Autonomous Driving","summary":"  End-to-end autonomous driving (E2E-AD) has rapidly emerged as a promising\napproach toward achieving full autonomy. However, existing E2E-AD systems\ntypically adopt a traditional multi-task framework, addressing perception,\nprediction, and planning tasks through separate task-specific heads. Despite\nbeing trained in a fully differentiable manner, they still encounter issues\nwith task coordination, and the system complexity remains high. In this work,\nwe introduce DiffAD, a novel diffusion probabilistic model that redefines\nautonomous driving as a conditional image generation task. By rasterizing\nheterogeneous targets onto a unified bird's-eye view (BEV) and modeling their\nlatent distribution, DiffAD unifies various driving objectives and jointly\noptimizes all driving tasks in a single framework, significantly reducing\nsystem complexity and harmonizing task coordination. The reverse process\niteratively refines the generated BEV image, resulting in more robust and\nrealistic driving behaviors. Closed-loop evaluations in Carla demonstrate the\nsuperiority of the proposed method, achieving a new state-of-the-art Success\nRate and Driving Score.\n","authors":["Tao Wang","Cong Zhang","Xingguang Qu","Kun Li","Weiwei Liu","Chang Huang"],"pdf_url":"https://arxiv.org/pdf/2503.12170v2.pdf","comment":"8 pages, 6 figures; Code released"},{"id":"http://arxiv.org/abs/2507.13857v1","updated":"2025-07-18T12:23:47Z","published":"2025-07-18T12:23:47Z","title":"Depth3DLane: Fusing Monocular 3D Lane Detection with Self-Supervised\n  Monocular Depth Estimation","summary":"  Monocular 3D lane detection is essential for autonomous driving, but\nchallenging due to the inherent lack of explicit spatial information.\nMulti-modal approaches rely on expensive depth sensors, while methods\nincorporating fully-supervised depth networks rely on ground-truth depth data\nthat is impractical to collect at scale. Additionally, existing methods assume\nthat camera parameters are available, limiting their applicability in scenarios\nlike crowdsourced high-definition (HD) lane mapping. To address these\nlimitations, we propose Depth3DLane, a novel dual-pathway framework that\nintegrates self-supervised monocular depth estimation to provide explicit\nstructural information, without the need for expensive sensors or additional\nground-truth depth data. Leveraging a self-supervised depth network to obtain a\npoint cloud representation of the scene, our bird's-eye view pathway extracts\nexplicit spatial information, while our front view pathway simultaneously\nextracts rich semantic information. Depth3DLane then uses 3D lane anchors to\nsample features from both pathways and infer accurate 3D lane geometry.\nFurthermore, we extend the framework to predict camera parameters on a\nper-frame basis and introduce a theoretically motivated fitting procedure to\nenhance stability on a per-segment basis. Extensive experiments demonstrate\nthat Depth3DLane achieves competitive performance on the OpenLane benchmark\ndataset. Furthermore, experimental results show that using learned parameters\ninstead of ground-truth parameters allows Depth3DLane to be applied in\nscenarios where camera calibration is infeasible, unlike previous methods.\n","authors":["Max van den Hoven","Kishaan Jeeveswaran","Pieter Piscaer","Thijs Wensveen","Elahe Arani","Bahram Zonooz"],"pdf_url":"https://arxiv.org/pdf/2507.13857v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07799v2","updated":"2025-07-18T11:52:19Z","published":"2024-11-12T13:53:22Z","title":"Horticultural Temporal Fruit Monitoring via 3D Instance Segmentation and\n  Re-Identification using Colored Point Clouds","summary":"  Accurate and consistent fruit monitoring over time is a key step toward\nautomated agricultural production systems. However, this task is inherently\ndifficult due to variations in fruit size, shape, occlusion, orientation, and\nthe dynamic nature of orchards where fruits may appear or disappear between\nobservations. In this article, we propose a novel method for fruit instance\nsegmentation and re-identification on 3D terrestrial point clouds collected\nover time. Our approach directly operates on dense colored point clouds,\ncapturing fine-grained 3D spatial detail. We segment individual fruits using a\nlearning-based instance segmentation method applied directly to the point\ncloud. For each segmented fruit, we extract a compact and discriminative\ndescriptor using a 3D sparse convolutional neural network. To track fruits\nacross different times, we introduce an attention-based matching network that\nassociates fruits with their counterparts from previous sessions. Matching is\nperformed using a probabilistic assignment scheme, selecting the most likely\nassociations across time. We evaluate our approach on real-world datasets of\nstrawberries and apples, demonstrating that it outperforms existing methods in\nboth instance segmentation and temporal re-identification, enabling robust and\nprecise fruit monitoring across complex and dynamic orchard environments.\n","authors":["Daniel Fusaro","Federico Magistri","Jens Behley","Alberto Pretto","Cyrill Stachniss"],"pdf_url":"https://arxiv.org/pdf/2411.07799v2.pdf","comment":"Submitted to Computers and Electronics in Agriculture"},{"id":"http://arxiv.org/abs/2505.19688v2","updated":"2025-07-18T10:36:52Z","published":"2025-05-26T08:49:06Z","title":"GeoPF: Infusing Geometry into Potential Fields for Reactive Planning in\n  Non-trivial Environments","summary":"  Reactive intelligence remains one of the cornerstones of versatile robotics\noperating in cluttered, dynamic, and human-centred environments. Among reactive\napproaches, potential fields (PF) continue to be widely adopted due to their\nsimplicity and real-time applicability. However, existing PF methods typically\noversimplify environmental representations by relying on isotropic, point- or\nsphere-based obstacle approximations. In human-centred settings, this\nsimplification results in overly conservative paths, cumbersome tuning, and\ncomputational overhead -- even breaking real-time requirements. In response, we\npropose the Geometric Potential Field (GeoPF), a reactive motion-planning\nframework that explicitly infuses geometric primitives -- points, lines,\nplanes, cubes, and cylinders -- their structure and spatial relationship in\nmodulating the real-time repulsive response. Extensive quantitative analyses\nconsistently show GeoPF's higher success rates, reduced tuning complexity (a\nsingle parameter set across experiments), and substantially lower computational\ncosts (up to 2 orders of magnitude) compared to traditional PF methods.\nReal-world experiments further validate GeoPF reliability, robustness, and\npractical ease of deployment, as well as its scalability to whole-body\navoidance. GeoPF provides a fresh perspective on reactive planning problems\ndriving geometric-aware temporal motion generation, enabling flexible and\nlow-latency motion planning suitable for modern robotic applications.\n","authors":["Yuhe Gong","Riddhiman Laha","Luis Figueredo"],"pdf_url":"https://arxiv.org/pdf/2505.19688v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.20487v3","updated":"2025-07-18T10:28:01Z","published":"2025-06-25T14:35:33Z","title":"A Survey of Behavior Foundation Model: Next-Generation Whole-Body\n  Control System of Humanoid Robots","summary":"  Humanoid robots are drawing significant attention as versatile platforms for\ncomplex motor control, human-robot interaction, and general-purpose physical\nintelligence. However, achieving efficient whole-body control (WBC) in\nhumanoids remains a fundamental challenge due to sophisticated dynamics,\nunderactuation, and diverse task requirements. While learning-based controllers\nhave shown promise for complex tasks, their reliance on labor-intensive and\ncostly retraining for new scenarios limits real-world applicability. To address\nthese limitations, behavior(al) foundation models (BFMs) have emerged as a new\nparadigm that leverages large-scale pre-training to learn reusable primitive\nskills and broad behavioral priors, enabling zero-shot or rapid adaptation to a\nwide range of downstream tasks. In this paper, we present a comprehensive\noverview of BFMs for humanoid WBC, tracing their development across diverse\npre-training pipelines. Furthermore, we discuss real-world applications,\ncurrent limitations, urgent challenges, and future opportunities, positioning\nBFMs as a key approach toward scalable and general-purpose humanoid\nintelligence. Finally, we provide a curated and long-term list of BFM papers\nand projects to facilitate more subsequent research, which is available at\nhttps://github.com/yuanmingqi/awesome-bfm-papers.\n","authors":["Mingqi Yuan","Tao Yu","Wenqi Ge","Xiuyong Yao","Huijiang Wang","Jiayu Chen","Xin Jin","Bo Li","Hua Chen","Wei Zhang","Wenjun Zeng"],"pdf_url":"https://arxiv.org/pdf/2506.20487v3.pdf","comment":"18 pages, 9 figures"},{"id":"http://arxiv.org/abs/2507.13787v1","updated":"2025-07-18T09:59:13Z","published":"2025-07-18T09:59:13Z","title":"Design Analysis of an Innovative Parallel Robot for Minimally Invasive\n  Pancreatic Surgery","summary":"  This paper focuses on the design of a parallel robot designed for robotic\nassisted minimally invasive pancreatic surgery. Two alternative architectures,\ncalled ATHENA-1 and ATHENA-2, each with 4 degrees of freedom (DOF) are\nproposed. Their kinematic schemes are presented, and the conceptual 3D CAD\nmodels are illustrated. Based on these, two Finite Element Method (FEM)\nsimulations were performed to determine which architecture has the higher\nstiffness. A workspace quantitative analysis is performed to further assess the\nusability of the two proposed parallel architectures related to the medical\ntasks. The obtained results are used to select the architecture which fit the\nrequired design criteria and will be used to develop the experimental model of\nthe surgical robot.\n","authors":["Doina Pisla","Alexandru Pusca","Andrei Caprariu","Adrian Pisla","Bogdan Gherman","Calin Vaida","Damien Chablat"],"pdf_url":"https://arxiv.org/pdf/2507.13787v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.18781v2","updated":"2025-07-18T08:42:53Z","published":"2024-12-25T05:02:22Z","title":"Robustness Evaluation of Offline Reinforcement Learning for Robot\n  Control Against Action Perturbations","summary":"  Offline reinforcement learning, which learns solely from datasets without\nenvironmental interaction, has gained attention. This approach, similar to\ntraditional online deep reinforcement learning, is particularly promising for\nrobot control applications. Nevertheless, its robustness against real-world\nchallenges, such as joint actuator faults in robots, remains a critical\nconcern. This study evaluates the robustness of existing offline reinforcement\nlearning methods using legged robots from OpenAI Gym based on average episodic\nrewards. For robustness evaluation, we simulate failures by incorporating both\nrandom and adversarial perturbations, representing worst-case scenarios, into\nthe joint torque signals. Our experiments show that existing offline\nreinforcement learning methods exhibit significant vulnerabilities to these\naction perturbations and are more vulnerable than online reinforcement learning\nmethods, highlighting the need for more robust approaches in this field.\n","authors":["Shingo Ayabe","Takuto Otomo","Hiroshi Kera","Kazuhiko Kawamoto"],"pdf_url":"https://arxiv.org/pdf/2412.18781v2.pdf","comment":"22 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.02145v4","updated":"2025-07-18T08:39:33Z","published":"2025-02-04T09:19:13Z","title":"From Words to Collisions: LLM-Guided Evaluation and Adversarial\n  Generation of Safety-Critical Driving Scenarios","summary":"  Ensuring the safety of autonomous vehicles requires virtual scenario-based\ntesting, which depends on the robust evaluation and generation of\nsafety-critical scenarios. So far, researchers have used scenario-based testing\nframeworks that rely heavily on handcrafted scenarios as safety metrics. To\nreduce the effort of human interpretation and overcome the limited scalability\nof these approaches, we combine Large Language Models (LLMs) with structured\nscenario parsing and prompt engineering to automatically evaluate and generate\nsafety-critical driving scenarios. We introduce Cartesian and Ego-centric\nprompt strategies for scenario evaluation, and an adversarial generation module\nthat modifies trajectories of risk-inducing vehicles (ego-attackers) to create\ncritical scenarios. We validate our approach using a 2D simulation framework\nand multiple pre-trained LLMs. The results show that the evaluation module\neffectively detects collision scenarios and infers scenario safety. Meanwhile,\nthe new generation module identifies high-risk agents and synthesizes\nrealistic, safety-critical scenarios. We conclude that an LLM equipped with\ndomain-informed prompting techniques can effectively evaluate and generate\nsafety-critical driving scenarios, reducing dependence on handcrafted metrics.\nWe release our open-source code and scenarios at:\nhttps://github.com/TUM-AVS/From-Words-to-Collisions.\n","authors":["Yuan Gao","Mattia Piccinini","Korbinian Moller","Amr Alanwar","Johannes Betz"],"pdf_url":"https://arxiv.org/pdf/2502.02145v4.pdf","comment":"Final Version and Paper Accepted at IEEE ITSC 2025"},{"id":"http://arxiv.org/abs/2507.13729v1","updated":"2025-07-18T08:20:16Z","published":"2025-07-18T08:20:16Z","title":"AGENTS-LLM: Augmentative GENeration of Challenging Traffic Scenarios\n  with an Agentic LLM Framework","summary":"  Rare, yet critical, scenarios pose a significant challenge in testing and\nevaluating autonomous driving planners. Relying solely on real-world driving\nscenes requires collecting massive datasets to capture these scenarios. While\nautomatic generation of traffic scenarios appears promising, data-driven models\nrequire extensive training data and often lack fine-grained control over the\noutput. Moreover, generating novel scenarios from scratch can introduce a\ndistributional shift from the original training scenes which undermines the\nvalidity of evaluations especially for learning-based planners. To sidestep\nthis, recent work proposes to generate challenging scenarios by augmenting\noriginal scenarios from the test set. However, this involves the manual\naugmentation of scenarios by domain experts. An approach that is unable to meet\nthe demands for scale in the evaluation of self-driving systems. Therefore,\nthis paper introduces a novel LLM-agent based framework for augmenting\nreal-world traffic scenarios using natural language descriptions, addressing\nthe limitations of existing methods. A key innovation is the use of an agentic\ndesign, enabling fine-grained control over the output and maintaining high\nperformance even with smaller, cost-effective LLMs. Extensive human expert\nevaluation demonstrates our framework's ability to accurately adhere to user\nintent, generating high quality augmented scenarios comparable to those created\nmanually.\n","authors":["Yu Yao","Salil Bhatnagar","Markus Mazzola","Vasileios Belagiannis","Igor Gilitschenski","Luigi Palmieri","Simon Razniewski","Marcel Hallgarten"],"pdf_url":"https://arxiv.org/pdf/2507.13729v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14180v2","updated":"2025-07-18T08:08:19Z","published":"2025-06-17T04:47:51Z","title":"Non-Overlap-Aware Egocentric Pose Estimation for Collaborative\n  Perception in Connected Autonomy","summary":"  Egocentric pose estimation is a fundamental capability for multi-robot\ncollaborative perception in connected autonomy, such as connected autonomous\nvehicles. During multi-robot operations, a robot needs to know the relative\npose between itself and its teammates with respect to its own coordinates.\nHowever, different robots usually observe completely different views that\ncontains similar objects, which leads to wrong pose estimation. In addition, it\nis unrealistic to allow robots to share their raw observations to detect\noverlap due to the limited communication bandwidth constraint. In this paper,\nwe introduce a novel method for Non-Overlap-Aware Egocentric Pose Estimation\n(NOPE), which performs egocentric pose estimation in a multi-robot team while\nidentifying the non-overlap views and satifying the communication bandwidth\nconstraint. NOPE is built upon an unified hierarchical learning framework that\nintegrates two levels of robot learning: (1) high-level deep graph matching for\ncorrespondence identification, which allows to identify if two views are\noverlapping or not, (2) low-level position-aware cross-attention graph learning\nfor egocentric pose estimation. To evaluate NOPE, we conduct extensive\nexperiments in both high-fidelity simulation and real-world scenarios.\nExperimental results have demonstrated that NOPE enables the novel capability\nfor non-overlapping-aware egocentric pose estimation and achieves state-of-art\nperformance compared with the existing methods. Our project page at\nhttps://hongh0.github.io/NOPE/.\n","authors":["Hong Huang","Dongkuan Xu","Hao Zhang","Peng Gao"],"pdf_url":"https://arxiv.org/pdf/2506.14180v2.pdf","comment":"IROS 2025"},{"id":"http://arxiv.org/abs/2503.07049v2","updated":"2025-07-18T07:48:59Z","published":"2025-03-10T08:35:38Z","title":"VMTS: Vision-Assisted Teacher-Student Reinforcement Learning for\n  Multi-Terrain Locomotion in Bipedal Robots","summary":"  Bipedal robots, due to their anthropomorphic design, offer substantial\npotential across various applications, yet their control is hindered by the\ncomplexity of their structure. Currently, most research focuses on\nproprioception-based methods, which lack the capability to overcome complex\nterrain. While visual perception is vital for operation in human-centric\nenvironments, its integration complicates control further. Recent reinforcement\nlearning (RL) approaches have shown promise in enhancing legged robot\nlocomotion, particularly with proprioception-based methods. However, terrain\nadaptability, especially for bipedal robots, remains a significant challenge,\nwith most research focusing on flat-terrain scenarios. In this paper, we\nintroduce a novel mixture of experts teacher-student network RL strategy, which\nenhances the performance of teacher-student policies based on visual inputs\nthrough a simple yet effective approach. Our method combines terrain selection\nstrategies with the teacher policy, resulting in superior performance compared\nto traditional models. Additionally, we introduce an alignment loss between the\nteacher and student networks, rather than enforcing strict similarity, to\nimprove the student's ability to navigate diverse terrains. We validate our\napproach experimentally on the Limx Dynamic P1 bipedal robot, demonstrating its\nfeasibility and robustness across multiple terrain types.\n","authors":["Fu Chen","Rui Wan","Peidong Liu","Nanxing Zheng","Bo Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.07049v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12440v3","updated":"2025-07-18T07:18:39Z","published":"2025-07-16T17:27:44Z","title":"EgoVLA: Learning Vision-Language-Action Models from Egocentric Human\n  Videos","summary":"  Real robot data collection for imitation learning has led to significant\nadvancements in robotic manipulation. However, the requirement for robot\nhardware in the process fundamentally constrains the scale of the data. In this\npaper, we explore training Vision-Language-Action (VLA) models using egocentric\nhuman videos. The benefit of using human videos is not only for their scale but\nmore importantly for the richness of scenes and tasks. With a VLA trained on\nhuman video that predicts human wrist and hand actions, we can perform Inverse\nKinematics and retargeting to convert the human actions to robot actions. We\nfine-tune the model using a few robot manipulation demonstrations to obtain the\nrobot policy, namely EgoVLA. We propose a simulation benchmark called Ego\nHumanoid Manipulation Benchmark, where we design diverse bimanual manipulation\ntasks with demonstrations. We fine-tune and evaluate EgoVLA with Ego Humanoid\nManipulation Benchmark and show significant improvements over baselines and\nablate the importance of human data. Videos can be found on our website:\nhttps://rchalyang.github.io/EgoVLA\n","authors":["Ruihan Yang","Qinxi Yu","Yecheng Wu","Rui Yan","Borui Li","An-Chieh Cheng","Xueyan Zou","Yunhao Fang","Xuxin Cheng","Ri-Zhao Qiu","Hongxu Yin","Sifei Liu","Song Han","Yao Lu","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2507.12440v3.pdf","comment":"More videos can be found on our website:\n  https://rchalyang.github.io/EgoVLA"},{"id":"http://arxiv.org/abs/2507.13702v1","updated":"2025-07-18T07:10:29Z","published":"2025-07-18T07:10:29Z","title":"SaWa-ML: Structure-Aware Pose Correction and Weight Adaptation-Based\n  Robust Multi-Robot Localization","summary":"  Multi-robot localization is a crucial task for implementing multi-robot\nsystems. Numerous researchers have proposed optimization-based multi-robot\nlocalization methods that use camera, IMU, and UWB sensors. Nevertheless,\ncharacteristics of individual robot odometry estimates and distance\nmeasurements between robots used in the optimization are not sufficiently\nconsidered. In addition, previous researches were heavily influenced by the\nodometry accuracy that is estimated from individual robots. Consequently,\nlong-term drift error caused by error accumulation is potentially inevitable.\nIn this paper, we propose a novel visual-inertial-range-based multi-robot\nlocalization method, named SaWa-ML, which enables geometric structure-aware\npose correction and weight adaptation-based robust multi-robot localization.\nOur contributions are twofold: (i) we leverage UWB sensor data, whose range\nerror does not accumulate over time, to first estimate the relative positions\nbetween robots and then correct the positions of each robot, thus reducing\nlong-term drift errors, (ii) we design adaptive weights for robot pose\ncorrection by considering the characteristics of the sensor data and\nvisual-inertial odometry estimates. The proposed method has been validated in\nreal-world experiments, showing a substantial performance increase compared\nwith state-of-the-art algorithms.\n","authors":["Junho Choi","Kihwan Ryoo","Jeewon Kim","Taeyun Kim","Eungchang Lee","Myeongwoo Jeong","Kevin Christiansen Marsim","Hyungtae Lim","Hyun Myung"],"pdf_url":"https://arxiv.org/pdf/2507.13702v1.pdf","comment":"This paper has been accepted to the 2025 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS)"},{"id":"http://arxiv.org/abs/2505.13916v2","updated":"2025-07-18T06:12:42Z","published":"2025-05-20T04:26:45Z","title":"Robotic Monitoring of Colorimetric Leaf Sensors for Precision\n  Agriculture","summary":"  Common remote sensing modalities (RGB, multispectral, hyperspectral imaging\nor LiDAR) are often used to indirectly measure crop health and do not directly\ncapture plant stress indicators. Commercially available direct leaf sensors are\nbulky, powered electronics that are expensive and interfere with crop growth.\nIn contrast, low-cost, passive and bio-degradable leaf sensors offer an\nopportunity to advance real-time monitoring as they directly interface with the\ncrop surface while not interfering with crop growth. To this end, we co-design\na sensor-detector system, where the sensor is a passive colorimetric leaf\nsensor that directly measures crop health in a precision agriculture setting,\nand the detector autonomously obtains optical signals from these leaf sensors.\nThe detector comprises a low size weight and power (SWaP) mobile ground robot\nwith an onboard monocular RGB camera and object detector to localize each leaf\nsensor, as well as a hyperspectral camera with a motorized mirror and halogen\nlight to acquire hyperspectral images. The sensor's crop health-dependent\noptical signals can be extracted from the hyperspectral images. The\nproof-of-concept system is demonstrated in row-crop environments both indoors\nand outdoors where it is able to autonomously navigate, locate and obtain a\nhyperspectral image of all leaf sensors present, and acquire interpretable\nspectral resonance with 80 $\\%$ accuracy within a required retrieval distance\nfrom the sensor.\n","authors":["Malakhi Hopkins","Alice Kate Li","Shobhita Kramadhati","Jackson Arnold","Akhila Mallavarapu","Chavez Lawrence","Varun Murali","Sanjeev J. Koppal","Cherie R. Kagan","Vijay Kumar"],"pdf_url":"https://arxiv.org/pdf/2505.13916v2.pdf","comment":"Revised version. Initial version was accepted to the Novel Approaches\n  for Precision Agriculture and Forestry with Autonomous Robots IEEE ICRA\n  Workshop - 2025"},{"id":"http://arxiv.org/abs/2507.14249v1","updated":"2025-07-18T06:09:30Z","published":"2025-07-18T06:09:30Z","title":"Real-Time Communication-Aware Ride-Sharing Route Planning for Urban Air\n  Mobility: A Multi-Source Hybrid Attention Reinforcement Learning Approach","summary":"  Urban Air Mobility (UAM) systems are rapidly emerging as promising solutions\nto alleviate urban congestion, with path planning becoming a key focus area.\nUnlike ground transportation, UAM trajectory planning has to prioritize\ncommunication quality for accurate location tracking in constantly changing\nenvironments to ensure safety. Meanwhile, a UAM system, serving as an air taxi,\nrequires adaptive planning to respond to real-time passenger requests,\nespecially in ride-sharing scenarios where passenger demands are unpredictable\nand dynamic. However, conventional trajectory planning strategies based on\npredefined routes lack the flexibility to meet varied passenger ride demands.\nTo address these challenges, this work first proposes constructing a radio map\nto evaluate the communication quality of urban airspace. Building on this, we\nintroduce a novel Multi-Source Hybrid Attention Reinforcement Learning\n(MSHA-RL) framework for the challenge of effectively focusing on passengers and\nUAM locations, which arises from the significant dimensional disparity between\nthe representations. This model first generates the alignment among diverse\ndata sources with large gap dimensions before employing hybrid attention to\nbalance global and local insights, thereby facilitating responsive, real-time\npath planning. Extensive experimental results demonstrate that the approach\nenables communication-compliant trajectory planning, reducing travel time and\nenhancing operational efficiency while prioritizing passenger safety.\n","authors":["Yuejiao Xie","Maonan Wang","Di Zhou","Man-On Pun","Zhu Han"],"pdf_url":"https://arxiv.org/pdf/2507.14249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13662v1","updated":"2025-07-18T05:13:02Z","published":"2025-07-18T05:13:02Z","title":"Iteratively Learning Muscle Memory for Legged Robots to Master Adaptive\n  and High Precision Locomotion","summary":"  This paper presents a scalable and adaptive control framework for legged\nrobots that integrates Iterative Learning Control (ILC) with a biologically\ninspired torque library (TL), analogous to muscle memory. The proposed method\naddresses key challenges in robotic locomotion, including accurate trajectory\ntracking under unmodeled dynamics and external disturbances. By leveraging the\nrepetitive nature of periodic gaits and extending ILC to nonperiodic tasks, the\nframework enhances accuracy and generalization across diverse locomotion\nscenarios. The control architecture is data-enabled, combining a physics-based\nmodel derived from hybrid-system trajectory optimization with real-time\nlearning to compensate for model uncertainties and external disturbances. A\ncentral contribution is the development of a generalized TL that stores learned\ncontrol profiles and enables rapid adaptation to changes in speed, terrain, and\ngravitational conditions-eliminating the need for repeated learning and\nsignificantly reducing online computation. The approach is validated on the\nbipedal robot Cassie and the quadrupedal robot A1 through extensive simulations\nand hardware experiments. Results demonstrate that the proposed framework\nreduces joint tracking errors by up to 85% within a few seconds and enables\nreliable execution of both periodic and nonperiodic gaits, including slope\ntraversal and terrain adaptation. Compared to state-of-the-art whole-body\ncontrollers, the learned skills eliminate the need for online computation\nduring execution and achieve control update rates exceeding 30x those of\nexisting methods. These findings highlight the effectiveness of integrating ILC\nwith torque memory as a highly data-efficient and practical solution for legged\nlocomotion in unstructured and dynamic environments.\n","authors":["Jing Cheng","Yasser G. Alqaham","Zhenyu Gan","Amit K. Sanyal"],"pdf_url":"https://arxiv.org/pdf/2507.13662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13654v1","updated":"2025-07-18T04:48:46Z","published":"2025-07-18T04:48:46Z","title":"A Study of Teleoperation Methods in a Simulated Virtual Eye Surgery\n  Environment","summary":"  This paper examines the performance of Inside and Outside Control modes at\nvarious scaling factors in a simulated vitreoretinal surgical setting. The\nIRISS teleoperated surgical system's console (cockpit) was adapted to project a\nsimulated microscope view of an intraocular setup to a virtual reality (VR)\nheadset. Five experienced vitreoretinal surgeons and five engineers with no\nsurgical experience used the system to perform tasks common to vitreoretinal\nsurgery. Experimental results indicate that Inside Control methods at higher\nscaling factors (20 or 30) achieved the best performance overall, though the\noptimal scaling factor may vary by task and complexity. Optimizing control\nmethods and scaling factors could lead to improvements in surgical efficiency\nand accuracy, as well as minimize risks in future robotic-assisted intraocular\nprocedures.\n","authors":["Haoran Wang","Yasamin Foroutani","Matthew Nepo","Mercedes Rodriguez","Ji Ma","Jean-Pierre Hubschman","Tsu-Chin Tsao","Jacob Rosen"],"pdf_url":"https://arxiv.org/pdf/2507.13654v1.pdf","comment":"9 pages, 11 figures"},{"id":"http://arxiv.org/abs/2507.13650v1","updated":"2025-07-18T04:37:18Z","published":"2025-07-18T04:37:18Z","title":"Safe Robotic Capsule Cleaning with Integrated Transpupillary and\n  Intraocular Optical Coherence Tomography","summary":"  Secondary cataract is one of the most common complications of vision loss due\nto the proliferation of residual lens materials that naturally grow on the lens\ncapsule after cataract surgery. A potential treatment is capsule cleaning, a\nsurgical procedure that requires enhanced visualization of the entire capsule\nand tool manipulation on the thin membrane. This article presents a robotic\nsystem capable of performing the capsule cleaning procedure by integrating a\nstandard transpupillary and an intraocular optical coherence tomography probe\non a surgical instrument for equatorial capsule visualization and real-time\ntool-to-tissue distance feedback. Using robot precision, the developed system\nenables complete capsule mapping in the pupillary and equatorial regions with\nin-situ calibration of refractive index and fiber offset, which are still\ncurrent challenges in obtaining an accurate capsule model. To demonstrate\neffectiveness, the capsule mapping strategy was validated through five\nexperimental trials on an eye phantom that showed reduced root-mean-square\nerrors in the constructed capsule model, while the cleaning strategy was\nperformed in three ex-vivo pig eyes without tissue damage.\n","authors":["Yu-Ting Lai","Yasamin Foroutani","Aya Barzelay","Tsu-Chin Tsao"],"pdf_url":"https://arxiv.org/pdf/2507.13650v1.pdf","comment":"12 pages, 27 figures"},{"id":"http://arxiv.org/abs/2507.13647v1","updated":"2025-07-18T04:31:49Z","published":"2025-07-18T04:31:49Z","title":"Improved particle swarm optimization algorithm: multi-target trajectory\n  optimization for swarm drones","summary":"  Real-time trajectory planning for unmanned aerial vehicles (UAVs) in dynamic\nenvironments remains a key challenge due to high computational demands and the\nneed for fast, adaptive responses. Traditional Particle Swarm Optimization\n(PSO) methods, while effective for offline planning, often struggle with\npremature convergence and latency in real-time scenarios. To overcome these\nlimitations, we propose PE-PSO, an enhanced PSO-based online trajectory\nplanner. The method introduces a persistent exploration mechanism to preserve\nswarm diversity and an entropy-based parameter adjustment strategy to\ndynamically adapt optimization behavior. UAV trajectories are modeled using\nB-spline curves, which ensure path smoothness while reducing optimization\ncomplexity. To extend this capability to UAV swarms, we develop a multi-agent\nframework that combines genetic algorithm (GA)-based task allocation with\ndistributed PE-PSO, supporting scalable and coordinated trajectory generation.\nThe distributed architecture allows for parallel computation and decentralized\ncontrol, enabling effective cooperation among agents while maintaining\nreal-time performance. Comprehensive simulations demonstrate that the proposed\nframework outperforms conventional PSO and other swarm-based planners across\nseveral metrics, including trajectory quality, energy efficiency, obstacle\navoidance, and computation time. These results confirm the effectiveness and\napplicability of PE-PSO in real-time multi-UAV operations under complex\nenvironmental conditions.\n","authors":["Minze Li","Wei Zhao","Ran Chen","Mingqiang Wei"],"pdf_url":"https://arxiv.org/pdf/2507.13647v1.pdf","comment":"8 papers,7 figures"},{"id":"http://arxiv.org/abs/2507.13613v1","updated":"2025-07-18T02:44:32Z","published":"2025-07-18T02:44:32Z","title":"Conformal Contraction for Robust Nonlinear Control with\n  Distribution-Free Uncertainty Quantification","summary":"  We present a novel robust control framework for continuous-time, perturbed\nnonlinear dynamical systems with uncertainty that depends nonlinearly on both\nthe state and control inputs. Unlike conventional approaches that impose\nstructural assumptions on the uncertainty, our framework enhances\ncontraction-based robust control with data-driven uncertainty prediction,\nremaining agnostic to the models of the uncertainty and predictor. We\nstatistically quantify how reliably the contraction conditions are satisfied\nunder dynamics with uncertainty via conformal prediction, thereby obtaining a\ndistribution-free and finite-time probabilistic guarantee for exponential\nboundedness of the trajectory tracking error. We further propose the\nprobabilistically robust control invariant (PRCI) tube for distributionally\nrobust motion planning, within which the perturbed system trajectories are\nguaranteed to stay with a finite probability, without explicit knowledge of the\nuncertainty model. Numerical simulations validate the effectiveness of the\nproposed robust control framework and the performance of the PRCI tube.\n","authors":["Sihang Wei","Melkior Ornik","Hiroyasu Tsukamoto"],"pdf_url":"https://arxiv.org/pdf/2507.13613v1.pdf","comment":"IEEE CDC 2025 submission (accepted)"},{"id":"http://arxiv.org/abs/2507.13602v1","updated":"2025-07-18T02:05:07Z","published":"2025-07-18T02:05:07Z","title":"Improving Low-Cost Teleoperation: Augmenting GELLO with Force","summary":"  In this work we extend the low-cost GELLO teleoperation system, initially\ndesigned for joint position control, with additional force information. Our\nfirst extension is to implement force feedback, allowing users to feel\nresistance when interacting with the environment. Our second extension is to\nadd force information into the data collection process and training of\nimitation learning models. We validate our additions by implementing these on a\nGELLO system with a Franka Panda arm as the follower robot, performing a user\nstudy, and comparing the performance of policies trained with and without force\ninformation on a range of simulated and real dexterous manipulation tasks.\nQualitatively, users with robotics experience preferred our controller, and the\naddition of force inputs improved task success on the majority of tasks.\n","authors":["Shivakanth Sujit","Luca Nunziante","Dan Ogawa Lillrank","Rousslan Fernand Julien Dossa","Kai Arulkumaran"],"pdf_url":"https://arxiv.org/pdf/2507.13602v1.pdf","comment":"Accepted at the 2025 IEEE/SICE International Symposium on System\n  Integration"}]},"2025-07-17T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2507.13539v1","updated":"2025-07-17T21:15:48Z","published":"2025-07-17T21:15:48Z","title":"SCOPE for Hexapod Gait Generation","summary":"  Evolutionary methods have previously been shown to be an effective learning\nmethod for walking gaits on hexapod robots. However, the ability of these\nalgorithms to evolve an effective policy rapidly degrades as the input space\nbecomes more complex. This degradation is due to the exponential growth of the\nsolution space, resulting from an increasing parameter count to handle a more\ncomplex input. In order to address this challenge, we introduce Sparse Cosine\nOptimized Policy Evolution (SCOPE). SCOPE utilizes the Discrete Cosine\nTransform (DCT) to learn directly from the feature coefficients of an input\nmatrix. By truncating the coefficient matrix returned by the DCT, we can reduce\nthe dimensionality of an input while retaining the highest energy features of\nthe original input. We demonstrate the effectiveness of this method by using\nSCOPE to learn the gait of a hexapod robot. The hexapod controller is given a\nmatrix input containing time-series information of previous poses, which are\nthen transformed to gait parameters by an evolved policy. In this task, the\naddition of SCOPE to a reference algorithm achieves a 20% increase in efficacy.\nSCOPE achieves this result by reducing the total input size of the time-series\npose data from 2700 to 54, a 98% decrease. Additionally, SCOPE is capable of\ncompressing an input to any output shape, provided that each output dimension\nis no greater than the corresponding input dimension. This paper demonstrates\nthat SCOPE is capable of significantly compressing the size of an input to an\nevolved controller, resulting in a statistically significant gain in efficacy.\n","authors":["Jim O'Connor","Jay B. Nash","Derin Gezgin","Gary B. Parker"],"pdf_url":"https://arxiv.org/pdf/2507.13539v1.pdf","comment":"IJCCI Conference on Evolutionary Computation and Theory and\n  Applications, 2025"},{"id":"http://arxiv.org/abs/2411.07146v2","updated":"2025-07-17T18:44:07Z","published":"2024-11-11T17:17:11Z","title":"Lost in Tracking Translation: A Comprehensive Analysis of Visual SLAM in\n  Human-Centered XR and IoT Ecosystems","summary":"  Advancements in tracking algorithms have empowered nascent applications\nacross various domains, from steering autonomous vehicles to guiding robots to\nenhancing augmented reality experiences for users. However, these algorithms\nare application-specific and do not work across applications with different\ntypes of motion; even a tracking algorithm designed for a given application\ndoes not work in scenarios deviating from highly standard conditions. For\nexample, a tracking algorithm designed for robot navigation inside a building\nwill not work for tracking the same robot in an outdoor environment. To\ndemonstrate this problem, we evaluate the performance of the state-of-the-art\ntracking methods across various applications and scenarios. To inform our\nanalysis, we first categorize algorithmic, environmental, and\nlocomotion-related challenges faced by tracking algorithms. We quantitatively\nevaluate the performance using multiple tracking algorithms and representative\ndatasets for a wide range of Internet of Things (IoT) and Extended Reality (XR)\napplications, including autonomous vehicles, drones, and humans. Our analysis\nshows that no tracking algorithm works across different applications and\nscenarios within applications. Ultimately, using the insights generated from\nour analysis, we discuss multiple approaches to improving the tracking\nperformance using input data characterization, leveraging intermediate\ninformation, and output evaluation.\n","authors":["Yasra Chandio","Khotso Selialia","Joseph DeGol","Luis Garcia","Fatima M. Anwar"],"pdf_url":"https://arxiv.org/pdf/2411.07146v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13468v1","updated":"2025-07-17T18:21:45Z","published":"2025-07-17T18:21:45Z","title":"ERR@HRI 2.0 Challenge: Multimodal Detection of Errors and Failures in\n  Human-Robot Conversations","summary":"  The integration of large language models (LLMs) into conversational robots\nhas made human-robot conversations more dynamic. Yet, LLM-powered\nconversational robots remain prone to errors, e.g., misunderstanding user\nintent, prematurely interrupting users, or failing to respond altogether.\nDetecting and addressing these failures is critical for preventing\nconversational breakdowns, avoiding task disruptions, and sustaining user\ntrust. To tackle this problem, the ERR@HRI 2.0 Challenge provides a multimodal\ndataset of LLM-powered conversational robot failures during human-robot\nconversations and encourages researchers to benchmark machine learning models\ndesigned to detect robot failures. The dataset includes 16 hours of dyadic\nhuman-robot interactions, incorporating facial, speech, and head movement\nfeatures. Each interaction is annotated with the presence or absence of robot\nerrors from the system perspective, and perceived user intention to correct for\na mismatch between robot behavior and user expectation. Participants are\ninvited to form teams and develop machine learning models that detect these\nfailures using multimodal data. Submissions will be evaluated using various\nperformance metrics, including detection accuracy and false positive rate. This\nchallenge represents another key step toward improving failure detection in\nhuman-robot interaction through social signal analysis.\n","authors":["Shiye Cao","Maia Stiber","Amama Mahmood","Maria Teresa Parreira","Wendy Ju","Micol Spitale","Hatice Gunes","Chien-Ming Huang"],"pdf_url":"https://arxiv.org/pdf/2507.13468v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13455v1","updated":"2025-07-17T18:01:18Z","published":"2025-07-17T18:01:18Z","title":"Hard-Stop Synthesis for Multi-DOF Compliant Mechanisms","summary":"  Compliant mechanisms have significant potential in precision applications due\nto their ability to guide motion without contact. However, an inherent\nvulnerability to fatigue and mechanical failure has hindered the translation of\ncompliant mechanisms to real-world applications. This is particularly\nchallenging in service environments where loading is complex and uncertain, and\nthe cost of failure is high. In such cases, mechanical hard stops are critical\nto prevent yielding and buckling. Conventional hard-stop designs, which rely on\nstacking single-DOF limits, must be overly restrictive in multi-DOF space to\nguarantee safety in the presence of unknown loads. In this study, we present a\nsystematic design synthesis method to guarantee overload protection in\ncompliant mechanisms by integrating coupled multi-DOF motion limits within a\nsingle pair of compact hard-stop surfaces. Specifically, we introduce a\ntheoretical and practical framework for optimizing the contact surface geometry\nto maximize the mechanisms multi-DOF working space while still ensuring that\nthe mechanism remains within its elastic regime. We apply this synthesis method\nto a case study of a caged-hinge mechanism for orthopaedic implants, and\nprovide numerical and experimental validation that the derived design offers\nreliable protection against fatigue, yielding, and buckling. This work\nestablishes a foundation for precision hard-stop design in compliant systems\noperating under uncertain loads, which is a crucial step toward enabling the\napplication of compliant mechanisms in real-world systems.\n","authors":["Dean Chen","Armin Pomeroy","Brandon T. Peterson","Will Flanagan","He Kai Lim","Alexandra Stavrakis","Nelson F. SooHoo","Jonathan B. Hopkins","Tyler R. Clites"],"pdf_url":"https://arxiv.org/pdf/2507.13455v1.pdf","comment":"42 pages, 17 figures. Under review at ASME Journal of Mechanical\n  Design"},{"id":"http://arxiv.org/abs/2507.13340v1","updated":"2025-07-17T17:57:57Z","published":"2025-07-17T17:57:57Z","title":"Latent Policy Steering with Embodiment-Agnostic Pretrained World Models","summary":"  Learning visuomotor policies via imitation has proven effective across a wide\nrange of robotic domains. However, the performance of these policies is heavily\ndependent on the number of training demonstrations, which requires expensive\ndata collection in the real world. In this work, we aim to reduce data\ncollection efforts when learning visuomotor robot policies by leveraging\nexisting or cost-effective data from a wide range of embodiments, such as\npublic robot datasets and the datasets of humans playing with objects (human\ndata from play). Our approach leverages two key insights. First, we use optic\nflow as an embodiment-agnostic action representation to train a World Model\n(WM) across multi-embodiment datasets, and finetune it on a small amount of\nrobot data from the target embodiment. Second, we develop a method, Latent\nPolicy Steering (LPS), to improve the output of a behavior-cloned policy by\nsearching in the latent space of the WM for better action sequences. In real\nworld experiments, we observe significant improvements in the performance of\npolicies trained with a small amount of data (over 50% relative improvement\nwith 30 demonstrations and over 20% relative improvement with 50\ndemonstrations) by combining the policy with a WM pretrained on two thousand\nepisodes sampled from the existing Open X-embodiment dataset across different\nrobots or a cost-effective human dataset from play.\n","authors":["Yiqi Wang","Mrinal Verghese","Jeff Schneider"],"pdf_url":"https://arxiv.org/pdf/2507.13340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13277v1","updated":"2025-07-17T16:38:14Z","published":"2025-07-17T16:38:14Z","title":"Evaluating Reinforcement Learning Algorithms for Navigation in Simulated\n  Robotic Quadrupeds: A Comparative Study Inspired by Guide Dog Behaviour","summary":"  Robots are increasingly integrated across industries, particularly in\nhealthcare. However, many valuable applications for quadrupedal robots remain\noverlooked. This research explores the effectiveness of three reinforcement\nlearning algorithms in training a simulated quadruped robot for autonomous\nnavigation and obstacle avoidance. The goal is to develop a robotic guide dog\nsimulation capable of path following and obstacle avoidance, with long-term\npotential for real-world assistance to guide dogs and visually impaired\nindividuals. It also seeks to expand research into medical 'pets', including\nrobotic guide and alert dogs.\n  A comparative analysis of thirteen related research papers shaped key\nevaluation criteria, including collision detection, pathfinding algorithms,\nsensor usage, robot type, and simulation platforms. The study focuses on sensor\ninputs, collision frequency, reward signals, and learning progression to\ndetermine which algorithm best supports robotic navigation in complex\nenvironments.\n  Custom-made environments were used to ensure fair evaluation of all three\nalgorithms under controlled conditions, allowing consistent data collection.\nResults show that Proximal Policy Optimization (PPO) outperformed Deep\nQ-Network (DQN) and Q-learning across all metrics, particularly in average and\nmedian steps to goal per episode.\n  By analysing these results, this study contributes to robotic navigation, AI\nand medical robotics, offering insights into the feasibility of AI-driven\nquadruped mobility and its role in assistive robotics.\n","authors":["Emma M. A. Harrison"],"pdf_url":"https://arxiv.org/pdf/2507.13277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11623v2","updated":"2025-07-17T16:00:19Z","published":"2025-07-15T18:01:49Z","title":"A Roadmap for Climate-Relevant Robotics Research","summary":"  Climate change is one of the defining challenges of the 21st century, and\nmany in the robotics community are looking for ways to contribute. This paper\npresents a roadmap for climate-relevant robotics research, identifying\nhigh-impact opportunities for collaboration between roboticists and experts\nacross climate domains such as energy, the built environment, transportation,\nindustry, land use, and Earth sciences. These applications include problems\nsuch as energy systems optimization, construction, precision agriculture,\nbuilding envelope retrofits, autonomous trucking, and large-scale environmental\nmonitoring. Critically, we include opportunities to apply not only physical\nrobots but also the broader robotics toolkit - including planning, perception,\ncontrol, and estimation algorithms - to climate-relevant problems. A central\ngoal of this roadmap is to inspire new research directions and collaboration by\nhighlighting specific, actionable problems at the intersection of robotics and\nclimate. This work represents a collaboration between robotics researchers and\ndomain experts in various climate disciplines, and it serves as an invitation\nto the robotics community to bring their expertise to bear on urgent climate\npriorities.\n","authors":["Alan Papalia","Charles Dawson","Laurentiu L. Anton","Norhan Magdy Bayomi","Bianca Champenois","Jung-Hoon Cho","Levi Cai","Joseph DelPreto","Kristen Edwards","Bilha-Catherine Githinji","Cameron Hickert","Vindula Jayawardana","Matthew Kramer","Shreyaa Raghavan","David Russell","Shide Salimi","Jingnan Shi","Soumya Sudhakar","Yanwei Wang","Shouyi Wang","Luca Carlone","Vijay Kumar","Daniela Rus","John E. Fernandez","Cathy Wu","George Kantor","Derek Young","Hanumant Singh"],"pdf_url":"https://arxiv.org/pdf/2507.11623v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13231v1","updated":"2025-07-17T15:41:57Z","published":"2025-07-17T15:41:57Z","title":"VITA: Vision-to-Action Flow Matching Policy","summary":"  We present VITA, a Vision-To-Action flow matching policy that evolves latent\nvisual representations into latent actions for visuomotor control. Traditional\nflow matching and diffusion policies sample from standard source distributions\n(e.g., Gaussian noise) and require additional conditioning mechanisms like\ncross-attention to condition action generation on visual information, creating\ntime and space overheads. VITA proposes a novel paradigm that treats latent\nimages as the flow source, learning an inherent mapping from vision to action\nwhile eliminating separate conditioning modules and preserving generative\nmodeling capabilities. Learning flows between fundamentally different\nmodalities like vision and action is challenging due to sparse action data\nlacking semantic structures and dimensional mismatches between high-dimensional\nvisual representations and raw actions. We address this by creating a\nstructured action latent space via an autoencoder as the flow matching target,\nup-sampling raw actions to match visual representation shapes. Crucially, we\nsupervise flow matching with both encoder targets and final action outputs\nthrough flow latent decoding, which backpropagates action reconstruction loss\nthrough sequential flow matching ODE solving steps for effective end-to-end\nlearning. Implemented as simple MLP layers, VITA is evaluated on challenging\nbi-manual manipulation tasks on the ALOHA platform, including 5 simulation and\n2 real-world tasks. Despite its simplicity, MLP-only VITA outperforms or\nmatches state-of-the-art generative policies while reducing inference latency\nby 50-130% compared to conventional flow matching policies requiring different\nconditioning mechanisms or complex architectures. To our knowledge, VITA is the\nfirst MLP-only flow matching policy capable of solving complex bi-manual\nmanipulation tasks like those in ALOHA benchmarks.\n","authors":["Dechen Gao","Boqi Zhao","Andrew Lee","Ian Chuang","Hanchu Zhou","Hang Wang","Zhe Zhao","Junshan Zhang","Iman Soltani"],"pdf_url":"https://arxiv.org/pdf/2507.13231v1.pdf","comment":"Project page: https://ucd-dare.github.io/VITA/"},{"id":"http://arxiv.org/abs/2507.13229v1","updated":"2025-07-17T15:40:18Z","published":"2025-07-17T15:40:18Z","title":"$S^2M^2$: Scalable Stereo Matching Model for Reliable Depth Estimation","summary":"  The pursuit of a generalizable stereo matching model, capable of performing\nacross varying resolutions and disparity ranges without dataset-specific\nfine-tuning, has revealed a fundamental trade-off. Iterative local search\nmethods achieve high scores on constrained benchmarks, but their core mechanism\ninherently limits the global consistency required for true generalization. On\nthe other hand, global matching architectures, while theoretically more robust,\nhave been historically rendered infeasible by prohibitive computational and\nmemory costs. We resolve this dilemma with $S^2M^2$: a global matching\narchitecture that achieves both state-of-the-art accuracy and high efficiency\nwithout relying on cost volume filtering or deep refinement stacks. Our design\nintegrates a multi-resolution transformer for robust long-range correspondence,\ntrained with a novel loss function that concentrates probability on feasible\nmatches. This approach enables a more robust joint estimation of disparity,\nocclusion, and confidence. $S^2M^2$ establishes a new state of the art on the\nMiddlebury v3 and ETH3D benchmarks, significantly outperforming prior methods\nacross most metrics while reconstructing high-quality details with competitive\nefficiency.\n","authors":["Junhong Min","Youngpil Jeon","Jimin Kim","Minyong Choi"],"pdf_url":"https://arxiv.org/pdf/2507.13229v1.pdf","comment":"8 pages, 5 figures, ICCV accepted paper"},{"id":"http://arxiv.org/abs/2503.00614v2","updated":"2025-07-17T15:37:40Z","published":"2025-03-01T20:49:26Z","title":"Sampling-Based Motion Planning with Discrete Configuration-Space\n  Symmetries","summary":"  When planning motions in a configuration space that has underlying symmetries\n(e.g. when manipulating one or multiple symmetric objects), the ideal planning\nalgorithm should take advantage of those symmetries to produce shorter\ntrajectories. However, finite symmetries lead to complicated changes to the\nunderlying topology of configuration space, preventing the use of standard\nalgorithms. We demonstrate how the key primitives used for sampling-based\nplanning can be efficiently implemented in spaces with finite symmetries. A\nrigorous theoretical analysis, building upon a study of the geometry of the\nconfiguration space, shows improvements in the sample complexity of several\nstandard algorithms. Furthermore, a comprehensive slate of experiments\ndemonstrates the practical improvements in both path length and runtime.\n","authors":["Thomas Cohn","Russ Tedrake"],"pdf_url":"https://arxiv.org/pdf/2503.00614v2.pdf","comment":"Accepted to IROS 2025. 8 pages, 2 figures, 4 tables. Interactive\n  results available at https://cohnt.github.io/projects/symmetries.html"},{"id":"http://arxiv.org/abs/2507.13225v1","updated":"2025-07-17T15:37:24Z","published":"2025-07-17T15:37:24Z","title":"Signal Temporal Logic Compliant Co-design of Planning and Control","summary":"  This work presents a novel co-design strategy that integrates trajectory\nplanning and control to handle STL-based tasks in autonomous robots. The method\nconsists of two phases: $(i)$ learning spatio-temporal motion primitives to\nencapsulate the inherent robot-specific constraints and $(ii)$ constructing an\nSTL-compliant motion plan from these primitives. Initially, we employ\nreinforcement learning to construct a library of control policies that perform\ntrajectories described by the motion primitives. Then, we map motion primitives\nto spatio-temporal characteristics. Subsequently, we present a sampling-based\nSTL-compliant motion planning strategy tailored to meet the STL specification.\nThe proposed model-free approach, which generates feasible STL-compliant motion\nplans across various environments, is validated on differential-drive and\nquadruped robots across various STL specifications. Demonstration videos are\navailable at https://tinyurl.com/m6zp7rsm.\n","authors":["Manas Sashank Juvvi","Tushar Dilip Kurne","Vaishnavi J","Shishir Kolathaya","Pushpak Jagtap"],"pdf_url":"https://arxiv.org/pdf/2507.13225v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17469v4","updated":"2025-07-17T15:33:49Z","published":"2024-09-26T02:02:58Z","title":"VertiSelector: Automatic Curriculum Learning for Wheeled Mobility on\n  Vertically Challenging Terrain","summary":"  Reinforcement Learning (RL) has the potential to enable extreme off-road\nmobility by circumventing complex kinodynamic modeling, planning, and control\nby simulated end-to-end trial-and-error learning experiences. However, most RL\nmethods are sample-inefficient when training in a large amount of manually\ndesigned simulation environments and struggle at generalizing to the real\nworld. To address these issues, we introduce VertiSelector (VS), an automatic\ncurriculum learning framework designed to enhance learning efficiency and\ngeneralization by selectively sampling training terrain. VS prioritizes\nvertically challenging terrain with higher Temporal Difference (TD) errors when\nrevisited, thereby allowing robots to learn at the edge of their evolving\ncapabilities. By dynamically adjusting the sampling focus, VS significantly\nboosts sample efficiency and generalization within the VW-Chrono simulator\nbuilt on the Chrono multi-physics engine. Furthermore, we provide simulation\nand physical results using VS on a Verti-4-Wheeler platform. These results\ndemonstrate that VS can achieve 23.08% improvement in terms of success rate by\nefficiently sampling during training and robustly generalizing to the real\nworld.\n","authors":["Tong Xu","Chenhui Pan","Xuesu Xiao"],"pdf_url":"https://arxiv.org/pdf/2409.17469v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.08388v3","updated":"2025-07-17T15:30:27Z","published":"2025-03-11T12:53:24Z","title":"V-Max: A Reinforcement Learning Framework for Autonomous Driving","summary":"  Learning-based decision-making has the potential to enable generalizable\nAutonomous Driving (AD) policies, reducing the engineering overhead of\nrule-based approaches. Imitation Learning (IL) remains the dominant paradigm,\nbenefiting from large-scale human demonstration datasets, but it suffers from\ninherent limitations such as distribution shift and imitation gaps.\nReinforcement Learning (RL) presents a promising alternative, yet its adoption\nin AD remains limited due to the lack of standardized and efficient research\nframeworks. To this end, we introduce V-Max, an open research framework\nproviding all the necessary tools to make RL practical for AD. V-Max is built\non Waymax, a hardware-accelerated AD simulator designed for large-scale\nexperimentation. We extend it using ScenarioNet's approach, enabling the fast\nsimulation of diverse AD datasets.\n","authors":["Valentin Charraut","Waël Doulazmi","Thomas Tournaire","Thibault Buhet"],"pdf_url":"https://arxiv.org/pdf/2503.08388v3.pdf","comment":"RLC 25 - Camera-ready"},{"id":"http://arxiv.org/abs/2507.13200v1","updated":"2025-07-17T15:10:12Z","published":"2025-07-17T15:10:12Z","title":"Few-shot transfer of tool-use skills using human demonstrations with\n  proximity and tactile sensing","summary":"  Tools extend the manipulation abilities of robots, much like they do for\nhumans. Despite human expertise in tool manipulation, teaching robots these\nskills faces challenges. The complexity arises from the interplay of two\nsimultaneous points of contact: one between the robot and the tool, and another\nbetween the tool and the environment. Tactile and proximity sensors play a\ncrucial role in identifying these complex contacts. However, learning tool\nmanipulation using these sensors remains challenging due to limited real-world\ndata and the large sim-to-real gap. To address this, we propose a few-shot\ntool-use skill transfer framework using multimodal sensing. The framework\ninvolves pre-training the base policy to capture contact states common in\ntool-use skills in simulation and fine-tuning it with human demonstrations\ncollected in the real-world target domain to bridge the domain gap. We validate\nthat this framework enables teaching surface-following tasks using tools with\ndiverse physical and geometric properties with a small number of demonstrations\non the Franka Emika robot arm. Our analysis suggests that the robot acquires\nnew tool-use skills by transferring the ability to recognise tool-environment\ncontact relationships from pre-trained to fine-tuned policies. Additionally,\ncombining proximity and tactile sensors enhances the identification of contact\nstates and environmental geometry.\n","authors":["Marina Y. Aoyama","Sethu Vijayakumar","Tetsuya Narita"],"pdf_url":"https://arxiv.org/pdf/2507.13200v1.pdf","comment":"8 pages, 9 figures, IEEE Robotics and Automation Letters"},{"id":"http://arxiv.org/abs/2507.12273v2","updated":"2025-07-17T14:54:27Z","published":"2025-07-16T14:22:00Z","title":"Next-Gen Museum Guides: Autonomous Navigation and Visitor Interaction\n  with an Agentic Robot","summary":"  Autonomous robots are increasingly being tested into public spaces to enhance\nuser experiences, particularly in cultural and educational settings. This paper\npresents the design, implementation, and evaluation of the autonomous museum\nguide robot Alter-Ego equipped with advanced navigation and interactive\ncapabilities. The robot leverages state-of-the-art Large Language Models (LLMs)\nto provide real-time, context aware question-and-answer (Q&A) interactions,\nallowing visitors to engage in conversations about exhibits. It also employs\nrobust simultaneous localization and mapping (SLAM) techniques, enabling\nseamless navigation through museum spaces and route adaptation based on user\nrequests. The system was tested in a real museum environment with 34\nparticipants, combining qualitative analysis of visitor-robot conversations and\nquantitative analysis of pre and post interaction surveys. Results showed that\nthe robot was generally well-received and contributed to an engaging museum\nexperience, despite some limitations in comprehension and responsiveness. This\nstudy sheds light on HRI in cultural spaces, highlighting not only the\npotential of AI-driven robotics to support accessibility and knowledge\nacquisition, but also the current limitations and challenges of deploying such\ntechnologies in complex, real-world environments.\n","authors":["Luca Garello","Francesca Cocchella","Alessandra Sciutti","Manuel Catalano","Francesco Rea"],"pdf_url":"https://arxiv.org/pdf/2507.12273v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.05548v6","updated":"2025-07-17T14:44:39Z","published":"2024-11-08T13:11:16Z","title":"Equivariant IMU Preintegration with Biases: a Galilean Group Approach","summary":"  This letter proposes a new approach for Inertial Measurement Unit (IMU)\npreintegration, a fundamental building block that can be leveraged in different\noptimization-based Inertial Navigation System (INS) localization solutions.\nInspired by recent advances in equivariant theory applied to biased INSs, we\nderive a discrete-time formulation of the IMU preintegration on\n${\\mathbf{Gal}(3) \\ltimes \\mathfrak{gal}(3)}$, the left-trivialization of the\ntangent group of the Galilean group $\\mathbf{Gal}(3)$. We define a novel\npreintegration error that geometrically couples the navigation states and the\nbias leading to lower linearization error. Our method improves in consistency\ncompared to existing preintegration approaches which treat IMU biases as a\nseparate state-space. Extensive validation against state-of-the-art methods,\nboth in simulation and with real-world IMU data, implementation in the Lie++\nlibrary, and open-source code are provided.\n","authors":["Giulio Delama","Alessandro Fornasier","Robert Mahony","Stephan Weiss"],"pdf_url":"https://arxiv.org/pdf/2411.05548v6.pdf","comment":null}]}}